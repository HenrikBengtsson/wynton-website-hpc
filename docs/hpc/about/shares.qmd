---
shares:
  pu_add_label: "2.2 GHz Opteron 6174 CPU"
  pu_add: 1.6
  slots_add: 12
  pu_times: 4
  pu_delta: 6.4           # pu_add * pu_times = 1.6 * 4
  slots_delta: 48         # slots_add * pu_times = 12 * 4
  pu_total_0: 20179.6     # specs.pu_total - pu_delta = 20186 - 6.4
  slots_total_0: 6975     # specs.member_q_total - slots_delta = 7023 - 48
  pu_delta_share: 0.000317 # pu_delta / pu_total_0 = 6.4 / 20179.6
  slots_delta_share: 2.21  # pu_delta_share * slots_total_0 = 0.000317 * 6975
  pu_0: 16.3
  pu_1: 22.7              # pu_0 + pu_delta = 16.3 + 6.4
  pu_1_share: 0.001125    # pu_1 / pu_total_0 = 22.7 / 20179.6
  slots_1: 7.85           # pu_1_share * slots_total_0 = 0.001125 * 6975
format: html
---

{{< include warning-hold-on-compute-requests.qmd >}}

<div class="alert alert-info" role="alert">
**Contributions to the {{< meta cluster.name >}} environment are non-expiring, e.g. contribute once and keep it for life!**
</div>

# Contributing Member Shares

## Compute Shares


Currently, the {{< meta cluster.name >}} cluster has in total _member.q<sub>total</sub>_ = {{< meta specs.member_q_total >}} slots available on the member.q queue.  Jobs on the _member.q_ queue will launch and finish sooner than jobs on the communal, lower-priority _long.q_ queue.  A member.q job will have higher-priority on the CPU than a long.q job in case they run on the same compute node.   It is only contributing members who have access to the member.q queue - non-contributing members will only have access to [queues](/hpc/scheduler/queues.html) such as the long.q queue.  **Contributors get _non-expiring, lifetime access_ to a  number of these member.q slots in proportion to their hardware contribution to the cluster.**  The number of member.q slots a particular hardware contribution, which can be monetary(\*) or physical(\*), adds, is based on how much compute power the contribution adds to the cluster.
The amount of compute power that contributed hardware adds is based on benchmarking(\*), which result in a _processing-unit score_ (PU) for the contribution.  Currently, there are in total _PU<sub>total</sub>_ = {{< meta specs.pu_total >}} _contributed_ processing units on {{< meta cluster.name >}}.

<div class="alert alert-info" role="alert">
**A lab's contributed processing units (_PU<sub>lab</sub>_) will never expire - it will remain the same until the lab makes additional contributions to the cluster.**
</div>

As other labs contribute to the cluster, the total computer power (_PU<sub>total</sub>_) and the total number of member.q slots (_member.q<sub>total</sub>_) will increase over time.   This will result in the lab's _relative_ compute share (_PU<sub>lab</sub>_ / _PU<sub>total</sub>_) to decrease over time while their number of member.q slots (_member.q<sub>lab</sub>_) will stay approximately(**) the same.


### Example: Additional contribution from the Charlie Lab

Assume that the last addition was from the Charlie Lab contributing {{< meta shares.pu_times >}} compute nodes.  Each of these machines has a {{< meta shares.slots_add >}}-core {{< meta shares.pu_add_label >}} and clocks in at {{< meta shares.pu_add >}} PUs based on the benchmarking, resulting in the processing power added for this lab, but also to the cluster as a whole, to be {{< meta shares.pu_times >}} \* {{< meta shares.pu_add >}} PUs = +{{< meta shares.pu_delta >}} PUs.  In addition to increasing the total amount of contributed PUs, the lab's contribution also increased the total number of member.q slots on the cluster by {{< meta shares.pu_times >}} \* {{< meta shares.slots_add >}} = +{{< meta shares.slots_delta >}} slots.

If this was Charlie Lab's first contribution to {{< meta cluster.name >}}, their share on the member.q queue will be _PU<sub>lab</sub>_ / _PU<sub>total</sub>_ = {{< meta shares.pu_delta >}} / {{< meta specs.pu_total >}} = 0.032%.  This PU share translates to _member.q<sub>lab</sub>_ = (_PU<sub>lab</sub>_ / _PU<sub>total</sub>_) \*_member.q<sub>total</sub>_ = 2 member.q slots (2.21 rounded off to the closest integer).
Instead, if they already had contributed, say, in total {{< meta shares.pu_0 >}} PUs in the past, their computational share would had become _PU<sub>lab</sub>_ = ({{< meta shares.pu_0 >}} + {{< meta shares.pu_delta >}}) / {{< meta specs.pu_total >}} = 0.113%, which, would corresponds to 8 member.q slots (7.85 rounded off).

<!--
All members of a lab will have access to the lab's member.q slots.  For example, if five out of seven member.q slots are currently in use when another lab member submits four ten-hour jobs, then two of those jobs will end up on the member.q queue whereas the other two will "spill over" to the lower-priority long.q queue.  In contrast, if those jobs were submitted by a non-contributing member, all four would end up on the long.q queue.
-->


### Current Compute Shares

Below table shows the current amount of contributions in terms of Processing Units (PU) and the corresponding number of member.q slots per contributing lab.



<script src="https://d3js.org/d3.v3.min.js"><!-- ~150 kB --></script>
<script src="https://cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"><!-- ~80 kB --></script>
<script src="https://cdn.datatables.net/1.10.16/js/dataTables.bootstrap.min.js"><!-- 2 kB --></script>

<table id="hosttable">
</table>

<!-- markdownlint-disable-file MD011 MD052 -->
<script type="text/javascript" charset="utf-8">
d3.text("/hpc/assets/data/compute_shares.tsv", "text/csv", function(host_table) {
  // extract date from header comments
  var timestamp = host_table.match(/^[#] Created on: [^\r\n]*[\r\n]+/mg, '')[0];
  timestamp = timestamp.replace(/^[#] Created on: /g, '');
  timestamp = timestamp.replace(/ [^ ]+/g, ''); // keep only the date
  timestamp = timestamp.trim();
  d3.select("#compute-shares-timestamp").text(timestamp);
  
  // drop header comments
  host_table = host_table.replace(/^[#][^\r\n]*[\r\n]+/mg, '');
  host_table = d3.tsv.parse(host_table);

  var table = d3.select("#hosttable");
  var thead, tbody, tfoot, tr, td, td_status;
  var value, value2;
  var pu_total = 0, slots_total = 0;
  
  /* For each row */
  var nodes = 0;
  host_table.forEach(function(row0) {
    var row = [row0["fshares"], row0["queue_slots"], row0["project"]];

    if (nodes == 0) {
      tr = table.append("thead").append("tr");
      tr.append("th").text("Processing Units (PU)");
      tr.append("th").text("Member.q Slots");
      tr.append("th").text("Lab Group");
      tbody = table.append("tbody");
    }

    tr = tbody.append("tr");
    for (key in row) td = tr.append("td").text(row[key]);

    pu_total += parseInt(row[0]);
    slots_total += parseInt(row[1]);

    nodes += 1;
  });

  tr = table.append("tfoot").append("tr");
  tr.append("td").text(pu_total + " PUs");
  tr.append("td").text(slots_total + " slots");

  $(document).ready(function() {
    $('#hosttable').DataTable({
      "pageLength": 50,
      "order": [[ 0, "desc" ]]
    });
  });
});
</script>

Source: [compute_shares.tsv](/hpc/assets/data/compute_shares.tsv) produced on <span id="compute-shares-timestamp"></span>.  These data were compiled from the current SGE configuration (`qconf -srqs member_queue_limits` and `qconf -sprj <project>`).  In SGE terms, a processing unit (PU) corresponds to a _functional share_ ("fshare").



<small>
(*) To be documented.<br>
(**) The reason for _member.q<sub>lab</sub>_ not remaining exactly the same when _PU<sub>lab</sub>_ does not change, is that the compute power per core is greater for newer hardware compared with older hardware. Because of this, a lab’s number of member.q slots is likely to, ever so slightly, decrease in the long run as the cluster keeps growing. But don’t worry, as the _average compute power per member.q slot increases over time_, your lab's total compute power on the member.q queue remains constant per definition (unless your lab adds further contributions).
</small>

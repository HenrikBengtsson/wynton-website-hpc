<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Wynton HPC Status – UCSF Wynton HPC Cluster</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-98e2a9b0d4696fd01dd9970aef6a652a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Create UCSF banner element
  var banner = document.createElement('div');
  banner.className = 'top-banner logo';
  banner.innerHTML = `
    <div class="inner">
      <div>
        <div class="logo"></div>
        <a class="logotype" href="https://www.ucsf.edu/">
          <span>University of California San Francisco</span>
        </a>
      </div>
      <div class="nav">
        <ul class="top-banner-menu">
          <li><a href="https://www.ucsf.edu/about">About UCSF</a></li>
          <li><a href="https://www.ucsf.edu/search">Search UCSF</a></li>
        </ul>
      </div>
    </div>
  `;
  
  // Insert banner before navbar
  var body = document.body;
  var navbar = document.querySelector('.navbar');
  if (navbar && navbar.parentNode) {
    navbar.parentNode.insertBefore(banner, navbar);
  } else {
    body.insertBefore(banner, body.firstChild);
  }
});
</script>
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Get the current page path
  var path = window.location.pathname;
  
  // Convert from .html to .md and adjust path
  // Remove /hpc/ from start if present for GitHub path
  var githubPath = path.replace(/\.html$/, '.md');
  if (githubPath.endsWith('/')) {
    githubPath += 'index.md';
  }
  
  // Create the edit link
  var repoUrl = 'https://github.com/ucsf-wynton/wynton-website-hpc';
  var editUrl = repoUrl + '/tree/master/docs' + githubPath;
  
  // Find the footer and add our custom links
  var footer = document.querySelector('.nav-footer');
  if (footer) {
    var rightDiv = footer.querySelector('.nav-footer-right');
    if (!rightDiv) {
      rightDiv = document.createElement('div');
      rightDiv.className = 'nav-footer-right';
      footer.appendChild(rightDiv);
    }
    
    rightDiv.innerHTML = '<ul class="footer-items list-unstyled">' +
      '<li class="nav-item">' +
      '<a class="nav-link" href="' + editUrl + '">' +
      '<i class="bi bi-pencil-square"></i> Edit this page' +
      '</a></li>' +
      '<li class="nav-item">' +
      '<a class="nav-link" href="' + repoUrl + '/issues/new">' +
      'Report an issue' +
      '</a></li>' +
      '</ul>';
  }
});
</script>


<link rel="stylesheet" href="../../hpc/assets/css/ucsf-banners.css">
<link rel="stylesheet" href="../../hpc/assets/css/bootstrap.min.css">
<link rel="stylesheet" href="../../hpc/assets/css/this_site.css">
<link rel="stylesheet" href="../../hpc/assets/css/broadcast.css">
<link rel="stylesheet" href="../../hpc/assets/css/checkbox-toggle.css">
<link rel="stylesheet" href="../../hpc/assets/css/docsearch.css">
<link rel="stylesheet" href="../../hpc/assets/css/quarto-customizations.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../hpc/assets/ico/favicon.ico" alt="" class="navbar-logo light-content">
    <img src="../../hpc/assets/ico/favicon.ico" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">UCSF Wynton HPC Cluster</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../hpc/status/index.html" aria-current="page"> 
<span class="menu-text">Status</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-get-started" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Get Started</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-get-started">    
        <li>
    <a class="dropdown-item" href="../../hpc/get-started/access-cluster.html">
 <span class="dropdown-text">1. Login &amp; Logout</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/get-started/development-prototyping.html">
 <span class="dropdown-text">2. Development / Prototyping</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/get-started/hello-world-job.html">
 <span class="dropdown-text">3. ‘Hello World’ Job</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/get-started/good-practices.html">
 <span class="dropdown-text">4. Good Practices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/get-started/duo-signup.html">
 <span class="dropdown-text">5. 2FA Registration</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/get-started/protected-quickstart.html">
 <span class="dropdown-text">6. Wynton Protected Quickstart</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-software" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Software</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-software">    
        <li>
    <a class="dropdown-item" href="../../hpc/software/core-software.html">
 <span class="dropdown-text">Core Software</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/scl.html">
 <span class="dropdown-text">Software Collections (SCL)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/software-modules.html">
 <span class="dropdown-text">Software Modules</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/software-repositories.html">
 <span class="dropdown-text">Software Repositories</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/apptainer.html">
 <span class="dropdown-text">Linux Containers (Apptainer/Singularity)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/sbgrid.html">
 <span class="dropdown-text">SBGrid Software (restricted access)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/missing-software.html">
 <span class="dropdown-text">Missing Software?</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/software/rocky-8-linux.html">
 <span class="dropdown-text">Migration to Rocky 8 Linux (Nov 2023)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-sge" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">SGE</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-sge">    
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/submit-jobs.html">
 <span class="dropdown-text">Submit Jobs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/list-jobs.html">
 <span class="dropdown-text">List Jobs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/kill-jobs.html">
 <span class="dropdown-text">Kill Jobs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/job-summary.html">
 <span class="dropdown-text">Job Summary</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/envvars.html">
 <span class="dropdown-text">Useful Env. Variables</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/using-local-scratch.html">
 <span class="dropdown-text">Use Local Scratch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/queues.html">
 <span class="dropdown-text">Available Queues</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/email-notifications.html">
 <span class="dropdown-text">Job Email Notifications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/gpu.html">
 <span class="dropdown-text">GPU Scheduling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/scheduler/advanced-usage.html">
 <span class="dropdown-text">Advanced Usage</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-file-transfers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">File Transfers</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-file-transfers">    
        <li>
    <a class="dropdown-item" href="../../hpc/transfers/files-and-directories.html">
 <span class="dropdown-text">Secure Copy File Transfers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/transfers/globus.html">
 <span class="dropdown-text">Globus File Transfers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/transfers/globus-sharing.html">
 <span class="dropdown-text">Globus File Sharing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/transfers/ucsf-box.html">
 <span class="dropdown-text">UCSF Box</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/transfers/dos2unix.html">
 <span class="dropdown-text">Windows-Unix File Transfers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/transfers/rclone.html">
 <span class="dropdown-text">rclone (mount Wynton file system locally)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-how-to" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">How To</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-how-to">    
        <li>
    <a class="dropdown-item" href="../../hpc/howto/matlab.html">
 <span class="dropdown-text">Work with MATLAB</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/python.html">
 <span class="dropdown-text">Work with Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/jupyter.html">
 <span class="dropdown-text">Work with Jupyter Notebook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/r.html">
 <span class="dropdown-text">Work with R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/rstudio.html">
 <span class="dropdown-text">Work with RStudio</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/conda.html">
 <span class="dropdown-text">Work with Conda</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/conda-stage.html">
 <span class="dropdown-text">Stage Conda Environment to Local Disk</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/install-from-source.html">
 <span class="dropdown-text">Install Software from Source</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/storage-size.html">
 <span class="dropdown-text">File Sizes and Disk Quotas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/group-quota.html">
 <span class="dropdown-text">Adjust Group Disk Quota</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/find-files.html">
 <span class="dropdown-text">Find Files</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/log-in-without-pwd.html">
 <span class="dropdown-text">Log in without Password</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/change-pwd.html">
 <span class="dropdown-text">Change &amp; Verify Password</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/gui-x11fwd.html">
 <span class="dropdown-text">GUIs (X2Go &amp; X11)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/howto/jupyterhub.html">
 <span class="dropdown-text">JupyterHub Server (DISCONTINUED)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-support" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Support</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-support">    
        <li>
    <a class="dropdown-item" href="../../hpc/support/index.html">
 <span class="dropdown-text">Support Channels</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/support/faq.html">
 <span class="dropdown-text">Frequently Asked Questions</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">About</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-about">    
        <li>
    <a class="dropdown-item" href="../../hpc/about/specs.html">
 <span class="dropdown-text">Specifications</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/gpus.html">
 <span class="dropdown-text">Available GPUs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/news.html">
 <span class="dropdown-text">News</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/pricing-storage.html">
 <span class="dropdown-text">Pricing: Extra Storage</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/pricing-compute.html">
 <span class="dropdown-text">Pricing: Extra Compute (CPU &amp; GPU)</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../hpc/about/wynton-protected.html">
 <span class="dropdown-text">Wynton Protected (P3 Data)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/join.html">
 <span class="dropdown-text">Join ♡</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/shares.html">
 <span class="dropdown-text">Contributing Member Shares</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../hpc/about/governance.html">
 <span class="dropdown-text">Governance</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/user-agreement.html">
 <span class="dropdown-text">User Agreement</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../hpc/about/citation.html">
 <span class="dropdown-text">How to Cite</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../hpc/about/contact.html">
 <span class="dropdown-text">Contact</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Wynton HPC Status</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- markdownlint-disable-file MD024 MD025 -->
<section id="ucsf-status" class="level1">
<h1>UCSF Wynton HPC Status</h1>
<section id="queue-metrics" class="level2">
<h2 class="anchored" data-anchor-id="queue-metrics">Queue Metrics</h2>
<ul class="nav nav-pills">
<li class="active">
<a data-toggle="pill" href="#by-day"><span style="font-weight: bold;">day</span></a>
</li>
<li>
<a data-toggle="pill" href="#by-week"><span style="font-weight: bold;">week</span></a>
</li>
<li>
<a data-toggle="pill" href="#by-month"><span style="font-weight: bold;">month</span></a>
</li>
<li>
<a data-toggle="pill" href="#by-year"><span style="font-weight: bold;">year</span></a>
</li>
</ul>
<div class="tab-content" style="margin-top: 1ex;">
<div id="by-day" class="tab-pane fade in active">
<img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/queues-day.png" alt="queues usage during the last day"><br> <img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/gpuq-day.png" alt="GPU queues usage during the last day"><br>
</div>
<div id="by-week" class="tab-pane fade in">
<img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/queues-week.png" alt="queues usage during the last week"><br> <img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/gpuq-week.png" alt="GPU queues usage during the last week"><br>
</div>
<div id="by-month" class="tab-pane fade in">
<img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/queues-month.png" alt="queues usage during the last month"><br> <img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/gpuq-month.png" alt="GPU queues usage during the last month"><br>
</div>
<div id="by-year" class="tab-pane fade in">
<img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/queues-year.png" alt="queues usage during the last year"><br> <img src="https://raw.githubusercontent.com/UCSF-HPC/wynton-slash/master/wynton/docs/status/figures/gpuq-year.png" alt="GPU queues usage during the last year"><br>
</div>
</div>
</section>
<section id="file-system-metrics" class="level2">
<h2 class="anchored" data-anchor-id="file-system-metrics">File-System Metrics</h2>
<p>Last known heartbeat: <span id="last-heartbeat">Loading…</span> <button id="reload-heartbeat" title="Reload">⟲</button></p>
<div class="status-panel" style="border: 1px solid #dec000; padding: 2ex; margin-bottom: 2ex;">
<div style="font-size: 150%; font-weight: bold;">
<span>/wynton/scratch/ lagginess</span><span style="float: right;"></span>
</div>
<div id="BeeGFSLoad_devX__wynton_scratch_hb">

</div>
</div>
<div class="status-panel" style="border: 1px solid #dec000; padding: 2ex; margin-bottom: 2ex;">
<div style="font-size: 150%; font-weight: bold;">
<span>/wynton/home/ lagginess</span><span style="float: right;"></span>
</div>
<div id="BeeGFSLoad_devX__wynton_home_cbi_hb">

</div>
</div>
<div class="status-panel" style="border: 1px solid #dec000; padding: 2ex; margin-bottom: 2ex;">
<div style="font-size: 150%; font-weight: bold;">
<span>/wynton/group/ lagginess</span><span style="float: right;"></span>
</div>
<div id="BeeGFSLoad_devX__wynton_group_cbi_hb">

</div>
</div>
<p><em>Figure: The total, relative processing time on the logarithmic scale for one benchmarking run to complete over time. The values presented are relative to the best case scenario when there is no load, in case the value is 1.0. The larger the relative time is, the more lag there is on file system. Annotation of lagginess ranges: 1-2: excellent (light green), 2-5: good (green), 5-20: sluggish (orange), 20-100: bad (red), 100 and above: critical (purple).</em></p>
<p>Details: These benchmarks are run every ten minutes from different hosts and toward different types of the file system. These metrics are based on a <a href="https://github.com/ucsf-wynton/wynton-bench/blob/d96937b51e6ee3a421afec3c793accb0acd82c51/bench-scripts/bench-files-tarball.sh#L93-L129">set of commands</a>, part of the <strong><a href="https://github.com/ucsf-wynton/wynton-bench">wynton-bench</a></strong> tool, that interacts with the file system that is being benchmarked. The relevant ones are: reading a large file from <code>/wynton/home/</code>, copying that large archive file to and from the BeeGFS path being benchmarked, extracting the archive to path being benchmarked, find one file among the extracted files, calculating the total file size, and re-archiving and compressing the extracted files. When there’s minimal load on <code>/wynton</code>, the processing time is ~19 seconds. In contrast, when benchmarking local <code>/scratch</code>, the total processing time is about three seconds.</p>
<p>When BeeGFS struggles to keep up with metadata and storage requests, the BeeGFS lagginess goes up. We can use <code>beegfs-ctl --serverstats --perserver --nodetype=meta</code> and <code>beegfs-ctl --serverstats --perserver --nodetype=storage</code> to see the amount of BeeGFS operations that are queued up (<code>qlen</code>) per metadata and storage server. When things run smoothly, the queue lengths should be near zero (<code>qlen</code> less than ten). When BeeGFS struggles to keep up, we typically find large <code>qlen</code> values for one or more servers. To see if the BeeGFS load is high due to file storage or metadata I/O performed by specific users, we can use <code>beegfs-ctl --userstats ...</code>. For example, <code>beegfs-ctl --userstats --names --interval=10 --maxlines=5 --nodetype=storage</code> summarizes <em>storage</em> operations every ten seconds and list the five users with the most operations. Similarly, <code>beegfs-ctl --userstats --names --interval=10 --maxlines=5 --nodetype=meta</code> shows <em>metadata</em> operations per user.</p>
</section>
<section id="miscellaneous-metrics" class="level2">
<h2 class="anchored" data-anchor-id="miscellaneous-metrics">Miscellaneous Metrics</h2>
<p>Detailed statistics on the file-system load and other cluster metrics can be found on the <a href="https://mon.wynton.ucsf.edu/grafana">Wynton HPC Grafana Dashboard</a>. To access this, make sure you are on the UCSF network. Use your Wynton HPC credential to log in.</p>
</section>
<section id="compute-nodes" class="level2">
<h2 class="anchored" data-anchor-id="compute-nodes">Compute Nodes</h2>
<div id="hosttablediv">
<div id="hosttablemessage">
Status on compute nodes unknown, which happens when for instance the job scheduler is down.
</div>
</div>
</section>
<section id="upcoming-incidents" class="level2">
<h2 class="anchored" data-anchor-id="upcoming-incidents">Upcoming Incidents</h2>
<section id="november-12-13-2025" class="level3">
<h3 class="anchored" data-anchor-id="november-12-13-2025">November 12-13, 2025</h3>
<section id="full-downtime" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Notice</strong>: The cluster will down for maintenance from 3:00 pm on Wednesday November 12 until 6:00 pm on Thursday November 13, 2025. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Jobs with runtimes that go into the maintenance window will be started after the downtime. Starting October 29 at 4:00pm, jobs relying on the default 14-day runtime will not be launched until after the downtime. UCSF Facilities will perform annual fire inspection activities to remain compliant with regulations. The network team will update a core switch. The Wynton team will take the opportunity to implement kernel updates during this period. <br><span class="timestamp">October 29, 12:00 PT</span></p>
<!--
comment: Scheduled downtime
start: 2025-11-12T15:00:00
stop: 2025-11-13T18:00:00
length: 27 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
</section>
<section id="current-incidents" class="level2">
<h2 class="anchored" data-anchor-id="current-incidents">Current Incidents</h2>
<section id="november-16-ongoing-2023" class="level3">
<h3 class="anchored" data-anchor-id="november-16-ongoing-2023">November 16-ongoing, 2023</h3>
<section id="sporadic-job-failure" class="level4">
<h4 class="anchored" data-anchor-id="sporadic-job-failure"><span style="color: orange;">Sporadic job failure</span></h4>
<p><strong>Update</strong>: There was another burst of “can’t get password entry for user” errors starting on 2025-01-26 around 15:30, causing jobs to fail immediately. We are restarting the SSSD service on the ~140 compute nodes we have identified suffer from this problem. <br><span class="timestamp">January 27, 11:45 PT</span></p>
<p><strong>Update</strong>: To lower the risk for this problem to occur, the SSSD timeout limit was increased from 10 seconds to 30 seconds. <br><span class="timestamp">November 20, 2023, 10:00 PT</span></p>
<p><strong>Update</strong>: The “can’t get password entry for user” error happens on some compute nodes where the System Security Services Daemon (SSSD) has failed. Until the cause for failed SSSD has been identified and resolved, the only solution is to resubmit the job. <br><span class="timestamp">November 17, 2023, 09:30 PT</span></p>
<p><strong>Notice</strong>: Some jobs end up in an error state (Eqw) with an error “can’t get password entry for user”alice”. Either user does not exist or error with NIS/LDAP etc.” <br><span class="timestamp">November 16, 2023, 17:00 PT</span></p>
<!--
start: 2023-11-17T16:00:00
stop: 
length: 
severity: 
affected: jobs
reason: scheduled
 -->
</section>
</section>
<section id="november-5-ongoing-2023" class="level3">
<h3 class="anchored" data-anchor-id="november-5-ongoing-2023">November 5-ongoing, 2023</h3>
<section id="passwords-cannot-be-reset" class="level4">
<h4 class="anchored" data-anchor-id="passwords-cannot-be-reset"><span style="color: orange;">Passwords cannot be reset</span></h4>
<p><strong>Notice</strong>: Passwords can be changed via the web interface. It is still not possible to change it via the command-line while logged in to Wynton. <br><span class="timestamp">November 13, 11:00 PT</span></p>
<p><strong>Notice</strong>: It is not possible to change or reset passwords since 2023-11-05. This problem was introduced while doing cluster-wide upgrades to Rocky 8. <br><span class="timestamp">November 11, 09:00 PT</span></p>
</section>
</section>
</section>
<section id="past-incidents" class="level2">
<h2 class="anchored" data-anchor-id="past-incidents">Past Incidents</h2>
<ul class="nav nav-pills">
<li class="active">
<a data-toggle="pill" href="#2025"><span style="font-weight: bold;">2025</span></a>
</li>
<li>
<a data-toggle="pill" href="#2024"><span style="font-weight: bold;">2024</span></a>
</li>
<li>
<a data-toggle="pill" href="#2023"><span style="font-weight: bold;">2023</span></a>
</li>
<li>
<a data-toggle="pill" href="#2022"><span style="font-weight: bold;">2022</span></a>
</li>
<li>
<a data-toggle="pill" href="#2021"><span style="font-weight: bold;">2021</span></a>
</li>
<li>
<a data-toggle="pill" href="#2020"><span style="font-weight: bold;">2020</span></a>
</li>
<li>
<a data-toggle="pill" href="#2019"><span style="font-weight: bold;">2019</span></a>
</li>
<li>
<a data-toggle="pill" href="#2018"><span style="font-weight: bold;">2018</span></a>
</li>
</ul>
<div class="tab-content" style="margin-top: 1ex;">
<div id="2025" class="tab-pane fadein active">
<section>
<section id="operational-summary-for-2025-this-far" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2025-this-far">Operational Summary for 2025 (this far)</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 0.0 hours (= 0.0 days)</li>
<li>Unscheduled: 505 hours (= 21.0 days)</li>
<li>Total: 505 hours (= 21.0 days)</li>
<li>External factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>2025-01-09 – 2025-01-09 ( 1.25 hours)</li>
<li>2025-01-17 – 2025-01-22 (81.75 hours)</li>
<li>2025-02-21 – 2025-03-07 (61.0 hours)</li>
<li>2025-03-31 – 2025-04-01 (17.0 hours)</li>
<li>2025-04-11 – 2025-04-14 (62.0 hours)</li>
<li>2025-05-29 – 2025-06-10 (282.0 hours)</li>
</ul></li>
<li>Total downtime: 505 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-other-reasons" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-other-reasons">Unscheduled downtimes due to other reasons</h4>
<ul>
<li>Impact: Less compute resources</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
</section>
<section id="may-29-june-10-2025" class="level3">
<h3 class="anchored" data-anchor-id="may-29-june-10-2025">May 29-June 10, 2025</h3>
<section id="major-file-system-failures" class="level4">
<h4 class="anchored" data-anchor-id="major-file-system-failures"><span style="color: orange;">Major file system failures</span></h4>
<p><strong>Resolved</strong>: Wynton logins are available as of noon today. At that time we will start unsuspending jobs. We lost about 50 TiBs (0.7%) of compressed data from 6550 TiBs with the group storage pool for files in <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code>. See Wynton Announcement email for further details. <br><span class="timestamp">June 10, 12:00 PT</span></p>
<p><strong>Update</strong>: We plan to resume operations by the weekend, given that the current backup and the necessary, manual one-at-the-time replacement of multiple drives completes in time. Files that lived on the failed storage pool are broken and cannot not be fully read, but possible partially. Home directories are unaffected. The affected files live under <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code>. We are scanning the file system to identify exactly which files are affected - this is a slow processes. We will share file lists with affected groups. Eventually, any broken files have to be deleted. <br><span class="timestamp">June 4, 14:00 PT</span></p>
<p><strong>Update</strong>: Wynton jobs and logins are still paused until further notice. Our team is working on determining all of the files that may be corrupt/unavailable and will work with the vendor on the best course of action. We do not yet have an estimate on when we will be back up. <br><span class="timestamp">May 30, 10:40 PT</span></p>
<p><strong>Notice</strong>: Jobs and logins have been paused until further notice. Our team is actively troubleshooting and coordinating with the vendor. A drive was replaced today and was in the process of resilvering when two more drives failed, totally three failed drives, which causes significant problems. Data corruption is expected. <br><span class="timestamp">May 29, 18:20 PT</span></p>
<!--
## When BeeGFS was down
start: 2025-05-29T18:00:00
stop: 2025-06-10T12:00:00
length: 282 hours
severity: major-outage
affected: beegfs, jobs, hosts
reason: beegfs
 -->
</section>
</section>
<section id="april-11-14-2025" class="level3">
<h3 class="anchored" data-anchor-id="april-11-14-2025">April 11-14, 2025</h3>
<section id="file-system-timeouts" class="level4">
<h4 class="anchored" data-anchor-id="file-system-timeouts"><span style="color: orange;">File system timeouts</span></h4>
<p><strong>Resolved</strong>: All cluster jobs and queues were unsuspended at 02:00 this night. <br><span class="timestamp">April 14, 08:15 PT</span></p>
<p><strong>Notice</strong>: All cluster jobs have been suspended in order to allow multiple metadata mirror resyncing processes to complete. These processes are what led to the hanging episodes that we have been seeing. Interactive nodes remain available. Resyncing processes are estimated to complete by Monday. <br><span class="timestamp">April 11, 12:00 PT</span></p>
<!--
## When BeeGFS was down
start: 2025-04-11T12:00:00
stop: 2025-04-14T02:00:00
length: 63h00m
severity: major-outage
affected: beegfs, jobs
reason: beegfs
 -->
</section>
</section>
<section id="march-31-april-1-2025" class="level3">
<h3 class="anchored" data-anchor-id="march-31-april-1-2025">March 31-April 1, 2025</h3>
<section id="file-system-timeouts-1" class="level4">
<h4 class="anchored" data-anchor-id="file-system-timeouts-1"><span style="color: orange;">File system timeouts</span></h4>
<p><strong>Resolved</strong>: Queues and jobs are re-enabled. <br><span class="timestamp">April 1, 12:00 PT</span></p>
<p><strong>Update</strong>: Login is re-enabled. Queues and jobs remains suspended. <br><span class="timestamp">March 31, 20:15 PT</span></p>
<p><strong>Notice</strong>: BeeGFS metadata servers are experiencing issues. We have suspended all queues and jobs and disabled logins. We will work with the file system vendor to resolve the issue. <br><span class="timestamp">March 31, 19:00 PT</span></p>
<!--
## When BeeGFS was down
start: 2025-03-31T19:00:00
stop: 2025-04-01T12:00:00
length: 17h00m
severity: major-outage
affected: beegfs, jobs
reason: beegfs
 -->
</section>
</section>
<section id="february-21-march-7-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-21-march-7-2025">February 21-March 7, 2025</h3>
<section id="file-system-timeouts-2" class="level4">
<h4 class="anchored" data-anchor-id="file-system-timeouts-2"><span style="color: orange;">File system timeouts</span></h4>
<p><strong>Resolved</strong>: We have resumed the scheduler and jobs are being processed again. We identified several problems related to the BeeGFS file system that could have contributed to the recent, severe performance degradation. Specifically, the process that automatically removes files older than 14 days from <code>/wynton/scratch/</code> failed to complete, which resulted in close to 100% full storage servers. We believe this issues started in November 2024 and has gone unnoticed until now. We do not understand why these cleanup processes had failed, but one hypothesis is that there are corrupt files or folders where the cleanup process gets stuck, preventing it from cleaning up elsewhere. It might be that these problems have caused our metadata servers resynchronizing over and over - resynchronization itself is an indication that something is wrong. We are in the process of robustifying our cleanup process, putting in monitoring systems to detect these issues before system degradation takes place. <br><span class="timestamp">March 7, 11:30 PT</span></p>
<p><strong>Notice</strong>: We have decided to again suspending all running jobs and disable the queue from taking on new jobs. <br><span class="timestamp">March 5, 15:00 PT</span></p>
<p><strong>Notice</strong>: Resynchronization of BeeGFS metadata server pair (42,52) finished after 23 hours. <br><span class="timestamp">March 4, 14:00 PT</span></p>
<p><strong>Notice</strong>: Resynchronization of BeeGFS metadata server pairs (32,22) and (23,33) started 2025-03-03, and (42,52) on 2025-03-04. <br><span class="timestamp">March 4, 09:00 PT</span></p>
<p><strong>Notice</strong>: The job queue has been re-enabled and all suspended jobs have been released. <br><span class="timestamp">February 28, 09:00 PT</span></p>
<p><strong>Notice</strong>: Login and file transfers to Wynton has been re-enabled. <br><span class="timestamp">February 28, 09:00 PT</span></p>
<p><strong>Notice</strong>: Resynchronization of BeeGFS metadata server pair (41,51) completed after 24 hours, and pair (63,73) completed after 18 hours. <br><span class="timestamp">February 28, 09:00 PT</span></p>
<p><strong>Notice</strong>: In order to speed up resynchronization of metadata servers, we have decided to minimize the load on the file system by suspending all running jobs, disable login to Wynton, and disable all file transfers to and from Wynton. <br><span class="timestamp">February 27, 16:30 PT</span></p>
<p><strong>Notice</strong>: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (51,73). <br><span class="timestamp">February 27, 16:15 PT</span></p>
<p><strong>Notice</strong>: Resynchronization of BeeGFS metadata server pair <code>meta22</code> and <code>meta32</code> completed after 30 hours. <br><span class="timestamp">February 27, 06:00 PT</span></p>
<p><strong>Notice</strong>: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (22,32). <br><span class="timestamp">February 26, 19:30 PT</span></p>
<p><strong>Notice</strong>: We are working with the vendor to try to resolve this problem. <br><span class="timestamp">February 26, 09:00 PT</span></p>
<p><strong>Notice</strong>: The file system is again very slow. delays when working interactively and jobs to slow down. <br><span class="timestamp">February 25, 15:15 PT</span></p>
<p><strong>Notice</strong>: The file system is again very slow. <br><span class="timestamp">February 25, 10:00 PT</span></p>
<p><strong>Notice</strong>: The file system is very slow, which result in long delays when working interactively and jobs to take longer than usual. <br><span class="timestamp">February 21, 16:00 PT</span></p>
<!--
## Total duration
start: 2025-02-21T16:00:00
stop: 2025-03-07T11:30:00
length: 331h300m
severity: blocking
affected: beegfs, jobs, data-transfer, interactive
reason: beegfs

## When BeeGFS was down
start: 2025-03-05T15:00:00
stop: 2025-03-07T11:30:00
length: 44h30m
severity: major-outage
affected: beegfs, jobs
reason: beegfs

## When BeeGFS was down
start: 2025-02-27T16:30:00
stop: 2025-02-28T09:00:00
length: 16h30m
severity: major-outage
affected: beegfs, jobs
reason: beegfs

## total length on scheduler being down due to BeeGFS: 81h45m
 -->
</section>
</section>
<section id="february-21-24-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-21-24-2025">February 21-24, 2025</h3>
<section id="kernel-maintenance" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: Login node <code>plog1</code> respects SSH keys again. <br><span class="timestamp">February 24, 2025, 11:15 PT</span></p>
<p><strong>Update</strong>: Login node <code>plog1</code> is available again, but does not respect SSH keys. <br><span class="timestamp">February 24, 2025, 10:30 PT</span></p>
<p><strong>Update</strong>: Data-transfer node <code>dt1</code> is available again. <br><span class="timestamp">February 24, 2025, 10:30 PT</span></p>
<p><strong>Update</strong>: With the exception for <code>plog1</code> and <code>dt1</code>, all login, data-transfer, and development nodes have been rebooted. Until <code>plog1</code> is available, PHI-users may use <code>pdt1</code> and <code>pdt2</code> to login into the cluster. <br><span class="timestamp">February 22, 2025, 13:30 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Saturday, February 22, 2025 at 13:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. <br><span class="timestamp">February 21, 2025, 12:15 PT</span></p>
</section>
</section>
<section id="february-22-24-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-22-24-2025">February 22-24, 2025</h3>
<section id="globus-issues" class="level4">
<h4 class="anchored" data-anchor-id="globus-issues"><span style="color: orange;">Globus issues</span></h4>
<p><strong>Resolved</strong>: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is available again after data-transfer node <code>dt1</code> coming online. <br><span class="timestamp">February 24, 2025, 10:30 PT</span></p>
<p><strong>Notice</strong>: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is unavailable, because data-transfer node <code>dt1</code> is unavailable. <br><span class="timestamp">February 22, 2025, 13:30 PT</span></p>
</section>
</section>
<section id="february-18-24-2025" class="level3">
<h3 class="anchored" data-anchor-id="february-18-24-2025">February 18-24, 2025</h3>
<section id="globus-issues-1" class="level4">
<h4 class="anchored" data-anchor-id="globus-issues-1"><span style="color: orange;">Globus issues</span></h4>
<p><strong>Resolved</strong>: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are functional again. <br><span class="timestamp">February 24, 2025, 09:30 PT</span></p>
<p><strong>Update</strong>: The vendor has escalated our support ticket. <br><span class="timestamp">February 19, 2025, 13:30 PT</span></p>
<p><strong>Notice</strong>: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are currently unavailable. The former gives an error on “Unknown user or wrong password”, and the latter “Authentication Required - Identity set contains an identity from an allowed domain, but it does not map to a valid username for this connector”. The regular ‘Wynton HPC’ Globus endpoint is unaffected and available. The problem has been there since at least 2025-02-14 at 22:36, when I user reported it. <br><span class="timestamp">February 19, 2025, 12:00 PT</span></p>
</section>
</section>
<section id="january-17-22-2025" class="level3">
<h3 class="anchored" data-anchor-id="january-17-22-2025">January 17-22, 2025</h3>
<section id="cluster-unavailable" class="level4">
<h4 class="anchored" data-anchor-id="cluster-unavailable"><span style="color: orange;">Cluster unavailable</span></h4>
<p><strong>Resolved</strong>: Wynton is fully operational again. The BeeGFS file system issue has been resolved. All data consistency has been verified. Working with the vendor, we have identified a potential bug in the BeeGFS quota system that caused the BeeGFS outage. That part is still under investigation in order to minimize and remove the risk of reoccurrence. <br><span class="timestamp">January 22, 12:15 PT</span></p>
<p><strong>Update</strong>: The login and data-transfer nodes are available again. <br><span class="timestamp">January 22, 11:00 PT</span></p>
<p><strong>Update</strong>: The third resynchronization completed successfully. <br><span class="timestamp">January 21, 18:30 PT</span></p>
<p><strong>Update</strong>: Further investigation of the failed resynchronization this morning indicated that the resynchronization did indeed keep running while it stopped producing any output and the underlying BeeGFS service was unresponsive. Because of this, we decided to not restart the resynchronization, but instead let it continue in the hope it will finish. But, by not restarting, Wynton will remain inaccessible. Our first objective is to not jeopardize the cluster, the second objective is to bring the system back online. <br><span class="timestamp">January 21, 15:15 PT</span></p>
<p><strong>Update</strong>: The cluster is unavailable again. The past resynchronization of the problematic BeeGFS metadata server failed again, which triggers the problem. We are communicating with the vendor for their support. <br><span class="timestamp">January 21, 09:45 PT</span></p>
<p><strong>Update</strong>: The cluster is available again, but the scheduler has been paused. No queued jobs are launched and running jobs have been suspended, but will resume when the pause of scheduler is removed. This is done to minimize the load on BeeGFS, which will simplify troubleshooting and increase the chances to stabilize BeeGFS. It is the same BeeGFS metadata server as before that is experiencing problems. <br><span class="timestamp">January 19, 13:45 PT</span></p>
<p><strong>Update</strong>: The cluster is unavailable again. <br><span class="timestamp">January 19, 12:45 PT</span></p>
<p><strong>Update</strong>: The cluster is working again. We have started a resynchronization of the problematic BeeGFS metadata server pair <code>meta22</code> and <code>meta32</code>. <br><span class="timestamp">January 18, 13:45 PT</span></p>
<p><strong>Update</strong>: First signs of the cluster coming back online again, e.g.&nbsp;queued jobs are launched, and it is possible to access the cluster via SSH. <br><span class="timestamp">January 18, 06:00 PT</span></p>
<p><strong>Update</strong>: Identifies a specific BeeGFS metadata server that is unresponsive. The BeeGFS vendor has been contacted. <br><span class="timestamp">January 18, 01:00 PT</span></p>
<p><strong>Update</strong>: The underlying problem appears to be BeeGFS. The storage servers are okay, but one or more metadata servers are unresponsive. <br><span class="timestamp">January 17, 21:30 PT</span></p>
<p><strong>Notice</strong>: The cluster is unavailable, e.g.&nbsp;i is not possible to access the login or the data-transfer nodes. <br><span class="timestamp">January 17, 19:45 PT</span></p>
<!--
## When BeeGFS was down
start: 2025-01-21T09:45:00
stop: 2025-01-22T12:15:00
length: 27h30m
severity: major-outage
affected: beegfs
reason: beegfs

## When BeeGFS was down
start: 2025-01-19T12:45:00
stop: 2025-01-19T13:45:00
length: 1h00m
severity: major-outage
affected: beegfs
reason: beegfs

## When scheduler was down
start: 2025-01-19T12:45:00
stop: 2025-01-22T12:15:00
length: 71h30m
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs

## When scheduler was down
start: 2025-01-17T19:45:00
stop: 2025-01-18T06:00:00
length: 10h15m
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs

## total length on scheduler being down due to BeeGFS: 81h45m
 -->
</section>
</section>
<section id="january-9-2025" class="level3">
<h3 class="anchored" data-anchor-id="january-9-2025">January 9, 2025</h3>
<section id="file-system-emergency-shutdown" class="level4">
<h4 class="anchored" data-anchor-id="file-system-emergency-shutdown"><span style="color: orange;">File-system emergency shutdown</span></h4>
<p><strong>Resolved</strong>: The cluster full operational again. Suspended jobs have been resumed. The BeeGFS issue has been resolved. Checked hardware and cables. Rebooted affected BeeGFS server. <br><span class="timestamp">January 9, 16:20 PT</span></p>
<p><strong>Notice</strong>: An issue with BeeGFS was detected. All Wynton jobs have been paused until further notice. <br><span class="timestamp">January 9, 15:10 PT</span></p>
<!--
start: 2025-01-09T15:10:00
stop: 2025-01-09T16:20:00
length: 1.2 hours
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs
 -->
</section>
</section>
</section>
</div>
<div id="2024" class="tab-pane fade">
<section>
<section id="operational-summary-for-2024" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2024">Operational Summary for 2024</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 137.0 hours (= 5.7 days) = 1.6%</li>
<li>Unscheduled: 142.3 hours (= 5.9 days) = 1.6%</li>
<li>Total: 279.3 hours (= 11.6 days) = 3.2%</li>
<li>External factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-1" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-1">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>2024-06-17 – 2024-06-18 (32.0 hours)</li>
<li>2024-10-14 – 2024-10-18 (105.0 hours)</li>
</ul></li>
<li>Total downtime: 137.0 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-1" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-1">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>2024-04-03 (~500 hours)</li>
</ul></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-1" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-1">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-1" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-1">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>2024-03-14 (13.0 hours)</li>
<li>2024-03-17 (15.0 hours)</li>
<li>2024-05-31 (2.3 hours)</li>
<li>2024-06-15 – 2024-06-21 (112.0 hours; excluding 32 hours scheduled maintenance)</li>
</ul></li>
<li>Total downtime: 142.3 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-other-reasons-1" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-other-reasons-1">Unscheduled downtimes due to other reasons</h4>
<ul>
<li>Impact: Less compute resources</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
</section>
<section id="october-14-18-2024" class="level3">
<h3 class="anchored" data-anchor-id="october-14-18-2024">October 14-18, 2024</h3>
<section id="full-downtime-1" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-1"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Resolved</strong>: The cluster is back online. <br><span class="timestamp">October 18, 17:00 PT</span></p>
<p><strong>Update</strong>: The cluster including all its storage is offline undergoing a scheduled maintenance. <br><span class="timestamp">October 14, 11:00 PT</span></p>
<p><strong>Notice</strong>: The cluster will be shut down for maintenance from 8:00 am on Monday October 14 until 5:00 pm on Friday October 18, 2024. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Starting 14 days before, the maximum job runtime will be decreased on a daily basis from the current 14 days down to one day so that jobs finish in time before the shutdown. Jobs with runtimes that go into the maintenance window will be started after the downtime. The reason for the downtime is that UCSF Facilities will perform maintenance affecting cooling in our data center. We will take this opportunity to perform system updates and BeeGFS maintenance. <br><span class="timestamp">September 20, 16:45 PT</span></p>
<!--
comment: Scheduled downtime
start: 2024-10-14T08:00:00
stop: 2024-10-18T17:00:00
length: 105 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="september-12-2024" class="level3">
<h3 class="anchored" data-anchor-id="september-12-2024">September 12, 2024</h3>
<section id="kernel-maintenance-1" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-1"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All interactive nodes have been updated and deployed with the new CGroups limits. <br><span class="timestamp">September 13, 13:00 PT</span></p>
<p><strong>Notice</strong>: All interactive nodes will be shutdown and rebooted on Thursday September 12 at 12:30 to update Linux kernels and deploy CGroups-controlled CPU and memory user limits. To avoid data loss, please save your work and logout before. Queued and running jobs are not affected. <br><span class="timestamp">September 11, 09:15 PT</span></p>
</section>
</section>
<section id="june-15-25-2024" class="level3">
<h3 class="anchored" data-anchor-id="june-15-25-2024">June 15-25, 2024</h3>
<section id="file-system-unreliable" class="level4">
<h4 class="anchored" data-anchor-id="file-system-unreliable"><span style="color: orange;">File-system unreliable</span></h4>
<p><strong>Resolved</strong>: 14,000 compute slots are now available, which corresponds to the majority of compute nodes. <br><span class="timestamp">June 25, 00:30 PT</span></p>
<p><strong>Update</strong>: We will go ahead and re-enable the remaining compute nodes. <br><span class="timestamp">June 24, 13:00 PT</span></p>
<p><strong>Update</strong>: Development nodes are available. We have also opened up 100 compute nodes. We will keep monitoring BeeGFS over the weekend with the plan to re-enable the remaining compute nodes if all go well. <br><span class="timestamp">June 21, 19:15 PT</span></p>
<p><strong>Update</strong>: The login and data-transfer nodes are available. We will continue to validate BeeGFS during the day with the intent to open up the development nodes and a portion of the compute nodes before the weekend. <br><span class="timestamp">June 21, 12:45 PT</span></p>
<p><strong>Update</strong>: We decided to replace the problematic chassis with a spare. The RAID file system has two failing drives, which are currently being restored. We expect this to finish up in the morning. Then, we will replace those two failing drives and proceed with another restore. If that succeeds, we plan to open up the login nodes to make files available again. After that, the goal is to slowly open up the queue and compute nodes over the weekend. <br><span class="timestamp">June 20, 23:30 PT</span></p>
<p><strong>Update</strong>: We had folks onsite today to complete some preventative maintenance on all of the disk chassis (and, in a fit of optimism, bring up all of the nodes to prepare for a return to production). As this maintenance involved new firmware, we had some hope that it might sort out our issues with the problematic chassis. Unfortunately, our testing was still able to cause an issue (read: crash). We’ve sent details from this latest crash to the vendor and we’ll be pushing hard to work with them tomorrow Thursday to sort things out. <br><span class="timestamp">June 20, 00:15 PT</span></p>
<p><strong>Update</strong>: The vendor is still working on diagnosing our disk chassis issue. That work will resume after Wednesday’s holiday. So, unfortunately, we will not be able to bring Wynton up on Wednesday. We hope to come up on Thursday, but it all depends on our testing and the vendor’s investigation. <br><span class="timestamp">June 19, 01:00 PT</span></p>
<p><strong>Update</strong>: We are working with both the system and chassis vendors to diagnose this and determine what the problem is and how to fix it. This process is taking much longer than we’d like, and it is looking increasingly unlikely that we’ll be in a position to bring Wynton back online today. <br><span class="timestamp">June 18, 14:00 PT</span></p>
<p><strong>Update</strong>: A disk chassis that hosts part of <code>/wynton/home</code> appears to be failing. It works for a while and then fails, which brings down <code>/wynton</code>. We are trying to keep it running as much as possible, but can’t make any promises. <br><span class="timestamp">June 16, 00:15 PT</span></p>
<p><strong>Notice</strong>: Wynton is currently down due to an unknown issue. The problem started around 15:00 on Saturday 2024-06-15. <br><span class="timestamp">June 15, 23:15 PT</span></p>
<!--
start: 2024-06-15T15:00:00
stop: 2024-06-21T19:00:00
length: 144 hours - 32.0 hours scheduled maintenance = 112 hours
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs
 -->
</section>
</section>
<section id="june-17-18-2024" class="level3">
<h3 class="anchored" data-anchor-id="june-17-18-2024">June 17-18, 2024</h3>
<section id="full-downtime-2" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-2"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Update</strong>: All but one of the planned maintenance upgrades were completed during this scheduled maintenance. The remain upgrade does not require a downtime and will be done in a near future without disrupting the cluster. <br><span class="timestamp">June 18, 17:00 PT</span></p>
<p><strong>Update</strong>: Wynton is down for maintenance as of 09:00 on Monday 2024-06-17. <br><span class="timestamp">June 17, 09:00 PT</span></p>
<p><strong>Notice</strong>: The cluster will be shut down for maintenance from 9 pm on Monday June 17 until 5:00 pm on Tuesday June 18, 2024. Starting June 3, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. <br><span class="timestamp">June 5, 09:00 PT</span></p>
<!--
comment: Scheduled downtime
start: 2024-06-17T09:00:00
stop: 2024-06-18T17:00:00
length: 32 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="june-7-june-10-2024" class="level3">
<h3 class="anchored" data-anchor-id="june-7-june-10-2024">June 7-June 10, 2024</h3>
<section id="development-nodes-are-inaccessible" class="level4">
<h4 class="anchored" data-anchor-id="development-nodes-are-inaccessible"><span style="color: orange;">Development nodes are inaccessible</span></h4>
<p><strong>Resolved</strong>: Development nodes are available again. <br><span class="timestamp">June 10, 10:25 PT</span></p>
<p><strong>Notice</strong>: Development nodes are inaccessible since Friday June 7 at 17:00. We will investigate the problem on Monday. <br><span class="timestamp">June 8, 05:45 PT</span></p>
<!--
start: 2024-06-07T17:05:00
stop: 2024-06-10T10:25:00
length: 65.0 hours
severity: major
affected: development-nodes
reason: unknown
 -->
</section>
</section>
<section id="may-31-2024" class="level3">
<h3 class="anchored" data-anchor-id="may-31-2024">May 31, 2024</h3>
<section id="file-system-failures" class="level4">
<h4 class="anchored" data-anchor-id="file-system-failures"><span style="color: orange;">File-system failures</span></h4>
<p><strong>Resolved</strong>: The BeeGFS issue has been resolved. Wynton is operational again. <br><span class="timestamp">May 31, 09:20 PT</span></p>
<p><strong>Notice</strong>: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started around 06:00. We’re working on it and will post updates as we know more. <br><span class="timestamp">May 31, 08:45 PT</span></p>
<!--
start: 2024-05-31T07:00:00
stop: 2024-05-31T09:20:00
length: 2.3 hours
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs
 -->
</section>
</section>
<section id="april-3-25-2024" class="level3">
<h3 class="anchored" data-anchor-id="april-3-25-2024">April 3-25, 2024</h3>
<section id="kernel-maintenance-2" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-2"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">April 25, 09:00 PT</span></p>
<p><strong>Update</strong>: Login, data-transfer, and development nodes have been rebooted. <br><span class="timestamp">April 4, 11:15 PT</span></p>
<p><strong>Update</strong>: A new set of kernel updates will be rolled out. Login, data-transfer, and development nodes will be rebooted briefly on Thursday April 11 at 11:00. All compute nodes will also have to be drained and rebooted, which might take up to two weeks. Some of the compute have been draining since last week, meaning that will only have been drain for at most another week. <br><span class="timestamp">April 10, 16:00 PT</span></p>
<p><strong>Update</strong>: Hosts <code>dt1</code> and <code>plog1</code> are now also available. <br><span class="timestamp">April 4, 12:15 PT</span></p>
<p><strong>Update</strong>: Login, data-transfer, and development nodes have been rebooted. It will take some more time before <code>dt1</code> and <code>plog1</code> are available again, because they did not come back as expected after the reboot. <br><span class="timestamp">April 4, 11:15 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Thursday April 4 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. <br><span class="timestamp">April 3, 17:30 PT</span></p>
</section>
</section>
<section id="march-17-18-2024" class="level3">
<h3 class="anchored" data-anchor-id="march-17-18-2024">March 17-18, 2024</h3>
<section id="file-system-failures-1" class="level4">
<h4 class="anchored" data-anchor-id="file-system-failures-1"><span style="color: orange;">File-system failures</span></h4>
<p><strong>Resolved</strong>: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. <br><span class="timestamp">March 18, 10:30 PT</span></p>
<p><strong>Notice</strong>: Wynton is currently down due to an unknown BeeGFS issues. The problem started around 19:30 on 2024-03-17. We’re working on it and will post updates as we know more. <br><span class="timestamp">March 18, 09:00 PT</span></p>
<!--
start: 2024-03-17T19:30:00
stop: 2024-03-18T10:30:00
length: 15.0 hours
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs
 -->
</section>
</section>
<section id="march-14-2024" class="level3">
<h3 class="anchored" data-anchor-id="march-14-2024">March 14, 2024</h3>
<section id="file-system-failures-2" class="level4">
<h4 class="anchored" data-anchor-id="file-system-failures-2"><span style="color: orange;">File-system failures</span></h4>
<p><strong>Resolved</strong>: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. <br><span class="timestamp">March 14, 15:15 PT</span></p>
<p><strong>Notice</strong>: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started at 02:11 this morning. We’re working on it and will post updates as we know more. <br><span class="timestamp">March 14, 09:15 PT</span></p>
<!--
start: 2024-03-14T02:11:00
stop: 2024-03-14T15:15:00
length: 13.0 hours
severity: major-outage
affected: jobs, beegfs, compute, *
reason: beegfs
 -->
</section>
</section>
<section id="february-2-3-2024" class="level3">
<h3 class="anchored" data-anchor-id="february-2-3-2024">February 2-3, 2024</h3>
<section id="kernel-maintenance-3" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-3"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All hosts are available. <br><span class="timestamp">February 3, 17:00 PT</span></p>
<p><strong>Update</strong>: Login, data-transfer, and development nodes have been rebooted. It will take some more time before <code>plog1</code>, <code>dt1</code>, and <code>dev2</code> are available again, because they did not come back as expected after the reboot. PHI users may use <code>pdt1</code> and <code>pdt2</code> to access the cluster. <br><span class="timestamp">February 2, 14:45 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Friday February 2 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. <br><span class="timestamp">February 1, 23:30 PT</span></p>
</section>
</section>
<section id="january-25-august-2024" class="level3">
<h3 class="anchored" data-anchor-id="january-25-august-2024">January 25-August, 2024</h3>
<section id="emergency-shutdown-due-to-cooling-issue" class="level4">
<h4 class="anchored" data-anchor-id="emergency-shutdown-due-to-cooling-issue"><span style="color: orange;">Emergency shutdown due to cooling issue</span></h4>
<p><strong>Resolved</strong>: UCSF Facilities has resolved the cooling issue and there are again two working chillers. As a fallback backup, the building is now connected to the campus chilled water loop. This was confirmed by UCSF Facilities on 2024-12-10. <br><span class="timestamp">July-August, 2024</span></p>
<p><strong>Update</strong>: UCSF Facilities performed testing for rerouting of updated chilled-water piping the building where the Wynton data center is hosted between 07-12 on 2024-05-08. <br><span class="timestamp">May 9, 12:30 PT</span></p>
<p><strong>Update</strong>: The compute and development nodes are available again. Jobs that were running when we did the emergency shutdown should be considered lost and need to be resubmitted. UCSF Facilities has re-established cooling, but there is currently no redundancy cooling system available, meaning there is a higher-than-usual risk for another failure. <br><span class="timestamp">January 25, 15:45 PT</span></p>
<p><strong>Notice</strong>: We are shutting down all Wynton compute and development nodes as an emergency action. This is due to a serious issue with the chilled-water system that feeds the cooling in the Wynton data center. By shutting down all of the compute nodes, we hope to slow the current temperature rise, while keeping the storage system, login and data-transfer nodes up. The will come back up again as soon as the UCSF Facilities has resolved the chilled-water system. ETA is currently unknown. <br><span class="timestamp">January 25, 11:25 PT</span></p>
<!--
start: 2024-01-25T11:25:00
stop: 2024-01-25T15:25:00
length: 4.0 hours
severity: major-outage
affected: jobs
reason: external
 -->
</section>
</section>
</section>
</div>
<div id="2023" class="tab-pane fade">
<section>
<section id="operational-summary-for-2023" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2023">Operational Summary for 2023</h3>
<ul>
<li>Full downtime:
<ul>
<li>Scheduled: 141.0 hours = 5.9 days = 1.6%</li>
<li>Unscheduled: 742.25 hours = 30.9 days = 8.5%</li>
<li>Total: 883.25 hours = 35.3 days = 10.1%</li>
<li>External factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-2" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-2">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>2023-02-22 (17.0 hours)</li>
<li>2023-05-17 (20.0 hours)</li>
<li>2023-10-30 – 2023-11-03 (104.0 hours)</li>
</ul></li>
<li>Total downtime: 141.0 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-2" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-2">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-2" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-2">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-2" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-2">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>2023-05-17 – 2023-06-01 (359.0 hours)</li>
<li>2023-10-27 – 2023-11-15 (347.25 hours, excluding the scheduled 5-day downtime)</li>
</ul></li>
<li>Total downtime: 742.25 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-other-reasons-2" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-other-reasons-2">Unscheduled downtimes due to other reasons</h4>
<ul>
<li>Impact: Less compute resources</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>N/A</li>
</ul></li>
<li>Total downtime: 0.0 hours of which 0.0 hours were due to external factors</li>
</ul>
</section>
</section>
<section id="november-15-december-15-2023" class="level3">
<h3 class="anchored" data-anchor-id="november-15-december-15-2023">November 15-December 15, 2023</h3>
<section id="upgrading-compute-nodes" class="level4">
<h4 class="anchored" data-anchor-id="upgrading-compute-nodes"><span style="color: orange;">Upgrading compute nodes</span></h4>
<p><strong>Resolved</strong>: All compute nodes are up and running. <br><span class="timestamp">December 15, 09:00 PT</span></p>
<p><strong>Update</strong>: A total of ~15,000 CPU cores are now up and running. <br><span class="timestamp">November 27, 15:00 PT</span></p>
<p><strong>Update</strong>: A total of ~14,000 CPU cores are now up and running. <br><span class="timestamp">November 26, 02:00 PT</span></p>
<p><strong>Update</strong>: A total of ~13,000 CPU cores are now up and running. <br><span class="timestamp">November 22, 15:30 PT</span></p>
<p><strong>Update</strong>: A total of ~12,000 CPU cores are now up and running. <br><span class="timestamp">November 22, 01:00 PT</span></p>
<p><strong>Update</strong>: A total of ~10,000 CPU cores are now up and running. <br><span class="timestamp">November 21, 01:00 PT</span></p>
<p><strong>Update</strong>: 98 compute nodes with a total of 2,780 CPU cores are now up and running. <br><span class="timestamp">November 16, 15:00 PT</span></p>
<p><strong>Notice</strong>: As we come back from the downtime, we start out with 36 out of 490 compute nodes available to process jobs. Work continues to migrating the remaining nodes to Rocky 8. <br><span class="timestamp">November 15, 14:15 PT</span></p>
<!--
start: 2023-11-15T09:00:00
stop: 2023-12-15T09:00:00
length: 720 hours
severity: low
affected: jobs
reason: scheduled
 -->
</section>
</section>
<section id="october-30-november-15-2023" class="level3">
<h3 class="anchored" data-anchor-id="october-30-november-15-2023">October 30-November 15, 2023</h3>
<section id="full-downtime-3" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-3"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Update</strong>: The job scheduler is available and jobs are running. The data-transfer nodes are available. At this time, 36 out of 490 compute nodes have been re-enabled. Work has begun booting up the remaining ones. The first jobs were processed around 09:00 this morning. <br><span class="timestamp">November 15, 14:15 PT</span></p>
<p><strong>Update</strong>: We plan to re-enable the job scheduler and start processing jobs by the end of today. It is possible to submit jobs already now, but they will remain queued until we re-enable the scheduler. <br><span class="timestamp">November 15, 10:30 PT</span></p>
<p><strong>Update</strong>: The BeeGFS issue has been resolved, which allows us to move forward on the remaining Rocky-8 updates. We hope to start bringing compute nodes online as soon as tomorrow (2023-11-15). <br><span class="timestamp">November 14, 13:15 PT</span></p>
<p><strong>Update</strong>: Still status quo; the BeeGFS issue holds us back from bringing the scheduler back up. We’re rather certain that we will not be able to resolve it today or tomorrow. <br><span class="timestamp">November 13, 13:45 PT</span></p>
<p><strong>Update</strong>: Login and development nodes are available. Write access to the BeeGFS file system has been re-enabled. Due to continued issues in getting BeeGFS back in stable state, we are still not ready for opening up the scheduler and compute nodes. <br><span class="timestamp">November 11, 00:30 PT</span></p>
<p><strong>Update</strong>: Unfortunately, we will not bring up Wynton to run jobs today. We are evaluating what, if anything, may be possible to bring up before the long weekend. The reason being that the required metadata resynchronization failed late yesterday. The vendor has provided us with a script to fix the failure. That script is running, and once it’s done, we’ll reattempt to resynchronize. <br><span class="timestamp">November 9, 10:30 PT</span></p>
<p><strong>Update</strong>: We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. At that time, we expect all login, all data-transfer, and most development nodes will be available. A large number of the compute nodes will also be available via the scheduler. <br><span class="timestamp">November 8, 10:30 PT</span></p>
<p><strong>Update</strong>: The team makes progress on the scheduled downtime activities, which was delayed due to the BeeGFS incident. We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. <br><span class="timestamp">November 7, 11:20 PT</span></p>
<p><strong>Notice</strong>: The cluster will be shut down for maintenance from 9 pm on Monday October 30 through end of business on Friday November 3, 2023 (2023W44). The operating system will be upgraded system wide (<em>all</em> machines) from CentOS 7.9 to Rocky 8 Linux, the BeeGFS will be upgrade, and old hardware will be replaced. UCSF Facilities will perform scheduled work. After the downtime, there will no longer be any machine running CentOS 7.9. All machines will have their <em>local</em> disks (including <code>/scratch</code> and <code>/tmp</code>) wiped. Anything under <code>/wynton</code> (including <code>/wynton/scratch</code>, <code>/wynton/home</code>, …) should be unaffected, but please note that Wynton does not back anything up, so we recommend you to back up critical data. For more information about the Rocky 8 Linux migration project and how you can prepare for it is available at on the <a href="../../hpc/software/rocky-8-linux.html">Migration to Rocky 8 Linux from CentOS 7</a> page. <br><span class="timestamp">October 13, 11:15 PT</span></p>
<!--
comment: Scheduled downtime
start: 2023-10-30T09:00:00
stop: 2023-11-03T17:00:00
length: 104 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled

comment: Pre-downtime BeeGFS outtage
start: 2023-10-27T13:45:00
stop: 2023-10-30T09:00:00
length: 66.25 hours
severity: major
affected: jobs, beegfs, compute, *
reason: unscheduled

comment: Extended-downtime outtage
start: 2023-11-03T17:00:00
stop: 2023-11-15T09:00:00
length: 11.71 days = 281.0 hours
severity: major
affected: jobs, beegfs, compute, *
reason: unscheduled

comment: Total scheduled outage = 104 hours = 4.33 days
comment: Total unscheduled outage = 66.25 + 281 = 347.25 hours = 14.47 days
comment: Total outage = 451.25 hours = 18.8 days
 -->
</section>
</section>
<section id="october-27-november-14-2023" class="level3">
<h3 class="anchored" data-anchor-id="october-27-november-14-2023">October 27-November 14, 2023</h3>
<section id="file-system-failures-3" class="level4">
<h4 class="anchored" data-anchor-id="file-system-failures-3"><span style="color: orange;">File-system failures</span></h4>
<p><strong>Resolved</strong>: The BeeGFS metadata resynchronization is complete around 02:30 this morning. <br><span class="timestamp">November 14, 13:15 PT</span></p>
<p><strong>Update</strong>: The BeeGFS metadata resynchronization is still unresolved. We are looking into other strategies, which we are currently testing. If those tests are successful, we will attempt to deploy the fix in the production. <br><span class="timestamp">November 13, 13:45 PT</span></p>
<p><strong>Update</strong>: After resynchronization of the BeeGFS metadata kept failing, we identified a possible culprit. We suspect BeeGFS cannot handle the folders with many millions of files, causing the resynchronization to fail. We keep working on stabilizing BeeGFS. <br><span class="timestamp">November 11, 00:45 PT</span></p>
<p><strong>Update</strong>: The BeeGFS metadata resynchronization that had been running for several hours, failed late yesterday. The vendor has provided us with a script tailored to fix the issue we ran into. That script is running, and once it’s done, we’ll start the resynchronization again. <br><span class="timestamp">November 9, 10:30 PT</span></p>
<p><strong>Update</strong>: The recovery from the BeeGFS incident goes as planned. We estimate to have resolved this issue by the end of November 9, 2023, when full read-write access to <code>/wynton</code> will be available again. <br><span class="timestamp">November 8, 10:30 PT</span></p>
<p><strong>Update</strong>: The Wynton team works on fixing and stabilizing the BeeGFS incident. We estimate to have resolved this issue by the end of November 9, 2023. <br><span class="timestamp">November 7, 11:20 PT</span></p>
<p><strong>Update</strong>: Read-only access to Wynton has been enabled for users to retrieve their files. Login nodes <code>log1</code> and <code>plog1</code> are available for this. If going through the Wynton 2FA, make sure to answer “no” (default) when prompted for “Remember connection authentication from 98.153.103.186 for 12 hours? [y/N]”; answering “yes” causes the SSH connection to fail. <br><span class="timestamp">November 5, 00:30 PT</span></p>
<p><strong>Update</strong>: Wynton admins can retrieve user files under <code>/wynton/</code> upon requests until 18:00 today, when the UCSF network will go down. We are not able to share the PHI data under <code>/wynton/protected/</code>. Please contact support with all details including full path of the data to be retrieved. <br><span class="timestamp">October 30, 15:30 PT</span></p>
<p><strong>Update</strong>: The BeeGFS issue is related to a CentOS 7-kernel bug in one of our BeeGFS metadata servers. To minimize the risk of data loss on the <code>/wynton</code> file system, we took the decision to shut down Wynton immediately. At the moment, we do not have an estimate on how long it will take to resolve this problem. It has to be resolved before we can begin the major upgrade scheduled for 2023W44. <br><span class="timestamp">October 27, 16:30 PT</span></p>
<p><strong>Notice</strong>: The BeeGFS file system, which hosts <code>/wynton</code>, is experiencing unexpected, major issues. Some or all files on <code>/wynton</code> cannot be accessed, and when attempted, an <code>Communication error on send</code> error is seen. The problem started around 13:45 on Friday 2023-10-27. <br><span class="timestamp">October 27, 15:10 PT</span></p>
<!--
start: 2023-10-27T13:45:00-08:00
stop: 2023-11-14T13:15:00-07:00
length: 447.25 hours
severity: filesystem-failure
affected: jobs, beegfs, compute, storage, *
reason: unscheduled
 -->
</section>
</section>
<section id="october-23-october-26-2023" class="level3">
<h3 class="anchored" data-anchor-id="october-23-october-26-2023">October 23-October 26, 2023</h3>
<p><strong>Resolve</strong>: Login node <code>log2</code> and data-transfer node <code>dt1</code> are available again. <br><span class="timestamp">October 26, 12:15 PT</span></p>
<p><strong>Update</strong>: Development node <code>dev2</code> is available again. <br><span class="timestamp">October 24, 12:45 PT</span></p>
<p><strong>Notice</strong>: Access to login node <code>log2</code>, data-transfer nodes <code>dt1</code>, and development node <code>dev2</code> will be disabled from Monday-Friday October 23-27, 2023 (2023W43) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node <code>log1</code>, data-transfer nodes <code>dt2</code>, and development nodes <code>dev1</code> and <code>dev3</code> are unaffected, so are the Wynton HPC Globus endpoints. <br><span class="timestamp">October 23, 11:10 PT</span></p>
</section>
<section id="october-16-october-20-2023" class="level3">
<h3 class="anchored" data-anchor-id="october-16-october-20-2023">October 16-October 20, 2023</h3>
<p><strong>Resolved</strong>: Login node <code>log1</code>, data-transfer nodes <code>dt2</code> and <code>pdt2</code> are available again and are now running Rocky 8. <br><span class="timestamp">October 20, 17:00 PT</span></p>
<p><strong>Notice</strong>: Data-transfer nodes <code>dt2</code> will be disabled this week instead of <code>dt1</code> as previously announced. <br><span class="timestamp">October 16, 14:30 PT</span></p>
<p><strong>Notice</strong>: Access to login node <code>log1</code>, data-transfer nodes <code>dt1</code>, and <code>pdt2</code> will be disabled from Monday-Friday October 16-20, 2023 (2023W42) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node <code>log2</code>, data-transfer nodes <code>dt2</code>, and <code>pdt1</code> are unaffected, so are the Wynton HPC Globus endpoints. <br><span class="timestamp">October 13, 11:20 PT</span></p>
</section>
<section id="june-1-2023---april-3-2024" class="level3">
<h3 class="anchored" data-anchor-id="june-1-2023---april-3-2024">June 1, 2023 - April 3, 2024</h3>
<section id="post-file-system-failure-incidents" class="level4">
<h4 class="anchored" data-anchor-id="post-file-system-failure-incidents"><span style="color: orange;">Post file-system failure incidents</span></h4>
<p><strong>Resolved</strong>: All corrupted and orphaned files have now been deleted. There might be orphaned directories remaining, which we leave to each user to remove, if they exist. <br><span class="timestamp">April 3, 2024, 11:15 PT</span></p>
<p><strong>Update</strong>: Reading files whose data was lost on the unrecovered storage targets back in May no longer results in an error message. Instead, the portion of the file that was lost will be replaced by null bytes. Obviously, this results in a file with corrupt content. The admins will be going through and deleting all the corrupted files as soon as possible. It’s a big task and will take some time. <br><span class="timestamp">July 13, 14:15 PT</span></p>
<p><strong>Update</strong>: The remaining two ZFS storage targets (22004 and 22006) are back online again. <br><span class="timestamp">July 11, 10:30 PT</span></p>
<p><strong>Update</strong>: Four out of the six ZFS storage targets have been brought back online. Two targets (22004 and 22006) remain offline. <strong>If you encounter a “Communication error on send” error, please do not delete or move the affected file.</strong> <br><span class="timestamp">July 6, 17:00 PT</span></p>
<p><strong>Update</strong>: Six ZFS storage targets (22001-22006) are down, because one of the recovered storage targets encountered latent damage that had gone undetected since the recovery in May. This locked up the server and thus all six targets on that server. <br><span class="timestamp">July 6, 08:30 PT</span></p>
<p><strong>Update</strong>: The final two ZFS storage targets are now serving the BeeGFS file system (<code>/wynton</code>) again. <br><span class="timestamp">June 30, 11:00 PT</span></p>
<p><strong>Update</strong>: We will be reintroducing the final two ZFS storage targets back into the BeeGFS file system (<code>/wynton</code>) on Friday June 30. The work will start at 10 am and should take an hour or so. During that time, there will be a couple of brief “blips” as we reconfigure the storage. <br><span class="timestamp">June 29, 23:55 PT</span></p>
<p><strong>Update</strong>: Organizing the data recovered from ZFS storage target 22004 into a form suitable for BeeGFS is taking long than expected. Thus far, we’ve properly replaced 10,354,873 of the 11,351,926 recovered files. Approximately one million files remain. We now hope to complete the work this week. The automatic clean up of old files on <code>/wynton/scratch</code> and <code>/wynton/protected/scratch</code> have been resumed. <br><span class="timestamp">June 27, 17:00 PT</span></p>
<p><strong>Update</strong>: There are two broken ZFS storage targets (22004 and 21002). We expect to recover most files on target 22004 (approximately 14 TB). The reason it takes this long to recover that storage target is that the file chunks are there, but we have to puzzle them together to reconstruct the original files, which is a slow process. We estimate this process to complete by the end of the week. The files on the other target, target 21002, are unfortunately not recoverable. <strong>If you encounter a “Communication error on send” error, please do not delete or move the affected file.</strong> <br><span class="timestamp">June 21, 23:30 PT</span></p>
<p><strong>Notice</strong>: There are two ZFS storage targets that are still failing and offline. We have hopes to be able to recover files from one of them. As of June 9, about 12 TB of low-level, raw file data (out of ~15 TB) was recovered. When that is completed, we will start the tedious work on reconstructing the actual files lost. The consultants are less optimistic about recovering data from second storage target, because it was much more damaged. They will give us the final verdict by the end of the week. <strong>If you encounter a “Communication error on send” error, please do not delete or move the affected file.</strong> <br><span class="timestamp">June 12, 16:00 PT</span></p>
</section>
</section>
<section id="may-16-june-1-2023" class="level3">
<h3 class="anchored" data-anchor-id="may-16-june-1-2023">May 16-June 1, 2023</h3>
<section id="full-downtime-followed-by-network-and-file-system-recovery" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-followed-by-network-and-file-system-recovery"><span style="color: orange;">Full downtime followed by network and file-system recovery</span></h4>
<p><strong>Resolved</strong>: The job scheduler is now available. Access to <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code> has been restored. <strong>If you encounter a “Communication error on send” error, please do not delete or move the affected file.</strong> <br><span class="timestamp">June 1, 16:00 PT</span></p>
<p><strong>Update</strong>: Wynton will be fully available later today, meaning the job scheduler and access to <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code> will be re-enabled. Note, two ZFS storage targets are still faulty and offline, but the work of trying to recover them will continue while we go live. This means that any files on the above re-opened <code>/wynton</code> subfolders that are stored, in part or in full, on those two offline storage targets will be inaccessible. Any attempt to read such files will result in a “Communication error on send” error and stall. To exit, press <kbd>Ctrl-C</kbd>. <strong>Importantly, do <em>not</em> attempt to remove, move, or update such files! That will make it impossible to recover them!</strong> <br><span class="timestamp">June 1, 12:15 PT</span></p>
<p><strong>Update</strong>: In total 22 (92%) out of 24 failed storage targets has been recovered. The consultant hopes to recover the bulk of the data from one of the two remaining damaged targets. The final damage target is heavily damaged, work on it will continue a few more days, but it is likely it cannot be recovered. The plan is to open up <code>/wynton/group</code> tomorrow Thursday with instructions what to expect for files on the damaged targets. The compute nodes and the job scheduler will also be enabled during the day tomorrow. <br><span class="timestamp">May 31, 22:45 PT</span></p>
<p><strong>Update</strong>: In total 22 (92%) out of 24 failed storage targets has been recovered. The remaining two targets are unlikely to be fully recovered. We’re hoping to restore the bulk of the files from them, but there is a risk that we will get none back. Then plan is to bring back <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code>, and re-enable the job queue, on Thursday. <br><span class="timestamp">May 31, 01:00 PT</span></p>
<p><strong>Update</strong>: The login, data-transfer, and development nodes (except <code>gpudev1</code>) are now online an available for use. The job scheduler and compute nodes are kept offline, to allow for continued recovery of the failed ZFS storage pools. For the same reason, folders under <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code> are locked down, except for groups who have mirrored storage. <code>/wynton/home</code> and <code>/wynton/scratch</code> are fully available. We have suspended the automatic cleanup of old files under <code>/wynton/scratch</code> and <code>/wynton/protected/scratch</code>. The ZFS consultant recovered 3 of the 6 remaining storage targets. We have now recovered in total 21 (88%) out of 24 failed targets. The recovery work will continue on Monday (sic!). <br><span class="timestamp">May 26, 17:00 PT</span></p>
<p><strong>Update</strong>: All 12 ZFS storage targets on one server pair have been recovered and are undergoing final verification, after which that server pair is back in production. On the remaining server pair with also 12 failed ZFS storage targets, 4 targets have been recovered, 4 possibly have been, and 4 are holding out. We’re continuing our work with the consultant on those targets. These storage servers were installed on 2023-03-28, so it is only files written after that date that may be affected. We are tentatively planning on bringing up the login, data transfer and development nodes tomorrow Friday, prior to the long weekend, but access to directories in <code>/wynton/group</code>, <code>/wynton/protected/group</code>, or <code>/wynton/protected/project</code> will be blocked with the exception for a few groups with mirrored storage. <code>/wynton/home</code> and <code>/wynton/scratch</code> would be fully accessible. <br><span class="timestamp">May 25, 17:00 PT</span></p>
<p><strong>Update</strong>: 8 more ZFS storage targets were recovered today. We have now recovered in total 17 (71%) out of 24 failed targets. The content of the recovered targets is now being verified. We will continue working with the consultant tomorrow on the remaining 7 storage targets. <br><span class="timestamp">May 24, 17:00 PT</span></p>
<p><strong>Update</strong>: The maintenance and upgrade of the Wynton network switch was successful and is now completed. We also made progress of recovering the failed ZFS storage targets - 9 (38%) out of 24 failed targets have been recovered. To maximize our chances at a full recovery, Wynton will be kept down until the consultant completes their initial assessment. Details: The contracted ZFS consultant started to work on recovering the failed ZFS storage targets that we have on four servers. During the two hours of work, they quickly recovered another three targets on on the first server, leaving us with only one failed target on that server. Attempts of the same recovery method on the second and third servers were not successful. There was no time today to work on the fourth server. The work to recover the remaining targets will resume tomorrow. After the initial recovery attempt has been attempted on all targets, the consultant, who is one of the lead ZFS developers, plans to load a development version of ZFS on the servers in order to perform more thorough and deep-reaching recovery attempts. <br><span class="timestamp">May 23, 17:00 PT</span></p>
<p><strong>Update</strong>: Wynton will be kept down until the ZFS-recovery consultant has completed their initial assessment. If they get everything back quickly, Wynton will come back up swiftly. If recovery takes longer, or is less certain, we will look at coming back up without the problematic storage targets. As the purchase is being finalized, we hope that the consultant can start their work either on Tuesday or Wednesday. The UCSF Networking Team is performing more maintenance on the switch tonight. <br><span class="timestamp">May 22, 23:30 PT</span></p>
<p><strong>Update</strong>: The cluster will be kept offline until at least Tuesday May 23. The BeeGFS file-system failure is because 24 out of 144 ZFS storage targets got corrupted. These 24 storage targets served our “group” storage, which means only files written to <code>/wynton/group</code>, <code>/wynton/protected/group</code>, and <code>/wynton/protected/project</code> within the past couple of months are affected. Files under <code>/wynton/home</code> and <code>/wynton/scratch</code> are not affected. We are scanning the BeeGFS file system to identify exactly which files are affected. Thus far, we have managed to recover 6 (25%) out of the 24 failed targets. The remaining 18 targets are more complicated and we are working with a vendor to start helping us recover them next week. <br><span class="timestamp">May 19, 10:15 PT</span></p>
<p><strong>Update</strong>: Automatic cleanup of <code>/wynton/scratch</code> has been disabled. <br><span class="timestamp">May 18, 23:00 PT</span></p>
<p><strong>Update</strong>: Several ZFS storage targets that are used by BeeGFS experienced failures during the scheduled maintenance window. There is a very high risk of partial data loss, but we will do everything possible to minimize the loss. In addition, the Wynton core network switch failed and needs to be replaced. The UCSF IT Infrastructure Network Services Team works with the vendor to get a rapid replacement. <br><span class="timestamp">May 17, 16:30 PT</span></p>
<p><strong>Update</strong>: The cluster is down and unavailable because of maintenance. <br><span class="timestamp">May 16, 21:00 PT</span></p>
<p><strong>Update</strong>: There will be a one-day downtime starting at 21:00 on Tuesday May 16 and ending at 17:00 on Wednesday May 17. This is aligned with a planned PG&amp;E power-outage maintenance on May 17. Starting May 2, the maximum job runtime will be decreased on a daily basis from the maximum 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will only be started after the downtime. The default run time is 14 days, so make sure to specify <code>qsub -l h_rt=&lt;run-time&gt; ...</code> if you want something shorter. <br><span class="timestamp">May 3, 10:00 PT</span></p>
<p><strong>Update</strong>: The updated plan is to only have a 24-hour downtime starting the evening of Tuesday May 16 and end by the end of Wednesday May 17. This is aligned with a planned PG&amp;E power-outage maintenance on May 17. <br><span class="timestamp">April 24, 11:00 PT</span></p>
<p><strong>Update</strong>: The updated plan is to have the downtime during the week of May 15, 2023 (2023W20). This is aligned with a planned PG&amp;E power-outage maintenance during the same week. <br><span class="timestamp">March 27, 11:00 PT</span></p>
<p><strong>Notice</strong>: We will performing a full-week major update to the cluster during late Spring 2023. Current plan is to do this during either the week of May 8, 2023 (2023W19) or the week of May 15, 2023 (2023W20). <br><span class="timestamp">February 27, 11:00 PT</span></p>
<!--
start: 2023-05-17T17:00:00
stop: 2023-06-01T16:00:00
length: 359 hours
severity: filesystem-failure
affected: jobs, beegfs, compute, storage, *
reason: unscheduled
 -->
<!--
start: 2023-05-16T21:00:00
stop: 2023-05-17T17:00:00
length: 20 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="february-22-23-2023" class="level3">
<h3 class="anchored" data-anchor-id="february-22-23-2023">February 22-23, 2023</h3>
<section id="full-downtime-4" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-4"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Resolved</strong>: The cluster maintenance has completed and the cluster is now fully operational again. <br><span class="timestamp">February 23, 14:00 PT</span></p>
<p><strong>Update</strong>: The cluster has been shut down for maintenance. <br><span class="timestamp">February 22, 21:00 PT</span></p>
<p><strong>Notice</strong>: The cluster will be shut down for maintenance from 9 pm on Wednesday February 22 until 5:00 pm on Thursday February 23, 2023. This is done to avoid possible file-system and hardware failures when the UCSF Facilities performs power-system maintenance. During this downtime, we will perform cluster maintenance. Starting February 8, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. <br><span class="timestamp">February 9, 09:00 PT</span></p>
<!--
start: 2023-02-22T21:00:00
stop: 2023-02-23T14:00:00
length: 17 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="january-24-2023" class="level3">
<h3 class="anchored" data-anchor-id="january-24-2023">January 24, 2023</h3>
<section id="no-access-to-login-and-data-transfer-hosts" class="level4">
<h4 class="anchored" data-anchor-id="no-access-to-login-and-data-transfer-hosts"><span style="color: orange;">No access to login and data-transfer hosts</span></h4>
<p><strong>Resolve</strong>: Network issues has been resolved and access to all login and data-transfer has been re-established. The problem was physical (a cable was disconnected). <br><span class="timestamp">January 24, 16:00 PT</span></p>
<p><strong>Notice</strong>: There is no access to non-PHI login and data-transfer hosts (log[1-2], dt[1-2]). We suspect a physical issue (e.g.&nbsp;somebody kicked a cable), which means we need to send someone onsite to fix the problem. <br><span class="timestamp">January 24, 14:45 PT</span></p>
</section>
</section>
<section id="january-11-2023" class="level3">
<h3 class="anchored" data-anchor-id="january-11-2023">January 11, 2023</h3>
<section id="no-internet-access-on-development-nodes" class="level4">
<h4 class="anchored" data-anchor-id="no-internet-access-on-development-nodes"><span style="color: orange;">No internet access on development nodes</span></h4>
<p><strong>Resolved</strong>: The network issue for the proxy servers has been fixed. All development nodes now have working internet access. <br><span class="timestamp">January 11, 16:00 PT</span></p>
<p><strong>Workarounds</strong>: Until this issue has been resolved, and depending on needs, you might try to use a data-transfer node.Some of the software tools on the development nodes are also available on the data-transfer nodes, e.g.&nbsp;<code>curl</code>, <code>wget</code>, and <code>git</code>. <br><span class="timestamp">January 11, 09:50 PT</span></p>
<p><strong>Notice</strong>: The development nodes have no internet access, because the network used by out proxy servers is down for unknown reasons. The problem most likely started on January 10 around 15:45. <br><span class="timestamp">January 11, 09:00 PT</span></p>
<!--
start: 2023-01-10T15:45:00
stop: 2023-01-11T15:45:00
length: 24 hours
severity: partial-outage
affected: develop
reason: network
 -->
</section>
</section>
</section>
</div>
<div id="2022" class="tab-pane fade">
<section>
<section id="operational-summary-for-2022" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2022">Operational Summary for 2022</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 94.0 hours = 3.9 days = 1.1%</li>
<li>Unscheduled: 220.0 hours = 9.2 days = 2.5%</li>
<li>Total: 314.0 hours = 13.1 days = 3.6%</li>
<li>External factors: 36% of the above downtime, corresponding to 114 hours (= 4.8 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-3" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-3">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>2022-02-08 (53.5 hours)</li>
<li>2022-09-27 (40.5 hours)</li>
</ul></li>
<li>Total downtime: 94.0 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-3" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-3">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2022-08-05 (up to 14 days)</li>
</ol></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-3" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-3">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ul>
<li>2022-09-06 (66 hours)</li>
</ul></li>
<li>Total downtime: 66 hours of which 66 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-3" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-3">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>2022-03-28 (1 hours): Major BeeGFS issues</li>
<li>2022-03-26 (5 hours): Major BeeGFS issues</li>
<li>2022-03-18 (100 hours): Major BeeGFS issues</li>
</ul></li>
<li>Total downtime: 106.0 hours of which 0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-other-reasons-3" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-other-reasons-3">Unscheduled downtimes due to other reasons</h4>
<ul>
<li>Impact: Less compute resources</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>2022-03-26 (48 hours): Data-center cooling issues</li>
</ul></li>
<li>Total downtime: 48 hours of which 48 hours were due to external factors</li>
</ul>
</section>
<section id="accounts" class="level4">
<h4 class="anchored" data-anchor-id="accounts">Accounts</h4>
<ul>
<li>Number of user account: 1,643 (change: +369 during the year)</li>
</ul>
</section>
</section>
<section id="november-2-2022" class="level3">
<h3 class="anchored" data-anchor-id="november-2-2022">November 2, 2022</h3>
<section id="major-beegfs-issues" class="level4">
<h4 class="anchored" data-anchor-id="major-beegfs-issues"><span style="color: orange;">Major BeeGFS issues</span></h4>
<p><strong>Resolved</strong>: The BeeGFS issues have been resolved. At 05:29 this morning, a local file system hosting one of our 12 BeeGFS meta daemons crashed. Normally, BeeGFS detects this and redirects processing to a secondary, backup daemon. In this incident, this failback did not get activated and a manual intervention was needed. <br><span class="timestamp">November 2, 09:30 PT</span></p>
<p><strong>Notice</strong>: The BeeGFS file system started to experience issues early morning on Tuesday 2022-11-02. The symptoms are missing files and folders. <br><span class="timestamp">November 2, 08:15 PT</span></p>
<!--
start: 2022-11-02T05:30:00
stop: 2022-11-02T09:15:00
length: 3.75 hours
severity: partial-outage
affected: jobs, beegfs, compute
reason: beegfs
 -->
</section>
</section>
<section id="november-1-2022" class="level3">
<h3 class="anchored" data-anchor-id="november-1-2022">November 1, 2022</h3>
<section id="scheduler-not-available" class="level4">
<h4 class="anchored" data-anchor-id="scheduler-not-available"><span style="color: orange;">Scheduler not available</span></h4>
<p><strong>Resolved</strong>: The job scheduler is responsive again, but we are not certain what caused the problem. We will keep monitoring the issue. <br><span class="timestamp">November 1, 16:30 PT</span></p>
<p><strong>Notice</strong>: The job scheduler, SGE, does not respond to user requests, e.g.&nbsp;<code>qstat</code> and <code>qsub</code>. No new jobs can be submitted at this time. The first reports on problems came in around 09:00 this morning. We are troubleshooting the problem. <br><span class="timestamp">November 1, 10:25 PT</span></p>
<!--
start: 2022-11-01T07:16:30
stop: 2022-11-01T09:09:00
length: 7.5 hours
severity: partial-outage
affected: jobs, compute
reason: scheduler
 -->
</section>
</section>
<section id="september-27-29-2022" class="level3">
<h3 class="anchored" data-anchor-id="september-27-29-2022">September 27-29, 2022</h3>
<section id="full-downtime-5" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-5"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Resolved</strong>: The cluster maintenance has completed and the cluster is now fully operational again. <br><span class="timestamp">September 29, 13:30 PT</span></p>
<p><strong>Update</strong>: The cluster has been shut down for maintenance. <br><span class="timestamp">September 27, 21:00 PT</span></p>
<p><strong>Notice</strong>: Wynton will be shut down on Tuesday September 27, 2022 at 21:00. We expect the cluster to be back up by the end of the workday on Thursday September 29. This is done to avoid file-system and hardware failures that otherwise may occur when the UCSF Facilities performs maintenance to the power system in Byers Hall. We will take the opportunity to perform cluster maintenance after the completion of the power-system maintenance. <br><span class="timestamp">September 14, 17:00 PT</span></p>
<!--
start: 2022-09-27T21:00:00
stop: 2022-09-29T13:30:00
length: 40.5 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="september-6-9-2022" class="level3">
<h3 class="anchored" data-anchor-id="september-6-9-2022">September 6-9, 2022</h3>
<section id="outage-following-campus-power-glitch" class="level4">
<h4 class="anchored" data-anchor-id="outage-following-campus-power-glitch"><span style="color: orange;">Outage following campus power glitch</span></h4>
<p><strong>Resolved</strong>: As of 09:20 on 2022-09-09, the cluster is back in full operation. The queues are enabled, jobs are running, and the development nodes are accepting logins. <br><span class="timestamp">September 9, 09:35 PT</span></p>
<p><strong>Update</strong>: Login and data-transfer nodes are disabled to minimize the risk for file corruption. <br><span class="timestamp">September 7, 12:45 PT</span></p>
<p><strong>Notice</strong>: The Wynton system experiencing system-wide issues, including the file system, due to a campus power glitch. To minimize the risk of corrupting the file system, it was decided to shut down the job scheduler and terminate all running jobs. The power outage at Mission Bay campus happened at 15:13. Despite diesel-generated backup power started up momentarily, it was enough to affect some of our servers. The job scheduler will be offline until the impact on Wynton is fully investigated. <br><span class="timestamp">September 6, 16:20 PT</span></p>
<!--
start: 2022-09-06T15:20:00
stop: 2022-09-09T09:20:00
length: 66 hours
severity: major-outage
affected: jobs, beegfs, compute
reason: power outage
 -->
</section>
</section>
<section id="august-5-9-2022" class="level3">
<h3 class="anchored" data-anchor-id="august-5-9-2022">August 5-9, 2022</h3>
<section id="kernel-maintenance-4" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-4"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">Aug 9, 12:00 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Monday August 8 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~14,500 cores) in the graph above. <br><span class="timestamp">Aug 5, 10:30 PT</span></p>
</section>
</section>
<section id="august-4-2022" class="level3">
<h3 class="anchored" data-anchor-id="august-4-2022">August 4, 2022</h3>
<section id="software-repository-maintenance" class="level4">
<h4 class="anchored" data-anchor-id="software-repository-maintenance"><span style="color: orange;">Software repository maintenance</span></h4>
<p><strong>Resolved</strong>: The Sali lab software module repository is back. <br><span class="timestamp">Aug 4, 12:00 PT</span></p>
<p><strong>Notice</strong>: The Sali lab software module repository is back will be unavailable from around 10:30-11:30 today August 4 for maintenance. <br><span class="timestamp">Aug 4, 03:30 PT</span></p>
</section>
</section>
<section id="march-28-april-6-2022" class="level3">
<h3 class="anchored" data-anchor-id="march-28-april-6-2022">March 28-April 6, 2022</h3>
<section id="major-beegfs-issues-1" class="level4">
<h4 class="anchored" data-anchor-id="major-beegfs-issues-1"><span style="color: orange;">Major BeeGFS issues</span></h4>
<p><strong>Resolved</strong>: The patch of the BeeGFS servers were successfully deployed by 14:30 and went without disruptions. As a side effect, rudimentary benchmarking shows that this patch also improves the overall performance. Since the troubleshooting, bug fixing, and testing started on 2022-03-28, we managed to keep the impact of the bugs to a minimum resulting in only one hour of BeeGFS stall. <br><span class="timestamp">April 6, 17:00 PT</span></p>
<p><strong>Update</strong>: The BeeGFS servers will be updated tomorrow April 6 at 14:00. The cluster should work as usual during the update. <br><span class="timestamp">April 5, 17:00 PT</span></p>
<p><strong>Update</strong>: Our load tests over the weekend went well. Next, we will do discrepancy validation tests between our current version and the patch versions. When those pass, we will do a final confirmation with the BeeGFS vendor. We hope to deploy the patch to Wynton in a few days. <br><span class="timestamp">April 4, 10:30 PT</span></p>
<p><strong>Update</strong>: After a few rounds, we now have a patch that we have confirmed work on our test BeeGFS system. The plan is to do additional high-load testing today and over the weekend. <br><span class="timestamp">April 1, 10:30 PT</span></p>
<p><strong>Update</strong>: The BeeGFS vendors will send us a patch by tomorrow Tuesday, which we will test on our separate BeeGFS test system. After being validated there, will will deploy it to the main system. We hope to have a patch deploy by the end of the week. <br><span class="timestamp">March 28, 11:30 PT</span></p>
<p><strong>Update</strong>: We have re-enabled the job scheduler after manually having resolved the BeeGFS meta server issues. We will keep monitoring the problem and send more debug data to the BeeGFS vendors. <br><span class="timestamp">March 28, 11:00 PT</span></p>
<p><strong>Notice</strong>: On Monday 2022-03-28 morning at 10:30 the BeeGFS hung again. We put a hold on the job scheduler for now. <br><span class="timestamp">March 28, 10:30 PT</span></p>
<!--
start: 2022-03-28T10:00:00
stop: 2022-03-28T11:00:00
length: 1.0 hours
severity: major-outage
affected: jobs, beegfs, compute
reason: beegfs
 -->
</section>
</section>
<section id="march-26-2022" class="level3">
<h3 class="anchored" data-anchor-id="march-26-2022">March 26, 2022</h3>
<section id="job-scheduler-is-disabled-due-to-cooling-issues" class="level4">
<h4 class="anchored" data-anchor-id="job-scheduler-is-disabled-due-to-cooling-issues"><span style="color: orange;">Job scheduler is disabled due to cooling issues</span></h4>
<p><strong>Resolved</strong>: The compute nodes and the job scheduler are up and running again. <br><span class="timestamp">March 26, 11:00 PT</span></p>
<p><strong>Notice</strong>: The job scheduler as disabled and running jobs where terminated on Saturday 2022-03-26 around 09:00. This was done due to an emergency shutdown because the ambient temperature in the data center started to rise around 08:00 and at 09:00 it hit the critical level, where our monitoring system automatically shuts down compute nodes to prevent further damage. This resulted in the room temperature coming down to normal levels again. We are waiting on UCSF Facilities to restore cooling in the data center. <br><span class="timestamp">March 26, 10:30 PT</span></p>
<!--
start: 2022-03-26T03:00:00
stop: 2022-03-26T09:00:00
length: 6 hours
severity: major-outage
affected: jobs
reason: external
 -->
</section>
</section>
<section id="march-26-2022-1" class="level3">
<h3 class="anchored" data-anchor-id="march-26-2022-1">March 26, 2022</h3>
<section id="major-beegfs-issues-2" class="level4">
<h4 class="anchored" data-anchor-id="major-beegfs-issues-2"><span style="color: orange;">Major BeeGFS issues</span></h4>
<p><strong>Resolved</strong>: Just after 03:00 on Saturday 2022-03-26 morning BeeGFS hung. Recover actions were taken at 07:30 and the problem was resolved before 08:00. We have tracked down the problem occur when a user runs more than one <code>rm -r /wynton/path/to/folder</code> concurrently on the same folder. This is a bug in BeeGFS that vendors is aware of. <br><span class="timestamp">March 26, 10:30 PT</span></p>
<!--
start: 2022-03-26T03:00:00
stop: 2022-03-26T08:00:00
length: 5.0 hours
severity: major-outage
affected: jobs, beegfs, compute
reason: beegfs
 -->
</section>
</section>
<section id="march-18-22-2022" class="level3">
<h3 class="anchored" data-anchor-id="march-18-22-2022">March 18-22, 2022</h3>
<section id="job-scheduler-is-disabled-because-of-beegfs-issues" class="level4">
<h4 class="anchored" data-anchor-id="job-scheduler-is-disabled-because-of-beegfs-issues"><span style="color: orange;">Job scheduler is disabled because of BeeGFS issues</span></h4>
<p><strong>Resolved</strong>: We have re-enabled the job scheduler, which now processes all queued jobs. We will keep working with the BeeGFS vendor to find a solution to avoid this issue from happening again. <br><span class="timestamp">March 22, 16:30 PT</span></p>
<p><strong>Update</strong>: The BeeGFS issue has been identified. We identified a job that appears to trigger a bug in BeeGFS, which we can reproduce. The BeeGFS vendor will work on a bug fix. The good news is that the job script that triggers the problem can be tweaked to avoid hitting the bug. This means we can enable the job scheduler as soon as all BeeGFS metadata servers have synchronized, which we expect to take a few hours. <br><span class="timestamp">March 22, 12:00 PT</span></p>
<p><strong>Update</strong>: The BeeGFS file system troubleshooting continues. The job queue is still disabled. You might experience login and non-responsive prompt issues while we troubleshoot this. We have met with the BeeGFS vendors this morning and we are collecting debug information to allow them to troubleshoot the problem on their end. At the same time, we hope to narrow in on the problem further on our end by trying to identify whether there is a particular job or software running on the queue that might cause this. Currently, we have no estimate when this problem will be fixed. We have another call scheduled with the vendor tomorrow morning. <br><span class="timestamp">March 21, 11:45 PT</span></p>
<p><strong>Update</strong>: The BeeGFS file system is back online and the cluster can be accessed again. However, we had to put SGE in maintenance mode, which means no jobs will be started until the underlying problem, which is still unknown, has been identified and resolved. The plan is to talk to the BeeGFS vendor as soon as possible after the weekend. Unfortunately, in order to stabilize BeeGFS, we had to kill, at 16:30 today, all running jobs and requeue them on the SGE job scheduler. They are now listed as status ‘Rq’. For troubleshooting purposes, please do <em>not</em> delete any of your ‘Rq’ jobs. <br><span class="timestamp">March 18, 17:05 PT</span></p>
<p><strong>Notification</strong>: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues. The problem, which started around 11:45 today, is being investigated. <br><span class="timestamp">March 18, 11:55 PT</span></p>
<!--
start: 2022-03-18T11:45:00
stop: 2022-03-22T15:45:00
length: 100.0 hours
severity: major-outage
affected: jobs, beegfs, compute
reason: beegfs
 -->
</section>
</section>
<section id="march-14-15-2022" class="level3">
<h3 class="anchored" data-anchor-id="march-14-15-2022">March 14-15, 2022</h3>
<section id="brief-network-outage" class="level4">
<h4 class="anchored" data-anchor-id="brief-network-outage"><span style="color: orange;">Brief network outage</span></h4>
<p><strong>Noticed</strong>: UCSF Network IT will be performing maintenance on several network switches in the evening and overnight on Monday March 14. This will <em>not</em> affect jobs running on the cluster. One of the switches is the one which provides Wynton with external network access. When that switch is rebooted, Wynton will be inaccessible for about 15 minutes. This is likely to happen somewhere between 22:00 and 23:00 that evening, but the outage window extends from 21:00 to 05:00 the following morning, so it could take place anywhere in that window. <br><span class="timestamp">March 11, 10:15 PT</span></p>
</section>
</section>
<section id="february-28-march-2-2022" class="level3">
<h3 class="anchored" data-anchor-id="february-28-march-2-2022">February 28-March 2, 2022</h3>
<section id="full-downtime-6" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-6"><span style="color: orange;">Full downtime</span></h4>
<p><strong>Resolved</strong>: Wynton is available again. <br><span class="timestamp">March 2, 15:30 PT</span></p>
<p><strong>Update</strong>: The Wynton environment is now offline for maintenance work. <br><span class="timestamp">February 28, 10:00 PT</span></p>
<p><strong>Clarification</strong>: The shutdown will take place early Monday morning February 28, 2022. Also, this is on a Monday and not on a Tuesday (as previously written below). <br><span class="timestamp">February 22, 11:45 PT</span></p>
<p><strong>Update</strong>: We confirm that this downtime will take place as scheduled. <br><span class="timestamp">February 14, 15:45 PT</span></p>
<p><strong>Notice</strong>: We are planning a full file-system maintenance starting on <del>Tuesday</del> Monday February 28, 2022. As this requires a full shutdown of the cluster environment, we will start decreasing the job queue, on February 14, two weeks prior to the shutdown. On February 14, jobs that requires 14 days or less to run will be launched. On February 15, only jobs that requires 13 days or less will be launched, and so on until the day of the downtime. Submitted jobs that would go into the downtime window if launched, will only be launched after the downtime window. <br><span class="timestamp">November 22, 11:45 PT</span></p>
<!--
start: 2022-02-28T10:00:00
stop: 2022-03-02T15:30:00
length: 53.5 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
</section>
</div>
<div id="2021" class="tab-pane fade">
<section>
<section id="operational-summary-for-2021" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2021">Operational Summary for 2021</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 64 hours = 2.7 days = 0.73%</li>
<li>Unscheduled: 58 hours = 2.4 days = 0.66%</li>
<li>Total: 122 hours = 5.1 days = 1.4%</li>
<li>External factors: 39% of the above downtime, corresponding to 47 hours (=2.0 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-4" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-4">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2021-05-25 (64 hours)</li>
</ol></li>
<li>Total downtime: 64 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-4" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-4">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2021-01-29 (up to 14 days)</li>
<li>2021-07-23 (up to 14 days)</li>
<li>2021-12-08 (up to 14 days)</li>
</ol></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-4" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-4">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ul>
<li>2021-08-26 (28 hours) - Planned Byers Hall power shutdown failed</li>
<li>2021-11-09 (10 hours) - Unplanned PG&amp;E power outage</li>
</ul></li>
<li>Total downtime: 38 hours of which 38 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-4" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-4">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ol type="1">
<li>2021-03-26 (9 hours) - Campus networks issues causing significant BeeGFS slowness</li>
<li>2021-07-23 (8 hours) - BeeGFS silently failed disks</li>
<li>2021-11-05 (3 hours) - BeeGFS non-responsive</li>
</ol></li>
<li>Total downtime: 20 hours of which 9 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-other-reasons-4" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-other-reasons-4">Unscheduled downtimes due to other reasons</h4>
<ul>
<li>Impact: Less compute resources</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ol type="1">
<li>2021-04-28 (210 hours) - GPU taken down due to server room cooling issues</li>
</ol></li>
<li>Total downtime: 0 hours</li>
</ul>
</section>
<section id="accounts-1" class="level4">
<h4 class="anchored" data-anchor-id="accounts-1">Accounts</h4>
<ul>
<li>Number of user account: 1,274 (change: +410 during the year)</li>
</ul>
</section>
</section>
<section id="december-8-december-23-2021" class="level3">
<h3 class="anchored" data-anchor-id="december-8-december-23-2021">December 8-December 23, 2021</h3>
<section id="kernel-maintenance-5" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-5"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">Dec 23, 12:00 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted tomorrow Thursday December 9 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~12,500 cores) in the graph above.<br>
<br><span class="timestamp">Dec 8, 16:30 PT</span></p>
</section>
</section>
<section id="december-19-21-2021" class="level3">
<h3 class="anchored" data-anchor-id="december-19-21-2021">December 19-21, 2021</h3>
<section id="globus-and-data-transfer-node-issue" class="level4">
<h4 class="anchored" data-anchor-id="globus-and-data-transfer-node-issue"><span style="color: orange;">Globus and data-transfer node issue</span></h4>
<p><strong>Resolved</strong>: Data-transfer node <code>dt1</code> and Globus file transfers are working again. <br><span class="timestamp">Dec 21, 13:20 PT</span></p>
<p><strong>Update</strong>: Globus file transfers to and from Wynton are not working. This is because Globus relies on the data-transfer node <code>dt1</code>, which is currently down. <br><span class="timestamp">Dec 20, 15:30 PT</span></p>
<p><strong>Notice</strong>: Data-transfer node <code>dt1</code> has issues. Please use <code>dt2</code> until resolved. The first report on this problem came yesterday at 21:30. <br><span class="timestamp">Dec 20, 09:30 PT</span></p>
</section>
</section>
<section id="november-9-2021" class="level3">
<h3 class="anchored" data-anchor-id="november-9-2021">November 9, 2021</h3>
<section id="partial-outage-due-to-campus-power-glitch" class="level4">
<h4 class="anchored" data-anchor-id="partial-outage-due-to-campus-power-glitch"><span style="color: orange;">Partial outage due to campus power glitch</span></h4>
<p><strong>Resolved</strong>: All hosts have been rebooted and are now up and running. <br><span class="timestamp">November 9, 11:00 PT</span></p>
<p><strong>Notice</strong>: There was a brief PG&amp;E power outage early Tuesday November 9 around 01:20. This affected the power on the Mission Bay campus, including the data center housing Wynton. The parts of our system with redundant power were fine, but many of the compute nodes are on PG&amp;E-power only and, therefore, went down. As a result, lots of jobs crashed. We will restart the nodes that crashed manually during the day today. <br><span class="timestamp">November 9, 09:10 PT</span></p>
<!--
start: 2021-11-09T01:20:00
stop: 2021-11-09T11:00:00
length: 10 hours
severity: partial-outage
affected: jobs, compute
reason: external
 -->
</section>
</section>
<section id="october-25-26-2021" class="level3">
<h3 class="anchored" data-anchor-id="october-25-26-2021">October 25-26, 2021</h3>
<section id="file-system-maintenance" class="level4">
<h4 class="anchored" data-anchor-id="file-system-maintenance"><span style="color: orange;">File-system maintenance</span></h4>
<p><strong>Resolved</strong>: Resynchronization of all file-system meta servers is complete, which concludes the maintenance. <br><span class="timestamp">October 26, 09:45 PT</span></p>
<p><strong>Update</strong>: The maintenance work has started. <br><span class="timestamp">October 25, 14:00 PT</span></p>
<p><strong>Notice</strong>: We will perform BeeGFS maintenance work starting Monday October 25 at 2:00 pm. During this work, the filesystem might be less performant. We don’t anticipate any downtime. <br><span class="timestamp">October 21, 12:10 PT</span></p>
</section>
</section>
<section id="august-26-september-10-2021" class="level3">
<h3 class="anchored" data-anchor-id="august-26-september-10-2021">August 26-September 10, 2021</h3>
<section id="byers-hall-power-outage-file-system-corruption" class="level4">
<h4 class="anchored" data-anchor-id="byers-hall-power-outage-file-system-corruption"><span style="color: orange;">Byers Hall power outage &amp; file-system corruption</span></h4>
<p><strong>Resolved</strong>: The corrupted filesystem has been recovered. <br><span class="timestamp">September 10, 17:20 PT</span></p>
<p><strong>Update</strong>: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on <code>/wynton/group/</code>. The file recovery tools are still running. <br><span class="timestamp">August 27, 13:05 PT</span></p>
<p><strong>Partially resolved</strong>: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on <code>/wynton/group/</code>. The file recovery tools are still running. <br><span class="timestamp">August 27, 13:05 PT</span></p>
<p><strong>Update</strong>: The BeeGFS filesystem recovering attempt keeps running. The current plan is to bring Wynton back online while keeping the problematic BeeGFS filesystem offline. <br><span class="timestamp">August 26, 23:05 PT</span></p>
<p><strong>Update</strong>: All of the BeeGFS servers are up and running, but one of the 108 filesystems that make up BeeGFS was corrupted by the sudden power outage. The bad filesystem is part of <code>/wynton/group/</code>. We estimate that 70 TB of data is affected. We are making every possible effort to restore this filesystem, which will take time. While we do so, Wynton will remain down. <br><span class="timestamp">August 26, 14:05 PT</span></p>
<p><strong>Notice</strong>: The cluster is down after an unplanned power outage in the main data center. The power is back online but several of our systems, including BeeGFS servers, did not come back up automatically and will require on-site, manual actions. <br><span class="timestamp">August 26, 09:15 PT</span></p>
<!--
start: 2021-08-26T09:15:00
stop: 2021-08-27T13:15:00
length: 28.0 hours
severity: major-outage
affected: jobs, beegfs, compute
reason: external
 -->
</section>
</section>
<section id="july-23-july-28-2021" class="level3">
<h3 class="anchored" data-anchor-id="july-23-july-28-2021">July 23-July 28, 2021</h3>
<section id="kernel-maintenance-6" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-6"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: The majority of the compute nodes have been rebooted after only four days, which was quicker than the maximum of 14 days. <br><span class="timestamp">July 28, 08:00 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted at 13:00 on Friday July 23 at 1:00 pm. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above.<br>
<br><span class="timestamp">July 23, 07:40 PT</span></p>
</section>
</section>
<section id="june-24-2021" class="level3">
<h3 class="anchored" data-anchor-id="june-24-2021">June 24, 2021</h3>
<section id="cluster-not-accessible-due-to-beegfs-issues" class="level4">
<h4 class="anchored" data-anchor-id="cluster-not-accessible-due-to-beegfs-issues"><span style="color: orange;">Cluster not accessible (due to BeeGFS issues)</span></h4>
<p><strong>Resolved</strong>: Wynton and BeeGFS is back online. The problem was due to failed disks. Unfortunately, about 10% of the space in <code>/wynton/scratch/</code> went bad, meaning some files are missing or corrupted. It is neither possible to recover them nor identify which files or folders are affected. In other words, expect some oddness if you had data under <code>/wynton/scratch/</code>. There will also be some hiccups over the next several days as we get everything in ZFS and BeeGFS back into an as stable state as possible. <br><span class="timestamp">June 24, 14:55 PT</span></p>
<p><strong>Update</strong>: We’re working hard on getting BeeGFS back up. We were not able to recover the bad storage target, so it looks like there will be some data loss on <code>/wynton/scratch/</code>. More updates soon. <br><span class="timestamp">June 24, 13:45 PT</span></p>
<p><strong>Notification</strong>: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues since early this morning. The problem is being investigated. <br><span class="timestamp">June 24, 07:00 PT</span></p>
<!--
start: 2021-06-24T07:00:00
stop: 2021-06-24T14:55:00
length: 8.0 hours
severity: major-outage
affected: jobs, beegfs, compute
reason: external
 -->
</section>
</section>
<section id="may-25-june-7-2021" class="level3">
<h3 class="anchored" data-anchor-id="may-25-june-7-2021">May 25-June 7, 2021</h3>
<section id="full-downtime-major-maintenance" class="level4">
<h4 class="anchored" data-anchor-id="full-downtime-major-maintenance"><span style="color: orange;">Full downtime (major maintenance)</span></h4>
<p><strong>Resolved</strong>: All remaining issues from the downtime have been resolved. <br> <span class="timestamp">June 7, 17:00 PT</span></p>
<p><strong>Update</strong>: Login node log2 can now be reached from the UCSF Housing WiFi network. <br> <span class="timestamp">June 7, 17:00 PT</span></p>
<p><strong>Update</strong>: dt2 can now be reached from outside the Wynton cluster. <br> <span class="timestamp">June 7, 13:15 PT</span></p>
<p><strong>Update</strong>: Login node log2 cannot be reached from the UCSF Housing WiFi network. If you are on that network, use log1 until this has been resolved. <br> <span class="timestamp">June 2, 07:00 PT</span></p>
<p><strong>Update</strong>: Both data transfer nodes are back online since a while, but dt2 can only be reached from within the Wynton cluster. <br> <span class="timestamp">June 1, 13:45 PT</span></p>
<p><strong>Update</strong>: A large number of of the remaining compute nodes have been booted up. There are now ~8,600 cores serving jobs. <br> <span class="timestamp">June 1, 10:15 PT</span></p>
<p><strong>Update</strong>: The development nodes are now back too. For the PHI pilot project, development node pgpudev1 is back up, but pdev1 is still down. <br> <span class="timestamp">May 28, 10:00 PT</span></p>
<p><strong>Update</strong>: Wynton is partially back up and running. Both login hosts are up (log1 and log2). The job scheduler, SGE, accepts new jobs and and launches queued jobs. Two thirds of the compute node slots are back up serving jobs. Work is done to bring up the the development nodes and the data transfer hosts (dt1 and dt2). <br> <span class="timestamp">May 27, 10:30 PT</span></p>
<p><strong>Update</strong>: We hit more than a few snags today. Our filesystem, BeeGFS, is up and running, but it still needs some work. The login hosts are up, but SGE is not and neither are the dev nodes. We will continue the work early tomorrow Thursday. <br> <span class="timestamp">May 26, 21:40 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC environment will be shut down late afternoon on Tuesday May 25, 2021, for maintenance. We expect the cluster to be back online late Wednesday May 26. To allow for an orderly shutdown of Wynton, the queues have been disabled starting at 3:30 pm on May 25. Between now and then, only jobs whose runtimes end before that time will be able to start. Jobs whose runtimes would run into the maintenance window will remain in the queue. <br> <span class="timestamp">May 10, 16:40 PT</span></p>
<p><strong>Preliminary notice</strong>: The Wynton HPC cluster will be undergoing a major upgrade on Wednesday May 26, 2021. As usual, starting 15 days prior to this day, on May 11, the maximum job run-time will be decreased on a daily basis so that all jobs finishes in time, e.g.&nbsp;if you submit a job on May 16 with a run-time longer than nine days, it will not be able to scheduled and it will be queued until after the downtime. <br> <span class="timestamp">May 3, 11:00 PT</span></p>
<!--
start: 2021-05-25T16:00:00
stop: 2021-06-28T10:00:00
length: 64 hours
severity: under-maintenance
affected: jobs, beegfs, compute
reason: scheduled
 -->
</section>
</section>
<section id="june-1-2-2021" class="level3">
<h3 class="anchored" data-anchor-id="june-1-2-2021">June 1-2, 2021</h3>
<section id="password-management-outage" class="level4">
<h4 class="anchored" data-anchor-id="password-management-outage"><span style="color: orange;">Password management outage</span></h4>
<p><strong>Resolved</strong>: Password updates works again. <br> <span class="timestamp">June 2, 10:30 PT</span></p>
<p><strong>Notice</strong>: Due to technical issues, it is currently not possible to change your Wynton password. If attempted from the web interface, you will get an error on “Password change not successful! (kadmin: Communication failure with server while initializing kadmin interface )”. If attempted using ‘passwd’, you will get “passwd: Authentication token manipulation error”. <br> <span class="timestamp">June 1, 10:30 PT</span></p>
</section>
</section>
<section id="april-28---may-7-2021" class="level3">
<h3 class="anchored" data-anchor-id="april-28---may-7-2021">April 28 - May 7, 2021</h3>
<section id="many-gpu-nodes-down-due-to-cooling-issues" class="level4">
<h4 class="anchored" data-anchor-id="many-gpu-nodes-down-due-to-cooling-issues"><span style="color: orange;">Many GPU nodes down (due to cooling issues)</span></h4>
<p><strong>Resolved</strong>: Cooling has been restored and all GPU nodes are back online again. <br> <span class="timestamp">May 7, 11:10 PT</span></p>
<p><strong>Update</strong>: Half of the GPU nodes that was taken down are back online. Hopefully, the remaining ones can be brought back up tomorrow when the cooling in the server room should be fully functioning again. <br> <span class="timestamp">May 6, 14:30 PT</span></p>
<p><strong>Notification</strong>: One of Wynton’s ancillary server rooms is having cooling issues. To reduce the heat load in the room, we had to turn off all the Wynton nodes in the room around 09:45 this morning. This affects GPU nodes named msg*gpu* and a few other regular nodes. We estimate that the UCSF Facilities to fix the cooling problem by early next week. <br> <span class="timestamp">April 28, 16:30 PT</span></p>
<!--
start: 2021-04-28T16:30:00
stop: 2021-05-07T11:10:00
length: 210 hours
severity: partial-outage
affected: jobs-gpu, compute-gpu
reason: external
 -->
</section>
</section>
<section id="march-26-2021" class="level3">
<h3 class="anchored" data-anchor-id="march-26-2021">March 26, 2021</h3>
<section id="cluster-not-accessible-due-to-network-outage" class="level4">
<h4 class="anchored" data-anchor-id="cluster-not-accessible-due-to-network-outage"><span style="color: orange;">Cluster not accessible (due to network outage)</span></h4>
<p><strong>Resolved</strong>: The malfunctioning network link between two of Wynton’s data centers, which affected our BeeGFS file system and Wynton HPC as a whole, has been restored. <br><span class="timestamp">March 26, 21:30 PT</span></p>
<p><strong>Notification</strong>: Campus network issues causing major Wynton HPC issues including extremely slow access to our BeeGFS file system. This was first reported around 11:30 today. A ticket has been filed with the UCSF Network. ETA is unknown. <br><span class="timestamp">March 26, 12:30 PT</span></p>
<!--
start: 2021-03-26T12:30:00
stop: 2021-03-26T21:30:00
length: 9 hours
severity: major-outage
affected: beegfs, compute
reason: external
 -->
</section>
</section>
<section id="january-29-february-12-2021" class="level3">
<h3 class="anchored" data-anchor-id="january-29-february-12-2021">January 29-February 12, 2021</h3>
<section id="kernel-maintenance-7" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-7"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. A few compute nodes remain offline that has to be rebooted manually, which will be done as opportunity is given. <br><span class="timestamp">February 13, 09:00 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Login, data-transfer, and development nodes will be rebooted at 13:00 on Monday February 1. <br><span class="timestamp">January 31, 17:00 PT</span></p>
</section>
</section>
<section id="february-1-3-2021" class="level3">
<h3 class="anchored" data-anchor-id="february-1-3-2021">February 1-3, 2021</h3>
<section id="development-node-not-available" class="level4">
<h4 class="anchored" data-anchor-id="development-node-not-available"><span style="color: orange;">Development node not available</span></h4>
<p><strong>Resolved</strong>: Development node <code>dev2</code> is available again. <br><span class="timestamp">February 3, 15:00 PT</span></p>
<p><strong>Notice</strong>: Development node <code>dev2</code> is down. It failed to come back up after the kernel upgrade on 2021-02-01. An on-site reboot is planned for Wednesday February 3. <br><span class="timestamp">February 2, 11:45 PT</span></p>
</section>
</section>
<section id="january-28-2021" class="level3">
<h3 class="anchored" data-anchor-id="january-28-2021">January 28, 2021</h3>
<section id="server-room-maintenance" class="level4">
<h4 class="anchored" data-anchor-id="server-room-maintenance"><span style="color: orange;">Server room maintenance</span></h4>
<p><strong>Notice</strong>: The air conditioning system in one of our server rooms will be upgraded on January 28. The compute nodes in this room will be powered down during the upgrade resulting in fewer compute nodes being available on the cluster. Starting 14 days prior to this date, compute nodes in this room will only accept jobs that will finish in time. <br><span class="timestamp">January 13, 10:00 PT</span></p>
</section>
</section>
</section>
</div>
<div id="2020" class="tab-pane fade">
<section>
<section id="operational-summary-for-2020" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2020">Operational Summary for 2020</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 123 hours = 5.1 days = 1.4%</li>
<li>Unscheduled: 91.5 hours = 3.8 days = 1.0%</li>
<li>Total: 214.5 hours = 8.9 days = 2.4%</li>
<li>External factors: 12% of the above downtime, corresponding to 26.5 hours (=1.1 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-5" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-5">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2020-08-10 (93 hours)</li>
<li>2020-12-07 (30 hours)</li>
</ol></li>
<li>Total downtime: 123 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-5" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-5">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2020-06-11 (up to 14 days)</li>
<li>2020-12-11 (up to 14 days)</li>
</ol></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-5" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-5">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ul>
<li>None</li>
</ul></li>
<li>Total downtime: 0 hours</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-5" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-5">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ol type="1">
<li>2020-01-22 (2.5 hours) - BeeGFS failure to failed upgrade</li>
<li>2020-01-29 (1.0 hours) - BeeGFS non-responsive</li>
<li>2020-02-05 (51.5 hours) - Legacy NetApp file system failed</li>
<li>2020-05-22 (0.5 hours) - BeeGFS non-responsive to failed upgrade</li>
<li>2020-08-19 (1.5 hours) - BeeGFS non-responsive</li>
<li>2020-10-21 (3 hours) - BeeGFS non-responsive</li>
<li>2020-11-05 (3 hours) - BeeGFS non-responsive</li>
</ol></li>
<li>Total downtime: 63.0 hours</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-other-reasons-5" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-other-reasons-5">Unscheduled downtimes due to other reasons</h4>
<ul>
<li>Impact: Less compute resources</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ol type="1">
<li>2020-05-28 (26.5 hours) - MSG Data Center outage affecting many GPU compute nodes</li>
<li>2020-07-04 (2 hours) - SGE scheduler failed</li>
<li>2020-11-04 (288 hours) - ~80 compute nodes lost due to network switch failure</li>
</ol></li>
<li>Total downtime: 28.5 hours of which 26.5 hours were due to external factors</li>
</ul>
</section>
<section id="accounts-2" class="level4">
<h4 class="anchored" data-anchor-id="accounts-2">Accounts</h4>
<ul>
<li>Number of user account: 864 (change: +386 during the year)</li>
</ul>
</section>
</section>
<section id="december-8-17-2020" class="level3">
<h3 class="anchored" data-anchor-id="december-8-17-2020">December 8-17, 2020</h3>
<section id="limited-accessibility-of-login-node-log1" class="level4">
<h4 class="anchored" data-anchor-id="limited-accessibility-of-login-node-log1"><span style="color: orange;">Limited accessibility of Login node log1</span></h4>
<p><strong>Resolved</strong>: Login node ‘log1.wynton.ucsf.edu’ can again be accessed from outside of the UCSF network. <br><span class="timestamp">December 17, 14:20 PT</span></p>
<p><strong>Notice</strong>: Login node ‘log1.wynton.ucsf.edu’ is only accessible from within UCSF network. This is a side effect of the recent network upgrades. We are waiting for The UCSF IT Network to resolve this for us. Until resolved, please use the alternative ‘log2.wynton.ucsf.edu’ login node when connecting from outside of the UCSF network. <br><span class="timestamp">December 8, 23:00 PT</span></p>
</section>
</section>
<section id="december-11-16-2020" class="level3">
<h3 class="anchored" data-anchor-id="december-11-16-2020">December 11-16, 2020</h3>
<section id="rebooting-compute-nodes" class="level4">
<h4 class="anchored" data-anchor-id="rebooting-compute-nodes"><span style="color: orange;">Rebooting compute nodes</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">December 16, 05:00 PT</span></p>
<p><strong>Notice</strong>: The new BeeGFS setting introduced during the upgrades earlier this week caused problems throughout the system and we need to roll them back. The compute nodes will no longer take on new jobs until they have been rebooted. A compute node will be automatically rebooted as soon as all of its running jobs have completed. Unfortunately, we have to kill jobs that run on compute nodes that are stalled and suffer from the BeeGFS issues. <br><span class="timestamp">December 11, 13:50 PT</span></p>
</section>
</section>
<section id="december-11-2020" class="level3">
<h3 class="anchored" data-anchor-id="december-11-2020">December 11, 2020</h3>
<section id="rebooting-login-and-development-nodes" class="level4">
<h4 class="anchored" data-anchor-id="rebooting-login-and-development-nodes"><span style="color: orange;">Rebooting login and development nodes</span></h4>
<p><strong>Resolved</strong>: All login and development nodes have been rebooted. <br><span class="timestamp">December 12, 17:00 PT</span></p>
<p><strong>Notice</strong>: Login node ‘log1.wynton.ucsf.edu’ and all the development nodes will be rebooted at 4:30 PM today Friday. This is needed in order to roll back the new BeeGFS setting introduced during the upgrades earlier this week. <br><span class="timestamp">December 11, 13:50 PT</span></p>
</section>
</section>
<section id="december-7-8-2020" class="level3">
<h3 class="anchored" data-anchor-id="december-7-8-2020">December 7-8, 2020</h3>
<section id="major-upgrades-full-downtime" class="level4">
<h4 class="anchored" data-anchor-id="major-upgrades-full-downtime"><span style="color: orange;">Major upgrades (full downtime)</span></h4>
<p><strong>Resolved</strong>: The upgrade has been completed. The cluster back online, including all of the login, data-transfer, and development nodes, as well as the majority of the compute nodes. The scheduler is processing jobs again. All hosts now run CentOS 7.9. <br><span class="timestamp">December 8, 16:30 PT</span></p>
<p><strong>Update</strong>: The upgrade is paused and will resume tomorrow. We hope to be bring all of the cluster back online by the end of tomorrow. For now, login node ‘log2’ (but not ‘log1’), and data-transfer nodes ‘dt1’, and ‘dt2’ are back online and can be used for accessing files. Development nodes ‘dev1’ and ‘dev3’ are also available (please make sure to leave room for others). The scheduler remains down, i.e.&nbsp;it is is not be possible to submit jobs. <br><span class="timestamp">December 7, 17:00 PT</span></p>
<p><strong>Update</strong>: The upgrades have started. Access to Wynton HPC has been disable as of 10:30 this morning. The schedulers stopped launching queued jobs as of 23:30 last night. <br><span class="timestamp">December 7, 10:30 PT</span></p>
<p><strong>Revised notice</strong>: We have decided to hold back on upgrading BeeGFS during the downtime and only focus on the remain parts including operating system and network upgrades. The scope of the work is still non-trivial. There is a risk that the downtime will extend into Thursday December 10. However, if everything go smoothly, we hope that Wynton HPC will be back up by the end of Monday or during the Tuesday. There will only be one continuous downtime, that is, when the cluster comes back up, it will stay up. <br><span class="timestamp">December 3, 09:00 PT</span></p>
<p><strong>Notice</strong>: Starting early Monday December 7, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the operating system, the network, and the BeeGFS file system. We anticipate that the cluster will be available again by the end of Tuesday December 8, when load testing of the upgraded BeeGFS file system will start. If these tests fail, we will have to unroll the BeeGFS upgrade, which in case we anticipate that the cluster is back online by the end of Wednesday December 9. <br><span class="timestamp">November 23, 16:50 PT</span></p>
<!--
start: 2020-12-07T10:30:00
stop: 2020-12-08T16:30:00
length: 30 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="november-4-16-2020" class="level3">
<h3 class="anchored" data-anchor-id="november-4-16-2020">November 4-16, 2020</h3>
<section id="compute-nodes-not-serving-jobs-due-to-network-switch-failure" class="level4">
<h4 class="anchored" data-anchor-id="compute-nodes-not-serving-jobs-due-to-network-switch-failure"><span style="color: orange;">Compute nodes not serving jobs (due to network switch failure)</span></h4>
<p><strong>Resolved</strong>: All 74 compute nodes that were taken off the job scheduler on 2020-11-04 are back up and running <br><span class="timestamp">November 16, 12:00 PT</span></p>
<p><strong>Notice</strong>: 74 compute nodes, including several GPU nodes, were taken off the job scheduler around 14:00 on 2020-11-04 due to a faulty network switch. The network switch needs to be replaced in order to resolve this. <br><span class="timestamp">November 4, 16:10 PT</span></p>
<!--
start: 2020-11-04T16:50:00
stop: 2020-11-16T16:30:00
length: 360 hours
severity: partial-outage
affected: jobs, network, compute, ...
reason: external
 -->
</section>
</section>
<section id="november-5-2020" class="level3">
<h3 class="anchored" data-anchor-id="november-5-2020">November 5, 2020</h3>
<section id="cluster-inaccessible-due-to-beegfs-issues" class="level4">
<h4 class="anchored" data-anchor-id="cluster-inaccessible-due-to-beegfs-issues"><span style="color: orange;">Cluster inaccessible (due to BeeGFS issues)</span></h4>
<p><strong>Resolved</strong>: Our BeeGFS file system was non-responsive during 01:20-04:00 on 2020-11-05 because one of the meta servers hung. <br><span class="timestamp">November 5, 08:55 PT</span></p>
<!--
start: 2020-11-05T01:20:00
stop: 2020-11-05T04:00:00
length: 3 hours
severity: major-outage
affected: jobs, beegfs, ...
reason: internal
 -->
</section>
</section>
<section id="october-21-2020" class="level3">
<h3 class="anchored" data-anchor-id="october-21-2020">October 21, 2020</h3>
<section id="cluster-inaccessible-due-to-beegfs-issues-1" class="level4">
<h4 class="anchored" data-anchor-id="cluster-inaccessible-due-to-beegfs-issues-1"><span style="color: orange;">Cluster inaccessible (due to BeeGFS issues)</span></h4>
<p><strong>Resolved</strong>: Our BeeGFS file system was non-responsive because one of its meta servers hung, which now has been restarted. <br><span class="timestamp">October 21, 11:15 PT</span></p>
<p><strong>Notice</strong>: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 09:30 today. <br><span class="timestamp">October 21, 10:45 PT</span></p>
<!--
start: 2020-10-21T09:30:00
stop: 2020-10-21T11:15:00
length: 3.0 hours
severity: major-outage
affected: jobs, beegfs, ...
reason: internal
 -->
</section>
</section>
<section id="august-19-2020" class="level3">
<h3 class="anchored" data-anchor-id="august-19-2020">August 19, 2020</h3>
<section id="cluster-inaccessible-due-to-beegfs-issues-2" class="level4">
<h4 class="anchored" data-anchor-id="cluster-inaccessible-due-to-beegfs-issues-2"><span style="color: orange;">Cluster inaccessible (due to BeeGFS issues)</span></h4>
<p><strong>Resolved</strong>: Our BeeGFS file system was non-responsive between 17:22 and 18:52 today because one of its meta servers hung while the other attempted to synchronize to it. <br><span class="timestamp">August 19, 19:00 PT</span></p>
<p><strong>Notice</strong>: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 17:30 today. <br><span class="timestamp">August 19, 18:15 PT</span></p>
<!--
start: 2020-08-19T17:22:00
stop: 2020-08-19T18:52:00
length: 1.5 hours
severity: major-outage
affected: jobs, beegfs, ...
reason: internal
 -->
</section>
</section>
<section id="august-10-13-2020" class="level3">
<h3 class="anchored" data-anchor-id="august-10-13-2020">August 10-13, 2020</h3>
<section id="network-and-hardware-upgrades-full-downtime" class="level4">
<h4 class="anchored" data-anchor-id="network-and-hardware-upgrades-full-downtime"><span style="color: orange;">Network and hardware upgrades (full downtime)</span></h4>
<p><strong>Resolved</strong>: The cluster is fully back up and running. Several compute nodes still need to be rebooted but we consider this upgrade cycle completed. The network upgrade took longer than expected, which delayed the processes. We hope to bring the new lab storage online during the next week. <br><span class="timestamp">August 13, 21:00 PT</span></p>
<p><strong>Update</strong>: All login, data-transfer, and development nodes are online. Additional compute nodes are being upgraded and are soon part of the pool serving jobs. <br><span class="timestamp">August 13, 14:50 PT</span></p>
<p><strong>Update</strong>: Login node log1, data-transfer node dt2, and the development nodes are available again. Compute nodes are going through an upgrade cycle and will soon start serving jobs again. The upgrade work is taking longer than expected and will continue tomorrow Thursday August 13. <br><span class="timestamp">August 12, 16:10 PT</span></p>
<p><strong>Notice</strong>: All of the Wynton HPC environment is down for maintenance and upgrades. <br><span class="timestamp">August 10, 00:00 PT</span></p>
<p><strong>Notice</strong>: Starting early Monday August 10, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the network and adding lab storage purchased by several groups. We anticipate that the cluster will be available again by the end of Wednesday August 12. <br><span class="timestamp">July 24, 15:45 PT</span></p>
<!--
start: 2020-08-10T00:00:00
stop: 2020-08-13T21:00:00
length: 93 hours
severity: under-maintenance
affected: jobs, beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="july-6-2020" class="level3">
<h3 class="anchored" data-anchor-id="july-6-2020">July 6, 2020</h3>
<section id="development-node-failures" class="level4">
<h4 class="anchored" data-anchor-id="development-node-failures"><span style="color: orange;">Development node failures</span></h4>
<p><strong>Resolved</strong>: All three development nodes have been rebooted. <br><span class="timestamp">July 6, 15:20 PT</span></p>
<p><strong>Notice</strong>: The three regular development nodes have all gotten themselves hung up on one particular process. This affects basic system operations and preventing such basic commands as <code>ps</code> and <code>w</code>. To clear this state, we’ll be doing an emergency reboot of the dev nodes at about 15:15. <br><span class="timestamp">July 6, 15:05 PT</span></p>
</section>
</section>
<section id="july-5-2020" class="level3">
<h3 class="anchored" data-anchor-id="july-5-2020">July 5, 2020</h3>
<section id="job-scheduler-non-working" class="level4">
<h4 class="anchored" data-anchor-id="job-scheduler-non-working"><span style="color: orange;">Job scheduler non-working</span></h4>
<p><strong>Resolved</strong>: The SGE scheduler produced errors when queried or when jobs were submitted or launched. The problem started 00:30 and lasted until 02:45 early Sunday 2020-07-05. <br><span class="timestamp">July 6, 22:00 PT</span></p>
<!--
start: 2020-07-06T00:30:00
stop: 2020-07-06T02:45:00
length: 2.0 hours
severity: partial-outage
affected: jobs, scheduler
reason: internal
 -->
</section>
</section>
<section id="june-11-26-2020" class="level3">
<h3 class="anchored" data-anchor-id="june-11-26-2020">June 11-26, 2020</h3>
<section id="kernel-maintenance-8" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-8"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">June 26, 10:45 PT</span></p>
<p><strong>Update</strong>: Development node dev3 is back online. <br><span class="timestamp">June 15, 11:15 PT</span></p>
<p><strong>Update</strong>: Development node dev3 is not available. It failed to reboot and requires on-site attention, which might not be possible for several days. All other log-in, data-transfer, and development nodes were rebooted successfully. <br><span class="timestamp">June 11, 15:45 PT</span></p>
<p><strong>Notice</strong>: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Thursday June 11. <br><span class="timestamp">June 11, 10:45 PT</span></p>
</section>
</section>
<section id="june-5-9-2020" class="level3">
<h3 class="anchored" data-anchor-id="june-5-9-2020">June 5-9, 2020</h3>
<section id="no-internet-access-on-development-nodes-1" class="level4">
<h4 class="anchored" data-anchor-id="no-internet-access-on-development-nodes-1"><span style="color: orange;">No internet access on development nodes</span></h4>
<p><strong>Resolved</strong>: Internet access from the development nodes is available again. A new web-proxy server had to be built and deploy. <br><span class="timestamp">June 9, 09:15 PT</span></p>
<p><strong>Notice</strong>: Internet access from the development nodes is not available. This is because the proxy server providing them with internet access had a critical hardware failure around 08-09 this morning. At the most, we cannot provide an estimate when we get to restore this server. <br><span class="timestamp">June 5, 16:45 PT</span></p>
</section>
</section>
<section id="may-18-22-2020" class="level3">
<h3 class="anchored" data-anchor-id="may-18-22-2020">May 18-22, 2020</h3>
<section id="file-system-maintenance-1" class="level4">
<h4 class="anchored" data-anchor-id="file-system-maintenance-1"><span style="color: orange;">File-system maintenance</span></h4>
<p><strong>Update</strong>: The upgrade of the BeeGFS filesystem introduced new issues. We decided to rollback the upgrade and we are working with the vendor. There is no upgrade planned for the near term. <br><span class="timestamp">June 8, 09:00 PT</span></p>
<p><strong>Update</strong>: The BeeGFS filesystem has been upgraded using a patch from the vendor. The patch was designed to lower the amount of resynchronization needed between the two metadata servers. Unfortunately, after the upgrade we observe an increase of resynchronization. We will keep monitoring the status. If the problem remains, we will consider a rollback to the BeeGFS version used prior to May 18. <br><span class="timestamp">May 22, 01:25 PT</span></p>
<p><strong>Update</strong>: For a short moment around 01:00 early Friday, both of our BeeGFS metadata servers were down. This may have lead to some applications experiencing I/O errors around this time. <br><span class="timestamp">May 22, 01:25 PT</span></p>
<p><strong>Notice</strong>: Work to improve the stability of the BeeGFS filesystem (<code>/wynton</code>) will be conducted during the week of May 18-22. This involves restarting the eight pairs of metadata server processes, which may result in several brief stalls of the file system. Each should last less than 5 minutes and operations will continue normally after each one. <br><span class="timestamp">May 6, 15:10 PT</span></p>
<!--
start: 2020-05-22T01:00:00
stop: 2020-05-22T01:15:00
length: 0.5 hours
severity: partial-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="may-28-29-2020" class="level3">
<h3 class="anchored" data-anchor-id="may-28-29-2020">May 28-29, 2020</h3>
<section id="gpu-compute-nodes-outage" class="level4">
<h4 class="anchored" data-anchor-id="gpu-compute-nodes-outage"><span style="color: orange;">GPU compute nodes outage</span></h4>
<p><strong>Resolved</strong>: The GPU compute nodes are now fully available to serve jobs. <br><span class="timestamp">May 29, 12:00 PT</span></p>
<p><strong>Update</strong>: The GPU compute nodes that went down yesterday have been rebooted. <br><span class="timestamp">May 29, 11:10 PT</span></p>
<p><strong>Investigating</strong>: A large number of GPU compute nodes in the MSG data center are currently down for unknown reasons. We are investigating the cause. <br><span class="timestamp">May 28, 09:35 PT</span></p>
<!--
start: 2020-05-28T09:35:00
stop: 2020-05-29T12:00:00
length: 26.5 hours
severity: partial-outage
affected: jobs, compute-gpu
reason: internal
 -->
</section>
</section>
<section id="february-5-7-2020" class="level3">
<h3 class="anchored" data-anchor-id="february-5-7-2020">February 5-7, 2020</h3>
<section id="major-outage-due-to-netapp-file-system-failure" class="level4">
<h4 class="anchored" data-anchor-id="major-outage-due-to-netapp-file-system-failure"><span style="color: orange;">Major outage due to NetApp file-system failure</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC system is considered fully functional again. The legacy, deprecated NetApp storage was lost. <br><span class="timestamp">February 10, 10:55 PT</span></p>
<p><strong>Update</strong>: The majority of the compute nodes have been rebooted and are now online and running jobs. We will actively monitor the system and assess the how everything works before we considered this incident resolved. <br><span class="timestamp">February 7, 13:40 PT</span></p>
<p><strong>Update</strong>: The login, development and data transfer nodes will be rebooted at 01:00 today Friday February 7. <br><span class="timestamp">February 7, 12:00 PT</span></p>
<p><strong>Update</strong>: The failed legacy NetApp server is the cause to the problems, e.g.&nbsp;compute nodes not being responsive causing problems for SGE etc. Because of this, <strong>all of the cluster - login, development, transfer, and computes nodes - will be rebooted tomorrow Friday 2020-02-07</strong>. <br><span class="timestamp">February 6, 10:00 PT</span></p>
<p><strong>Notice</strong>: Wynton HPC is experience major issues due to NetApp file-system failure, despite this is being deprecated and not used much these days. The first user report on this came in around 09:00 and the job-queue logs suggests the problem began around 02:00. It will take a while for everything to come back up and there will be brief BeeGFS outage while we reboot the BeeGFS management node. <br><span class="timestamp">February 5, 10:15 PT</span></p>
<!--
start: 2020-02-05T10:15:00
stop: 2020-02-07T13:40:00
length: 51.5 hours
severity: major-outage
affected: jobs, compute, ...
reason: internal
 -->
</section>
</section>
<section id="january-29-2020" class="level3">
<h3 class="anchored" data-anchor-id="january-29-2020">January 29, 2020</h3>
<section id="beegfs-failure" class="level4">
<h4 class="anchored" data-anchor-id="beegfs-failure"><span style="color: orange;">BeeGFS failure</span></h4>
<p><strong>Resolved</strong>: The BeeGFS file-system issue has been resolved by rebooting two meta servers. <br><span class="timestamp">January 29, 17:00 PT</span></p>
<p><strong>Notice</strong>: There’s currently an issue with the BeeGFS file system. Users reporting that they cannot log in. <br><span class="timestamp">January 29, 16:00 PT</span></p>
<!--
start: 2020-01-29T16:00:00
stop: 2020-01-29T17:00:00
length: 1.0 hours
severity: major-outage
affected: jobs, beegfs, ...
reason: internal
 -->
</section>
</section>
<section id="january-22-2020" class="level3">
<h3 class="anchored" data-anchor-id="january-22-2020">January 22, 2020</h3>
<section id="file-system-maintenance-2" class="level4">
<h4 class="anchored" data-anchor-id="file-system-maintenance-2"><span style="color: orange;">File-system maintenance</span></h4>
<p><strong>Resolved</strong>: The BeeGFS upgrade issue has been resolved. <br><span class="timestamp">Jan 22, 14:30 PT</span></p>
<p><strong>Update</strong>: The planned upgrade caused unexpected problems to the BeeGFS file system resulting in <code>/wynton/group</code> becoming unstable. <br><span class="timestamp">Jan 22, 13:35 PT</span></p>
<p><strong>Notice</strong>: One of the BeeGFS servers, which serve our cluster-wide file system, will be swapped out starting at noon (11:59am) on Wednesday January 22, 2020 and the work is expected to last one hour. We don’t anticipate any downtime because the BeeGFS servers are mirrored for availability. <br><span class="timestamp">Jan 16, 14:40 PT</span></p>
<!--
start: 2020-01-22T12:00:00
stop: 2020-01-22T14:30:00
length: 2.5 hours
severity: under-maintenance
affected: jobs, beegfs, *
reason: scheduled
 -->
</section>
</section>
<section id="december-20-2019---january-4-2020" class="level3">
<h3 class="anchored" data-anchor-id="december-20-2019---january-4-2020">December 20, 2019 - January 4, 2020</h3>
<section id="kernel-maintenance-9" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-9"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been updated and rebooted. <br><span class="timestamp">Jan 4, 11:00 PT</span></p>
<p><strong>Notice</strong>: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. <br><span class="timestamp">December 20, 10:20 PT</span></p>
</section>
</section>
</section>
</div>
<div id="2019" class="tab-pane fade">
<section>
<section id="operational-summary-for-2019" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2019">Operational Summary for 2019</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 96 hours = 4.0 days = 1.1%</li>
<li>Unscheduled: 83.5 hours = 3.5 days = 1.0%</li>
<li>Total: 179.5 hours = 7.5 days = 2.0%</li>
<li>External factors: 15% of the above downtime, corresponding to 26 hours (=1.1 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-6" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-6">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2021-01-09 (1.0 hours) - job scheduler updates</li>
<li>2021-07-08 (95 hours)</li>
</ol></li>
<li>Total downtime: 96.0 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-6" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-6">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2019-01-22 (up to 14 days)</li>
<li>2019-03-21 (up to 14 days)</li>
<li>2019-10-29 (up to 14 days)</li>
<li>2019-12-22 (up to 14 days)</li>
</ol></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-6" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-6">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ol type="1">
<li>2019-07-30 (6.5 hour) - Byers Hall power outage</li>
<li>2019-08-15 (5.5 hour) - Diller power outage</li>
<li>2019-10-25 (1.0 hour) - Byers Hall power outage</li>
<li>2019-10-22 (13.0 hour) - Diller power backup failed during power maintenance</li>
</ol></li>
<li>Total downtime: 26.0 hours of which 26.0 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-6" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-6">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ol type="1">
<li>2019-01-08 (2.0 hours) - BeeGFS server non-responsive</li>
<li>2019-01-14 (1.5 hours) - BeeGFS non-responsive</li>
<li>2019-05-15 (24.5 hours) - BeeGFS non-responsive</li>
<li>2019-05-17 (5.0 hours) - BeeGFS slowdown</li>
<li>2019-06-17 (10.5 hours) - BeeGFS non-responsive</li>
<li>2019-08-23 (4.0 hours) - BeeGFS server non-responsive</li>
<li>2019-09-24 (3.0 hours) - BeeGFS server non-responsive</li>
<li>2019-12-18 (3.5 hours) - Network switch upgrade</li>
<li>2019-12-22 (5.5 hours) - BeeGFS server non-responsive</li>
</ol></li>
<li>Total downtime: 58.5 hours of which 0 hours were due to external factors</li>
</ul>
</section>
<section id="accounts-3" class="level4">
<h4 class="anchored" data-anchor-id="accounts-3">Accounts</h4>
<ul>
<li>Number of user account: 478 (change: +280 during the year)</li>
</ul>
</section>
</section>
<section id="december-20-2019---january-4-2020-1" class="level3">
<h3 class="anchored" data-anchor-id="december-20-2019---january-4-2020-1">December 20, 2019 - January 4, 2020</h3>
<section id="kernel-maintenance-10" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-10"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been updated and rebooted. <br><span class="timestamp">Jan 4, 11:00 PT</span></p>
<p><strong>Notice</strong>: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. <br><span class="timestamp">December 20, 10:20 PT</span></p>
</section>
</section>
<section id="december-22-2019" class="level3">
<h3 class="anchored" data-anchor-id="december-22-2019">December 22, 2019</h3>
<section id="beegfs-failure-1" class="level4">
<h4 class="anchored" data-anchor-id="beegfs-failure-1"><span style="color: orange;">BeeGFS failure</span></h4>
<p><strong>Resolved</strong>: No further hiccups were needed during the BeeGFS resynchronization. Everything is working as expected. <br><span class="timestamp">December 23, 10:00 PT</span></p>
<p><strong>Update</strong>: The issues with login was because the responsiveness of one of the BeeGFS file servers became unreliable around 04:20. Rebooting that server resolved the problem. The cluster is fully functional again although slower than usual until the file system have been resynced. After this, there might be a need for one more, brief, reboot. <br><span class="timestamp">December 22, 14:40 PT</span></p>
<p><strong>Notice</strong>: It is not possible to log in to the Wynton HPC environment. The reason is currently not known. <br><span class="timestamp">December 22, 09:15 PT</span></p>
<!--
start: 2019-12-22T09:15:00
stop: 2019-12-22T14:40:00
length: 5.5 hours
severity: major-outage
affected: jobs, beegfs, *
reason: internal
 -->
</section>
</section>
<section id="december-18-2019" class="level3">
<h3 class="anchored" data-anchor-id="december-18-2019">December 18, 2019</h3>
<section id="networklogin-issues" class="level4">
<h4 class="anchored" data-anchor-id="networklogin-issues"><span style="color: orange;">Network/login issues</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC environment is fully functional again. The BeeGFS filesystem was not working properly during 18:30-22:10 on December 18 resulting in no login access to the cluster and job file I/O being backed up. <br><span class="timestamp">December 19, 08:50 PT</span></p>
<p><strong>Update</strong>: The BeeGFS filesystem is non-responsive, which we believe is due to the network switch upgrade. <br><span class="timestamp">December 18, 21:00 PT</span></p>
<p><strong>Notice</strong>: One of two network switches will be upgraded on Wednesday December 18 starting at 18:00 and lasting a few hours. We do not expect this to impact the Wynton HPC environment other than slowing down the network performance to 50%. <br><span class="timestamp">December 17, 10:00 PT</span></p>
<!--
start: 2019-12-18T18:30:00
stop: 2019-12-19T22:10:00
length: 3.5 hours
severity: major-outage
affected: beegfs, *
reason: internal
 -->
</section>
</section>
<section id="october-29-november-11-2019" class="level3">
<h3 class="anchored" data-anchor-id="october-29-november-11-2019">October 29-November 11, 2019</h3>
<section id="kernel-maintenance-11" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-11"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been updated and rebooted. <br><span class="timestamp">Nov 11, 01:00 PT</span></p>
<p><strong>Notice</strong>: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). GPU nodes will be rebooted as soon as all GPU jobs complete. During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,000 cores) in the graph above. <br><span class="timestamp">Oct 29, 16:30 PT</span></p>
</section>
</section>
<section id="october-25-2019" class="level3">
<h3 class="anchored" data-anchor-id="october-25-2019">October 25, 2019</h3>
<section id="byers-hall-power-outage-glitch" class="level4">
<h4 class="anchored" data-anchor-id="byers-hall-power-outage-glitch"><span style="color: orange;">Byers Hall power outage glitch</span></h4>
<p><strong>Resolved</strong>: Development node <code>qb3-dev2</code> was rebooted. Data-transfer node <code>dt1.wynton.ucsf.edu</code> is kept offline because it is scheduled to be upgraded next week. <br><span class="timestamp">October 28, 15:00 PT</span></p>
<p><strong>Update</strong>: Most compute nodes that went down due to the power glitch has been rebooted. Data-transfer node <code>dt1.wynton.ucsf.edu</code> and development node <code>qb3-dev2</code> are still down - they will be brought back online on Monday October 28. <br><span class="timestamp">October 25, 14:00 PT</span></p>
<p><strong>Notice</strong>: A very brief power outage in the Byers Hall building caused several compute nodes in its Data Center to go down. Jobs that were running on those compute nodes at the time of the power failure did unfortunately fail. Log-in, data-transfer, and development nodes were also affected. All these hosts are currently being rebooted. <br><span class="timestamp">October 25, 13:00 PT</span></p>
<!--
start: 2019-10-25T12:00:00
stop: 2019-10-25T13:00:00
length: 1.0 hours
severity: partial-outage
affected: jobs, compute
reason: external
 -->
</section>
</section>
<section id="october-24-2019" class="level3">
<h3 class="anchored" data-anchor-id="october-24-2019">October 24, 2019</h3>
<section id="login-non-functional" class="level4">
<h4 class="anchored" data-anchor-id="login-non-functional"><span style="color: orange;">Login non-functional</span></h4>
<p><strong>Resolved</strong>: Log in works again. <br><span class="timestamp">October 24, 09:45 PT</span></p>
<p><strong>Notice</strong>: It is not possible to log in to the Wynton HPC environment. This is due to a recent misconfiguration of the LDAP server. <br><span class="timestamp">October 24, 09:30 PT</span></p>
</section>
</section>
<section id="october-22-23-2019" class="level3">
<h3 class="anchored" data-anchor-id="october-22-23-2019">October 22-23, 2019</h3>
<section id="beegfs-failure-2" class="level4">
<h4 class="anchored" data-anchor-id="beegfs-failure-2"><span style="color: orange;">BeeGFS failure</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC BeeGFS file system is fully functional again. During the outage, <code>/wynton/group</code> and <code>/wynton/scratch</code> was not working properly, whereas <code>/wynton/home</code> was unaffected. <br><span class="timestamp">October 23, 10:35 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC BeeGFS file system is non-functional. It is expected to be resolved by noon on October 23. The underlying problem is that the power backup at the Diller data center did not work as expected during a planned power maintenance. <br><span class="timestamp">October 22, 21:45 PT</span></p>
<!--
start: 2019-10-22T21:45:00
stop: 2019-10-23T10:35:00
length: 13.0 hours
severity: major-outage
affected: jobs, beegfs
reason: external
 -->
</section>
</section>
<section id="september-24-2019" class="level3">
<h3 class="anchored" data-anchor-id="september-24-2019">September 24, 2019</h3>
<section id="beegfs-failure-3" class="level4">
<h4 class="anchored" data-anchor-id="beegfs-failure-3"><span style="color: orange;">BeeGFS failure</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC environment is up and running again. <br><span class="timestamp">September 24, 20:25 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC environment is unresponsive. Problem is being investigated. <br><span class="timestamp">September 24, 17:30 PT</span></p>
<!--
start: 2019-09-24T17:30:00
stop: 2019-09-24T20:25:00
length: 3.0 hours
severity: major-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="august-23-2019" class="level3">
<h3 class="anchored" data-anchor-id="august-23-2019">August 23, 2019</h3>
<section id="beegfs-failure-4" class="level4">
<h4 class="anchored" data-anchor-id="beegfs-failure-4"><span style="color: orange;">BeeGFS failure</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC environment is up and running again. The reason for this downtime was the BeeGFS file server became unresponsive. <br><span class="timestamp">August 23, 20:45 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC environment is unresponsive. <br><span class="timestamp">August 23, 16:45 PT</span></p>
<!--
start: 2019-08-23T16:45:00
stop: 2019-08-23T20:45:00
length: 4.0 hours
severity: major-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="august-15-2019" class="level3">
<h3 class="anchored" data-anchor-id="august-15-2019">August 15, 2019</h3>
<section id="power-outage" class="level4">
<h4 class="anchored" data-anchor-id="power-outage"><span style="color: orange;">Power outage</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC environment is up and running again. <br><span class="timestamp">August 15, 21:00 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC environment is down due to a non-planned power outage at the Diller data center. Jobs running on compute nodes located in that data center, were terminated. Jobs running elsewhere may also have been affected because <code>/wynton/home</code> went down as well (despite it being mirrored). <br><span class="timestamp">August 15, 15:45 PT</span></p>
<!--
start: 2019-08-15T15:45:00
stop: 2019-08-15T21:00:00
length: 5.5 hours
severity: major-outage
affected: jobs, beegfs
reason: external
 -->
</section>
</section>
<section id="july-30-2019" class="level3">
<h3 class="anchored" data-anchor-id="july-30-2019">July 30, 2019</h3>
<section id="power-outage-1" class="level4">
<h4 class="anchored" data-anchor-id="power-outage-1"><span style="color: orange;">Power outage</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC environment is up and running again. <br><span class="timestamp">July 30, 14:40 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC environment is down due to a non-planned power outage at the main data center. <br><span class="timestamp">July 30, 08:20 PT</span></p>
<!--
start: 2019-07-30T08:20:00
stop: 2019-07-30T14:40:00
length: 6.5 hours
severity: major-outage
affected: jobs, beegfs
reason: external
 -->
</section>
</section>
<section id="july-8-12-2019" class="level3">
<h3 class="anchored" data-anchor-id="july-8-12-2019">July 8-12, 2019</h3>
<section id="full-system-downtime" class="level4">
<h4 class="anchored" data-anchor-id="full-system-downtime"><span style="color: orange;">Full system downtime</span></h4>
<p><strong>Resolved</strong>: The Wynton HPC environment and the BeeGFS file system are fully functional after updates and upgrades. <br><span class="timestamp">July 12, 11:15 PT</span></p>
<p><strong>Notice</strong>: The Wynton HPC environment is down for maintenance. <br><span class="timestamp">July 8, 12:00 PT</span></p>
<p><strong>Notice</strong>: Updates to the BeeGFS file system and the operating system that require to bring down all of Wynton HPC will start on the morning of Monday July 8. Please make sure to log out before then. The downtime might last the full week. <br><span class="timestamp">July 1, 14:15 PT</span></p>
<!--
start: 2019-07-08T12:00:00
stop: 2019-07-12T11:15:00
length: 95 hours
severity: under-maintenance
affected: beegfs, compute, *
reason: scheduled
 -->
</section>
</section>
<section id="june-17-18-2019" class="level3">
<h3 class="anchored" data-anchor-id="june-17-18-2019">June 17-18, 2019</h3>
<section id="significant-file-system-outage" class="level4">
<h4 class="anchored" data-anchor-id="significant-file-system-outage"><span style="color: orange;">Significant file-system outage</span></h4>
<p><strong>Resolved</strong>: The BeeGFS file system is fully functional again. <br><span class="timestamp">June 18, 01:30 PT</span></p>
<p><strong>Investigating</strong>: Parts of <code>/wynton/scratch</code> and <code>/wynton/group</code> are currently unavailable. The <code>/wynton/home</code> space should be unaffected. <br><span class="timestamp">June 17, 15:05 PT</span></p>
<!--
start: 2019-06-17T15:05:00
stop: 2019-06-18T01:30:00
length: 10.5 hours
severity: partial-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="may-17-2019" class="level3">
<h3 class="anchored" data-anchor-id="may-17-2019">May 17, 2019</h3>
<section id="major-outage-due-to-file-system-issues" class="level4">
<h4 class="anchored" data-anchor-id="major-outage-due-to-file-system-issues"><span style="color: orange;">Major outage due to file-system issues</span></h4>
<p><strong>Resolved</strong>: The BeeGFS file system and the cluster is functional again. <br><span class="timestamp">May 17, 16:00 PT</span></p>
<p><strong>Investigating</strong>: There is a major slowdown of the BeeGFS file system (<code>/wynton</code>), which in turn causes significant problems throughout the Wynton HPC environment. <br><span class="timestamp">May 17, 10:45 PT</span></p>
<!--
start: 2019-05-17T10:45:00
stop: 2019-05-17T16:00:00
length: 5.0 hours
severity: major-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="may-15-16-2019" class="level3">
<h3 class="anchored" data-anchor-id="may-15-16-2019">May 15-16, 2019</h3>
<section id="major-outage-due-to-file-system-issues-1" class="level4">
<h4 class="anchored" data-anchor-id="major-outage-due-to-file-system-issues-1"><span style="color: orange;">Major outage due to file-system issues</span></h4>
<p><strong>Resolved</strong>: The BeeGFS file system, and thereby also the cluster itself, is functional again. <br><span class="timestamp">May 16, 10:30 PT</span></p>
<p><strong>Investigating</strong>: The BeeGFS file system (<code>/wynton</code>) is experiencing major issues. This caused all on Wynton HPC to become non-functional. <br><span class="timestamp">May 15, 10:00 PT</span></p>
<!--
start: 2019-05-15T10:00:00
stop: 2019-05-16T10:30:00
length: 24.5 hours
severity: major-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="may-15-2019" class="level3">
<h3 class="anchored" data-anchor-id="may-15-2019">May 15, 2019</h3>
<section id="networklogin-issues-1" class="level4">
<h4 class="anchored" data-anchor-id="networklogin-issues-1"><span style="color: orange;">Network/login issues</span></h4>
<p><strong>Resolved</strong>: The UCSF-wide network issue that affected access to Wynton HPC has been resolved. <br><span class="timestamp">May 15, 15:30 PT</span></p>
<p><strong>Update</strong>: The login issue is related to UCSF-wide network issues. <br><span class="timestamp">May 15, 13:30 PT</span></p>
<p><strong>Investigating</strong>: There are issues logging in to Wynton HPC. <br><span class="timestamp">May 15, 10:15 PT</span></p>
</section>
</section>
<section id="march-21-april-5-2019" class="level3">
<h3 class="anchored" data-anchor-id="march-21-april-5-2019">March 21-April 5, 2019</h3>
<section id="kernel-maintenance-12" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-12"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">April 5, 12:00 PT</span></p>
<p><strong>Update</strong>: Nearly all compute nodes have been rebooted (~5,200 cores are now available). <br><span class="timestamp">Mar 29, 12:00 PT</span></p>
<p><strong>Notice</strong>: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 5,424 cores) in the graph above. <br><span class="timestamp">Mar 21, 15:30 PT</span></p>
</section>
</section>
<section id="march-22-2019" class="level3">
<h3 class="anchored" data-anchor-id="march-22-2019">March 22, 2019</h3>
<section id="kernel-maintenance-13" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-13"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: The login, development and transfer hosts have been rebooted. <br><span class="timestamp">March 22, 10:35 PT</span></p>
<p><strong>Notice</strong>: On Friday March 22 at 10:30am, all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. These hosts should be offline for less than 5 minutes. <br><span class="timestamp">Mar 21, 15:30 PT</span></p>
</section>
</section>
<section id="january-22-february-5-2019" class="level3">
<h3 class="anchored" data-anchor-id="january-22-february-5-2019">January 22-February 5, 2019</h3>
<section id="kernel-maintenance-14" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-14"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: All compute nodes have been rebooted. <br><span class="timestamp">Feb 5, 11:30 PT</span></p>
<p><strong>Notice</strong>: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 1,944 cores) in the graph above. <br><span class="timestamp">Jan 22, 16:45 PT</span></p>
</section>
</section>
<section id="january-23-2019" class="level3">
<h3 class="anchored" data-anchor-id="january-23-2019">January 23, 2019</h3>
<section id="kernel-maintenance-15" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-15"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: The login, development and transfer hosts have been rebooted. <br><span class="timestamp">Jan 23, 13:00 PT</span></p>
<p><strong>Notice</strong>: On Wednesday January 23 at 12:00 (noon), all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. The hosts should be offline for less than 5 minutes. <br><span class="timestamp">Jan 22, 16:45 PT</span></p>
</section>
</section>
<section id="january-14-2019" class="level3">
<h3 class="anchored" data-anchor-id="january-14-2019">January 14, 2019</h3>
<section id="blocking-file-system-issues" class="level4">
<h4 class="anchored" data-anchor-id="blocking-file-system-issues"><span style="color: orange;">Blocking file-system issues</span></h4>
<p><strong>Resolved</strong>: The file system under <code>/wynton/</code> is back up again. We are looking into the cause and taking steps to prevent this from happening again. <br><span class="timestamp">Jan 9, 12:45 PT</span></p>
<p><strong>Investigating</strong>: The file system under <code>/wynton/</code> went down around 11:30 resulting is several critical failures including the scheduler failing. <br><span class="timestamp">Jan 14, 11:55 PT</span></p>
<!--
start: 2019-01-14T11:30:00
stop: 2019-01-14T12:45:00
length: 1.5 hours
severity: major-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
<section id="january-9-2019" class="level3">
<h3 class="anchored" data-anchor-id="january-9-2019">January 9, 2019</h3>
<section id="job-scheduler-maintenance-downtime" class="level4">
<h4 class="anchored" data-anchor-id="job-scheduler-maintenance-downtime"><span style="color: orange;">Job scheduler maintenance downtime</span></h4>
<p><strong>Resolved</strong>: The SGE job scheduler is now back online and accepts new job submission again. <br><span class="timestamp">Jan 9, 12:45 PT</span></p>
<p><strong>Update</strong>: The downtime of the job scheduler will begin on Wednesday January 9 @ noon and is expected to be completed by 1:00pm. <br><span class="timestamp">Jan 8, 16:00 PT</span></p>
<p><strong>Notice</strong>: There will be a short job-scheduler downtime on Wednesday January 9 due to SGE maintenance. During this downtime, already running jobs will keep running and queued jobs will remain in the queue, but no new jobs can be submitted. <br><span class="timestamp">Dec 20, 12:00 PT</span></p>
<!--
start: 2019-01-09T12:00:00
stop: 2019-07-12T12:45:00
length: 1.0 hours
severity: under-maintenance
affected: n/a
reason: scheduled
 -->
</section>
</section>
<section id="january-8-2019" class="level3">
<h3 class="anchored" data-anchor-id="january-8-2019">January 8, 2019</h3>
<section id="file-system-server-crash" class="level4">
<h4 class="anchored" data-anchor-id="file-system-server-crash"><span style="color: orange;">File-system server crash</span></h4>
<p><strong>Investigating</strong>: One of the parallel file-system servers (BeeGFS) appears to have crashed on Monday January 7 at 07:30 and was recovered on 9:20pm. Right now we are monitoring its stability, and investigating the cause and what impact it might have had. Currently, we believe users might have experienced I/O errors on <code>/wynton/scratch/</code> whereas <code>/wynton/home/</code> was not affected. <br><span class="timestamp">Jan 8, 10:15 PT</span></p>
<!--
start: 2019-01-08T07:30:00
stop: 2019-01-08T09:20:00
length: 2.0 hours
severity: partial-outage
affected: jobs, beegfs
reason: internal
 -->
</section>
</section>
</section>
</div>
<div id="2018" class="tab-pane fade">
<section>
<section id="operational-summary-for-2018-q3-q4" class="level3">
<h3 class="anchored" data-anchor-id="operational-summary-for-2018-q3-q4">Operational Summary for 2018 Q3-Q4</h3>
<ul>
<li><p>Full downtime:</p>
<ul>
<li>Scheduled: 0 hours = 0.0%</li>
<li>Unscheduled: 84 hours = 3.5 days = 1.9%</li>
<li>Total: 84 hours = 3.5 days = 1.9%</li>
<li>External factors: 100% of the above downtime, corresponding to 84 hours (=3.5 days), were due to external factors</li>
</ul></li>
</ul>
<section id="scheduled-maintenance-downtimes-7" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-maintenance-downtimes-7">Scheduled maintenance downtimes</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: None</li>
<li>Occurrences:
<ul>
<li>None</li>
</ul></li>
<li>Total downtime: 0.0 hours</li>
</ul>
</section>
<section id="scheduled-kernel-maintenance-7" class="level4">
<h4 class="anchored" data-anchor-id="scheduled-kernel-maintenance-7">Scheduled kernel maintenance</h4>
<ul>
<li>Impact: Fewer compute nodes than usual until rebooted</li>
<li>Damage: None</li>
<li>Occurrences:
<ol type="1">
<li>2018-09-28 (up to 14 days)</li>
</ol></li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-power-outage-7" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-power-outage-7">Unscheduled downtimes due to power outage</h4>
<ul>
<li>Impact: No file access, no compute resources available</li>
<li>Damage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions</li>
<li>Occurrences:
<ol type="1">
<li>2018-06-17 (23 hours) - Campus power outage</li>
<li>2018-11-08 (19 hours) - Byers Hall power maintenance without notice</li>
<li>2018-12-14 (42 hours) - Sandler Building power outage</li>
</ol></li>
<li>Total downtime: 84 hours of which 84 hours were due to external factors</li>
</ul>
</section>
<section id="unscheduled-downtimes-due-to-file-system-failures-7" class="level4">
<h4 class="anchored" data-anchor-id="unscheduled-downtimes-due-to-file-system-failures-7">Unscheduled downtimes due to file-system failures</h4>
<ul>
<li>Impact: No file access</li>
<li>Damage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible</li>
<li>Occurrences:
<ul>
<li>None.</li>
</ul></li>
<li>Total downtime: 0.0 hours</li>
</ul>
</section>
<section id="accounts-4" class="level4">
<h4 class="anchored" data-anchor-id="accounts-4">Accounts</h4>
<ul>
<li>Number of user account: 198 (change: +103 during the year)</li>
</ul>
</section>
</section>
<section id="december-21-2018" class="level3">
<h3 class="anchored" data-anchor-id="december-21-2018">December 21, 2018</h3>
<section id="partial-file-system-failure" class="level4">
<h4 class="anchored" data-anchor-id="partial-file-system-failure"><span style="color: orange;">Partial file system failure</span></h4>
<p><strong>Resolved</strong>: Parts of the new BeeGFS file system was non-functional for approx. 1.5 hours during Friday December 21 when a brief maintenance task failed. <br><span class="timestamp">Dec 21, 20:50 PT</span></p>
<!--
start: 2018-12-21T12:00:00
stop: 2018-12-21T13:30:00
length: 1.5 hours
severity: major-outage
affected: jobs, beegfs, *
reason: internal
 -->
</section>
</section>
<section id="december-12-20-2018" class="level3">
<h3 class="anchored" data-anchor-id="december-12-20-2018">December 12-20, 2018</h3>
<section id="nodes-down" class="level4">
<h4 class="anchored" data-anchor-id="nodes-down"><span style="color: orange;">Nodes down</span></h4>
<p><strong>Resolved</strong>: All of the `msg-* compute nodes but one are operational. <br><span class="timestamp">Dec 20, 16:40 PT</span></p>
<p><strong>Notice</strong>: Starting Wednesday December 12 around 11:00, several <code>msg-*</code> compute nodes went down (~200 cores in total). The cause of this is unknown. Because it might be related to the BeeGFS migration project, the troubleshooting of this incident will most likely not start until the BeeGFS project is completed, which is projected to be done on Wednesday December 19. <br><span class="timestamp">Dec 17, 17:00 PT</span></p>
<!--
start: 2018-12-20T16:40:00
stop: 2018-12-17T17:00:00
length: 72 hours
severity: partial-outage
affected: jobs, compute, *
reason: internal
 -->
</section>
</section>
<section id="december-18-2018" class="level3">
<h3 class="anchored" data-anchor-id="december-18-2018">December 18, 2018</h3>
<section id="development-node-does-not-respond" class="level4">
<h4 class="anchored" data-anchor-id="development-node-does-not-respond"><span style="color: orange;">Development node does not respond</span></h4>
<p><strong>Resolved</strong>: Development node <code>qb3-dev1</code> is functional. <br><span class="timestamp">Dec 18, 20:50 PT</span></p>
<p><strong>Investigating</strong>: Development node <code>qb3-dev1</code> does not respond to SSH. This will be investigated the first thing tomorrow morning (Wednesday December 19). In the meanwhile, development node <code>qb3-gpudev1</code>, which is “under construction”, may be used. <br><span class="timestamp">Dec 18, 16:30 PT</span></p>
</section>
</section>
<section id="november-28-december-19-2018" class="level3">
<h3 class="anchored" data-anchor-id="november-28-december-19-2018">November 28-December 19, 2018</h3>
<section id="installation-of-new-larger-and-faster-storage-space" class="level4">
<h4 class="anchored" data-anchor-id="installation-of-new-larger-and-faster-storage-space"><span style="color: orange;">Installation of new, larger, and faster storage space</span></h4>
<p><strong>Resolved</strong>: <code>/wynton/scratch</code> is now back online and ready to be used. <br><span class="timestamp">Dec 19, 14:20 PT</span></p>
<p><strong>Update</strong>: The plan is to bring <code>/wynton/scratch</code> back online before the end of the day tomorrow (Wednesday December 19). The planned SGE downtime has been rescheduled to Wednesday January 9. Moreover, we will start providing the new 500-GiB <code>/wynton/home/</code> storage to users who explicitly request it (before Friday December 21) and who also promise to <strong>move</strong> the content under their current <code>/netapp/home/</code> to the new location. Sorry, users on both QB3 and Wynton HPC will <em>not</em> be able to migrate until the QB3 cluster has been incorporated into Wynton HPC (see <a href="../../hpc/about/roadmap.html">Roadmap</a>) or they giving up their QB3 account. <br><span class="timestamp">Dec 18, 16:45 PT</span></p>
<p><strong>Update</strong>: The installation and migration to the new BeeGFS parallel file servers is on track and we expect to go live as planned on Wednesday December 19. We are working on fine tuning the configuration, running performance tests, and resilience tests. <br><span class="timestamp">Dec 17, 10:15 PT</span></p>
<p><strong>Update</strong>: <code>/wynton/scratch</code> has been taken offline. <br><span class="timestamp">Dec 12, 10:20 PT</span></p>
<p><strong>Reminder</strong>: All of <code>/wynton/scratch</code> will be taken offline and completely wiped starting Wednesday December 12 at 8:00am. <br><span class="timestamp">Dec 11, 14:45 PT</span></p>
<p><strong>Notice</strong>: On Wednesday December 12, 2018, the global scratch space <code>/wynton/scratch</code> will be taken offline and completely erased. Over the week following this, we will be adding to and reconfiguring the storage system in order to provide all users with new, larger, and faster (home) storage space. The new storage will served using BeeGFS, which is a new much faster file system - a system we have been prototyping and tested via <code>/wynton/scratch</code>. Once migrated to the new storage, a user’s home directory quota will be increased from 200 GiB to 500 GiB. In order to do this, the following upgrade schedule is planned:</p>
<ul>
<li><p>Wednesday November 28-December 19 (21 days): <strong>To all users, please refrain from using <code>/wynton/scratch</code> - use local, node-specific <code>/scratch</code> if possible (see below). The sooner we can take it down, the higher the chance is that we can get everything in place before December 19.</strong></p></li>
<li><p>Wednesday December 12-19 (8 days): <strong><code>/wynton/scratch</code> will be unavailable and completely wiped</strong>. For computational scratch space, please <a href="../../hpc/scheduler/using-local-scratch.html">use local <code>/scratch</code> unique to each compute node</a>. For <em>global</em> scratch needs, the old and much slower <code>/scrapp</code> and <code>/scrapp2</code> may also be used.</p></li>
<li><p><del>Wednesday December 19, 2018 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline. No jobs will be able to be submitted until it is restarted.</del></p></li>
<li><p>Wednesday December 19, 2018: The upgraded Wynton HPC with the new storage will be available including <code>/wynton/scratch</code>.</p></li>
<li><p>Wednesday January 9, 2019 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline temporarily. No jobs will be able to be submitted until it is restarted.</p></li>
</ul>
<p>It is our hope to be able to keep the user’s home accounts, login nodes, the transfer nodes, and the development nodes available throughout this upgrade period.</p>
<p><em>NOTE: If our new setup proves more challenging than anticipated, then we will postpone the SGE downtime to after the holidays, on Wednesday January 9, 2019. Wynton HPC will remain operational over the holidays, though without <code>/wynton/scratch</code>.</em> <br><span class="timestamp">Dec 6, 14:30 PT [edited Dec 18, 17:15 PT]</span></p>
<!--
start: 2018-12-12T10:20:00
stop: 2018-12-19T14:20:00
length: 172 hours
severity: under-maintenance
affected: beegfs-scratch, *
reason: scheduled
 -->
</section>
</section>
<section id="december-12-14-2018" class="level3">
<h3 class="anchored" data-anchor-id="december-12-14-2018">December 12-14, 2018</h3>
<section id="power-failure" class="level4">
<h4 class="anchored" data-anchor-id="power-failure"><span style="color: orange;">Power failure</span></h4>
<p><strong>Resolved</strong>: All <code>mac-*</code> compute nodes are up and functional. <br><span class="timestamp">Dec 14, 12:00 PT</span></p>
<p><strong>Investigating</strong>: The compute nodes named <code>mac-*</code> (in the Sandler building) went down due to power failure on Wednesday December 12 starting around 05:50. Nodes are being rebooted. <br><span class="timestamp">Dec 12, 09:05 PT</span></p>
<!--
start: 2018-12-12T05:50:00
stop: 2018-12-14T12:00:00
length: 42 hours
severity: partial-outage
affected: jobs, compute
reason: external
 -->
</section>
</section>
<section id="november-8-2018" class="level3">
<h3 class="anchored" data-anchor-id="november-8-2018">November 8, 2018</h3>
<section id="partial-shutdown-due-to-planned-power-outage" class="level4">
<h4 class="anchored" data-anchor-id="partial-shutdown-due-to-planned-power-outage"><span style="color: orange;">Partial shutdown due to planned power outage</span></h4>
<p><strong>Resolved</strong>: The cluster is full functional. It turns out that none of the compute nodes, and therefore none of the running jobs, were affected by the power outage. <br><span class="timestamp">Nov 8, 11:00 PT</span></p>
<p><strong>Update</strong>: The queue-metric graphs are being updated again. <br><span class="timestamp">Nov 8, 11:00 PT</span></p>
<p><strong>Update</strong>: The login nodes, the development nodes and the data transfer node are now functional. <br><span class="timestamp">Nov 8, 10:10 PT</span></p>
<p><strong>Update</strong>: Login node <code>wynlog1</code> is also affected by the power outage. Use <code>wynlog2</code> instead. <br><span class="timestamp">Nov 8, 09:10 PT</span></p>
<p><strong>Notice</strong>: Parts of the Wynton HPC cluster will be shut down on November 8 at 4:00am. This shutdown takes place due to the UCSF Facilities shutting down power in the Byers Hall. Jobs running on affected compute nodes will be terminated abruptly. Compute nodes with battery backup or in other buildings will not be affected. Nodes will be rebooted as soon as the power comes back. <del>To follow the reboot progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above.</del> Unfortunately, the above queue-metric graphs cannot be updated during the power outage. <br><span class="timestamp">Nov 7, 15:45 PT</span></p>
<!--
start: 2018-11-08T04:00:00
stop: 2018-11-08T23:00:00
length: 19 hours
severity: under-maintenance, partial-outage
affected: jobs, compute, *
reason: scheduled, external
 -->
</section>
</section>
<section id="september-28---october-11-2018" class="level3">
<h3 class="anchored" data-anchor-id="september-28---october-11-2018">September 28 - October 11, 2018</h3>
<section id="kernel-maintenance-16" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-16"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: The compute nodes has been rebooted and are accepting new jobs. For the record, on day 5 approx. 300 cores were back online, on day 7 approx. 600 cores were back online, on day 8 approx. 1,500 cores were back online, and on day 9 the majority of the 1,832 cores were back online. <br><span class="timestamp">Oct 11, 09:00 PT</span></p>
<p><strong>Notice</strong>: On September 28, a kernel update was applied to all compute nodes. To begin running the new kernel, each node must be rebooted. To achieve this as quickly as possible and without any loss of running jobs, the queues on the nodes were all disabled (i.e., they stopped accepting new jobs). Each node will reboot itself and re-enable its own queues as soon as all of its running jobs have completed. Since the maximum allowed run time for a job is two weeks, it may take until October 11 before all nodes have been rebooted and accepting new jobs. In the meanwhile, there will be fewer available slots on the queue than usual. To follow the progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. <br><span class="timestamp">Sept 28, 16:30 PT</span></p>
</section>
</section>
<section id="october-1-2018" class="level3">
<h3 class="anchored" data-anchor-id="october-1-2018">October 1, 2018</h3>
<section id="kernel-maintenance-17" class="level4">
<h4 class="anchored" data-anchor-id="kernel-maintenance-17"><span style="color: orange;">Kernel maintenance</span></h4>
<p><strong>Resolved</strong>: The login, development, and data transfer hosts have been rebooted. <br><span class="timestamp">Oct 1, 13:30 PT</span></p>
<p><strong>Notice</strong>: On Monday October 1 at 01:00, all of the login, development, and data transfer hosts will be rebooted. <br><span class="timestamp">Sept 28, 16:30 PT</span></p>
</section>
</section>
<section id="september-13-2018" class="level3">
<h3 class="anchored" data-anchor-id="september-13-2018">September 13, 2018</h3>
<section id="scheduler-unreachable" class="level4">
<h4 class="anchored" data-anchor-id="scheduler-unreachable"><span style="color: orange;">Scheduler unreachable</span></h4>
<p><strong>Resolved</strong>: Around 11:00 on Wednesday September 12, the SGE scheduler (“qmaster”) became unreachable such that the scheduler could not be queried and no new jobs could be submitted. Jobs that relied on run-time access to the scheduler may have failed. The problem, which was due to a misconfiguration being introduced, was resolved early morning on Thursday September 13. <br><span class="timestamp">Sept 13, 09:50 PT</span></p>
<!--
start: 2018-09-12T11:00:00
stop: 2018-09-13T08:00:00
length: 21 hours
severity: partial-outage
affected: jobs, scheduler, compute, *
reason: internal
 -->
</section>
</section>
<section id="august-1-2018" class="level3">
<h3 class="anchored" data-anchor-id="august-1-2018">August 1, 2018</h3>
<section id="partial-shutdown" class="level4">
<h4 class="anchored" data-anchor-id="partial-shutdown"><span style="color: orange;">Partial shutdown</span></h4>
<p><strong>Resolved</strong>: Nodes were rebooted on August 1 shortly after the power came back. <br><span class="timestamp">Aug 2, 08:15 PT</span></p>
<p><strong>Notice</strong>: On Wednesday August 1 at 6:45am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. <br><span class="timestamp">Jul 30, 20:45 PT</span></p>
<!--
start: 2018-08-01T07:15:00
stop: 2018-08-01T06:45:00
length: 0.5 hours
severity: scheduled, partial-outage
affected: jobs, scheduler, compute, *
reason: external
 -->
</section>
</section>
<section id="july-30-2018" class="level3">
<h3 class="anchored" data-anchor-id="july-30-2018">July 30, 2018</h3>
<section id="partial-shutdown-1" class="level4">
<h4 class="anchored" data-anchor-id="partial-shutdown-1"><span style="color: orange;">Partial shutdown</span></h4>
<p><strong>Resolved</strong>: The nodes brought down during the July 30 partial shutdown has been rebooted. Unfortunately, the same partial shutdown has to be repeated within a few days because the work in server room was not completed. Exact date for the next shutdown is not known at this point. <br><span class="timestamp">Jul 30, 09:55 PT</span></p>
<p><strong>Notice</strong>: On Monday July 30 at 7:00am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. <br><span class="timestamp">Jul 29, 21:20 PT</span></p>
</section>
</section>
<section id="june-16-26-2018" class="level3">
<h3 class="anchored" data-anchor-id="june-16-26-2018">June 16-26, 2018</h3>
<section id="power-outage-2" class="level4">
<h4 class="anchored" data-anchor-id="power-outage-2"><span style="color: orange;">Power outage</span></h4>
<p><strong>Resolved</strong>: The Nvidia-driver issue occurring on some of the GPU compute nodes has been fixed. <br><span class="timestamp">Jun 26, 11:55 PT</span></p>
<p><strong>Update</strong>: Some of the compute nodes with GPUs are still down due to issues with the Nvidia drivers. <br><span class="timestamp">Jun 19, 13:50 PT</span></p>
<p><strong>Update</strong>: The login nodes and and the development nodes are functional. Some compute nodes that went down are back up, but not all. <br><span class="timestamp">Jun 18, 10:45 PT</span></p>
<p><strong>Investigating</strong>: The UCSF Mission Bay Campus experienced a power outage on Saturday June 16 causing parts of Wynton HPC to go down. One of the login nodes (wynlog1), the development node (qb3-dev1), and parts of the compute nodes are currently non-functional. <br><span class="timestamp">Jun 17, 15:00 PT</span></p>
<!--
start: 2018-06-17T15:00:00
stop: 2018-06-19T13:50:00
length: 23 hours
severity: partial-outage
affected: jobs, scheduler, compute, *
reason: external
 -->
</section>
</section>
</section>
</div>
</div>
<!-- DO NOT EDIT ANYTHING BELOW -->
<!-------------------------------------------------------------------------
   Heartbeat
 -------------------------------------------------------------------------->
<p><span id="last-heartbeat">loading…</span> <button id="reload-heartbeat">Reload</button></p>
<script>
(async function initHeartbeatFromTSV() {
  const urls = [
    "https://raw.githubusercontent.com/UCSF-HPC/wynton-slash2/master/wynton-bench/wynton-bench_dev1.wynton.ucsf.edu__wynton_scratch_hb.tsv",
    "https://raw.githubusercontent.com/UCSF-HPC/wynton-slash2/master/wynton-bench/wynton-bench_dev2.wynton.ucsf.edu__wynton_scratch_hb.tsv",
    "https://raw.githubusercontent.com/UCSF-HPC/wynton-slash2/master/wynton-bench/wynton-bench_dev3.wynton.ucsf.edu__wynton_scratch_hb.tsv",
  ];

  const el = document.getElementById("last-heartbeat");
  const reloadBtn = document.getElementById("reload-heartbeat");
  if (!el) return;

  const MAX_BUFFER = 32 * 1024; // keep only the last ~32KB per file

  function pad(n) { return String(n).padStart(2, "0"); }
  function toLocalIsoSeconds(d) {
    const t = new Date(Math.floor(d.getTime() / 1000) * 1000);
    const yyyy = t.getFullYear();
    const mm = pad(t.getMonth() + 1);
    const dd = pad(t.getDate());
    const hh = pad(t.getHours());
    const mi = pad(t.getMinutes());
    const ss = pad(t.getSeconds());
    const tzOffMin = -t.getTimezoneOffset();
    const sign = tzOffMin >= 0 ? "+" : "-";
    const offH = pad(Math.floor(Math.abs(tzOffMin) / 60));
    const offM = pad(Math.abs(tzOffMin) % 60);
    return `${yyyy}-${mm}-${dd}T${hh}:${mi}:${ss}${sign}${offH}:${offM}`;
  }

  async function readLastLine(url) {
    const res = await fetch(url + "?t=" + Date.now(), { cache: "no-store" });
    if (!res.ok || !res.body) throw new Error(`HTTP ${res.status} for ${url}`);
    const reader = res.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let buf = "";

    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      buf += decoder.decode(value, { stream: true });
      if (buf.length > MAX_BUFFER) {
        // Keep only tail; try not to cut inside a line
        const cutFrom = buf.length - MAX_BUFFER;
        const nl = buf.indexOf("\n", cutFrom);
        buf = nl >= 0 ? buf.slice(nl + 1) : buf.slice(cutFrom);
      }
    }
    buf += decoder.decode();

    const lines = buf.split(/\r?\n/).filter(line => line.trim().length > 0);
    if (lines.length === 0) throw new Error("No non-empty lines");
    const lastLine = lines[lines.length - 1];

    // TSV preferred; fallback to CSV if needed
    const delim = lastLine.includes("\t") ? "\t" : ",";
    const [tsRaw, durRaw] = lastLine.split(delim);

    const ts = (tsRaw || "").replace(/^"|"$/g, "");
    const start = new Date(ts);
    const durSec = Number((durRaw || "").trim());
    const validStart = !Number.isNaN(start.getTime());
    const validDur = !Number.isNaN(durSec);

    const end = validStart ? new Date(start.getTime() + (validDur ? durSec * 1000 : 0)) : null;

    return { url, start: validStart ? start : null, end, durationSec: validDur ? durSec : null };
  }

  // Hmm ... v.start appears to be the actual "end" timestamp, not v.end.
  // This was discovered when the latency was >200x. /HB 2025-08-26
  async function updateHeartbeat() {
    el.textContent = "loading…";
    try {    
      const results = await Promise.allSettled(urls.map(readLastLine));
      const ok = results.filter(r => r.status === "fulfilled").map(r => r.value).filter(v => v.start);
      if (ok.length === 0) {
        el.textContent = "—";
        el.title = "No valid (timestamp+duration) found";
        return;
      }
      // Pick newest end time (timestamp + duration)
      const newest = ok.reduce((a, b) => (a.start > b.start ? a : b));

      const localIso = toLocalIsoSeconds(newest.start);

      // Age as "XmYYs ago"
      const diff_ms = Date.now() - newest.start.getTime();
      const diff_s = diff_ms / 1000;
      const hours = Math.floor(diff_s / 3600);
      const mins = Math.floor((diff_s - 3600 * hours) / 60);
      const secs = Math.floor((diff_s - 3600 * hours - 60 * mins));
      var ageStr;
      if (hours > 0) {
        ageStr = `${hours}h${mins.toString().padStart(2, "0")}m${secs.toString().padStart(2, "0")}s ago`;
      } else {
        ageStr = `${mins}m${secs.toString().padStart(2, "0")}s ago`;
      }

      el.textContent = `${ageStr} (${localIso})`;
      const tipParts = [`from: ${newest.url}`];
      if (newest.durationSec != null) tipParts.push(`duration: ${newest.durationSec}s`);
      el.title = tipParts.join(" | ");
    } catch (err) {
      console.error("Failed to aggregate heartbeats:", err);
      el.textContent = "—";
    }
  }

  // Initial load
  updateHeartbeat();

  // Reload button
  if (reloadBtn) reloadBtn.addEventListener("click", updateHeartbeat);
})();
</script>
<!-------------------------------------------------------------------------
   Compute Nodes Status Table
 -------------------------------------------------------------------------->
<script src="https://d3js.org/d3.v3.min.js"><!-- ~150 kB --></script>
<script src="https://cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"><!-- ~80 kB --></script>
<script src="https://cdn.datatables.net/1.10.16/js/dataTables.bootstrap.min.js"><!-- 2 kB --></script>
<!-- markdownlint-disable-file MD052 -->
<script type="text/javascript" charset="utf-8">
d3.text("/hpc/assets/data/host_table.tsv", "text/csv", function(host_table) {
  // drop header comments
  host_table = host_table.replace(/^[#][^\r\n]*[\r\n]+/mg, '');
  host_table = d3.tsv.parse(host_table);

  d3.text("https://raw.githubusercontent.com/UCSF-HPC/wynton-slash2/master/status/qstat_nodes_in_state_au.tsv", "text/csv", function(host_status) {
    host_status = d3.tsv.parse(host_status);

    var tbody, tr, td, td_status;
    var value;
    var nodes = 0, gpu_nodes = 0;
    var cores = 0, gpu_cores = 0;
    var nodes_with_issues = 0, cores_with_issues = 0;
    var gpu_nodes_with_issues = 0, gpu_cores_with_issues = 0;
    var cores_node;
    var hostname;
    
    /* For each row */
    host_table.forEach(function(row) {
      hostname = row["Node"];
      
      nodes += 1;
      cores_node = parseInt(row["Physical Cores"]);
      cores += cores_node;

      if (hostname.includes("gpu")) {
        gpu_nodes += 1;
        gpu_cores += cores_node;
      }
      
      // No issues?
      if (host_status.filter(function(d) { return d.queuename == hostname }).length == 0) return;

      /* Ignore column on /tmp size, iff it exists */
      delete row["Local `/tmp`"];

      if (nodes_with_issues == 0) {
        var table = d3.select("#hosttablediv").append("details").append("table");
        table.id = "hosttable";
        tr = table.append("thead").append("tr");
        tr.append("th").text("Status");
        for (key in row) tr.append("th").text(key.replace(/\`/g, ""));
        tbody = table.append("tbody");
      }

      nodes_with_issues += 1;      
      cores_with_issues += cores_node;
      if (hostname.includes("gpu")) {
        gpu_nodes_with_issues += 1;      
        gpu_cores_with_issues += cores_node;
      }
  
      tr = tbody.append("tr");
      td_status = tr.append("td").text("⚠");  // "⚠" or "✖"
      for (key in row) td = tr.append("td").text(row[key]);
    });


    /* WORKAROUND: The host table is not updates; instead pull in the static information. /HB 2020-12-16 */
    /* TODO: These values need to be dynamically loaded from the data files */
    nodes = 400;  /* Approximate value */
    cores = 10000;  /* Approximate value */
    gpu_nodes = 50;  /* Approximate value */
    
    var div = document.getElementById("hosttablemessage");
    div.innerText = "";
    if (nodes_with_issues > 0) {
      var text = document.createTextNode("Currently, " + nodes_with_issues + " (" + (100*nodes_with_issues/nodes).toFixed(1) + "%) " +  " of " + nodes + " compute nodes, corresponding to " + cores_with_issues + " (" + (100*cores_with_issues/cores).toFixed(1) + "%) " + " of " + cores + " CPU cores, are unavailable. Out of these, " + gpu_nodes_with_issues + " (" + (100*gpu_cores_with_issues/gpu_cores).toFixed(1) + "%) of " + gpu_nodes + " GPU compute nodes are unavailable.");
      div.appendChild(text);
      text = document.createTextNode(" A compute node is considered unavailable when its queuing state is \"disabled\" (d), \"unheard/unreachable\" (u), or \"error\" (E) (according to ");
      div.appendChild(text);
      var code = document.createElement("code");
      code.innerText = "qstat -f -qs duE";
      div.appendChild(code);
      text = document.createTextNode(" queried every five minutes), which means they will not take on any new jobs.");
      div.appendChild(text);
    } else {
      var text = document.createTextNode("All " + nodes + " nodes, with a total of " + cores + " cores, are functional.");
      div.appendChild(text);
    }
    
    $(document).ready(function() {
      $('#hosttable').DataTable({
        paging: false,
        searching: false,
        order: [[ 1, "asc" ]]
      });
    });
  });
});
</script>
<!-------------------------------------------------------------------------
   BeeGFS Lagginess Graphs
 -------------------------------------------------------------------------->
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script>
function date_to_string(d) {
  var YY = d.getFullYear().toString();
  var mm = (d.getMonth()+1).toString().length==2?(d.getMonth()+1).toString():"0"+(d.getMonth()+1).toString();
  var dd = d.getDate().toString().length==2?d.getDate().toString():"0"+d.getDate().toString();
  var HH = d.getHours().toString().length==2?d.getHours().toString():"0"+d.getHours().toString();
  var MM = (parseInt(d.getMinutes()/5)*5).toString().length==2?(parseInt(d.getMinutes()/5)*5).toString():"0"+(parseInt(d.getMinutes()/5)*5).toString();
  var SS = "00";
  return YY + "-" + mm + "-" + dd + " " + HH + ":" + MM + ":" + SS;
}


url_path = "https://raw.githubusercontent.com/UCSF-HPC/wynton-slash2/master/wynton-bench";
host_set = "devX";
hosts = ["dev1", "dev2", "dev3"];
drives = ["wynton_scratch_hb", "wynton_home_cbi_hb", "wynton_group_cbi_hb"];
// Baseline is when there is no load on the file system (rough estimate)
baseline = 19.0;
var multiple_beegfs_tracks = true;

var now = new Date();
var from = new Date(now - 24 * 60 * 60 * 1000);

var max_lagginess = 1000;

var beegfs_load = {
  type: "scatter",
  mode: "lines",
  name: 'BeeGFS Load',
  x: [],
  y: [],
  line: {color: '#23527c'}
}

var layout = {
  height: 300,
  margin: { l: 50, r: 30, b: 40, t: 60, pad: 4 },
  xaxis: {
    autorange: false,
    range: [date_to_string(from), date_to_string(now)],
    rangeselector: {buttons: [
        {
          count: 1,
          label: '1d',
          step: 'day',
          stepmode: 'backward'
        },
        {
          count: 7,
          label: '1w',
          step: 'day',
          stepmode: 'backward'
        },
        {
          count: 1,
          label: '1m',
          step: 'month',
          stepmode: 'backward'
        },
        {
          count: 12,
          label: '1y',
          step: 'month',
          stepmode: 'backward'
        },
        {
          step: 'all',
          label: 'all'
        }
      ]},
    type: 'date'
  },
  yaxis: {
    autorange: false,
    range: [-0.1, Math.log10(max_lagginess)],
    type: 'log'
  }
};

function unpack(rows, key) {
  return rows.map(function(row) { 
    var value = row[key];
    if (key == "duration") {
      value = parseFloat(value);
      value = value / baseline;
      if (value < 1/2) {
        // Unreasonable value?
        value = null;
      } else if (value < 1.0) {
        // Round up to baseline
        value = 1.0;
      }
    }
    return value;
  });
}

var data = [];

drives.forEach(function(drive) {
  var id = "BeeGFSLoad_" + host_set + "__" + drive;
  
  var url = url_path + "/" + "wynton-bench_" + hosts[0] + ".wynton.ucsf.edu__" + drive + ".tsv";
  Plotly.d3.tsv(url, function(err, rows) {
    // Lagginess data
    var trace = JSON.parse(JSON.stringify(beegfs_load));
    trace.name = hosts[0];
    trace.x = unpack(rows, 'timestamp');
    trace.y = unpack(rows, 'duration');
    trace.line = { color: '#23527c', width: 3 };

    // Pain-level annotations (excellent, good, sluggish, bad, critical)
    var painTicks = [2, 5, 20, 100, 10000];
    var painLabels = ['', '', '', '', ''];
          
    var painTrace = {
      x: trace.x,
      y: painTicks,
      yaxis: 'y2',
      mode: 'markers',
      marker: { opacity: 0 },
      showlegend: false,
      hoverinfo: 'skip'
    };
    
    var updatedLayout = Object.assign({}, layout, {
      yaxis2: {
        overlaying: 'y',
        side: 'right',
        tickvals: painTicks,
        ticktext: painLabels,
        showgrid: false,
        zeroline: false
      },
      shapes: [
        { type: 'rect', xref: 'paper', x0: 0, x1: 1, yref: 'y', y0: 1, y1: painTicks[1], fillcolor: 'rgba(0, 200, 0, 0.2)', line: { width: 0 }, layer: 'below' },                 // Excellent
        
        { type: 'rect', xref: 'paper', x0: 0, x1: 1, yref: 'y', y0: painTicks[0], y1: painTicks[1], fillcolor: 'rgba(255, 215, 0, 0.3)', line: { width: 0 }, layer: 'below' },    // Good
        
        { type: 'rect', xref: 'paper', x0: 0, x1: 1, yref: 'y', y0: painTicks[1], y1: painTicks[2], fillcolor: 'rgba(255, 140, 0, 0.3)', line: { width: 0 }, layer: 'below' },    // Sluggish
        
        { type: 'rect', xref: 'paper', x0: 0, x1: 1, yref: 'y', y0: painTicks[2], y1: painTicks[3], fillcolor: 'rgba(255, 0, 0, 0.4)', line: { width: 0 }, layer: 'below' },      // Bad
        
        { type: 'rect', xref: 'paper', x0: 0, x1: 1, yref: 'y', y0: painTicks[3], y1: max_lagginess, fillcolor: 'rgba(200, 100, 255, 0.4)', line: { width: 0 }, layer: 'below' }  // Critical
      ]
    });  

    Plotly.newPlot(id, [trace, painTrace], updatedLayout);
  })

  // Change to 'true' to show multiple traces
  if (multiple_beegfs_tracks) {
    if (hosts.length >= 2) {
      var url = url_path + "/" + "wynton-bench_" + hosts[1] + ".wynton.ucsf.edu__" + drive + ".tsv";
      Plotly.d3.tsv(url, function(err, rows) {
        var trace = JSON.parse(JSON.stringify(beegfs_load));
        trace.name = hosts[1];
        trace.x = unpack(rows, 'timestamp');
        trace.y = unpack(rows, 'duration');
        trace.line = { color: '#F88379', width: 3 };
        Plotly.addTraces(id, [trace]);
      })
    }
  
    if (hosts.length >= 3) {
      var url = url_path + "/" + "wynton-bench_" + hosts[2] + ".wynton.ucsf.edu__" + drive + ".tsv";
      Plotly.d3.tsv(url, function(err, rows) {
        var trace = JSON.parse(JSON.stringify(beegfs_load));
        trace.name = hosts[2];
        trace.x = unpack(rows, 'timestamp');
        trace.y = unpack(rows, 'duration');
        trace.line = { color: '#9dc183', width: 3 };
        Plotly.addTraces(id, [trace]);
      })
    }
  }  
});
</script>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/wynton\.ucsf\.edu\/hpc\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a href="https://www.ucsf.edu/">UCSF</a> :: <a href="https://wynton.ucsf.edu/">Wynton</a> :: <a href="../../hpc/">Wynton HPC</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>
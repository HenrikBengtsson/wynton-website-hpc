[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Redirecting to https://wynton.ucsf.edu/hpc/index.html."
  },
  {
    "objectID": "hpc/support/faq.html",
    "href": "hpc/support/faq.html",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "For detailed information on data classifications and guidelines for using Wynton Protected, please consult the User Agreement and Disclaimer.\n\n\n\n\nQ. My jobs wait in the queue for a long time before they run - why?\nA. A job’s waiting time in the queue depends on many factors:\n\nThe overall number of jobs already in the queue by you and others, and the run time of the jobs already running on the cluster (Status page).\nThe amount of resources your job requests, e.g.\n\nthe more CPU cores it needs, the longer it has to wait (-pe &lt;type&gt; &lt;slots&gt;)\nthe more RAM it needs, the longer it has to wait (-l mem_free=&lt;amount&gt;)\nthe longer run-time it needs, the longer it has to wait (-l h_rt=&lt;time&gt;)\n\nIf you request impossible resource requests, the job will never run, as explained below.\nJobs that you ran in the past do not impact the waiting time for new jobs.\n\nQ. My submitted job is stuck in the queue - why is it not running?\nA. There could be several reason why your job is not running:\n\nIf you have access to the members.q queue, it could be that you and other users in your lab are currently using all your slots, which in case your jobs are being queued in the communal long.q queue instead.\nThe queue where your job is sitting may be full. If so, your job will eventually run.\nYou might have asked for compute resources that cannot be met, e.g. more memory or more cores than available on any compute node, e.g. -l mem_free=4048G or -pe smp 512. If so, your job will never run. Either lower the job’s resource needs using qdel, or, alternatively, remove the job (qdel) and submit (qsub) a new one with adjusted resources.\nIf there is a downtime scheduled, you might have asked for a run-time that goes into the downtime period. If so, your job cannot be started until after the downtime. If you don’t specify the run-time when you submit your job, the default is 14 days.\nqstat -j &lt;job_id&gt; will provide details on why a particular job is not running. qstat -u '*' will show all jobs and their priority scores in the queue.\n\n\n\n\nQ. I tried to delete some jobs, and now they’re stuck in the state “dr”. How can I get rid of them?\nA. The most likely cause of this is that node (or nodes) running your jobs crashed. Since the node(s) can’t report back to SGE and confirm the job deletion, the state of the jobs doesn’t change. To force the issue:\nqdel -f $JOB_ID [-t $SGE_TASK_ID]\n\n\n\nQ. What is the difference between the idgpu, iogpu, and atgpu parts used for GPU compute node names?\nA. They denotes CPU architecture: io is for “Intel Octo-core” (i.e. Intel CPUs with 8 cores per CPU), id is for “Intel Dodeca-core” (12 cores per CPU, but now encompasses all Intel nodes with more 12+ nodes), and at is for “AMD Triginti-core” (32 cores).\n\n\n\nQ. I just started to get SSL-related errors when using qsub and qstat that I have never seen before;\nerror: commlib error: ssl connect error (SSL handshake error)\nssl error (the used certificate is expired)\nunable to contact qmaster using port 6444 on host \"q\"\nA. Your Wynton HPC account has expired. If so, you should already have received an email from us with instructions on how to request the renewal. If you have responded to that email, then it’s a mistake on our end (sorry) - please drop us another email.\n\n\n\n\n\n\nQ. I was running Python on a development node when it suddenly terminated with a single message “Killed”. What happened?\nA. Sudden termination of Python, R, and other software tools with only a “Killed” message is likely due to overuse of the memory. The “Killed” message is produced by the shell whenever the operating system terminates the process (by signaling SIGKILL). The operating system may decide to terminate any process that uses too much memory, where the memory limit is 96 GB/user. To confirm it was overuse of memory, type echo \"Exit code: $?\" immediately after the process was terminated. If you get exit code 137, then it is likely that you ran out of memory.\n\n\n\nQ. I’m getting errors like ‘caught illegal operation’, ‘illegal operand’, and ‘Illegal instruction (core dumped)’\nA. These type of errors are often because a software tool is called a CPU instruction (“operand”) that the CPU on the current machine does not understand or support. The {{ site.cluster.nickname }} cluster comprise of a heterogeneous set of compute nodes with a wide variety of CPU generations. Specifically, all nodes support CPU instructions of x86-64 version 2 or newer. However, some software tools requires x86-64 v3 or newer. If you submit a job requiring x86-64 v3, it might end up on a x86-64 v2 compute node by chance, resulting in the above type of errors. To ensure that this does not happen, declare the x86-64 level that your job requires, e.g. -l   x86-64-v=3.\n\n\n\nQ. x2go authenticates, but then immediately disconnects without launching.\nA. The first thing to check is your BeeGFS home directory quota. x2go creates a number of temporary files related to your sessions. If it cannot create those files, it will authenticate and then disconnect. Short version: beegfs-ctl --getquota --storagepoolid=11 --uid \"$USER\". For more information on quotas and file systems, see the page on File Sizes and Disk Quotas.\nQ. How do I terminate a saved x2go session if it seems “stuck” or “unresponsive”?\nA. Log in to the destination server you were connecting to (not the proxy/jump host) via ssh, type x2golistsessions.\nIf something comes up, run x2goterminate-session $SESSION, replacing $SESSION by the second field in the output of x2golistsessions.\nThen try connecting again with your x2go client.\nQ. I am getting timeout errors when trying to connect via X2Go from a macOS computer, the X2Go status hangs on “connecting”; In the X2Go logs you will see:\nInfo: Forwarding X11 connections to display '/private/tmp/com.apple.launchd.C24DSqSnIF/org.xquartz:0'.\nInfo: Forwarding auxiliary X11 connections to display '/private/tmp/com.apple.launchd.C24DSqSnIF/org.xquartz:0'.\nSession: Session started at 'Tue Mar  2 13:00:37 2021'.\nConnection timeout, abortingSession: Terminating session at 'Tue Mar  2 13:01:05 2021'.\nInfo: Waiting the cleanup timeout to complete.\nSession: Session terminated at 'Tue Mar  2 13:01:07 2021'.\nA. This appears to be a communication problem between X2Go and XQuartz. The only way we’ve found to resolve this issue is to Completely remove XQuartz from the macOS computer and then re-install XQuartz. Please follow recommendations for completely removing the XQuartz application and all related files. (Search for any files or folders with the program’s name or developer’s name in the ~/Library/Preferences/, ~/Library/Application Support/ and ~/Library/Caches/ folders.) After re-installing XQuartz, X2Go should work again. If not, please contact the Wynton team.\n\n\n\nQ. I tried to change my shell using the unix command chsh and I got an error telling me, “chsh: user ‘alice’ does not exist”.\nA. First, let me assure you, your account does exist! You are logged in, after all. However, {{ site.cluster.nickname}} account attributes are managed via a remote directory system which is not manipulable via local tools like chsh. If you would like to change your shell, Please get in touch with the Wynton team, let us know your preferred shell, and we will change it for you. Note: The {{ site.cluster.nickname}} team supports csh/tcsh and sh/bash login shells. Any other shell than these may result in reduced functionality or errors which may be beyond the scope of our support.\n\n\n\nQ. When I SSH to one of the development nodes, nothing happens - it seems to be stuck.\nA. One reason for this could be that you previously started compute-heavy processes on the problematic development node, and those processes are now consuming all of your CPU quota on that node. Each user can use up to two CPU cores (200%) worth of compute per development node. If you are already saturate a development node like this, then you, and only you, will experience an very slow response time whenever you try to login into that same node. When this happens, try to login into another development node that you are not running on previously. If you can access another node promptly, then it is likely that you are indeed saturating your CPU quota on that other machine. The best you can do in this situation is to either (a) wait for your processes to finish, or (b) wait until SSH completes, which might take several minutes or longer. When you eventually get to the prompt, you can terminate all your processes on the current node by calling killall. This will terminate all your running processes on that machine abruptly and you will be logged out from that machine as well. Afterward, you should be able to access the machine as usual.\n\n\n\nQ. I cannot SSH into the development nodes - I get ‘IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!’ and ‘Host key verification failed.’ What is going on?\nA. This most likely happens because we have re-built the problematic development node resulting in its internal security keys having changed since you last access that machine. If the problem error looks like:\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the ECDSA key sent by the remote host is\nSHA256:FaARLbkE3sHP2a33Zgqa/sNXTqqWzZAeu6T43wST4ok.\nPlease contact your system administrator.\nAdd correct host key in /wynton/home/boblab/alice/.ssh/known_hosts to get rid of this message.\nOffending ECDSA key in /wynton/home/boblab/alice/.ssh/known_hosts:18\nECDSA host key for dev2 has changed and you have requested strict checking.\nHost key verification failed.\nthen the solution is to remove that offending key from your personal ~/.ssh/known_hosts file on Wynton. If you get this error when you try to access, say, dev2, then use:\n$ ssh-keygen -R dev2\nto remove all SSH keys associated with that machine. Alternatively, you can manually remove the problematic key by looking at:\nOffending ECDSA key in /wynton/home/boblab/alice/.ssh/known_hosts:18\nto identify that we want to remove the key on line 18. To remove that line, use:\n$ sed --in-place=\".bak\" \"18d\" ~/.ssh/known_hosts\nThen retry.\n\n\n\nQ. When I try to SSH to one of the login or data-transfer nodes, I get an error ssh: connect to host log2.wynton.ucsf.edu port 22: Connection timed out\" error. What’s wrong?\nA. This error suggests that you cannot reach the Wynton login node. This will happen if you are connected to the ‘UCSFguest’ WiFi or the UCSF ‘eduroam’ WiFi available on the UCSF campuses. If so, please make sure to use the ‘UCSFwpa’ WiFi, the ‘UCSFhousing’ WiFi (if you have access), or connect to the UCSF VPN, and then retry.\n\n\n\nQ. Why do I get “incorrect password attempts” when using sudo despite entering my password correctly?\nA. The sudo command is only available to system administrators. It is a command used to run a specific software as root, that is, with administrator privileges. First of all, for security reasons, users do not have the rights to use sudo. Second, sudo is often used to install a software tool centrally on the current machine for all users. If you think about it, it would wreak havoc if any user would be able to install or update software that other users use.\nBy the way, if you ever get prompted for your {{ site.cluster.name }} password, please stop and think! Except for when you access the cluster, or one of the development nodes, no software should ever need to know you password. So, if you get asked for your {{ site.cluster.name }} password, do not enter it.\nNow, if you do call sudo, the system administrators will be notified automatically, and you will most likely get a follow-up email from them. Please respond to such a message, if you get one. That said, if you end up using sudo, press Ctrl-C when you are prompted for your password. This will prevent the command from being completed, e.g.\n$ sudo make install\n\nWe trust you have received the usual lecture from the local System\nAdministrator. It usually boils down to these three things:\n\n    #1) Respect the privacy of others.\n    #2) Think before you type.\n    #3) With great power comes great responsibility.\n\n[sudo] password for alice: &lt;Ctrl-C&gt;\n\n$ \n\n\n\n\n\n\nQ. Is data on Wynton backed up?\nA. Data on Wynton is not backed up! Users and labs are responsible to back up their own data outside of the Wynton HPC environment.\n\n\n\nQ. Is it possible to have a common folder where our lab group members can share files and software?\nA. Labs who purchase additional storage will get a /wynton/group/your_group/ folder. Files written in that folder will not count toward users disk quota.\n\n\n\n\n\n\nQ. When I run Jupyter, I now get an error saying /opt/rh/rh-python38/root/usr/bin/python3: bad interpreter: No such file or directory. How can I fix it.\nA. This is because the Python software, here Jupyter, was installed using the rh-python38 SCL, which was only available on CentOS 7. To fix this, you need to reinstall the broken software, which in this example means reinstall Jupyter.\n\n\n\n\n\n\nQ. I might have corrupted my Bash startup files. How do I reset them?\nA. To get a fresh ~/.bashrc file, make a backup of your old one and copy the default one by:\n$ cp ~/.bashrc ~/.bashrc.20231115\n$ cp /etc/skel/.bashrc ~/\nA. To get a fresh ~/.bash_profile file, make a backup of your old one and copy the default one by:\n$ cp ~/.bash_profile ~/.bash_profile.20231115\n$ cp /etc/skel/.bash_profile ~/\n\n\n\nQ. How can I view PNG and JPEG image files?\nA. There are few alternatives for viewing images files over SSH (easy-to-hard):\n\nOne solution is to view the image files directly in the SSH terminal, but the resolution will depend on what type of terminal you run locally. First, call module load CBI chafa. Then, if you are using a terminal like iTerm2 or WezTerm, then use chafa --format=iterm image.png. If you are using a terminal like Kitty, then use chafa --format=kitty image.png. If nothing else works, try chafa image.png (the smaller font-size the great image resolution). [easiest]\nAnother solution is to view the images using regular GUI tools viewed remotely over X11. This requires that you are connected to Wynton with X11 Forwarding enabled. This allows you to use firefox image.png, which launched the Firebox browser on Wynton and opens the image.png file. Another tool is display image.png, which is available via module load Sali ImageMagick. [easy, but slower]\nA third solution is to browser and view the image files via browser-embedded GUIs such as RStudio Server and Jupyter Notebook. [more work]\n\n\n\n\nQ. Can I use Microsoft VS Code with Wynton?\nA. Yes, but, importantly, do not use Microsoft Visual Studio Code (VS Code) on the login nodes. Because of this, you need to configure VS Code on your local machine to connect directly to a development nodes instead. To achieve this, start by configuring your local SSH environment to connect directly to a development node via a “jump host” directive as described in ‘Connect directly to a development node’. When you know that works, consult VS Code’s documentation regarding using an SSH “Jump Host” or “Ssh Proxy”. In short, configure VS Code by clicking on the SSH extension, “new remote”, and then enter ssh dev1. This should allow VS Code to connect directly to dev1.\n\n\n\nQ. Why don’t you like screenshots of output and error messages?\nA. We, and others, strongly prefer that you cut-and-paste textual output from your SSH terminal in your email, Slack, … messages. There are several reason for this. First, and most importantly, screenshots are not accessible, which means they are useless to a person using a screen reader. For others, text in screenshots might be very hard to read and require zooming in on the image. Second, it is not possible to cut-and-paste from a screenshot, which adds unnecessary friction to anyone trying to reproduce your problem and help you. Third, contrary to plain text, screenshots are not searchable in email clients and on Slack.\n\n\n\n\n\n\nQ. Our lab would contribute to Wynton HPC in order to increase our priority. How can we do this?\nA. We welcome donations of any size. In return, your lab will receive a number of slots in member.q equivalent to the number of cores in a current Standard Node that your contribution would purchase. See Pricing for Extra Compute for more details.\n\n\n\nQ. Our lab has some old nodes we’d like to contribute to {{ site.cluster.name }} in return for priority. Will you take them?\nA. We no longer accept donations of old hardware.\n\n\n\nQ. Does Wynton Provide Server Hosting, Data Storage, or Application Hosting Services?\nA. Wynton HPC provides High Performance Computing resources to the UCSF Research community. We are not able to provide server hosting, application hosting, data storage, or consulting services outside of our core mission in Research High Performance Computing. For these, and related services, we suggest you contact UCSF IT regarding their offerings, e.g. UCSF IT Virtual Server Hosting, UCSF IT Physical Server Hosting, and UCSF IT Cloud Services."
  },
  {
    "objectID": "hpc/support/faq.html#jobs",
    "href": "hpc/support/faq.html#jobs",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Q. My jobs wait in the queue for a long time before they run - why?\nA. A job’s waiting time in the queue depends on many factors:\n\nThe overall number of jobs already in the queue by you and others, and the run time of the jobs already running on the cluster (Status page).\nThe amount of resources your job requests, e.g.\n\nthe more CPU cores it needs, the longer it has to wait (-pe &lt;type&gt; &lt;slots&gt;)\nthe more RAM it needs, the longer it has to wait (-l mem_free=&lt;amount&gt;)\nthe longer run-time it needs, the longer it has to wait (-l h_rt=&lt;time&gt;)\n\nIf you request impossible resource requests, the job will never run, as explained below.\nJobs that you ran in the past do not impact the waiting time for new jobs.\n\nQ. My submitted job is stuck in the queue - why is it not running?\nA. There could be several reason why your job is not running:\n\nIf you have access to the members.q queue, it could be that you and other users in your lab are currently using all your slots, which in case your jobs are being queued in the communal long.q queue instead.\nThe queue where your job is sitting may be full. If so, your job will eventually run.\nYou might have asked for compute resources that cannot be met, e.g. more memory or more cores than available on any compute node, e.g. -l mem_free=4048G or -pe smp 512. If so, your job will never run. Either lower the job’s resource needs using qdel, or, alternatively, remove the job (qdel) and submit (qsub) a new one with adjusted resources.\nIf there is a downtime scheduled, you might have asked for a run-time that goes into the downtime period. If so, your job cannot be started until after the downtime. If you don’t specify the run-time when you submit your job, the default is 14 days.\nqstat -j &lt;job_id&gt; will provide details on why a particular job is not running. qstat -u '*' will show all jobs and their priority scores in the queue.\n\n\n\n\nQ. I tried to delete some jobs, and now they’re stuck in the state “dr”. How can I get rid of them?\nA. The most likely cause of this is that node (or nodes) running your jobs crashed. Since the node(s) can’t report back to SGE and confirm the job deletion, the state of the jobs doesn’t change. To force the issue:\nqdel -f $JOB_ID [-t $SGE_TASK_ID]\n\n\n\nQ. What is the difference between the idgpu, iogpu, and atgpu parts used for GPU compute node names?\nA. They denotes CPU architecture: io is for “Intel Octo-core” (i.e. Intel CPUs with 8 cores per CPU), id is for “Intel Dodeca-core” (12 cores per CPU, but now encompasses all Intel nodes with more 12+ nodes), and at is for “AMD Triginti-core” (32 cores).\n\n\n\nQ. I just started to get SSL-related errors when using qsub and qstat that I have never seen before;\nerror: commlib error: ssl connect error (SSL handshake error)\nssl error (the used certificate is expired)\nunable to contact qmaster using port 6444 on host \"q\"\nA. Your Wynton HPC account has expired. If so, you should already have received an email from us with instructions on how to request the renewal. If you have responded to that email, then it’s a mistake on our end (sorry) - please drop us another email."
  },
  {
    "objectID": "hpc/support/faq.html#something-is-not-working",
    "href": "hpc/support/faq.html#something-is-not-working",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Q. I was running Python on a development node when it suddenly terminated with a single message “Killed”. What happened?\nA. Sudden termination of Python, R, and other software tools with only a “Killed” message is likely due to overuse of the memory. The “Killed” message is produced by the shell whenever the operating system terminates the process (by signaling SIGKILL). The operating system may decide to terminate any process that uses too much memory, where the memory limit is 96 GB/user. To confirm it was overuse of memory, type echo \"Exit code: $?\" immediately after the process was terminated. If you get exit code 137, then it is likely that you ran out of memory.\n\n\n\nQ. I’m getting errors like ‘caught illegal operation’, ‘illegal operand’, and ‘Illegal instruction (core dumped)’\nA. These type of errors are often because a software tool is called a CPU instruction (“operand”) that the CPU on the current machine does not understand or support. The {{ site.cluster.nickname }} cluster comprise of a heterogeneous set of compute nodes with a wide variety of CPU generations. Specifically, all nodes support CPU instructions of x86-64 version 2 or newer. However, some software tools requires x86-64 v3 or newer. If you submit a job requiring x86-64 v3, it might end up on a x86-64 v2 compute node by chance, resulting in the above type of errors. To ensure that this does not happen, declare the x86-64 level that your job requires, e.g. -l   x86-64-v=3.\n\n\n\nQ. x2go authenticates, but then immediately disconnects without launching.\nA. The first thing to check is your BeeGFS home directory quota. x2go creates a number of temporary files related to your sessions. If it cannot create those files, it will authenticate and then disconnect. Short version: beegfs-ctl --getquota --storagepoolid=11 --uid \"$USER\". For more information on quotas and file systems, see the page on File Sizes and Disk Quotas.\nQ. How do I terminate a saved x2go session if it seems “stuck” or “unresponsive”?\nA. Log in to the destination server you were connecting to (not the proxy/jump host) via ssh, type x2golistsessions.\nIf something comes up, run x2goterminate-session $SESSION, replacing $SESSION by the second field in the output of x2golistsessions.\nThen try connecting again with your x2go client.\nQ. I am getting timeout errors when trying to connect via X2Go from a macOS computer, the X2Go status hangs on “connecting”; In the X2Go logs you will see:\nInfo: Forwarding X11 connections to display '/private/tmp/com.apple.launchd.C24DSqSnIF/org.xquartz:0'.\nInfo: Forwarding auxiliary X11 connections to display '/private/tmp/com.apple.launchd.C24DSqSnIF/org.xquartz:0'.\nSession: Session started at 'Tue Mar  2 13:00:37 2021'.\nConnection timeout, abortingSession: Terminating session at 'Tue Mar  2 13:01:05 2021'.\nInfo: Waiting the cleanup timeout to complete.\nSession: Session terminated at 'Tue Mar  2 13:01:07 2021'.\nA. This appears to be a communication problem between X2Go and XQuartz. The only way we’ve found to resolve this issue is to Completely remove XQuartz from the macOS computer and then re-install XQuartz. Please follow recommendations for completely removing the XQuartz application and all related files. (Search for any files or folders with the program’s name or developer’s name in the ~/Library/Preferences/, ~/Library/Application Support/ and ~/Library/Caches/ folders.) After re-installing XQuartz, X2Go should work again. If not, please contact the Wynton team.\n\n\n\nQ. I tried to change my shell using the unix command chsh and I got an error telling me, “chsh: user ‘alice’ does not exist”.\nA. First, let me assure you, your account does exist! You are logged in, after all. However, {{ site.cluster.nickname}} account attributes are managed via a remote directory system which is not manipulable via local tools like chsh. If you would like to change your shell, Please get in touch with the Wynton team, let us know your preferred shell, and we will change it for you. Note: The {{ site.cluster.nickname}} team supports csh/tcsh and sh/bash login shells. Any other shell than these may result in reduced functionality or errors which may be beyond the scope of our support.\n\n\n\nQ. When I SSH to one of the development nodes, nothing happens - it seems to be stuck.\nA. One reason for this could be that you previously started compute-heavy processes on the problematic development node, and those processes are now consuming all of your CPU quota on that node. Each user can use up to two CPU cores (200%) worth of compute per development node. If you are already saturate a development node like this, then you, and only you, will experience an very slow response time whenever you try to login into that same node. When this happens, try to login into another development node that you are not running on previously. If you can access another node promptly, then it is likely that you are indeed saturating your CPU quota on that other machine. The best you can do in this situation is to either (a) wait for your processes to finish, or (b) wait until SSH completes, which might take several minutes or longer. When you eventually get to the prompt, you can terminate all your processes on the current node by calling killall. This will terminate all your running processes on that machine abruptly and you will be logged out from that machine as well. Afterward, you should be able to access the machine as usual.\n\n\n\nQ. I cannot SSH into the development nodes - I get ‘IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!’ and ‘Host key verification failed.’ What is going on?\nA. This most likely happens because we have re-built the problematic development node resulting in its internal security keys having changed since you last access that machine. If the problem error looks like:\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the ECDSA key sent by the remote host is\nSHA256:FaARLbkE3sHP2a33Zgqa/sNXTqqWzZAeu6T43wST4ok.\nPlease contact your system administrator.\nAdd correct host key in /wynton/home/boblab/alice/.ssh/known_hosts to get rid of this message.\nOffending ECDSA key in /wynton/home/boblab/alice/.ssh/known_hosts:18\nECDSA host key for dev2 has changed and you have requested strict checking.\nHost key verification failed.\nthen the solution is to remove that offending key from your personal ~/.ssh/known_hosts file on Wynton. If you get this error when you try to access, say, dev2, then use:\n$ ssh-keygen -R dev2\nto remove all SSH keys associated with that machine. Alternatively, you can manually remove the problematic key by looking at:\nOffending ECDSA key in /wynton/home/boblab/alice/.ssh/known_hosts:18\nto identify that we want to remove the key on line 18. To remove that line, use:\n$ sed --in-place=\".bak\" \"18d\" ~/.ssh/known_hosts\nThen retry.\n\n\n\nQ. When I try to SSH to one of the login or data-transfer nodes, I get an error ssh: connect to host log2.wynton.ucsf.edu port 22: Connection timed out\" error. What’s wrong?\nA. This error suggests that you cannot reach the Wynton login node. This will happen if you are connected to the ‘UCSFguest’ WiFi or the UCSF ‘eduroam’ WiFi available on the UCSF campuses. If so, please make sure to use the ‘UCSFwpa’ WiFi, the ‘UCSFhousing’ WiFi (if you have access), or connect to the UCSF VPN, and then retry.\n\n\n\nQ. Why do I get “incorrect password attempts” when using sudo despite entering my password correctly?\nA. The sudo command is only available to system administrators. It is a command used to run a specific software as root, that is, with administrator privileges. First of all, for security reasons, users do not have the rights to use sudo. Second, sudo is often used to install a software tool centrally on the current machine for all users. If you think about it, it would wreak havoc if any user would be able to install or update software that other users use.\nBy the way, if you ever get prompted for your {{ site.cluster.name }} password, please stop and think! Except for when you access the cluster, or one of the development nodes, no software should ever need to know you password. So, if you get asked for your {{ site.cluster.name }} password, do not enter it.\nNow, if you do call sudo, the system administrators will be notified automatically, and you will most likely get a follow-up email from them. Please respond to such a message, if you get one. That said, if you end up using sudo, press Ctrl-C when you are prompted for your password. This will prevent the command from being completed, e.g.\n$ sudo make install\n\nWe trust you have received the usual lecture from the local System\nAdministrator. It usually boils down to these three things:\n\n    #1) Respect the privacy of others.\n    #2) Think before you type.\n    #3) With great power comes great responsibility.\n\n[sudo] password for alice: &lt;Ctrl-C&gt;\n\n$"
  },
  {
    "objectID": "hpc/support/faq.html#files-and-folders",
    "href": "hpc/support/faq.html#files-and-folders",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Q. Is data on Wynton backed up?\nA. Data on Wynton is not backed up! Users and labs are responsible to back up their own data outside of the Wynton HPC environment.\n\n\n\nQ. Is it possible to have a common folder where our lab group members can share files and software?\nA. Labs who purchase additional storage will get a /wynton/group/your_group/ folder. Files written in that folder will not count toward users disk quota."
  },
  {
    "objectID": "hpc/support/faq.html#things-that-used-to-work-before-we-migrated-to-rocky-8",
    "href": "hpc/support/faq.html#things-that-used-to-work-before-we-migrated-to-rocky-8",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Q. When I run Jupyter, I now get an error saying /opt/rh/rh-python38/root/usr/bin/python3: bad interpreter: No such file or directory. How can I fix it.\nA. This is because the Python software, here Jupyter, was installed using the rh-python38 SCL, which was only available on CentOS 7. To fix this, you need to reinstall the broken software, which in this example means reinstall Jupyter."
  },
  {
    "objectID": "hpc/support/faq.html#miscellaneous",
    "href": "hpc/support/faq.html#miscellaneous",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Q. I might have corrupted my Bash startup files. How do I reset them?\nA. To get a fresh ~/.bashrc file, make a backup of your old one and copy the default one by:\n$ cp ~/.bashrc ~/.bashrc.20231115\n$ cp /etc/skel/.bashrc ~/\nA. To get a fresh ~/.bash_profile file, make a backup of your old one and copy the default one by:\n$ cp ~/.bash_profile ~/.bash_profile.20231115\n$ cp /etc/skel/.bash_profile ~/\n\n\n\nQ. How can I view PNG and JPEG image files?\nA. There are few alternatives for viewing images files over SSH (easy-to-hard):\n\nOne solution is to view the image files directly in the SSH terminal, but the resolution will depend on what type of terminal you run locally. First, call module load CBI chafa. Then, if you are using a terminal like iTerm2 or WezTerm, then use chafa --format=iterm image.png. If you are using a terminal like Kitty, then use chafa --format=kitty image.png. If nothing else works, try chafa image.png (the smaller font-size the great image resolution). [easiest]\nAnother solution is to view the images using regular GUI tools viewed remotely over X11. This requires that you are connected to Wynton with X11 Forwarding enabled. This allows you to use firefox image.png, which launched the Firebox browser on Wynton and opens the image.png file. Another tool is display image.png, which is available via module load Sali ImageMagick. [easy, but slower]\nA third solution is to browser and view the image files via browser-embedded GUIs such as RStudio Server and Jupyter Notebook. [more work]\n\n\n\n\nQ. Can I use Microsoft VS Code with Wynton?\nA. Yes, but, importantly, do not use Microsoft Visual Studio Code (VS Code) on the login nodes. Because of this, you need to configure VS Code on your local machine to connect directly to a development nodes instead. To achieve this, start by configuring your local SSH environment to connect directly to a development node via a “jump host” directive as described in ‘Connect directly to a development node’. When you know that works, consult VS Code’s documentation regarding using an SSH “Jump Host” or “Ssh Proxy”. In short, configure VS Code by clicking on the SSH extension, “new remote”, and then enter ssh dev1. This should allow VS Code to connect directly to dev1.\n\n\n\nQ. Why don’t you like screenshots of output and error messages?\nA. We, and others, strongly prefer that you cut-and-paste textual output from your SSH terminal in your email, Slack, … messages. There are several reason for this. First, and most importantly, screenshots are not accessible, which means they are useless to a person using a screen reader. For others, text in screenshots might be very hard to read and require zooming in on the image. Second, it is not possible to cut-and-paste from a screenshot, which adds unnecessary friction to anyone trying to reproduce your problem and help you. Third, contrary to plain text, screenshots are not searchable in email clients and on Slack."
  },
  {
    "objectID": "hpc/support/faq.html#contributing-to",
    "href": "hpc/support/faq.html#contributing-to",
    "title": "Frequently Asked Questions (FAQs)",
    "section": "",
    "text": "Q. Our lab would contribute to Wynton HPC in order to increase our priority. How can we do this?\nA. We welcome donations of any size. In return, your lab will receive a number of slots in member.q equivalent to the number of cores in a current Standard Node that your contribution would purchase. See Pricing for Extra Compute for more details.\n\n\n\nQ. Our lab has some old nodes we’d like to contribute to {{ site.cluster.name }} in return for priority. Will you take them?\nA. We no longer accept donations of old hardware.\n\n\n\nQ. Does Wynton Provide Server Hosting, Data Storage, or Application Hosting Services?\nA. Wynton HPC provides High Performance Computing resources to the UCSF Research community. We are not able to provide server hosting, application hosting, data storage, or consulting services outside of our core mission in Research High Performance Computing. For these, and related services, we suggest you contact UCSF IT regarding their offerings, e.g. UCSF IT Virtual Server Hosting, UCSF IT Physical Server Hosting, and UCSF IT Cloud Services."
  },
  {
    "objectID": "hpc/status/incidents-2021.html",
    "href": "hpc/status/incidents-2021.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2021\n\nFull downtime:\n\nScheduled: 64 hours = 2.7 days = 0.73%\nUnscheduled: 58 hours = 2.4 days = 0.66%\nTotal: 122 hours = 5.1 days = 1.4%\nExternal factors: 39% of the above downtime, corresponding to 47 hours (=2.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2021-05-25 (64 hours)\n\nTotal downtime: 64 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2021-01-29 (up to 14 days)\n2021-07-23 (up to 14 days)\n2021-12-08 (up to 14 days)\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2021-08-26 (28 hours) - Planned Byers Hall power shutdown failed\n2021-11-09 (10 hours) - Unplanned PG&E power outage\n\nTotal downtime: 38 hours of which 38 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2021-03-26 (9 hours) - Campus networks issues causing significant BeeGFS slowness\n2021-07-23 (8 hours) - BeeGFS silently failed disks\n2021-11-05 (3 hours) - BeeGFS non-responsive\n\nTotal downtime: 20 hours of which 9 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2021-04-28 (210 hours) - GPU taken down due to server room cooling issues\n\nTotal downtime: 0 hours\n\n\n\nAccounts\n\nNumber of user account: 1,274 (change: +410 during the year)\n\n\n\n\nDecember 8-December 23, 2021\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. Dec 23, 12:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted tomorrow Thursday December 9 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~12,500 cores) in the graph above.\nDec 8, 16:30 PT\n\n\n\nDecember 19-21, 2021\n\nGlobus and data-transfer node issue\nResolved: Data-transfer node dt1 and Globus file transfers are working again. Dec 21, 13:20 PT\nUpdate: Globus file transfers to and from Wynton are not working. This is because Globus relies on the data-transfer node dt1, which is currently down. Dec 20, 15:30 PT\nNotice: Data-transfer node dt1 has issues. Please use dt2 until resolved. The first report on this problem came yesterday at 21:30. Dec 20, 09:30 PT\n\n\n\nNovember 9, 2021\n\nPartial outage due to campus power glitch\nResolved: All hosts have been rebooted and are now up and running. November 9, 11:00 PT\nNotice: There was a brief PG&E power outage early Tuesday November 9 around 01:20. This affected the power on the Mission Bay campus, including the data center housing Wynton. The parts of our system with redundant power were fine, but many of the compute nodes are on PG&E-power only and, therefore, went down. As a result, lots of jobs crashed. We will restart the nodes that crashed manually during the day today. November 9, 09:10 PT\n\n\n\n\nOctober 25-26, 2021\n\nFile-system maintenance\nResolved: Resynchronization of all file-system meta servers is complete, which concludes the maintenance. October 26, 09:45 PT\nUpdate: The maintenance work has started. October 25, 14:00 PT\nNotice: We will perform BeeGFS maintenance work starting Monday October 25 at 2:00 pm. During this work, the filesystem might be less performant. We don’t anticipate any downtime. October 21, 12:10 PT\n\n\n\nAugust 26-September 10, 2021\n\nByers Hall power outage & file-system corruption\nResolved: The corrupted filesystem has been recovered. September 10, 17:20 PT\nUpdate: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on /wynton/group/. The file recovery tools are still running. August 27, 13:05 PT\nPartially resolved: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on /wynton/group/. The file recovery tools are still running. August 27, 13:05 PT\nUpdate: The BeeGFS filesystem recovering attempt keeps running. The current plan is to bring Wynton back online while keeping the problematic BeeGFS filesystem offline. August 26, 23:05 PT\nUpdate: All of the BeeGFS servers are up and running, but one of the 108 filesystems that make up BeeGFS was corrupted by the sudden power outage. The bad filesystem is part of /wynton/group/. We estimate that 70 TB of data is affected. We are making every possible effort to restore this filesystem, which will take time. While we do so, Wynton will remain down. August 26, 14:05 PT\nNotice: The cluster is down after an unplanned power outage in the main data center. The power is back online but several of our systems, including BeeGFS servers, did not come back up automatically and will require on-site, manual actions. August 26, 09:15 PT\n\n\n\n\nJuly 23-July 28, 2021\n\nKernel maintenance\nResolved: The majority of the compute nodes have been rebooted after only four days, which was quicker than the maximum of 14 days. July 28, 08:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted at 13:00 on Friday July 23 at 1:00 pm. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above.\nJuly 23, 07:40 PT\n\n\n\nJune 24, 2021\n\nCluster not accessible (due to BeeGFS issues)\nResolved: Wynton and BeeGFS is back online. The problem was due to failed disks. Unfortunately, about 10% of the space in /wynton/scratch/ went bad, meaning some files are missing or corrupted. It is neither possible to recover them nor identify which files or folders are affected. In other words, expect some oddness if you had data under /wynton/scratch/. There will also be some hiccups over the next several days as we get everything in ZFS and BeeGFS back into an as stable state as possible. June 24, 14:55 PT\nUpdate: We’re working hard on getting BeeGFS back up. We were not able to recover the bad storage target, so it looks like there will be some data loss on /wynton/scratch/. More updates soon. June 24, 13:45 PT\nNotification: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues since early this morning. The problem is being investigated. June 24, 07:00 PT\n\n\n\n\nMay 25-June 7, 2021\n\nFull downtime (major maintenance)\nResolved: All remaining issues from the downtime have been resolved.  June 7, 17:00 PT\nUpdate: Login node log2 can now be reached from the UCSF Housing WiFi network.  June 7, 17:00 PT\nUpdate: dt2 can now be reached from outside the Wynton cluster.  June 7, 13:15 PT\nUpdate: Login node log2 cannot be reached from the UCSF Housing WiFi network. If you are on that network, use log1 until this has been resolved.  June 2, 07:00 PT\nUpdate: Both data transfer nodes are back online since a while, but dt2 can only be reached from within the Wynton cluster.  June 1, 13:45 PT\nUpdate: A large number of of the remaining compute nodes have been booted up. There are now ~8,600 cores serving jobs.  June 1, 10:15 PT\nUpdate: The development nodes are now back too. For the PHI pilot project, development node pgpudev1 is back up, but pdev1 is still down.  May 28, 10:00 PT\nUpdate: Wynton is partially back up and running. Both login hosts are up (log1 and log2). The job scheduler, SGE, accepts new jobs and and launches queued jobs. Two thirds of the compute node slots are back up serving jobs. Work is done to bring up the the development nodes and the data transfer hosts (dt1 and dt2).  May 27, 10:30 PT\nUpdate: We hit more than a few snags today. Our filesystem, BeeGFS, is up and running, but it still needs some work. The login hosts are up, but SGE is not and neither are the dev nodes. We will continue the work early tomorrow Thursday.  May 26, 21:40 PT\nNotice: The Wynton HPC environment will be shut down late afternoon on Tuesday May 25, 2021, for maintenance. We expect the cluster to be back online late Wednesday May 26. To allow for an orderly shutdown of Wynton, the queues have been disabled starting at 3:30 pm on May 25. Between now and then, only jobs whose runtimes end before that time will be able to start. Jobs whose runtimes would run into the maintenance window will remain in the queue.  May 10, 16:40 PT\nPreliminary notice: The Wynton HPC cluster will be undergoing a major upgrade on Wednesday May 26, 2021. As usual, starting 15 days prior to this day, on May 11, the maximum job run-time will be decreased on a daily basis so that all jobs finishes in time, e.g. if you submit a job on May 16 with a run-time longer than nine days, it will not be able to scheduled and it will be queued until after the downtime.  May 3, 11:00 PT\n\n\n\n\nJune 1-2, 2021\n\nPassword management outage\nResolved: Password updates works again.  June 2, 10:30 PT\nNotice: Due to technical issues, it is currently not possible to change your Wynton password. If attempted from the web interface, you will get an error on “Password change not successful! (kadmin: Communication failure with server while initializing kadmin interface )”. If attempted using ‘passwd’, you will get “passwd: Authentication token manipulation error”.  June 1, 10:30 PT\n\n\n\nApril 28 - May 7, 2021\n\nMany GPU nodes down (due to cooling issues)\nResolved: Cooling has been restored and all GPU nodes are back online again.  May 7, 11:10 PT\nUpdate: Half of the GPU nodes that was taken down are back online. Hopefully, the remaining ones can be brought back up tomorrow when the cooling in the server room should be fully functioning again.  May 6, 14:30 PT\nNotification: One of Wynton’s ancillary server rooms is having cooling issues. To reduce the heat load in the room, we had to turn off all the Wynton nodes in the room around 09:45 this morning. This affects GPU nodes named msg*gpu* and a few other regular nodes. We estimate that the UCSF Facilities to fix the cooling problem by early next week.  April 28, 16:30 PT\n\n\n\n\nMarch 26, 2021\n\nCluster not accessible (due to network outage)\nResolved: The malfunctioning network link between two of Wynton’s data centers, which affected our BeeGFS file system and Wynton HPC as a whole, has been restored. March 26, 21:30 PT\nNotification: Campus network issues causing major Wynton HPC issues including extremely slow access to our BeeGFS file system. This was first reported around 11:30 today. A ticket has been filed with the UCSF Network. ETA is unknown. March 26, 12:30 PT\n\n\n\n\nJanuary 29-February 12, 2021\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. A few compute nodes remain offline that has to be rebooted manually, which will be done as opportunity is given. February 13, 09:00 PT\nNotice: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Login, data-transfer, and development nodes will be rebooted at 13:00 on Monday February 1. January 31, 17:00 PT\n\n\n\nFebruary 1-3, 2021\n\nDevelopment node not available\nResolved: Development node dev2 is available again. February 3, 15:00 PT\nNotice: Development node dev2 is down. It failed to come back up after the kernel upgrade on 2021-02-01. An on-site reboot is planned for Wednesday February 3. February 2, 11:45 PT\n\n\n\nJanuary 28, 2021\n\nServer room maintenance\nNotice: The air conditioning system in one of our server rooms will be upgraded on January 28. The compute nodes in this room will be powered down during the upgrade resulting in fewer compute nodes being available on the cluster. Starting 14 days prior to this date, compute nodes in this room will only accept jobs that will finish in time. January 13, 10:00 PT"
  },
  {
    "objectID": "hpc/status/incidents-2018.html",
    "href": "hpc/status/incidents-2018.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2018 Q3-Q4\n\nFull downtime:\n\nScheduled: 0 hours = 0.0%\nUnscheduled: 84 hours = 3.5 days = 1.9%\nTotal: 84 hours = 3.5 days = 1.9%\nExternal factors: 100% of the above downtime, corresponding to 84 hours (=3.5 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nNone\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2018-09-28 (up to 14 days)\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2018-06-17 (23 hours) - Campus power outage\n2018-11-08 (19 hours) - Byers Hall power maintenance without notice\n2018-12-14 (42 hours) - Sandler Building power outage\n\nTotal downtime: 84 hours of which 84 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nNone.\n\nTotal downtime: 0.0 hours\n\n\n\nAccounts\n\nNumber of user account: 198 (change: +103 during the year)\n\n\n\n\nDecember 21, 2018\n\nPartial file system failure\nResolved: Parts of the new BeeGFS file system was non-functional for approx. 1.5 hours during Friday December 21 when a brief maintenance task failed. Dec 21, 20:50 PT\n\n\n\n\nDecember 12-20, 2018\n\nNodes down\nResolved: All of the `msg-* compute nodes but one are operational. Dec 20, 16:40 PT\nNotice: Starting Wednesday December 12 around 11:00, several msg-* compute nodes went down (~200 cores in total). The cause of this is unknown. Because it might be related to the BeeGFS migration project, the troubleshooting of this incident will most likely not start until the BeeGFS project is completed, which is projected to be done on Wednesday December 19. Dec 17, 17:00 PT\n\n\n\n\nDecember 18, 2018\n\nDevelopment node does not respond\nResolved: Development node qb3-dev1 is functional. Dec 18, 20:50 PT\nInvestigating: Development node qb3-dev1 does not respond to SSH. This will be investigated the first thing tomorrow morning (Wednesday December 19). In the meanwhile, development node qb3-gpudev1, which is “under construction”, may be used. Dec 18, 16:30 PT\n\n\n\nNovember 28-December 19, 2018\n\nInstallation of new, larger, and faster storage space\nResolved: /wynton/scratch is now back online and ready to be used. Dec 19, 14:20 PT\nUpdate: The plan is to bring /wynton/scratch back online before the end of the day tomorrow (Wednesday December 19). The planned SGE downtime has been rescheduled to Wednesday January 9. Moreover, we will start providing the new 500-GiB /wynton/home/ storage to users who explicitly request it (before Friday December 21) and who also promise to move the content under their current /netapp/home/ to the new location. Sorry, users on both QB3 and Wynton HPC will not be able to migrate until the QB3 cluster has been incorporated into Wynton HPC (see Roadmap) or they giving up their QB3 account. Dec 18, 16:45 PT\nUpdate: The installation and migration to the new BeeGFS parallel file servers is on track and we expect to go live as planned on Wednesday December 19. We are working on fine tuning the configuration, running performance tests, and resilience tests. Dec 17, 10:15 PT\nUpdate: /wynton/scratch has been taken offline. Dec 12, 10:20 PT\nReminder: All of /wynton/scratch will be taken offline and completely wiped starting Wednesday December 12 at 8:00am. Dec 11, 14:45 PT\nNotice: On Wednesday December 12, 2018, the global scratch space /wynton/scratch will be taken offline and completely erased. Over the week following this, we will be adding to and reconfiguring the storage system in order to provide all users with new, larger, and faster (home) storage space. The new storage will served using BeeGFS, which is a new much faster file system - a system we have been prototyping and tested via /wynton/scratch. Once migrated to the new storage, a user’s home directory quota will be increased from 200 GiB to 500 GiB. In order to do this, the following upgrade schedule is planned:\n\nWednesday November 28-December 19 (21 days): To all users, please refrain from using /wynton/scratch - use local, node-specific /scratch if possible (see below). The sooner we can take it down, the higher the chance is that we can get everything in place before December 19.\nWednesday December 12-19 (8 days): /wynton/scratch will be unavailable and completely wiped. For computational scratch space, please use local /scratch unique to each compute node. For global scratch needs, the old and much slower /scrapp and /scrapp2 may also be used.\nWednesday December 19, 2018 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline. No jobs will be able to be submitted until it is restarted.\nWednesday December 19, 2018: The upgraded Wynton HPC with the new storage will be available including /wynton/scratch.\nWednesday January 9, 2019 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline temporarily. No jobs will be able to be submitted until it is restarted.\n\nIt is our hope to be able to keep the user’s home accounts, login nodes, the transfer nodes, and the development nodes available throughout this upgrade period.\nNOTE: If our new setup proves more challenging than anticipated, then we will postpone the SGE downtime to after the holidays, on Wednesday January 9, 2019. Wynton HPC will remain operational over the holidays, though without /wynton/scratch. Dec 6, 14:30 PT [edited Dec 18, 17:15 PT]\n\n\n\n\nDecember 12-14, 2018\n\nPower failure\nResolved: All mac-* compute nodes are up and functional. Dec 14, 12:00 PT\nInvestigating: The compute nodes named mac-* (in the Sandler building) went down due to power failure on Wednesday December 12 starting around 05:50. Nodes are being rebooted. Dec 12, 09:05 PT\n\n\n\n\nNovember 8, 2018\n\nPartial shutdown due to planned power outage\nResolved: The cluster is full functional. It turns out that none of the compute nodes, and therefore none of the running jobs, were affected by the power outage. Nov 8, 11:00 PT\nUpdate: The queue-metric graphs are being updated again. Nov 8, 11:00 PT\nUpdate: The login nodes, the development nodes and the data transfer node are now functional. Nov 8, 10:10 PT\nUpdate: Login node wynlog1 is also affected by the power outage. Use wynlog2 instead. Nov 8, 09:10 PT\nNotice: Parts of the Wynton HPC cluster will be shut down on November 8 at 4:00am. This shutdown takes place due to the UCSF Facilities shutting down power in the Byers Hall. Jobs running on affected compute nodes will be terminated abruptly. Compute nodes with battery backup or in other buildings will not be affected. Nodes will be rebooted as soon as the power comes back. To follow the reboot progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. Unfortunately, the above queue-metric graphs cannot be updated during the power outage. Nov 7, 15:45 PT\n\n\n\n\nSeptember 28 - October 11, 2018\n\nKernel maintenance\nResolved: The compute nodes has been rebooted and are accepting new jobs. For the record, on day 5 approx. 300 cores were back online, on day 7 approx. 600 cores were back online, on day 8 approx. 1,500 cores were back online, and on day 9 the majority of the 1,832 cores were back online. Oct 11, 09:00 PT\nNotice: On September 28, a kernel update was applied to all compute nodes. To begin running the new kernel, each node must be rebooted. To achieve this as quickly as possible and without any loss of running jobs, the queues on the nodes were all disabled (i.e., they stopped accepting new jobs). Each node will reboot itself and re-enable its own queues as soon as all of its running jobs have completed. Since the maximum allowed run time for a job is two weeks, it may take until October 11 before all nodes have been rebooted and accepting new jobs. In the meanwhile, there will be fewer available slots on the queue than usual. To follow the progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. Sept 28, 16:30 PT\n\n\n\nOctober 1, 2018\n\nKernel maintenance\nResolved: The login, development, and data transfer hosts have been rebooted. Oct 1, 13:30 PT\nNotice: On Monday October 1 at 01:00, all of the login, development, and data transfer hosts will be rebooted. Sept 28, 16:30 PT\n\n\n\nSeptember 13, 2018\n\nScheduler unreachable\nResolved: Around 11:00 on Wednesday September 12, the SGE scheduler (“qmaster”) became unreachable such that the scheduler could not be queried and no new jobs could be submitted. Jobs that relied on run-time access to the scheduler may have failed. The problem, which was due to a misconfiguration being introduced, was resolved early morning on Thursday September 13. Sept 13, 09:50 PT\n\n\n\n\nAugust 1, 2018\n\nPartial shutdown\nResolved: Nodes were rebooted on August 1 shortly after the power came back. Aug 2, 08:15 PT\nNotice: On Wednesday August 1 at 6:45am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. Jul 30, 20:45 PT\n\n\n\n\nJuly 30, 2018\n\nPartial shutdown\nResolved: The nodes brought down during the July 30 partial shutdown has been rebooted. Unfortunately, the same partial shutdown has to be repeated within a few days because the work in server room was not completed. Exact date for the next shutdown is not known at this point. Jul 30, 09:55 PT\nNotice: On Monday July 30 at 7:00am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. Jul 29, 21:20 PT\n\n\n\nJune 16-26, 2018\n\nPower outage\nResolved: The Nvidia-driver issue occurring on some of the GPU compute nodes has been fixed. Jun 26, 11:55 PT\nUpdate: Some of the compute nodes with GPUs are still down due to issues with the Nvidia drivers. Jun 19, 13:50 PT\nUpdate: The login nodes and and the development nodes are functional. Some compute nodes that went down are back up, but not all. Jun 18, 10:45 PT\nInvestigating: The UCSF Mission Bay Campus experienced a power outage on Saturday June 16 causing parts of Wynton HPC to go down. One of the login nodes (wynlog1), the development node (qb3-dev1), and parts of the compute nodes are currently non-functional. Jun 17, 15:00 PT"
  },
  {
    "objectID": "hpc/status/index.html",
    "href": "hpc/status/index.html",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "day\n\n\nweek\n\n\nmonth\n\n\nyear\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\nLast known heartbeat: Loading… ⟲\n\n\n/wynton/scratch/ lagginess\n\n\n\n\n\n\n\n/wynton/home/ lagginess\n\n\n\n\n\n\n\n/wynton/group/ lagginess\n\n\n\n\n\nFigure: The total, relative processing time on the logarithmic scale for one benchmarking run to complete over time. The values presented are relative to the best case scenario when there is no load, in case the value is 1.0. The larger the relative time is, the more lag there is on file system. Annotation of lagginess ranges: 1-2: excellent (light green), 2-5: good (green), 5-20: sluggish (orange), 20-100: bad (red), 100 and above: critical (purple).\nDetails: These benchmarks are run every ten minutes from different hosts and toward different types of the file system. These metrics are based on a set of commands, part of the wynton-bench tool, that interacts with the file system that is being benchmarked. The relevant ones are: reading a large file from /wynton/home/, copying that large archive file to and from the BeeGFS path being benchmarked, extracting the archive to path being benchmarked, find one file among the extracted files, calculating the total file size, and re-archiving and compressing the extracted files. When there’s minimal load on /wynton, the processing time is ~19 seconds. In contrast, when benchmarking local /scratch, the total processing time is about three seconds.\nWhen BeeGFS struggles to keep up with metadata and storage requests, the BeeGFS lagginess goes up. We can use beegfs-ctl --serverstats --perserver --nodetype=meta and beegfs-ctl --serverstats --perserver --nodetype=storage to see the amount of BeeGFS operations that are queued up (qlen) per metadata and storage server. When things run smoothly, the queue lengths should be near zero (qlen less than ten). When BeeGFS struggles to keep up, we typically find large qlen values for one or more servers. To see if the BeeGFS load is high due to file storage or metadata I/O performed by specific users, we can use beegfs-ctl --userstats .... For example, beegfs-ctl --userstats --names --interval=10 --maxlines=5 --nodetype=storage summarizes storage operations every ten seconds and list the five users with the most operations. Similarly, beegfs-ctl --userstats --names --interval=10 --maxlines=5 --nodetype=meta shows metadata operations per user.\n\n\n\nDetailed statistics on the file-system load and other cluster metrics can be found on the Wynton HPC Grafana Dashboard. To access this, make sure you are on the UCSF network. Use your Wynton HPC credential to log in.\n\n\n\n\n\nStatus on compute nodes unknown, which happens when for instance the job scheduler is down.\n\n\n\n\n\n\n\n\n\nNotice: The cluster will down for maintenance from 3:00 pm on Wednesday November 12 until 6:00 pm on Thursday November 13, 2025. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Jobs with runtimes that go into the maintenance window will be started after the downtime. Starting October 29 at 4:00pm, jobs relying on the default 14-day runtime will not be launched until after the downtime. UCSF Facilities will perform annual fire inspection activities to remain compliant with regulations. The network team will update a core switch. The Wynton team will take the opportunity to implement kernel updates during this period. October 29, 12:00 PT\n\n\n\n\n\n\n\n\n\n\nUpdate: There was another burst of “can’t get password entry for user” errors starting on 2025-01-26 around 15:30, causing jobs to fail immediately. We are restarting the SSSD service on the ~140 compute nodes we have identified suffer from this problem. January 27, 11:45 PT\nUpdate: To lower the risk for this problem to occur, the SSSD timeout limit was increased from 10 seconds to 30 seconds. November 20, 2023, 10:00 PT\nUpdate: The “can’t get password entry for user” error happens on some compute nodes where the System Security Services Daemon (SSSD) has failed. Until the cause for failed SSSD has been identified and resolved, the only solution is to resubmit the job. November 17, 2023, 09:30 PT\nNotice: Some jobs end up in an error state (Eqw) with an error “can’t get password entry for user”alice”. Either user does not exist or error with NIS/LDAP etc.” November 16, 2023, 17:00 PT\n\n\n\n\n\n\n\nNotice: Passwords can be changed via the web interface. It is still not possible to change it via the command-line while logged in to Wynton. November 13, 11:00 PT\nNotice: It is not possible to change or reset passwords since 2023-11-05. This problem was introduced while doing cluster-wide upgrades to Rocky 8. November 11, 09:00 PT\n\n\n\n\n\n\n\n2025\n\n\n2024\n\n\n2023\n\n\n2022\n\n\n2021\n\n\n2020\n\n\n2019\n\n\n2018\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 505 hours (= 21.0 days)\nTotal: 505 hours (= 21.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2025-01-09 – 2025-01-09 ( 1.25 hours)\n2025-01-17 – 2025-01-22 (81.75 hours)\n2025-02-21 – 2025-03-07 (61.0 hours)\n2025-03-31 – 2025-04-01 (17.0 hours)\n2025-04-11 – 2025-04-14 (62.0 hours)\n2025-05-29 – 2025-06-10 (282.0 hours)\n\nTotal downtime: 505 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\n\n\nResolved: Wynton logins are available as of noon today. At that time we will start unsuspending jobs. We lost about 50 TiBs (0.7%) of compressed data from 6550 TiBs with the group storage pool for files in /wynton/group, /wynton/protected/group, and /wynton/protected/project. See Wynton Announcement email for further details. June 10, 12:00 PT\nUpdate: We plan to resume operations by the weekend, given that the current backup and the necessary, manual one-at-the-time replacement of multiple drives completes in time. Files that lived on the failed storage pool are broken and cannot not be fully read, but possible partially. Home directories are unaffected. The affected files live under /wynton/group, /wynton/protected/group, and /wynton/protected/project. We are scanning the file system to identify exactly which files are affected - this is a slow processes. We will share file lists with affected groups. Eventually, any broken files have to be deleted. June 4, 14:00 PT\nUpdate: Wynton jobs and logins are still paused until further notice. Our team is working on determining all of the files that may be corrupt/unavailable and will work with the vendor on the best course of action. We do not yet have an estimate on when we will be back up. May 30, 10:40 PT\nNotice: Jobs and logins have been paused until further notice. Our team is actively troubleshooting and coordinating with the vendor. A drive was replaced today and was in the process of resilvering when two more drives failed, totally three failed drives, which causes significant problems. Data corruption is expected. May 29, 18:20 PT\n\n\n\n\n\n\n\nResolved: All cluster jobs and queues were unsuspended at 02:00 this night. April 14, 08:15 PT\nNotice: All cluster jobs have been suspended in order to allow multiple metadata mirror resyncing processes to complete. These processes are what led to the hanging episodes that we have been seeing. Interactive nodes remain available. Resyncing processes are estimated to complete by Monday. April 11, 12:00 PT\n\n\n\n\n\n\n\nResolved: Queues and jobs are re-enabled. April 1, 12:00 PT\nUpdate: Login is re-enabled. Queues and jobs remains suspended. March 31, 20:15 PT\nNotice: BeeGFS metadata servers are experiencing issues. We have suspended all queues and jobs and disabled logins. We will work with the file system vendor to resolve the issue. March 31, 19:00 PT\n\n\n\n\n\n\n\nResolved: We have resumed the scheduler and jobs are being processed again. We identified several problems related to the BeeGFS file system that could have contributed to the recent, severe performance degradation. Specifically, the process that automatically removes files older than 14 days from /wynton/scratch/ failed to complete, which resulted in close to 100% full storage servers. We believe this issues started in November 2024 and has gone unnoticed until now. We do not understand why these cleanup processes had failed, but one hypothesis is that there are corrupt files or folders where the cleanup process gets stuck, preventing it from cleaning up elsewhere. It might be that these problems have caused our metadata servers resynchronizing over and over - resynchronization itself is an indication that something is wrong. We are in the process of robustifying our cleanup process, putting in monitoring systems to detect these issues before system degradation takes place. March 7, 11:30 PT\nNotice: We have decided to again suspending all running jobs and disable the queue from taking on new jobs. March 5, 15:00 PT\nNotice: Resynchronization of BeeGFS metadata server pair (42,52) finished after 23 hours. March 4, 14:00 PT\nNotice: Resynchronization of BeeGFS metadata server pairs (32,22) and (23,33) started 2025-03-03, and (42,52) on 2025-03-04. March 4, 09:00 PT\nNotice: The job queue has been re-enabled and all suspended jobs have been released. February 28, 09:00 PT\nNotice: Login and file transfers to Wynton has been re-enabled. February 28, 09:00 PT\nNotice: Resynchronization of BeeGFS metadata server pair (41,51) completed after 24 hours, and pair (63,73) completed after 18 hours. February 28, 09:00 PT\nNotice: In order to speed up resynchronization of metadata servers, we have decided to minimize the load on the file system by suspending all running jobs, disable login to Wynton, and disable all file transfers to and from Wynton. February 27, 16:30 PT\nNotice: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (51,73). February 27, 16:15 PT\nNotice: Resynchronization of BeeGFS metadata server pair meta22 and meta32 completed after 30 hours. February 27, 06:00 PT\nNotice: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (22,32). February 26, 19:30 PT\nNotice: We are working with the vendor to try to resolve this problem. February 26, 09:00 PT\nNotice: The file system is again very slow. delays when working interactively and jobs to slow down. February 25, 15:15 PT\nNotice: The file system is again very slow. February 25, 10:00 PT\nNotice: The file system is very slow, which result in long delays when working interactively and jobs to take longer than usual. February 21, 16:00 PT\n\n\n\n\n\n\n\nResolved: Login node plog1 respects SSH keys again. February 24, 2025, 11:15 PT\nUpdate: Login node plog1 is available again, but does not respect SSH keys. February 24, 2025, 10:30 PT\nUpdate: Data-transfer node dt1 is available again. February 24, 2025, 10:30 PT\nUpdate: With the exception for plog1 and dt1, all login, data-transfer, and development nodes have been rebooted. Until plog1 is available, PHI-users may use pdt1 and pdt2 to login into the cluster. February 22, 2025, 13:30 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Saturday, February 22, 2025 at 13:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. February 21, 2025, 12:15 PT\n\n\n\n\n\n\nResolved: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is available again after data-transfer node dt1 coming online. February 24, 2025, 10:30 PT\nNotice: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is unavailable, because data-transfer node dt1 is unavailable. February 22, 2025, 13:30 PT\n\n\n\n\n\n\nResolved: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are functional again. February 24, 2025, 09:30 PT\nUpdate: The vendor has escalated our support ticket. February 19, 2025, 13:30 PT\nNotice: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are currently unavailable. The former gives an error on “Unknown user or wrong password”, and the latter “Authentication Required - Identity set contains an identity from an allowed domain, but it does not map to a valid username for this connector”. The regular ‘Wynton HPC’ Globus endpoint is unaffected and available. The problem has been there since at least 2025-02-14 at 22:36, when I user reported it. February 19, 2025, 12:00 PT\n\n\n\n\n\n\nResolved: Wynton is fully operational again. The BeeGFS file system issue has been resolved. All data consistency has been verified. Working with the vendor, we have identified a potential bug in the BeeGFS quota system that caused the BeeGFS outage. That part is still under investigation in order to minimize and remove the risk of reoccurrence. January 22, 12:15 PT\nUpdate: The login and data-transfer nodes are available again. January 22, 11:00 PT\nUpdate: The third resynchronization completed successfully. January 21, 18:30 PT\nUpdate: Further investigation of the failed resynchronization this morning indicated that the resynchronization did indeed keep running while it stopped producing any output and the underlying BeeGFS service was unresponsive. Because of this, we decided to not restart the resynchronization, but instead let it continue in the hope it will finish. But, by not restarting, Wynton will remain inaccessible. Our first objective is to not jeopardize the cluster, the second objective is to bring the system back online. January 21, 15:15 PT\nUpdate: The cluster is unavailable again. The past resynchronization of the problematic BeeGFS metadata server failed again, which triggers the problem. We are communicating with the vendor for their support. January 21, 09:45 PT\nUpdate: The cluster is available again, but the scheduler has been paused. No queued jobs are launched and running jobs have been suspended, but will resume when the pause of scheduler is removed. This is done to minimize the load on BeeGFS, which will simplify troubleshooting and increase the chances to stabilize BeeGFS. It is the same BeeGFS metadata server as before that is experiencing problems. January 19, 13:45 PT\nUpdate: The cluster is unavailable again. January 19, 12:45 PT\nUpdate: The cluster is working again. We have started a resynchronization of the problematic BeeGFS metadata server pair meta22 and meta32. January 18, 13:45 PT\nUpdate: First signs of the cluster coming back online again, e.g. queued jobs are launched, and it is possible to access the cluster via SSH. January 18, 06:00 PT\nUpdate: Identifies a specific BeeGFS metadata server that is unresponsive. The BeeGFS vendor has been contacted. January 18, 01:00 PT\nUpdate: The underlying problem appears to be BeeGFS. The storage servers are okay, but one or more metadata servers are unresponsive. January 17, 21:30 PT\nNotice: The cluster is unavailable, e.g. i is not possible to access the login or the data-transfer nodes. January 17, 19:45 PT\n\n\n\n\n\n\n\nResolved: The cluster full operational again. Suspended jobs have been resumed. The BeeGFS issue has been resolved. Checked hardware and cables. Rebooted affected BeeGFS server. January 9, 16:20 PT\nNotice: An issue with BeeGFS was detected. All Wynton jobs have been paused until further notice. January 9, 15:10 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 137.0 hours (= 5.7 days) = 1.6%\nUnscheduled: 142.3 hours (= 5.9 days) = 1.6%\nTotal: 279.3 hours (= 11.6 days) = 3.2%\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2024-06-17 – 2024-06-18 (32.0 hours)\n2024-10-14 – 2024-10-18 (105.0 hours)\n\nTotal downtime: 137.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2024-04-03 (~500 hours)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2024-03-14 (13.0 hours)\n2024-03-17 (15.0 hours)\n2024-05-31 (2.3 hours)\n2024-06-15 – 2024-06-21 (112.0 hours; excluding 32 hours scheduled maintenance)\n\nTotal downtime: 142.3 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\n\n\nResolved: The cluster is back online. October 18, 17:00 PT\nUpdate: The cluster including all its storage is offline undergoing a scheduled maintenance. October 14, 11:00 PT\nNotice: The cluster will be shut down for maintenance from 8:00 am on Monday October 14 until 5:00 pm on Friday October 18, 2024. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Starting 14 days before, the maximum job runtime will be decreased on a daily basis from the current 14 days down to one day so that jobs finish in time before the shutdown. Jobs with runtimes that go into the maintenance window will be started after the downtime. The reason for the downtime is that UCSF Facilities will perform maintenance affecting cooling in our data center. We will take this opportunity to perform system updates and BeeGFS maintenance. September 20, 16:45 PT\n\n\n\n\n\n\n\nResolved: All interactive nodes have been updated and deployed with the new CGroups limits. September 13, 13:00 PT\nNotice: All interactive nodes will be shutdown and rebooted on Thursday September 12 at 12:30 to update Linux kernels and deploy CGroups-controlled CPU and memory user limits. To avoid data loss, please save your work and logout before. Queued and running jobs are not affected. September 11, 09:15 PT\n\n\n\n\n\n\nResolved: 14,000 compute slots are now available, which corresponds to the majority of compute nodes. June 25, 00:30 PT\nUpdate: We will go ahead and re-enable the remaining compute nodes. June 24, 13:00 PT\nUpdate: Development nodes are available. We have also opened up 100 compute nodes. We will keep monitoring BeeGFS over the weekend with the plan to re-enable the remaining compute nodes if all go well. June 21, 19:15 PT\nUpdate: The login and data-transfer nodes are available. We will continue to validate BeeGFS during the day with the intent to open up the development nodes and a portion of the compute nodes before the weekend. June 21, 12:45 PT\nUpdate: We decided to replace the problematic chassis with a spare. The RAID file system has two failing drives, which are currently being restored. We expect this to finish up in the morning. Then, we will replace those two failing drives and proceed with another restore. If that succeeds, we plan to open up the login nodes to make files available again. After that, the goal is to slowly open up the queue and compute nodes over the weekend. June 20, 23:30 PT\nUpdate: We had folks onsite today to complete some preventative maintenance on all of the disk chassis (and, in a fit of optimism, bring up all of the nodes to prepare for a return to production). As this maintenance involved new firmware, we had some hope that it might sort out our issues with the problematic chassis. Unfortunately, our testing was still able to cause an issue (read: crash). We’ve sent details from this latest crash to the vendor and we’ll be pushing hard to work with them tomorrow Thursday to sort things out. June 20, 00:15 PT\nUpdate: The vendor is still working on diagnosing our disk chassis issue. That work will resume after Wednesday’s holiday. So, unfortunately, we will not be able to bring Wynton up on Wednesday. We hope to come up on Thursday, but it all depends on our testing and the vendor’s investigation. June 19, 01:00 PT\nUpdate: We are working with both the system and chassis vendors to diagnose this and determine what the problem is and how to fix it. This process is taking much longer than we’d like, and it is looking increasingly unlikely that we’ll be in a position to bring Wynton back online today. June 18, 14:00 PT\nUpdate: A disk chassis that hosts part of /wynton/home appears to be failing. It works for a while and then fails, which brings down /wynton. We are trying to keep it running as much as possible, but can’t make any promises. June 16, 00:15 PT\nNotice: Wynton is currently down due to an unknown issue. The problem started around 15:00 on Saturday 2024-06-15. June 15, 23:15 PT\n\n\n\n\n\n\n\nUpdate: All but one of the planned maintenance upgrades were completed during this scheduled maintenance. The remain upgrade does not require a downtime and will be done in a near future without disrupting the cluster. June 18, 17:00 PT\nUpdate: Wynton is down for maintenance as of 09:00 on Monday 2024-06-17. June 17, 09:00 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Monday June 17 until 5:00 pm on Tuesday June 18, 2024. Starting June 3, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. June 5, 09:00 PT\n\n\n\n\n\n\n\nResolved: Development nodes are available again. June 10, 10:25 PT\nNotice: Development nodes are inaccessible since Friday June 7 at 17:00. We will investigate the problem on Monday. June 8, 05:45 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS issue has been resolved. Wynton is operational again. May 31, 09:20 PT\nNotice: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started around 06:00. We’re working on it and will post updates as we know more. May 31, 08:45 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. April 25, 09:00 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. April 4, 11:15 PT\nUpdate: A new set of kernel updates will be rolled out. Login, data-transfer, and development nodes will be rebooted briefly on Thursday April 11 at 11:00. All compute nodes will also have to be drained and rebooted, which might take up to two weeks. Some of the compute have been draining since last week, meaning that will only have been drain for at most another week. April 10, 16:00 PT\nUpdate: Hosts dt1 and plog1 are now also available. April 4, 12:15 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. It will take some more time before dt1 and plog1 are available again, because they did not come back as expected after the reboot. April 4, 11:15 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Thursday April 4 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. April 3, 17:30 PT\n\n\n\n\n\n\nResolved: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. March 18, 10:30 PT\nNotice: Wynton is currently down due to an unknown BeeGFS issues. The problem started around 19:30 on 2024-03-17. We’re working on it and will post updates as we know more. March 18, 09:00 PT\n\n\n\n\n\n\n\nResolved: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. March 14, 15:15 PT\nNotice: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started at 02:11 this morning. We’re working on it and will post updates as we know more. March 14, 09:15 PT\n\n\n\n\n\n\n\nResolved: All hosts are available. February 3, 17:00 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. It will take some more time before plog1, dt1, and dev2 are available again, because they did not come back as expected after the reboot. PHI users may use pdt1 and pdt2 to access the cluster. February 2, 14:45 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Friday February 2 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. February 1, 23:30 PT\n\n\n\n\n\n\nResolved: UCSF Facilities has resolved the cooling issue and there are again two working chillers. As a fallback backup, the building is now connected to the campus chilled water loop. This was confirmed by UCSF Facilities on 2024-12-10. July-August, 2024\nUpdate: UCSF Facilities performed testing for rerouting of updated chilled-water piping the building where the Wynton data center is hosted between 07-12 on 2024-05-08. May 9, 12:30 PT\nUpdate: The compute and development nodes are available again. Jobs that were running when we did the emergency shutdown should be considered lost and need to be resubmitted. UCSF Facilities has re-established cooling, but there is currently no redundancy cooling system available, meaning there is a higher-than-usual risk for another failure. January 25, 15:45 PT\nNotice: We are shutting down all Wynton compute and development nodes as an emergency action. This is due to a serious issue with the chilled-water system that feeds the cooling in the Wynton data center. By shutting down all of the compute nodes, we hope to slow the current temperature rise, while keeping the storage system, login and data-transfer nodes up. The will come back up again as soon as the UCSF Facilities has resolved the chilled-water system. ETA is currently unknown. January 25, 11:25 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 141.0 hours = 5.9 days = 1.6%\nUnscheduled: 742.25 hours = 30.9 days = 8.5%\nTotal: 883.25 hours = 35.3 days = 10.1%\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2023-02-22 (17.0 hours)\n2023-05-17 (20.0 hours)\n2023-10-30 – 2023-11-03 (104.0 hours)\n\nTotal downtime: 141.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2023-05-17 – 2023-06-01 (359.0 hours)\n2023-10-27 – 2023-11-15 (347.25 hours, excluding the scheduled 5-day downtime)\n\nTotal downtime: 742.25 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\n\n\nResolved: All compute nodes are up and running. December 15, 09:00 PT\nUpdate: A total of ~15,000 CPU cores are now up and running. November 27, 15:00 PT\nUpdate: A total of ~14,000 CPU cores are now up and running. November 26, 02:00 PT\nUpdate: A total of ~13,000 CPU cores are now up and running. November 22, 15:30 PT\nUpdate: A total of ~12,000 CPU cores are now up and running. November 22, 01:00 PT\nUpdate: A total of ~10,000 CPU cores are now up and running. November 21, 01:00 PT\nUpdate: 98 compute nodes with a total of 2,780 CPU cores are now up and running. November 16, 15:00 PT\nNotice: As we come back from the downtime, we start out with 36 out of 490 compute nodes available to process jobs. Work continues to migrating the remaining nodes to Rocky 8. November 15, 14:15 PT\n\n\n\n\n\n\n\nUpdate: The job scheduler is available and jobs are running. The data-transfer nodes are available. At this time, 36 out of 490 compute nodes have been re-enabled. Work has begun booting up the remaining ones. The first jobs were processed around 09:00 this morning. November 15, 14:15 PT\nUpdate: We plan to re-enable the job scheduler and start processing jobs by the end of today. It is possible to submit jobs already now, but they will remain queued until we re-enable the scheduler. November 15, 10:30 PT\nUpdate: The BeeGFS issue has been resolved, which allows us to move forward on the remaining Rocky-8 updates. We hope to start bringing compute nodes online as soon as tomorrow (2023-11-15). November 14, 13:15 PT\nUpdate: Still status quo; the BeeGFS issue holds us back from bringing the scheduler back up. We’re rather certain that we will not be able to resolve it today or tomorrow. November 13, 13:45 PT\nUpdate: Login and development nodes are available. Write access to the BeeGFS file system has been re-enabled. Due to continued issues in getting BeeGFS back in stable state, we are still not ready for opening up the scheduler and compute nodes. November 11, 00:30 PT\nUpdate: Unfortunately, we will not bring up Wynton to run jobs today. We are evaluating what, if anything, may be possible to bring up before the long weekend. The reason being that the required metadata resynchronization failed late yesterday. The vendor has provided us with a script to fix the failure. That script is running, and once it’s done, we’ll reattempt to resynchronize. November 9, 10:30 PT\nUpdate: We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. At that time, we expect all login, all data-transfer, and most development nodes will be available. A large number of the compute nodes will also be available via the scheduler. November 8, 10:30 PT\nUpdate: The team makes progress on the scheduled downtime activities, which was delayed due to the BeeGFS incident. We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. November 7, 11:20 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Monday October 30 through end of business on Friday November 3, 2023 (2023W44). The operating system will be upgraded system wide (all machines) from CentOS 7.9 to Rocky 8 Linux, the BeeGFS will be upgrade, and old hardware will be replaced. UCSF Facilities will perform scheduled work. After the downtime, there will no longer be any machine running CentOS 7.9. All machines will have their local disks (including /scratch and /tmp) wiped. Anything under /wynton (including /wynton/scratch, /wynton/home, …) should be unaffected, but please note that Wynton does not back anything up, so we recommend you to back up critical data. For more information about the Rocky 8 Linux migration project and how you can prepare for it is available at on the Migration to Rocky 8 Linux from CentOS 7 page. October 13, 11:15 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS metadata resynchronization is complete around 02:30 this morning. November 14, 13:15 PT\nUpdate: The BeeGFS metadata resynchronization is still unresolved. We are looking into other strategies, which we are currently testing. If those tests are successful, we will attempt to deploy the fix in the production. November 13, 13:45 PT\nUpdate: After resynchronization of the BeeGFS metadata kept failing, we identified a possible culprit. We suspect BeeGFS cannot handle the folders with many millions of files, causing the resynchronization to fail. We keep working on stabilizing BeeGFS. November 11, 00:45 PT\nUpdate: The BeeGFS metadata resynchronization that had been running for several hours, failed late yesterday. The vendor has provided us with a script tailored to fix the issue we ran into. That script is running, and once it’s done, we’ll start the resynchronization again. November 9, 10:30 PT\nUpdate: The recovery from the BeeGFS incident goes as planned. We estimate to have resolved this issue by the end of November 9, 2023, when full read-write access to /wynton will be available again. November 8, 10:30 PT\nUpdate: The Wynton team works on fixing and stabilizing the BeeGFS incident. We estimate to have resolved this issue by the end of November 9, 2023. November 7, 11:20 PT\nUpdate: Read-only access to Wynton has been enabled for users to retrieve their files. Login nodes log1 and plog1 are available for this. If going through the Wynton 2FA, make sure to answer “no” (default) when prompted for “Remember connection authentication from 98.153.103.186 for 12 hours? [y/N]”; answering “yes” causes the SSH connection to fail. November 5, 00:30 PT\nUpdate: Wynton admins can retrieve user files under /wynton/ upon requests until 18:00 today, when the UCSF network will go down. We are not able to share the PHI data under /wynton/protected/. Please contact support with all details including full path of the data to be retrieved. October 30, 15:30 PT\nUpdate: The BeeGFS issue is related to a CentOS 7-kernel bug in one of our BeeGFS metadata servers. To minimize the risk of data loss on the /wynton file system, we took the decision to shut down Wynton immediately. At the moment, we do not have an estimate on how long it will take to resolve this problem. It has to be resolved before we can begin the major upgrade scheduled for 2023W44. October 27, 16:30 PT\nNotice: The BeeGFS file system, which hosts /wynton, is experiencing unexpected, major issues. Some or all files on /wynton cannot be accessed, and when attempted, an Communication error on send error is seen. The problem started around 13:45 on Friday 2023-10-27. October 27, 15:10 PT\n\n\n\n\n\nResolve: Login node log2 and data-transfer node dt1 are available again. October 26, 12:15 PT\nUpdate: Development node dev2 is available again. October 24, 12:45 PT\nNotice: Access to login node log2, data-transfer nodes dt1, and development node dev2 will be disabled from Monday-Friday October 23-27, 2023 (2023W43) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node log1, data-transfer nodes dt2, and development nodes dev1 and dev3 are unaffected, so are the Wynton HPC Globus endpoints. October 23, 11:10 PT\n\n\n\nResolved: Login node log1, data-transfer nodes dt2 and pdt2 are available again and are now running Rocky 8. October 20, 17:00 PT\nNotice: Data-transfer nodes dt2 will be disabled this week instead of dt1 as previously announced. October 16, 14:30 PT\nNotice: Access to login node log1, data-transfer nodes dt1, and pdt2 will be disabled from Monday-Friday October 16-20, 2023 (2023W42) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node log2, data-transfer nodes dt2, and pdt1 are unaffected, so are the Wynton HPC Globus endpoints. October 13, 11:20 PT\n\n\n\n\n\nResolved: All corrupted and orphaned files have now been deleted. There might be orphaned directories remaining, which we leave to each user to remove, if they exist. April 3, 2024, 11:15 PT\nUpdate: Reading files whose data was lost on the unrecovered storage targets back in May no longer results in an error message. Instead, the portion of the file that was lost will be replaced by null bytes. Obviously, this results in a file with corrupt content. The admins will be going through and deleting all the corrupted files as soon as possible. It’s a big task and will take some time. July 13, 14:15 PT\nUpdate: The remaining two ZFS storage targets (22004 and 22006) are back online again. July 11, 10:30 PT\nUpdate: Four out of the six ZFS storage targets have been brought back online. Two targets (22004 and 22006) remain offline. If you encounter a “Communication error on send” error, please do not delete or move the affected file. July 6, 17:00 PT\nUpdate: Six ZFS storage targets (22001-22006) are down, because one of the recovered storage targets encountered latent damage that had gone undetected since the recovery in May. This locked up the server and thus all six targets on that server. July 6, 08:30 PT\nUpdate: The final two ZFS storage targets are now serving the BeeGFS file system (/wynton) again. June 30, 11:00 PT\nUpdate: We will be reintroducing the final two ZFS storage targets back into the BeeGFS file system (/wynton) on Friday June 30. The work will start at 10 am and should take an hour or so. During that time, there will be a couple of brief “blips” as we reconfigure the storage. June 29, 23:55 PT\nUpdate: Organizing the data recovered from ZFS storage target 22004 into a form suitable for BeeGFS is taking long than expected. Thus far, we’ve properly replaced 10,354,873 of the 11,351,926 recovered files. Approximately one million files remain. We now hope to complete the work this week. The automatic clean up of old files on /wynton/scratch and /wynton/protected/scratch have been resumed. June 27, 17:00 PT\nUpdate: There are two broken ZFS storage targets (22004 and 21002). We expect to recover most files on target 22004 (approximately 14 TB). The reason it takes this long to recover that storage target is that the file chunks are there, but we have to puzzle them together to reconstruct the original files, which is a slow process. We estimate this process to complete by the end of the week. The files on the other target, target 21002, are unfortunately not recoverable. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 21, 23:30 PT\nNotice: There are two ZFS storage targets that are still failing and offline. We have hopes to be able to recover files from one of them. As of June 9, about 12 TB of low-level, raw file data (out of ~15 TB) was recovered. When that is completed, we will start the tedious work on reconstructing the actual files lost. The consultants are less optimistic about recovering data from second storage target, because it was much more damaged. They will give us the final verdict by the end of the week. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 12, 16:00 PT\n\n\n\n\n\n\nResolved: The job scheduler is now available. Access to /wynton/group, /wynton/protected/group, and /wynton/protected/project has been restored. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 1, 16:00 PT\nUpdate: Wynton will be fully available later today, meaning the job scheduler and access to /wynton/group, /wynton/protected/group, and /wynton/protected/project will be re-enabled. Note, two ZFS storage targets are still faulty and offline, but the work of trying to recover them will continue while we go live. This means that any files on the above re-opened /wynton subfolders that are stored, in part or in full, on those two offline storage targets will be inaccessible. Any attempt to read such files will result in a “Communication error on send” error and stall. To exit, press Ctrl-C. Importantly, do not attempt to remove, move, or update such files! That will make it impossible to recover them! June 1, 12:15 PT\nUpdate: In total 22 (92%) out of 24 failed storage targets has been recovered. The consultant hopes to recover the bulk of the data from one of the two remaining damaged targets. The final damage target is heavily damaged, work on it will continue a few more days, but it is likely it cannot be recovered. The plan is to open up /wynton/group tomorrow Thursday with instructions what to expect for files on the damaged targets. The compute nodes and the job scheduler will also be enabled during the day tomorrow. May 31, 22:45 PT\nUpdate: In total 22 (92%) out of 24 failed storage targets has been recovered. The remaining two targets are unlikely to be fully recovered. We’re hoping to restore the bulk of the files from them, but there is a risk that we will get none back. Then plan is to bring back /wynton/group, /wynton/protected/group, and /wynton/protected/project, and re-enable the job queue, on Thursday. May 31, 01:00 PT\nUpdate: The login, data-transfer, and development nodes (except gpudev1) are now online an available for use. The job scheduler and compute nodes are kept offline, to allow for continued recovery of the failed ZFS storage pools. For the same reason, folders under /wynton/group, /wynton/protected/group, and /wynton/protected/project are locked down, except for groups who have mirrored storage. /wynton/home and /wynton/scratch are fully available. We have suspended the automatic cleanup of old files under /wynton/scratch and /wynton/protected/scratch. The ZFS consultant recovered 3 of the 6 remaining storage targets. We have now recovered in total 21 (88%) out of 24 failed targets. The recovery work will continue on Monday (sic!). May 26, 17:00 PT\nUpdate: All 12 ZFS storage targets on one server pair have been recovered and are undergoing final verification, after which that server pair is back in production. On the remaining server pair with also 12 failed ZFS storage targets, 4 targets have been recovered, 4 possibly have been, and 4 are holding out. We’re continuing our work with the consultant on those targets. These storage servers were installed on 2023-03-28, so it is only files written after that date that may be affected. We are tentatively planning on bringing up the login, data transfer and development nodes tomorrow Friday, prior to the long weekend, but access to directories in /wynton/group, /wynton/protected/group, or /wynton/protected/project will be blocked with the exception for a few groups with mirrored storage. /wynton/home and /wynton/scratch would be fully accessible. May 25, 17:00 PT\nUpdate: 8 more ZFS storage targets were recovered today. We have now recovered in total 17 (71%) out of 24 failed targets. The content of the recovered targets is now being verified. We will continue working with the consultant tomorrow on the remaining 7 storage targets. May 24, 17:00 PT\nUpdate: The maintenance and upgrade of the Wynton network switch was successful and is now completed. We also made progress of recovering the failed ZFS storage targets - 9 (38%) out of 24 failed targets have been recovered. To maximize our chances at a full recovery, Wynton will be kept down until the consultant completes their initial assessment. Details: The contracted ZFS consultant started to work on recovering the failed ZFS storage targets that we have on four servers. During the two hours of work, they quickly recovered another three targets on on the first server, leaving us with only one failed target on that server. Attempts of the same recovery method on the second and third servers were not successful. There was no time today to work on the fourth server. The work to recover the remaining targets will resume tomorrow. After the initial recovery attempt has been attempted on all targets, the consultant, who is one of the lead ZFS developers, plans to load a development version of ZFS on the servers in order to perform more thorough and deep-reaching recovery attempts. May 23, 17:00 PT\nUpdate: Wynton will be kept down until the ZFS-recovery consultant has completed their initial assessment. If they get everything back quickly, Wynton will come back up swiftly. If recovery takes longer, or is less certain, we will look at coming back up without the problematic storage targets. As the purchase is being finalized, we hope that the consultant can start their work either on Tuesday or Wednesday. The UCSF Networking Team is performing more maintenance on the switch tonight. May 22, 23:30 PT\nUpdate: The cluster will be kept offline until at least Tuesday May 23. The BeeGFS file-system failure is because 24 out of 144 ZFS storage targets got corrupted. These 24 storage targets served our “group” storage, which means only files written to /wynton/group, /wynton/protected/group, and /wynton/protected/project within the past couple of months are affected. Files under /wynton/home and /wynton/scratch are not affected. We are scanning the BeeGFS file system to identify exactly which files are affected. Thus far, we have managed to recover 6 (25%) out of the 24 failed targets. The remaining 18 targets are more complicated and we are working with a vendor to start helping us recover them next week. May 19, 10:15 PT\nUpdate: Automatic cleanup of /wynton/scratch has been disabled. May 18, 23:00 PT\nUpdate: Several ZFS storage targets that are used by BeeGFS experienced failures during the scheduled maintenance window. There is a very high risk of partial data loss, but we will do everything possible to minimize the loss. In addition, the Wynton core network switch failed and needs to be replaced. The UCSF IT Infrastructure Network Services Team works with the vendor to get a rapid replacement. May 17, 16:30 PT\nUpdate: The cluster is down and unavailable because of maintenance. May 16, 21:00 PT\nUpdate: There will be a one-day downtime starting at 21:00 on Tuesday May 16 and ending at 17:00 on Wednesday May 17. This is aligned with a planned PG&E power-outage maintenance on May 17. Starting May 2, the maximum job runtime will be decreased on a daily basis from the maximum 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will only be started after the downtime. The default run time is 14 days, so make sure to specify qsub -l h_rt=&lt;run-time&gt; ... if you want something shorter. May 3, 10:00 PT\nUpdate: The updated plan is to only have a 24-hour downtime starting the evening of Tuesday May 16 and end by the end of Wednesday May 17. This is aligned with a planned PG&E power-outage maintenance on May 17. April 24, 11:00 PT\nUpdate: The updated plan is to have the downtime during the week of May 15, 2023 (2023W20). This is aligned with a planned PG&E power-outage maintenance during the same week. March 27, 11:00 PT\nNotice: We will performing a full-week major update to the cluster during late Spring 2023. Current plan is to do this during either the week of May 8, 2023 (2023W19) or the week of May 15, 2023 (2023W20). February 27, 11:00 PT\n\n\n\n\n\n\n\n\nResolved: The cluster maintenance has completed and the cluster is now fully operational again. February 23, 14:00 PT\nUpdate: The cluster has been shut down for maintenance. February 22, 21:00 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Wednesday February 22 until 5:00 pm on Thursday February 23, 2023. This is done to avoid possible file-system and hardware failures when the UCSF Facilities performs power-system maintenance. During this downtime, we will perform cluster maintenance. Starting February 8, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. February 9, 09:00 PT\n\n\n\n\n\n\n\nResolve: Network issues has been resolved and access to all login and data-transfer has been re-established. The problem was physical (a cable was disconnected). January 24, 16:00 PT\nNotice: There is no access to non-PHI login and data-transfer hosts (log[1-2], dt[1-2]). We suspect a physical issue (e.g. somebody kicked a cable), which means we need to send someone onsite to fix the problem. January 24, 14:45 PT\n\n\n\n\n\n\nResolved: The network issue for the proxy servers has been fixed. All development nodes now have working internet access. January 11, 16:00 PT\nWorkarounds: Until this issue has been resolved, and depending on needs, you might try to use a data-transfer node.Some of the software tools on the development nodes are also available on the data-transfer nodes, e.g. curl, wget, and git. January 11, 09:50 PT\nNotice: The development nodes have no internet access, because the network used by out proxy servers is down for unknown reasons. The problem most likely started on January 10 around 15:45. January 11, 09:00 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 94.0 hours = 3.9 days = 1.1%\nUnscheduled: 220.0 hours = 9.2 days = 2.5%\nTotal: 314.0 hours = 13.1 days = 3.6%\nExternal factors: 36% of the above downtime, corresponding to 114 hours (= 4.8 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2022-02-08 (53.5 hours)\n2022-09-27 (40.5 hours)\n\nTotal downtime: 94.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2022-08-05 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2022-09-06 (66 hours)\n\nTotal downtime: 66 hours of which 66 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2022-03-28 (1 hours): Major BeeGFS issues\n2022-03-26 (5 hours): Major BeeGFS issues\n2022-03-18 (100 hours): Major BeeGFS issues\n\nTotal downtime: 106.0 hours of which 0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2022-03-26 (48 hours): Data-center cooling issues\n\nTotal downtime: 48 hours of which 48 hours were due to external factors\n\n\n\n\n\nNumber of user account: 1,643 (change: +369 during the year)\n\n\n\n\n\n\n\nResolved: The BeeGFS issues have been resolved. At 05:29 this morning, a local file system hosting one of our 12 BeeGFS meta daemons crashed. Normally, BeeGFS detects this and redirects processing to a secondary, backup daemon. In this incident, this failback did not get activated and a manual intervention was needed. November 2, 09:30 PT\nNotice: The BeeGFS file system started to experience issues early morning on Tuesday 2022-11-02. The symptoms are missing files and folders. November 2, 08:15 PT\n\n\n\n\n\n\n\nResolved: The job scheduler is responsive again, but we are not certain what caused the problem. We will keep monitoring the issue. November 1, 16:30 PT\nNotice: The job scheduler, SGE, does not respond to user requests, e.g. qstat and qsub. No new jobs can be submitted at this time. The first reports on problems came in around 09:00 this morning. We are troubleshooting the problem. November 1, 10:25 PT\n\n\n\n\n\n\n\nResolved: The cluster maintenance has completed and the cluster is now fully operational again. September 29, 13:30 PT\nUpdate: The cluster has been shut down for maintenance. September 27, 21:00 PT\nNotice: Wynton will be shut down on Tuesday September 27, 2022 at 21:00. We expect the cluster to be back up by the end of the workday on Thursday September 29. This is done to avoid file-system and hardware failures that otherwise may occur when the UCSF Facilities performs maintenance to the power system in Byers Hall. We will take the opportunity to perform cluster maintenance after the completion of the power-system maintenance. September 14, 17:00 PT\n\n\n\n\n\n\n\nResolved: As of 09:20 on 2022-09-09, the cluster is back in full operation. The queues are enabled, jobs are running, and the development nodes are accepting logins. September 9, 09:35 PT\nUpdate: Login and data-transfer nodes are disabled to minimize the risk for file corruption. September 7, 12:45 PT\nNotice: The Wynton system experiencing system-wide issues, including the file system, due to a campus power glitch. To minimize the risk of corrupting the file system, it was decided to shut down the job scheduler and terminate all running jobs. The power outage at Mission Bay campus happened at 15:13. Despite diesel-generated backup power started up momentarily, it was enough to affect some of our servers. The job scheduler will be offline until the impact on Wynton is fully investigated. September 6, 16:20 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. Aug 9, 12:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Monday August 8 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~14,500 cores) in the graph above. Aug 5, 10:30 PT\n\n\n\n\n\n\nResolved: The Sali lab software module repository is back. Aug 4, 12:00 PT\nNotice: The Sali lab software module repository is back will be unavailable from around 10:30-11:30 today August 4 for maintenance. Aug 4, 03:30 PT\n\n\n\n\n\n\nResolved: The patch of the BeeGFS servers were successfully deployed by 14:30 and went without disruptions. As a side effect, rudimentary benchmarking shows that this patch also improves the overall performance. Since the troubleshooting, bug fixing, and testing started on 2022-03-28, we managed to keep the impact of the bugs to a minimum resulting in only one hour of BeeGFS stall. April 6, 17:00 PT\nUpdate: The BeeGFS servers will be updated tomorrow April 6 at 14:00. The cluster should work as usual during the update. April 5, 17:00 PT\nUpdate: Our load tests over the weekend went well. Next, we will do discrepancy validation tests between our current version and the patch versions. When those pass, we will do a final confirmation with the BeeGFS vendor. We hope to deploy the patch to Wynton in a few days. April 4, 10:30 PT\nUpdate: After a few rounds, we now have a patch that we have confirmed work on our test BeeGFS system. The plan is to do additional high-load testing today and over the weekend. April 1, 10:30 PT\nUpdate: The BeeGFS vendors will send us a patch by tomorrow Tuesday, which we will test on our separate BeeGFS test system. After being validated there, will will deploy it to the main system. We hope to have a patch deploy by the end of the week. March 28, 11:30 PT\nUpdate: We have re-enabled the job scheduler after manually having resolved the BeeGFS meta server issues. We will keep monitoring the problem and send more debug data to the BeeGFS vendors. March 28, 11:00 PT\nNotice: On Monday 2022-03-28 morning at 10:30 the BeeGFS hung again. We put a hold on the job scheduler for now. March 28, 10:30 PT\n\n\n\n\n\n\n\nResolved: The compute nodes and the job scheduler are up and running again. March 26, 11:00 PT\nNotice: The job scheduler as disabled and running jobs where terminated on Saturday 2022-03-26 around 09:00. This was done due to an emergency shutdown because the ambient temperature in the data center started to rise around 08:00 and at 09:00 it hit the critical level, where our monitoring system automatically shuts down compute nodes to prevent further damage. This resulted in the room temperature coming down to normal levels again. We are waiting on UCSF Facilities to restore cooling in the data center. March 26, 10:30 PT\n\n\n\n\n\n\n\nResolved: Just after 03:00 on Saturday 2022-03-26 morning BeeGFS hung. Recover actions were taken at 07:30 and the problem was resolved before 08:00. We have tracked down the problem occur when a user runs more than one rm -r /wynton/path/to/folder concurrently on the same folder. This is a bug in BeeGFS that vendors is aware of. March 26, 10:30 PT\n\n\n\n\n\n\n\nResolved: We have re-enabled the job scheduler, which now processes all queued jobs. We will keep working with the BeeGFS vendor to find a solution to avoid this issue from happening again. March 22, 16:30 PT\nUpdate: The BeeGFS issue has been identified. We identified a job that appears to trigger a bug in BeeGFS, which we can reproduce. The BeeGFS vendor will work on a bug fix. The good news is that the job script that triggers the problem can be tweaked to avoid hitting the bug. This means we can enable the job scheduler as soon as all BeeGFS metadata servers have synchronized, which we expect to take a few hours. March 22, 12:00 PT\nUpdate: The BeeGFS file system troubleshooting continues. The job queue is still disabled. You might experience login and non-responsive prompt issues while we troubleshoot this. We have met with the BeeGFS vendors this morning and we are collecting debug information to allow them to troubleshoot the problem on their end. At the same time, we hope to narrow in on the problem further on our end by trying to identify whether there is a particular job or software running on the queue that might cause this. Currently, we have no estimate when this problem will be fixed. We have another call scheduled with the vendor tomorrow morning. March 21, 11:45 PT\nUpdate: The BeeGFS file system is back online and the cluster can be accessed again. However, we had to put SGE in maintenance mode, which means no jobs will be started until the underlying problem, which is still unknown, has been identified and resolved. The plan is to talk to the BeeGFS vendor as soon as possible after the weekend. Unfortunately, in order to stabilize BeeGFS, we had to kill, at 16:30 today, all running jobs and requeue them on the SGE job scheduler. They are now listed as status ‘Rq’. For troubleshooting purposes, please do not delete any of your ‘Rq’ jobs. March 18, 17:05 PT\nNotification: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues. The problem, which started around 11:45 today, is being investigated. March 18, 11:55 PT\n\n\n\n\n\n\n\nNoticed: UCSF Network IT will be performing maintenance on several network switches in the evening and overnight on Monday March 14. This will not affect jobs running on the cluster. One of the switches is the one which provides Wynton with external network access. When that switch is rebooted, Wynton will be inaccessible for about 15 minutes. This is likely to happen somewhere between 22:00 and 23:00 that evening, but the outage window extends from 21:00 to 05:00 the following morning, so it could take place anywhere in that window. March 11, 10:15 PT\n\n\n\n\n\n\nResolved: Wynton is available again. March 2, 15:30 PT\nUpdate: The Wynton environment is now offline for maintenance work. February 28, 10:00 PT\nClarification: The shutdown will take place early Monday morning February 28, 2022. Also, this is on a Monday and not on a Tuesday (as previously written below). February 22, 11:45 PT\nUpdate: We confirm that this downtime will take place as scheduled. February 14, 15:45 PT\nNotice: We are planning a full file-system maintenance starting on Tuesday Monday February 28, 2022. As this requires a full shutdown of the cluster environment, we will start decreasing the job queue, on February 14, two weeks prior to the shutdown. On February 14, jobs that requires 14 days or less to run will be launched. On February 15, only jobs that requires 13 days or less will be launched, and so on until the day of the downtime. Submitted jobs that would go into the downtime window if launched, will only be launched after the downtime window. November 22, 11:45 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 64 hours = 2.7 days = 0.73%\nUnscheduled: 58 hours = 2.4 days = 0.66%\nTotal: 122 hours = 5.1 days = 1.4%\nExternal factors: 39% of the above downtime, corresponding to 47 hours (=2.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2021-05-25 (64 hours)\n\nTotal downtime: 64 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2021-01-29 (up to 14 days)\n2021-07-23 (up to 14 days)\n2021-12-08 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2021-08-26 (28 hours) - Planned Byers Hall power shutdown failed\n2021-11-09 (10 hours) - Unplanned PG&E power outage\n\nTotal downtime: 38 hours of which 38 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2021-03-26 (9 hours) - Campus networks issues causing significant BeeGFS slowness\n2021-07-23 (8 hours) - BeeGFS silently failed disks\n2021-11-05 (3 hours) - BeeGFS non-responsive\n\nTotal downtime: 20 hours of which 9 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2021-04-28 (210 hours) - GPU taken down due to server room cooling issues\n\nTotal downtime: 0 hours\n\n\n\n\n\nNumber of user account: 1,274 (change: +410 during the year)\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. Dec 23, 12:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted tomorrow Thursday December 9 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~12,500 cores) in the graph above.\nDec 8, 16:30 PT\n\n\n\n\n\n\nResolved: Data-transfer node dt1 and Globus file transfers are working again. Dec 21, 13:20 PT\nUpdate: Globus file transfers to and from Wynton are not working. This is because Globus relies on the data-transfer node dt1, which is currently down. Dec 20, 15:30 PT\nNotice: Data-transfer node dt1 has issues. Please use dt2 until resolved. The first report on this problem came yesterday at 21:30. Dec 20, 09:30 PT\n\n\n\n\n\n\nResolved: All hosts have been rebooted and are now up and running. November 9, 11:00 PT\nNotice: There was a brief PG&E power outage early Tuesday November 9 around 01:20. This affected the power on the Mission Bay campus, including the data center housing Wynton. The parts of our system with redundant power were fine, but many of the compute nodes are on PG&E-power only and, therefore, went down. As a result, lots of jobs crashed. We will restart the nodes that crashed manually during the day today. November 9, 09:10 PT\n\n\n\n\n\n\n\nResolved: Resynchronization of all file-system meta servers is complete, which concludes the maintenance. October 26, 09:45 PT\nUpdate: The maintenance work has started. October 25, 14:00 PT\nNotice: We will perform BeeGFS maintenance work starting Monday October 25 at 2:00 pm. During this work, the filesystem might be less performant. We don’t anticipate any downtime. October 21, 12:10 PT\n\n\n\n\n\n\nResolved: The corrupted filesystem has been recovered. September 10, 17:20 PT\nUpdate: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on /wynton/group/. The file recovery tools are still running. August 27, 13:05 PT\nPartially resolved: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on /wynton/group/. The file recovery tools are still running. August 27, 13:05 PT\nUpdate: The BeeGFS filesystem recovering attempt keeps running. The current plan is to bring Wynton back online while keeping the problematic BeeGFS filesystem offline. August 26, 23:05 PT\nUpdate: All of the BeeGFS servers are up and running, but one of the 108 filesystems that make up BeeGFS was corrupted by the sudden power outage. The bad filesystem is part of /wynton/group/. We estimate that 70 TB of data is affected. We are making every possible effort to restore this filesystem, which will take time. While we do so, Wynton will remain down. August 26, 14:05 PT\nNotice: The cluster is down after an unplanned power outage in the main data center. The power is back online but several of our systems, including BeeGFS servers, did not come back up automatically and will require on-site, manual actions. August 26, 09:15 PT\n\n\n\n\n\n\n\nResolved: The majority of the compute nodes have been rebooted after only four days, which was quicker than the maximum of 14 days. July 28, 08:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted at 13:00 on Friday July 23 at 1:00 pm. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above.\nJuly 23, 07:40 PT\n\n\n\n\n\n\nResolved: Wynton and BeeGFS is back online. The problem was due to failed disks. Unfortunately, about 10% of the space in /wynton/scratch/ went bad, meaning some files are missing or corrupted. It is neither possible to recover them nor identify which files or folders are affected. In other words, expect some oddness if you had data under /wynton/scratch/. There will also be some hiccups over the next several days as we get everything in ZFS and BeeGFS back into an as stable state as possible. June 24, 14:55 PT\nUpdate: We’re working hard on getting BeeGFS back up. We were not able to recover the bad storage target, so it looks like there will be some data loss on /wynton/scratch/. More updates soon. June 24, 13:45 PT\nNotification: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues since early this morning. The problem is being investigated. June 24, 07:00 PT\n\n\n\n\n\n\n\nResolved: All remaining issues from the downtime have been resolved.  June 7, 17:00 PT\nUpdate: Login node log2 can now be reached from the UCSF Housing WiFi network.  June 7, 17:00 PT\nUpdate: dt2 can now be reached from outside the Wynton cluster.  June 7, 13:15 PT\nUpdate: Login node log2 cannot be reached from the UCSF Housing WiFi network. If you are on that network, use log1 until this has been resolved.  June 2, 07:00 PT\nUpdate: Both data transfer nodes are back online since a while, but dt2 can only be reached from within the Wynton cluster.  June 1, 13:45 PT\nUpdate: A large number of of the remaining compute nodes have been booted up. There are now ~8,600 cores serving jobs.  June 1, 10:15 PT\nUpdate: The development nodes are now back too. For the PHI pilot project, development node pgpudev1 is back up, but pdev1 is still down.  May 28, 10:00 PT\nUpdate: Wynton is partially back up and running. Both login hosts are up (log1 and log2). The job scheduler, SGE, accepts new jobs and and launches queued jobs. Two thirds of the compute node slots are back up serving jobs. Work is done to bring up the the development nodes and the data transfer hosts (dt1 and dt2).  May 27, 10:30 PT\nUpdate: We hit more than a few snags today. Our filesystem, BeeGFS, is up and running, but it still needs some work. The login hosts are up, but SGE is not and neither are the dev nodes. We will continue the work early tomorrow Thursday.  May 26, 21:40 PT\nNotice: The Wynton HPC environment will be shut down late afternoon on Tuesday May 25, 2021, for maintenance. We expect the cluster to be back online late Wednesday May 26. To allow for an orderly shutdown of Wynton, the queues have been disabled starting at 3:30 pm on May 25. Between now and then, only jobs whose runtimes end before that time will be able to start. Jobs whose runtimes would run into the maintenance window will remain in the queue.  May 10, 16:40 PT\nPreliminary notice: The Wynton HPC cluster will be undergoing a major upgrade on Wednesday May 26, 2021. As usual, starting 15 days prior to this day, on May 11, the maximum job run-time will be decreased on a daily basis so that all jobs finishes in time, e.g. if you submit a job on May 16 with a run-time longer than nine days, it will not be able to scheduled and it will be queued until after the downtime.  May 3, 11:00 PT\n\n\n\n\n\n\n\nResolved: Password updates works again.  June 2, 10:30 PT\nNotice: Due to technical issues, it is currently not possible to change your Wynton password. If attempted from the web interface, you will get an error on “Password change not successful! (kadmin: Communication failure with server while initializing kadmin interface )”. If attempted using ‘passwd’, you will get “passwd: Authentication token manipulation error”.  June 1, 10:30 PT\n\n\n\n\n\n\nResolved: Cooling has been restored and all GPU nodes are back online again.  May 7, 11:10 PT\nUpdate: Half of the GPU nodes that was taken down are back online. Hopefully, the remaining ones can be brought back up tomorrow when the cooling in the server room should be fully functioning again.  May 6, 14:30 PT\nNotification: One of Wynton’s ancillary server rooms is having cooling issues. To reduce the heat load in the room, we had to turn off all the Wynton nodes in the room around 09:45 this morning. This affects GPU nodes named msg*gpu* and a few other regular nodes. We estimate that the UCSF Facilities to fix the cooling problem by early next week.  April 28, 16:30 PT\n\n\n\n\n\n\n\nResolved: The malfunctioning network link between two of Wynton’s data centers, which affected our BeeGFS file system and Wynton HPC as a whole, has been restored. March 26, 21:30 PT\nNotification: Campus network issues causing major Wynton HPC issues including extremely slow access to our BeeGFS file system. This was first reported around 11:30 today. A ticket has been filed with the UCSF Network. ETA is unknown. March 26, 12:30 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. A few compute nodes remain offline that has to be rebooted manually, which will be done as opportunity is given. February 13, 09:00 PT\nNotice: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Login, data-transfer, and development nodes will be rebooted at 13:00 on Monday February 1. January 31, 17:00 PT\n\n\n\n\n\n\nResolved: Development node dev2 is available again. February 3, 15:00 PT\nNotice: Development node dev2 is down. It failed to come back up after the kernel upgrade on 2021-02-01. An on-site reboot is planned for Wednesday February 3. February 2, 11:45 PT\n\n\n\n\n\n\nNotice: The air conditioning system in one of our server rooms will be upgraded on January 28. The compute nodes in this room will be powered down during the upgrade resulting in fewer compute nodes being available on the cluster. Starting 14 days prior to this date, compute nodes in this room will only accept jobs that will finish in time. January 13, 10:00 PT\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 123 hours = 5.1 days = 1.4%\nUnscheduled: 91.5 hours = 3.8 days = 1.0%\nTotal: 214.5 hours = 8.9 days = 2.4%\nExternal factors: 12% of the above downtime, corresponding to 26.5 hours (=1.1 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2020-08-10 (93 hours)\n2020-12-07 (30 hours)\n\nTotal downtime: 123 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2020-06-11 (up to 14 days)\n2020-12-11 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nNone\n\nTotal downtime: 0 hours\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2020-01-22 (2.5 hours) - BeeGFS failure to failed upgrade\n2020-01-29 (1.0 hours) - BeeGFS non-responsive\n2020-02-05 (51.5 hours) - Legacy NetApp file system failed\n2020-05-22 (0.5 hours) - BeeGFS non-responsive to failed upgrade\n2020-08-19 (1.5 hours) - BeeGFS non-responsive\n2020-10-21 (3 hours) - BeeGFS non-responsive\n2020-11-05 (3 hours) - BeeGFS non-responsive\n\nTotal downtime: 63.0 hours\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2020-05-28 (26.5 hours) - MSG Data Center outage affecting many GPU compute nodes\n2020-07-04 (2 hours) - SGE scheduler failed\n2020-11-04 (288 hours) - ~80 compute nodes lost due to network switch failure\n\nTotal downtime: 28.5 hours of which 26.5 hours were due to external factors\n\n\n\n\n\nNumber of user account: 864 (change: +386 during the year)\n\n\n\n\n\n\n\nResolved: Login node ‘log1.wynton.ucsf.edu’ can again be accessed from outside of the UCSF network. December 17, 14:20 PT\nNotice: Login node ‘log1.wynton.ucsf.edu’ is only accessible from within UCSF network. This is a side effect of the recent network upgrades. We are waiting for The UCSF IT Network to resolve this for us. Until resolved, please use the alternative ‘log2.wynton.ucsf.edu’ login node when connecting from outside of the UCSF network. December 8, 23:00 PT\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. December 16, 05:00 PT\nNotice: The new BeeGFS setting introduced during the upgrades earlier this week caused problems throughout the system and we need to roll them back. The compute nodes will no longer take on new jobs until they have been rebooted. A compute node will be automatically rebooted as soon as all of its running jobs have completed. Unfortunately, we have to kill jobs that run on compute nodes that are stalled and suffer from the BeeGFS issues. December 11, 13:50 PT\n\n\n\n\n\n\nResolved: All login and development nodes have been rebooted. December 12, 17:00 PT\nNotice: Login node ‘log1.wynton.ucsf.edu’ and all the development nodes will be rebooted at 4:30 PM today Friday. This is needed in order to roll back the new BeeGFS setting introduced during the upgrades earlier this week. December 11, 13:50 PT\n\n\n\n\n\n\nResolved: The upgrade has been completed. The cluster back online, including all of the login, data-transfer, and development nodes, as well as the majority of the compute nodes. The scheduler is processing jobs again. All hosts now run CentOS 7.9. December 8, 16:30 PT\nUpdate: The upgrade is paused and will resume tomorrow. We hope to be bring all of the cluster back online by the end of tomorrow. For now, login node ‘log2’ (but not ‘log1’), and data-transfer nodes ‘dt1’, and ‘dt2’ are back online and can be used for accessing files. Development nodes ‘dev1’ and ‘dev3’ are also available (please make sure to leave room for others). The scheduler remains down, i.e. it is is not be possible to submit jobs. December 7, 17:00 PT\nUpdate: The upgrades have started. Access to Wynton HPC has been disable as of 10:30 this morning. The schedulers stopped launching queued jobs as of 23:30 last night. December 7, 10:30 PT\nRevised notice: We have decided to hold back on upgrading BeeGFS during the downtime and only focus on the remain parts including operating system and network upgrades. The scope of the work is still non-trivial. There is a risk that the downtime will extend into Thursday December 10. However, if everything go smoothly, we hope that Wynton HPC will be back up by the end of Monday or during the Tuesday. There will only be one continuous downtime, that is, when the cluster comes back up, it will stay up. December 3, 09:00 PT\nNotice: Starting early Monday December 7, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the operating system, the network, and the BeeGFS file system. We anticipate that the cluster will be available again by the end of Tuesday December 8, when load testing of the upgraded BeeGFS file system will start. If these tests fail, we will have to unroll the BeeGFS upgrade, which in case we anticipate that the cluster is back online by the end of Wednesday December 9. November 23, 16:50 PT\n\n\n\n\n\n\n\nResolved: All 74 compute nodes that were taken off the job scheduler on 2020-11-04 are back up and running November 16, 12:00 PT\nNotice: 74 compute nodes, including several GPU nodes, were taken off the job scheduler around 14:00 on 2020-11-04 due to a faulty network switch. The network switch needs to be replaced in order to resolve this. November 4, 16:10 PT\n\n\n\n\n\n\n\nResolved: Our BeeGFS file system was non-responsive during 01:20-04:00 on 2020-11-05 because one of the meta servers hung. November 5, 08:55 PT\n\n\n\n\n\n\n\nResolved: Our BeeGFS file system was non-responsive because one of its meta servers hung, which now has been restarted. October 21, 11:15 PT\nNotice: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 09:30 today. October 21, 10:45 PT\n\n\n\n\n\n\n\nResolved: Our BeeGFS file system was non-responsive between 17:22 and 18:52 today because one of its meta servers hung while the other attempted to synchronize to it. August 19, 19:00 PT\nNotice: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 17:30 today. August 19, 18:15 PT\n\n\n\n\n\n\n\nResolved: The cluster is fully back up and running. Several compute nodes still need to be rebooted but we consider this upgrade cycle completed. The network upgrade took longer than expected, which delayed the processes. We hope to bring the new lab storage online during the next week. August 13, 21:00 PT\nUpdate: All login, data-transfer, and development nodes are online. Additional compute nodes are being upgraded and are soon part of the pool serving jobs. August 13, 14:50 PT\nUpdate: Login node log1, data-transfer node dt2, and the development nodes are available again. Compute nodes are going through an upgrade cycle and will soon start serving jobs again. The upgrade work is taking longer than expected and will continue tomorrow Thursday August 13. August 12, 16:10 PT\nNotice: All of the Wynton HPC environment is down for maintenance and upgrades. August 10, 00:00 PT\nNotice: Starting early Monday August 10, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the network and adding lab storage purchased by several groups. We anticipate that the cluster will be available again by the end of Wednesday August 12. July 24, 15:45 PT\n\n\n\n\n\n\n\nResolved: All three development nodes have been rebooted. July 6, 15:20 PT\nNotice: The three regular development nodes have all gotten themselves hung up on one particular process. This affects basic system operations and preventing such basic commands as ps and w. To clear this state, we’ll be doing an emergency reboot of the dev nodes at about 15:15. July 6, 15:05 PT\n\n\n\n\n\n\nResolved: The SGE scheduler produced errors when queried or when jobs were submitted or launched. The problem started 00:30 and lasted until 02:45 early Sunday 2020-07-05. July 6, 22:00 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. June 26, 10:45 PT\nUpdate: Development node dev3 is back online. June 15, 11:15 PT\nUpdate: Development node dev3 is not available. It failed to reboot and requires on-site attention, which might not be possible for several days. All other log-in, data-transfer, and development nodes were rebooted successfully. June 11, 15:45 PT\nNotice: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Thursday June 11. June 11, 10:45 PT\n\n\n\n\n\n\nResolved: Internet access from the development nodes is available again. A new web-proxy server had to be built and deploy. June 9, 09:15 PT\nNotice: Internet access from the development nodes is not available. This is because the proxy server providing them with internet access had a critical hardware failure around 08-09 this morning. At the most, we cannot provide an estimate when we get to restore this server. June 5, 16:45 PT\n\n\n\n\n\n\nUpdate: The upgrade of the BeeGFS filesystem introduced new issues. We decided to rollback the upgrade and we are working with the vendor. There is no upgrade planned for the near term. June 8, 09:00 PT\nUpdate: The BeeGFS filesystem has been upgraded using a patch from the vendor. The patch was designed to lower the amount of resynchronization needed between the two metadata servers. Unfortunately, after the upgrade we observe an increase of resynchronization. We will keep monitoring the status. If the problem remains, we will consider a rollback to the BeeGFS version used prior to May 18. May 22, 01:25 PT\nUpdate: For a short moment around 01:00 early Friday, both of our BeeGFS metadata servers were down. This may have lead to some applications experiencing I/O errors around this time. May 22, 01:25 PT\nNotice: Work to improve the stability of the BeeGFS filesystem (/wynton) will be conducted during the week of May 18-22. This involves restarting the eight pairs of metadata server processes, which may result in several brief stalls of the file system. Each should last less than 5 minutes and operations will continue normally after each one. May 6, 15:10 PT\n\n\n\n\n\n\n\nResolved: The GPU compute nodes are now fully available to serve jobs. May 29, 12:00 PT\nUpdate: The GPU compute nodes that went down yesterday have been rebooted. May 29, 11:10 PT\nInvestigating: A large number of GPU compute nodes in the MSG data center are currently down for unknown reasons. We are investigating the cause. May 28, 09:35 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC system is considered fully functional again. The legacy, deprecated NetApp storage was lost. February 10, 10:55 PT\nUpdate: The majority of the compute nodes have been rebooted and are now online and running jobs. We will actively monitor the system and assess the how everything works before we considered this incident resolved. February 7, 13:40 PT\nUpdate: The login, development and data transfer nodes will be rebooted at 01:00 today Friday February 7. February 7, 12:00 PT\nUpdate: The failed legacy NetApp server is the cause to the problems, e.g. compute nodes not being responsive causing problems for SGE etc. Because of this, all of the cluster - login, development, transfer, and computes nodes - will be rebooted tomorrow Friday 2020-02-07. February 6, 10:00 PT\nNotice: Wynton HPC is experience major issues due to NetApp file-system failure, despite this is being deprecated and not used much these days. The first user report on this came in around 09:00 and the job-queue logs suggests the problem began around 02:00. It will take a while for everything to come back up and there will be brief BeeGFS outage while we reboot the BeeGFS management node. February 5, 10:15 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file-system issue has been resolved by rebooting two meta servers. January 29, 17:00 PT\nNotice: There’s currently an issue with the BeeGFS file system. Users reporting that they cannot log in. January 29, 16:00 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS upgrade issue has been resolved. Jan 22, 14:30 PT\nUpdate: The planned upgrade caused unexpected problems to the BeeGFS file system resulting in /wynton/group becoming unstable. Jan 22, 13:35 PT\nNotice: One of the BeeGFS servers, which serve our cluster-wide file system, will be swapped out starting at noon (11:59am) on Wednesday January 22, 2020 and the work is expected to last one hour. We don’t anticipate any downtime because the BeeGFS servers are mirrored for availability. Jan 16, 14:40 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been updated and rebooted. Jan 4, 11:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. December 20, 10:20 PT\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 96 hours = 4.0 days = 1.1%\nUnscheduled: 83.5 hours = 3.5 days = 1.0%\nTotal: 179.5 hours = 7.5 days = 2.0%\nExternal factors: 15% of the above downtime, corresponding to 26 hours (=1.1 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2021-01-09 (1.0 hours) - job scheduler updates\n2021-07-08 (95 hours)\n\nTotal downtime: 96.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2019-01-22 (up to 14 days)\n2019-03-21 (up to 14 days)\n2019-10-29 (up to 14 days)\n2019-12-22 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2019-07-30 (6.5 hour) - Byers Hall power outage\n2019-08-15 (5.5 hour) - Diller power outage\n2019-10-25 (1.0 hour) - Byers Hall power outage\n2019-10-22 (13.0 hour) - Diller power backup failed during power maintenance\n\nTotal downtime: 26.0 hours of which 26.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2019-01-08 (2.0 hours) - BeeGFS server non-responsive\n2019-01-14 (1.5 hours) - BeeGFS non-responsive\n2019-05-15 (24.5 hours) - BeeGFS non-responsive\n2019-05-17 (5.0 hours) - BeeGFS slowdown\n2019-06-17 (10.5 hours) - BeeGFS non-responsive\n2019-08-23 (4.0 hours) - BeeGFS server non-responsive\n2019-09-24 (3.0 hours) - BeeGFS server non-responsive\n2019-12-18 (3.5 hours) - Network switch upgrade\n2019-12-22 (5.5 hours) - BeeGFS server non-responsive\n\nTotal downtime: 58.5 hours of which 0 hours were due to external factors\n\n\n\n\n\nNumber of user account: 478 (change: +280 during the year)\n\n\n\n\n\n\n\nResolved: All compute nodes have been updated and rebooted. Jan 4, 11:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. December 20, 10:20 PT\n\n\n\n\n\n\nResolved: No further hiccups were needed during the BeeGFS resynchronization. Everything is working as expected. December 23, 10:00 PT\nUpdate: The issues with login was because the responsiveness of one of the BeeGFS file servers became unreliable around 04:20. Rebooting that server resolved the problem. The cluster is fully functional again although slower than usual until the file system have been resynced. After this, there might be a need for one more, brief, reboot. December 22, 14:40 PT\nNotice: It is not possible to log in to the Wynton HPC environment. The reason is currently not known. December 22, 09:15 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is fully functional again. The BeeGFS filesystem was not working properly during 18:30-22:10 on December 18 resulting in no login access to the cluster and job file I/O being backed up. December 19, 08:50 PT\nUpdate: The BeeGFS filesystem is non-responsive, which we believe is due to the network switch upgrade. December 18, 21:00 PT\nNotice: One of two network switches will be upgraded on Wednesday December 18 starting at 18:00 and lasting a few hours. We do not expect this to impact the Wynton HPC environment other than slowing down the network performance to 50%. December 17, 10:00 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been updated and rebooted. Nov 11, 01:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). GPU nodes will be rebooted as soon as all GPU jobs complete. During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,000 cores) in the graph above. Oct 29, 16:30 PT\n\n\n\n\n\n\nResolved: Development node qb3-dev2 was rebooted. Data-transfer node dt1.wynton.ucsf.edu is kept offline because it is scheduled to be upgraded next week. October 28, 15:00 PT\nUpdate: Most compute nodes that went down due to the power glitch has been rebooted. Data-transfer node dt1.wynton.ucsf.edu and development node qb3-dev2 are still down - they will be brought back online on Monday October 28. October 25, 14:00 PT\nNotice: A very brief power outage in the Byers Hall building caused several compute nodes in its Data Center to go down. Jobs that were running on those compute nodes at the time of the power failure did unfortunately fail. Log-in, data-transfer, and development nodes were also affected. All these hosts are currently being rebooted. October 25, 13:00 PT\n\n\n\n\n\n\n\nResolved: Log in works again. October 24, 09:45 PT\nNotice: It is not possible to log in to the Wynton HPC environment. This is due to a recent misconfiguration of the LDAP server. October 24, 09:30 PT\n\n\n\n\n\n\nResolved: The Wynton HPC BeeGFS file system is fully functional again. During the outage, /wynton/group and /wynton/scratch was not working properly, whereas /wynton/home was unaffected. October 23, 10:35 PT\nNotice: The Wynton HPC BeeGFS file system is non-functional. It is expected to be resolved by noon on October 23. The underlying problem is that the power backup at the Diller data center did not work as expected during a planned power maintenance. October 22, 21:45 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. September 24, 20:25 PT\nNotice: The Wynton HPC environment is unresponsive. Problem is being investigated. September 24, 17:30 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. The reason for this downtime was the BeeGFS file server became unresponsive. August 23, 20:45 PT\nNotice: The Wynton HPC environment is unresponsive. August 23, 16:45 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. August 15, 21:00 PT\nNotice: The Wynton HPC environment is down due to a non-planned power outage at the Diller data center. Jobs running on compute nodes located in that data center, were terminated. Jobs running elsewhere may also have been affected because /wynton/home went down as well (despite it being mirrored). August 15, 15:45 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. July 30, 14:40 PT\nNotice: The Wynton HPC environment is down due to a non-planned power outage at the main data center. July 30, 08:20 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment and the BeeGFS file system are fully functional after updates and upgrades. July 12, 11:15 PT\nNotice: The Wynton HPC environment is down for maintenance. July 8, 12:00 PT\nNotice: Updates to the BeeGFS file system and the operating system that require to bring down all of Wynton HPC will start on the morning of Monday July 8. Please make sure to log out before then. The downtime might last the full week. July 1, 14:15 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file system is fully functional again. June 18, 01:30 PT\nInvestigating: Parts of /wynton/scratch and /wynton/group are currently unavailable. The /wynton/home space should be unaffected. June 17, 15:05 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file system and the cluster is functional again. May 17, 16:00 PT\nInvestigating: There is a major slowdown of the BeeGFS file system (/wynton), which in turn causes significant problems throughout the Wynton HPC environment. May 17, 10:45 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file system, and thereby also the cluster itself, is functional again. May 16, 10:30 PT\nInvestigating: The BeeGFS file system (/wynton) is experiencing major issues. This caused all on Wynton HPC to become non-functional. May 15, 10:00 PT\n\n\n\n\n\n\n\nResolved: The UCSF-wide network issue that affected access to Wynton HPC has been resolved. May 15, 15:30 PT\nUpdate: The login issue is related to UCSF-wide network issues. May 15, 13:30 PT\nInvestigating: There are issues logging in to Wynton HPC. May 15, 10:15 PT\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. April 5, 12:00 PT\nUpdate: Nearly all compute nodes have been rebooted (~5,200 cores are now available). Mar 29, 12:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 5,424 cores) in the graph above. Mar 21, 15:30 PT\n\n\n\n\n\n\nResolved: The login, development and transfer hosts have been rebooted. March 22, 10:35 PT\nNotice: On Friday March 22 at 10:30am, all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. These hosts should be offline for less than 5 minutes. Mar 21, 15:30 PT\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. Feb 5, 11:30 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 1,944 cores) in the graph above. Jan 22, 16:45 PT\n\n\n\n\n\n\nResolved: The login, development and transfer hosts have been rebooted. Jan 23, 13:00 PT\nNotice: On Wednesday January 23 at 12:00 (noon), all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. The hosts should be offline for less than 5 minutes. Jan 22, 16:45 PT\n\n\n\n\n\n\nResolved: The file system under /wynton/ is back up again. We are looking into the cause and taking steps to prevent this from happening again. Jan 9, 12:45 PT\nInvestigating: The file system under /wynton/ went down around 11:30 resulting is several critical failures including the scheduler failing. Jan 14, 11:55 PT\n\n\n\n\n\n\n\nResolved: The SGE job scheduler is now back online and accepts new job submission again. Jan 9, 12:45 PT\nUpdate: The downtime of the job scheduler will begin on Wednesday January 9 @ noon and is expected to be completed by 1:00pm. Jan 8, 16:00 PT\nNotice: There will be a short job-scheduler downtime on Wednesday January 9 due to SGE maintenance. During this downtime, already running jobs will keep running and queued jobs will remain in the queue, but no new jobs can be submitted. Dec 20, 12:00 PT\n\n\n\n\n\n\n\nInvestigating: One of the parallel file-system servers (BeeGFS) appears to have crashed on Monday January 7 at 07:30 and was recovered on 9:20pm. Right now we are monitoring its stability, and investigating the cause and what impact it might have had. Currently, we believe users might have experienced I/O errors on /wynton/scratch/ whereas /wynton/home/ was not affected. Jan 8, 10:15 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 0 hours = 0.0%\nUnscheduled: 84 hours = 3.5 days = 1.9%\nTotal: 84 hours = 3.5 days = 1.9%\nExternal factors: 100% of the above downtime, corresponding to 84 hours (=3.5 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nNone\n\nTotal downtime: 0.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2018-09-28 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2018-06-17 (23 hours) - Campus power outage\n2018-11-08 (19 hours) - Byers Hall power maintenance without notice\n2018-12-14 (42 hours) - Sandler Building power outage\n\nTotal downtime: 84 hours of which 84 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nNone.\n\nTotal downtime: 0.0 hours\n\n\n\n\n\nNumber of user account: 198 (change: +103 during the year)\n\n\n\n\n\n\n\nResolved: Parts of the new BeeGFS file system was non-functional for approx. 1.5 hours during Friday December 21 when a brief maintenance task failed. Dec 21, 20:50 PT\n\n\n\n\n\n\n\nResolved: All of the `msg-* compute nodes but one are operational. Dec 20, 16:40 PT\nNotice: Starting Wednesday December 12 around 11:00, several msg-* compute nodes went down (~200 cores in total). The cause of this is unknown. Because it might be related to the BeeGFS migration project, the troubleshooting of this incident will most likely not start until the BeeGFS project is completed, which is projected to be done on Wednesday December 19. Dec 17, 17:00 PT\n\n\n\n\n\n\n\nResolved: Development node qb3-dev1 is functional. Dec 18, 20:50 PT\nInvestigating: Development node qb3-dev1 does not respond to SSH. This will be investigated the first thing tomorrow morning (Wednesday December 19). In the meanwhile, development node qb3-gpudev1, which is “under construction”, may be used. Dec 18, 16:30 PT\n\n\n\n\n\n\nResolved: /wynton/scratch is now back online and ready to be used. Dec 19, 14:20 PT\nUpdate: The plan is to bring /wynton/scratch back online before the end of the day tomorrow (Wednesday December 19). The planned SGE downtime has been rescheduled to Wednesday January 9. Moreover, we will start providing the new 500-GiB /wynton/home/ storage to users who explicitly request it (before Friday December 21) and who also promise to move the content under their current /netapp/home/ to the new location. Sorry, users on both QB3 and Wynton HPC will not be able to migrate until the QB3 cluster has been incorporated into Wynton HPC (see Roadmap) or they giving up their QB3 account. Dec 18, 16:45 PT\nUpdate: The installation and migration to the new BeeGFS parallel file servers is on track and we expect to go live as planned on Wednesday December 19. We are working on fine tuning the configuration, running performance tests, and resilience tests. Dec 17, 10:15 PT\nUpdate: /wynton/scratch has been taken offline. Dec 12, 10:20 PT\nReminder: All of /wynton/scratch will be taken offline and completely wiped starting Wednesday December 12 at 8:00am. Dec 11, 14:45 PT\nNotice: On Wednesday December 12, 2018, the global scratch space /wynton/scratch will be taken offline and completely erased. Over the week following this, we will be adding to and reconfiguring the storage system in order to provide all users with new, larger, and faster (home) storage space. The new storage will served using BeeGFS, which is a new much faster file system - a system we have been prototyping and tested via /wynton/scratch. Once migrated to the new storage, a user’s home directory quota will be increased from 200 GiB to 500 GiB. In order to do this, the following upgrade schedule is planned:\n\nWednesday November 28-December 19 (21 days): To all users, please refrain from using /wynton/scratch - use local, node-specific /scratch if possible (see below). The sooner we can take it down, the higher the chance is that we can get everything in place before December 19.\nWednesday December 12-19 (8 days): /wynton/scratch will be unavailable and completely wiped. For computational scratch space, please use local /scratch unique to each compute node. For global scratch needs, the old and much slower /scrapp and /scrapp2 may also be used.\nWednesday December 19, 2018 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline. No jobs will be able to be submitted until it is restarted.\nWednesday December 19, 2018: The upgraded Wynton HPC with the new storage will be available including /wynton/scratch.\nWednesday January 9, 2019 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline temporarily. No jobs will be able to be submitted until it is restarted.\n\nIt is our hope to be able to keep the user’s home accounts, login nodes, the transfer nodes, and the development nodes available throughout this upgrade period.\nNOTE: If our new setup proves more challenging than anticipated, then we will postpone the SGE downtime to after the holidays, on Wednesday January 9, 2019. Wynton HPC will remain operational over the holidays, though without /wynton/scratch. Dec 6, 14:30 PT [edited Dec 18, 17:15 PT]\n\n\n\n\n\n\n\nResolved: All mac-* compute nodes are up and functional. Dec 14, 12:00 PT\nInvestigating: The compute nodes named mac-* (in the Sandler building) went down due to power failure on Wednesday December 12 starting around 05:50. Nodes are being rebooted. Dec 12, 09:05 PT\n\n\n\n\n\n\n\nResolved: The cluster is full functional. It turns out that none of the compute nodes, and therefore none of the running jobs, were affected by the power outage. Nov 8, 11:00 PT\nUpdate: The queue-metric graphs are being updated again. Nov 8, 11:00 PT\nUpdate: The login nodes, the development nodes and the data transfer node are now functional. Nov 8, 10:10 PT\nUpdate: Login node wynlog1 is also affected by the power outage. Use wynlog2 instead. Nov 8, 09:10 PT\nNotice: Parts of the Wynton HPC cluster will be shut down on November 8 at 4:00am. This shutdown takes place due to the UCSF Facilities shutting down power in the Byers Hall. Jobs running on affected compute nodes will be terminated abruptly. Compute nodes with battery backup or in other buildings will not be affected. Nodes will be rebooted as soon as the power comes back. To follow the reboot progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. Unfortunately, the above queue-metric graphs cannot be updated during the power outage. Nov 7, 15:45 PT\n\n\n\n\n\n\n\nResolved: The compute nodes has been rebooted and are accepting new jobs. For the record, on day 5 approx. 300 cores were back online, on day 7 approx. 600 cores were back online, on day 8 approx. 1,500 cores were back online, and on day 9 the majority of the 1,832 cores were back online. Oct 11, 09:00 PT\nNotice: On September 28, a kernel update was applied to all compute nodes. To begin running the new kernel, each node must be rebooted. To achieve this as quickly as possible and without any loss of running jobs, the queues on the nodes were all disabled (i.e., they stopped accepting new jobs). Each node will reboot itself and re-enable its own queues as soon as all of its running jobs have completed. Since the maximum allowed run time for a job is two weeks, it may take until October 11 before all nodes have been rebooted and accepting new jobs. In the meanwhile, there will be fewer available slots on the queue than usual. To follow the progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. Sept 28, 16:30 PT\n\n\n\n\n\n\nResolved: The login, development, and data transfer hosts have been rebooted. Oct 1, 13:30 PT\nNotice: On Monday October 1 at 01:00, all of the login, development, and data transfer hosts will be rebooted. Sept 28, 16:30 PT\n\n\n\n\n\n\nResolved: Around 11:00 on Wednesday September 12, the SGE scheduler (“qmaster”) became unreachable such that the scheduler could not be queried and no new jobs could be submitted. Jobs that relied on run-time access to the scheduler may have failed. The problem, which was due to a misconfiguration being introduced, was resolved early morning on Thursday September 13. Sept 13, 09:50 PT\n\n\n\n\n\n\n\nResolved: Nodes were rebooted on August 1 shortly after the power came back. Aug 2, 08:15 PT\nNotice: On Wednesday August 1 at 6:45am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. Jul 30, 20:45 PT\n\n\n\n\n\n\n\nResolved: The nodes brought down during the July 30 partial shutdown has been rebooted. Unfortunately, the same partial shutdown has to be repeated within a few days because the work in server room was not completed. Exact date for the next shutdown is not known at this point. Jul 30, 09:55 PT\nNotice: On Monday July 30 at 7:00am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. Jul 29, 21:20 PT\n\n\n\n\n\n\nResolved: The Nvidia-driver issue occurring on some of the GPU compute nodes has been fixed. Jun 26, 11:55 PT\nUpdate: Some of the compute nodes with GPUs are still down due to issues with the Nvidia drivers. Jun 19, 13:50 PT\nUpdate: The login nodes and and the development nodes are functional. Some compute nodes that went down are back up, but not all. Jun 18, 10:45 PT\nInvestigating: The UCSF Mission Bay Campus experienced a power outage on Saturday June 16 causing parts of Wynton HPC to go down. One of the login nodes (wynlog1), the development node (qb3-dev1), and parts of the compute nodes are currently non-functional. Jun 17, 15:00 PT\n\n\n\n\n\n\n\n\nloading… Reload"
  },
  {
    "objectID": "hpc/status/index.html#queue-metrics",
    "href": "hpc/status/index.html#queue-metrics",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "day\n\n\nweek\n\n\nmonth\n\n\nyear"
  },
  {
    "objectID": "hpc/status/index.html#file-system-metrics",
    "href": "hpc/status/index.html#file-system-metrics",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "Last known heartbeat: Loading… ⟲\n\n\n/wynton/scratch/ lagginess\n\n\n\n\n\n\n\n/wynton/home/ lagginess\n\n\n\n\n\n\n\n/wynton/group/ lagginess\n\n\n\n\n\nFigure: The total, relative processing time on the logarithmic scale for one benchmarking run to complete over time. The values presented are relative to the best case scenario when there is no load, in case the value is 1.0. The larger the relative time is, the more lag there is on file system. Annotation of lagginess ranges: 1-2: excellent (light green), 2-5: good (green), 5-20: sluggish (orange), 20-100: bad (red), 100 and above: critical (purple).\nDetails: These benchmarks are run every ten minutes from different hosts and toward different types of the file system. These metrics are based on a set of commands, part of the wynton-bench tool, that interacts with the file system that is being benchmarked. The relevant ones are: reading a large file from /wynton/home/, copying that large archive file to and from the BeeGFS path being benchmarked, extracting the archive to path being benchmarked, find one file among the extracted files, calculating the total file size, and re-archiving and compressing the extracted files. When there’s minimal load on /wynton, the processing time is ~19 seconds. In contrast, when benchmarking local /scratch, the total processing time is about three seconds.\nWhen BeeGFS struggles to keep up with metadata and storage requests, the BeeGFS lagginess goes up. We can use beegfs-ctl --serverstats --perserver --nodetype=meta and beegfs-ctl --serverstats --perserver --nodetype=storage to see the amount of BeeGFS operations that are queued up (qlen) per metadata and storage server. When things run smoothly, the queue lengths should be near zero (qlen less than ten). When BeeGFS struggles to keep up, we typically find large qlen values for one or more servers. To see if the BeeGFS load is high due to file storage or metadata I/O performed by specific users, we can use beegfs-ctl --userstats .... For example, beegfs-ctl --userstats --names --interval=10 --maxlines=5 --nodetype=storage summarizes storage operations every ten seconds and list the five users with the most operations. Similarly, beegfs-ctl --userstats --names --interval=10 --maxlines=5 --nodetype=meta shows metadata operations per user."
  },
  {
    "objectID": "hpc/status/index.html#miscellaneous-metrics",
    "href": "hpc/status/index.html#miscellaneous-metrics",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "Detailed statistics on the file-system load and other cluster metrics can be found on the Wynton HPC Grafana Dashboard. To access this, make sure you are on the UCSF network. Use your Wynton HPC credential to log in."
  },
  {
    "objectID": "hpc/status/index.html#compute-nodes",
    "href": "hpc/status/index.html#compute-nodes",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "Status on compute nodes unknown, which happens when for instance the job scheduler is down."
  },
  {
    "objectID": "hpc/status/index.html#upcoming-incidents",
    "href": "hpc/status/index.html#upcoming-incidents",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "Notice: The cluster will down for maintenance from 3:00 pm on Wednesday November 12 until 6:00 pm on Thursday November 13, 2025. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Jobs with runtimes that go into the maintenance window will be started after the downtime. Starting October 29 at 4:00pm, jobs relying on the default 14-day runtime will not be launched until after the downtime. UCSF Facilities will perform annual fire inspection activities to remain compliant with regulations. The network team will update a core switch. The Wynton team will take the opportunity to implement kernel updates during this period. October 29, 12:00 PT"
  },
  {
    "objectID": "hpc/status/index.html#current-incidents",
    "href": "hpc/status/index.html#current-incidents",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "Update: There was another burst of “can’t get password entry for user” errors starting on 2025-01-26 around 15:30, causing jobs to fail immediately. We are restarting the SSSD service on the ~140 compute nodes we have identified suffer from this problem. January 27, 11:45 PT\nUpdate: To lower the risk for this problem to occur, the SSSD timeout limit was increased from 10 seconds to 30 seconds. November 20, 2023, 10:00 PT\nUpdate: The “can’t get password entry for user” error happens on some compute nodes where the System Security Services Daemon (SSSD) has failed. Until the cause for failed SSSD has been identified and resolved, the only solution is to resubmit the job. November 17, 2023, 09:30 PT\nNotice: Some jobs end up in an error state (Eqw) with an error “can’t get password entry for user”alice”. Either user does not exist or error with NIS/LDAP etc.” November 16, 2023, 17:00 PT\n\n\n\n\n\n\n\nNotice: Passwords can be changed via the web interface. It is still not possible to change it via the command-line while logged in to Wynton. November 13, 11:00 PT\nNotice: It is not possible to change or reset passwords since 2023-11-05. This problem was introduced while doing cluster-wide upgrades to Rocky 8. November 11, 09:00 PT"
  },
  {
    "objectID": "hpc/status/index.html#past-incidents",
    "href": "hpc/status/index.html#past-incidents",
    "title": "Wynton HPC Status",
    "section": "",
    "text": "2025\n\n\n2024\n\n\n2023\n\n\n2022\n\n\n2021\n\n\n2020\n\n\n2019\n\n\n2018\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 505 hours (= 21.0 days)\nTotal: 505 hours (= 21.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2025-01-09 – 2025-01-09 ( 1.25 hours)\n2025-01-17 – 2025-01-22 (81.75 hours)\n2025-02-21 – 2025-03-07 (61.0 hours)\n2025-03-31 – 2025-04-01 (17.0 hours)\n2025-04-11 – 2025-04-14 (62.0 hours)\n2025-05-29 – 2025-06-10 (282.0 hours)\n\nTotal downtime: 505 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\n\n\nResolved: Wynton logins are available as of noon today. At that time we will start unsuspending jobs. We lost about 50 TiBs (0.7%) of compressed data from 6550 TiBs with the group storage pool for files in /wynton/group, /wynton/protected/group, and /wynton/protected/project. See Wynton Announcement email for further details. June 10, 12:00 PT\nUpdate: We plan to resume operations by the weekend, given that the current backup and the necessary, manual one-at-the-time replacement of multiple drives completes in time. Files that lived on the failed storage pool are broken and cannot not be fully read, but possible partially. Home directories are unaffected. The affected files live under /wynton/group, /wynton/protected/group, and /wynton/protected/project. We are scanning the file system to identify exactly which files are affected - this is a slow processes. We will share file lists with affected groups. Eventually, any broken files have to be deleted. June 4, 14:00 PT\nUpdate: Wynton jobs and logins are still paused until further notice. Our team is working on determining all of the files that may be corrupt/unavailable and will work with the vendor on the best course of action. We do not yet have an estimate on when we will be back up. May 30, 10:40 PT\nNotice: Jobs and logins have been paused until further notice. Our team is actively troubleshooting and coordinating with the vendor. A drive was replaced today and was in the process of resilvering when two more drives failed, totally three failed drives, which causes significant problems. Data corruption is expected. May 29, 18:20 PT\n\n\n\n\n\n\n\nResolved: All cluster jobs and queues were unsuspended at 02:00 this night. April 14, 08:15 PT\nNotice: All cluster jobs have been suspended in order to allow multiple metadata mirror resyncing processes to complete. These processes are what led to the hanging episodes that we have been seeing. Interactive nodes remain available. Resyncing processes are estimated to complete by Monday. April 11, 12:00 PT\n\n\n\n\n\n\n\nResolved: Queues and jobs are re-enabled. April 1, 12:00 PT\nUpdate: Login is re-enabled. Queues and jobs remains suspended. March 31, 20:15 PT\nNotice: BeeGFS metadata servers are experiencing issues. We have suspended all queues and jobs and disabled logins. We will work with the file system vendor to resolve the issue. March 31, 19:00 PT\n\n\n\n\n\n\n\nResolved: We have resumed the scheduler and jobs are being processed again. We identified several problems related to the BeeGFS file system that could have contributed to the recent, severe performance degradation. Specifically, the process that automatically removes files older than 14 days from /wynton/scratch/ failed to complete, which resulted in close to 100% full storage servers. We believe this issues started in November 2024 and has gone unnoticed until now. We do not understand why these cleanup processes had failed, but one hypothesis is that there are corrupt files or folders where the cleanup process gets stuck, preventing it from cleaning up elsewhere. It might be that these problems have caused our metadata servers resynchronizing over and over - resynchronization itself is an indication that something is wrong. We are in the process of robustifying our cleanup process, putting in monitoring systems to detect these issues before system degradation takes place. March 7, 11:30 PT\nNotice: We have decided to again suspending all running jobs and disable the queue from taking on new jobs. March 5, 15:00 PT\nNotice: Resynchronization of BeeGFS metadata server pair (42,52) finished after 23 hours. March 4, 14:00 PT\nNotice: Resynchronization of BeeGFS metadata server pairs (32,22) and (23,33) started 2025-03-03, and (42,52) on 2025-03-04. March 4, 09:00 PT\nNotice: The job queue has been re-enabled and all suspended jobs have been released. February 28, 09:00 PT\nNotice: Login and file transfers to Wynton has been re-enabled. February 28, 09:00 PT\nNotice: Resynchronization of BeeGFS metadata server pair (41,51) completed after 24 hours, and pair (63,73) completed after 18 hours. February 28, 09:00 PT\nNotice: In order to speed up resynchronization of metadata servers, we have decided to minimize the load on the file system by suspending all running jobs, disable login to Wynton, and disable all file transfers to and from Wynton. February 27, 16:30 PT\nNotice: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (51,73). February 27, 16:15 PT\nNotice: Resynchronization of BeeGFS metadata server pair meta22 and meta32 completed after 30 hours. February 27, 06:00 PT\nNotice: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (22,32). February 26, 19:30 PT\nNotice: We are working with the vendor to try to resolve this problem. February 26, 09:00 PT\nNotice: The file system is again very slow. delays when working interactively and jobs to slow down. February 25, 15:15 PT\nNotice: The file system is again very slow. February 25, 10:00 PT\nNotice: The file system is very slow, which result in long delays when working interactively and jobs to take longer than usual. February 21, 16:00 PT\n\n\n\n\n\n\n\nResolved: Login node plog1 respects SSH keys again. February 24, 2025, 11:15 PT\nUpdate: Login node plog1 is available again, but does not respect SSH keys. February 24, 2025, 10:30 PT\nUpdate: Data-transfer node dt1 is available again. February 24, 2025, 10:30 PT\nUpdate: With the exception for plog1 and dt1, all login, data-transfer, and development nodes have been rebooted. Until plog1 is available, PHI-users may use pdt1 and pdt2 to login into the cluster. February 22, 2025, 13:30 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Saturday, February 22, 2025 at 13:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. February 21, 2025, 12:15 PT\n\n\n\n\n\n\nResolved: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is available again after data-transfer node dt1 coming online. February 24, 2025, 10:30 PT\nNotice: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is unavailable, because data-transfer node dt1 is unavailable. February 22, 2025, 13:30 PT\n\n\n\n\n\n\nResolved: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are functional again. February 24, 2025, 09:30 PT\nUpdate: The vendor has escalated our support ticket. February 19, 2025, 13:30 PT\nNotice: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are currently unavailable. The former gives an error on “Unknown user or wrong password”, and the latter “Authentication Required - Identity set contains an identity from an allowed domain, but it does not map to a valid username for this connector”. The regular ‘Wynton HPC’ Globus endpoint is unaffected and available. The problem has been there since at least 2025-02-14 at 22:36, when I user reported it. February 19, 2025, 12:00 PT\n\n\n\n\n\n\nResolved: Wynton is fully operational again. The BeeGFS file system issue has been resolved. All data consistency has been verified. Working with the vendor, we have identified a potential bug in the BeeGFS quota system that caused the BeeGFS outage. That part is still under investigation in order to minimize and remove the risk of reoccurrence. January 22, 12:15 PT\nUpdate: The login and data-transfer nodes are available again. January 22, 11:00 PT\nUpdate: The third resynchronization completed successfully. January 21, 18:30 PT\nUpdate: Further investigation of the failed resynchronization this morning indicated that the resynchronization did indeed keep running while it stopped producing any output and the underlying BeeGFS service was unresponsive. Because of this, we decided to not restart the resynchronization, but instead let it continue in the hope it will finish. But, by not restarting, Wynton will remain inaccessible. Our first objective is to not jeopardize the cluster, the second objective is to bring the system back online. January 21, 15:15 PT\nUpdate: The cluster is unavailable again. The past resynchronization of the problematic BeeGFS metadata server failed again, which triggers the problem. We are communicating with the vendor for their support. January 21, 09:45 PT\nUpdate: The cluster is available again, but the scheduler has been paused. No queued jobs are launched and running jobs have been suspended, but will resume when the pause of scheduler is removed. This is done to minimize the load on BeeGFS, which will simplify troubleshooting and increase the chances to stabilize BeeGFS. It is the same BeeGFS metadata server as before that is experiencing problems. January 19, 13:45 PT\nUpdate: The cluster is unavailable again. January 19, 12:45 PT\nUpdate: The cluster is working again. We have started a resynchronization of the problematic BeeGFS metadata server pair meta22 and meta32. January 18, 13:45 PT\nUpdate: First signs of the cluster coming back online again, e.g. queued jobs are launched, and it is possible to access the cluster via SSH. January 18, 06:00 PT\nUpdate: Identifies a specific BeeGFS metadata server that is unresponsive. The BeeGFS vendor has been contacted. January 18, 01:00 PT\nUpdate: The underlying problem appears to be BeeGFS. The storage servers are okay, but one or more metadata servers are unresponsive. January 17, 21:30 PT\nNotice: The cluster is unavailable, e.g. i is not possible to access the login or the data-transfer nodes. January 17, 19:45 PT\n\n\n\n\n\n\n\nResolved: The cluster full operational again. Suspended jobs have been resumed. The BeeGFS issue has been resolved. Checked hardware and cables. Rebooted affected BeeGFS server. January 9, 16:20 PT\nNotice: An issue with BeeGFS was detected. All Wynton jobs have been paused until further notice. January 9, 15:10 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 137.0 hours (= 5.7 days) = 1.6%\nUnscheduled: 142.3 hours (= 5.9 days) = 1.6%\nTotal: 279.3 hours (= 11.6 days) = 3.2%\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2024-06-17 – 2024-06-18 (32.0 hours)\n2024-10-14 – 2024-10-18 (105.0 hours)\n\nTotal downtime: 137.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2024-04-03 (~500 hours)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2024-03-14 (13.0 hours)\n2024-03-17 (15.0 hours)\n2024-05-31 (2.3 hours)\n2024-06-15 – 2024-06-21 (112.0 hours; excluding 32 hours scheduled maintenance)\n\nTotal downtime: 142.3 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\n\n\nResolved: The cluster is back online. October 18, 17:00 PT\nUpdate: The cluster including all its storage is offline undergoing a scheduled maintenance. October 14, 11:00 PT\nNotice: The cluster will be shut down for maintenance from 8:00 am on Monday October 14 until 5:00 pm on Friday October 18, 2024. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Starting 14 days before, the maximum job runtime will be decreased on a daily basis from the current 14 days down to one day so that jobs finish in time before the shutdown. Jobs with runtimes that go into the maintenance window will be started after the downtime. The reason for the downtime is that UCSF Facilities will perform maintenance affecting cooling in our data center. We will take this opportunity to perform system updates and BeeGFS maintenance. September 20, 16:45 PT\n\n\n\n\n\n\n\nResolved: All interactive nodes have been updated and deployed with the new CGroups limits. September 13, 13:00 PT\nNotice: All interactive nodes will be shutdown and rebooted on Thursday September 12 at 12:30 to update Linux kernels and deploy CGroups-controlled CPU and memory user limits. To avoid data loss, please save your work and logout before. Queued and running jobs are not affected. September 11, 09:15 PT\n\n\n\n\n\n\nResolved: 14,000 compute slots are now available, which corresponds to the majority of compute nodes. June 25, 00:30 PT\nUpdate: We will go ahead and re-enable the remaining compute nodes. June 24, 13:00 PT\nUpdate: Development nodes are available. We have also opened up 100 compute nodes. We will keep monitoring BeeGFS over the weekend with the plan to re-enable the remaining compute nodes if all go well. June 21, 19:15 PT\nUpdate: The login and data-transfer nodes are available. We will continue to validate BeeGFS during the day with the intent to open up the development nodes and a portion of the compute nodes before the weekend. June 21, 12:45 PT\nUpdate: We decided to replace the problematic chassis with a spare. The RAID file system has two failing drives, which are currently being restored. We expect this to finish up in the morning. Then, we will replace those two failing drives and proceed with another restore. If that succeeds, we plan to open up the login nodes to make files available again. After that, the goal is to slowly open up the queue and compute nodes over the weekend. June 20, 23:30 PT\nUpdate: We had folks onsite today to complete some preventative maintenance on all of the disk chassis (and, in a fit of optimism, bring up all of the nodes to prepare for a return to production). As this maintenance involved new firmware, we had some hope that it might sort out our issues with the problematic chassis. Unfortunately, our testing was still able to cause an issue (read: crash). We’ve sent details from this latest crash to the vendor and we’ll be pushing hard to work with them tomorrow Thursday to sort things out. June 20, 00:15 PT\nUpdate: The vendor is still working on diagnosing our disk chassis issue. That work will resume after Wednesday’s holiday. So, unfortunately, we will not be able to bring Wynton up on Wednesday. We hope to come up on Thursday, but it all depends on our testing and the vendor’s investigation. June 19, 01:00 PT\nUpdate: We are working with both the system and chassis vendors to diagnose this and determine what the problem is and how to fix it. This process is taking much longer than we’d like, and it is looking increasingly unlikely that we’ll be in a position to bring Wynton back online today. June 18, 14:00 PT\nUpdate: A disk chassis that hosts part of /wynton/home appears to be failing. It works for a while and then fails, which brings down /wynton. We are trying to keep it running as much as possible, but can’t make any promises. June 16, 00:15 PT\nNotice: Wynton is currently down due to an unknown issue. The problem started around 15:00 on Saturday 2024-06-15. June 15, 23:15 PT\n\n\n\n\n\n\n\nUpdate: All but one of the planned maintenance upgrades were completed during this scheduled maintenance. The remain upgrade does not require a downtime and will be done in a near future without disrupting the cluster. June 18, 17:00 PT\nUpdate: Wynton is down for maintenance as of 09:00 on Monday 2024-06-17. June 17, 09:00 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Monday June 17 until 5:00 pm on Tuesday June 18, 2024. Starting June 3, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. June 5, 09:00 PT\n\n\n\n\n\n\n\nResolved: Development nodes are available again. June 10, 10:25 PT\nNotice: Development nodes are inaccessible since Friday June 7 at 17:00. We will investigate the problem on Monday. June 8, 05:45 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS issue has been resolved. Wynton is operational again. May 31, 09:20 PT\nNotice: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started around 06:00. We’re working on it and will post updates as we know more. May 31, 08:45 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. April 25, 09:00 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. April 4, 11:15 PT\nUpdate: A new set of kernel updates will be rolled out. Login, data-transfer, and development nodes will be rebooted briefly on Thursday April 11 at 11:00. All compute nodes will also have to be drained and rebooted, which might take up to two weeks. Some of the compute have been draining since last week, meaning that will only have been drain for at most another week. April 10, 16:00 PT\nUpdate: Hosts dt1 and plog1 are now also available. April 4, 12:15 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. It will take some more time before dt1 and plog1 are available again, because they did not come back as expected after the reboot. April 4, 11:15 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Thursday April 4 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. April 3, 17:30 PT\n\n\n\n\n\n\nResolved: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. March 18, 10:30 PT\nNotice: Wynton is currently down due to an unknown BeeGFS issues. The problem started around 19:30 on 2024-03-17. We’re working on it and will post updates as we know more. March 18, 09:00 PT\n\n\n\n\n\n\n\nResolved: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. March 14, 15:15 PT\nNotice: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started at 02:11 this morning. We’re working on it and will post updates as we know more. March 14, 09:15 PT\n\n\n\n\n\n\n\nResolved: All hosts are available. February 3, 17:00 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. It will take some more time before plog1, dt1, and dev2 are available again, because they did not come back as expected after the reboot. PHI users may use pdt1 and pdt2 to access the cluster. February 2, 14:45 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Friday February 2 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. February 1, 23:30 PT\n\n\n\n\n\n\nResolved: UCSF Facilities has resolved the cooling issue and there are again two working chillers. As a fallback backup, the building is now connected to the campus chilled water loop. This was confirmed by UCSF Facilities on 2024-12-10. July-August, 2024\nUpdate: UCSF Facilities performed testing for rerouting of updated chilled-water piping the building where the Wynton data center is hosted between 07-12 on 2024-05-08. May 9, 12:30 PT\nUpdate: The compute and development nodes are available again. Jobs that were running when we did the emergency shutdown should be considered lost and need to be resubmitted. UCSF Facilities has re-established cooling, but there is currently no redundancy cooling system available, meaning there is a higher-than-usual risk for another failure. January 25, 15:45 PT\nNotice: We are shutting down all Wynton compute and development nodes as an emergency action. This is due to a serious issue with the chilled-water system that feeds the cooling in the Wynton data center. By shutting down all of the compute nodes, we hope to slow the current temperature rise, while keeping the storage system, login and data-transfer nodes up. The will come back up again as soon as the UCSF Facilities has resolved the chilled-water system. ETA is currently unknown. January 25, 11:25 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 141.0 hours = 5.9 days = 1.6%\nUnscheduled: 742.25 hours = 30.9 days = 8.5%\nTotal: 883.25 hours = 35.3 days = 10.1%\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2023-02-22 (17.0 hours)\n2023-05-17 (20.0 hours)\n2023-10-30 – 2023-11-03 (104.0 hours)\n\nTotal downtime: 141.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2023-05-17 – 2023-06-01 (359.0 hours)\n2023-10-27 – 2023-11-15 (347.25 hours, excluding the scheduled 5-day downtime)\n\nTotal downtime: 742.25 hours of which 0.0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\n\n\n\nResolved: All compute nodes are up and running. December 15, 09:00 PT\nUpdate: A total of ~15,000 CPU cores are now up and running. November 27, 15:00 PT\nUpdate: A total of ~14,000 CPU cores are now up and running. November 26, 02:00 PT\nUpdate: A total of ~13,000 CPU cores are now up and running. November 22, 15:30 PT\nUpdate: A total of ~12,000 CPU cores are now up and running. November 22, 01:00 PT\nUpdate: A total of ~10,000 CPU cores are now up and running. November 21, 01:00 PT\nUpdate: 98 compute nodes with a total of 2,780 CPU cores are now up and running. November 16, 15:00 PT\nNotice: As we come back from the downtime, we start out with 36 out of 490 compute nodes available to process jobs. Work continues to migrating the remaining nodes to Rocky 8. November 15, 14:15 PT\n\n\n\n\n\n\n\nUpdate: The job scheduler is available and jobs are running. The data-transfer nodes are available. At this time, 36 out of 490 compute nodes have been re-enabled. Work has begun booting up the remaining ones. The first jobs were processed around 09:00 this morning. November 15, 14:15 PT\nUpdate: We plan to re-enable the job scheduler and start processing jobs by the end of today. It is possible to submit jobs already now, but they will remain queued until we re-enable the scheduler. November 15, 10:30 PT\nUpdate: The BeeGFS issue has been resolved, which allows us to move forward on the remaining Rocky-8 updates. We hope to start bringing compute nodes online as soon as tomorrow (2023-11-15). November 14, 13:15 PT\nUpdate: Still status quo; the BeeGFS issue holds us back from bringing the scheduler back up. We’re rather certain that we will not be able to resolve it today or tomorrow. November 13, 13:45 PT\nUpdate: Login and development nodes are available. Write access to the BeeGFS file system has been re-enabled. Due to continued issues in getting BeeGFS back in stable state, we are still not ready for opening up the scheduler and compute nodes. November 11, 00:30 PT\nUpdate: Unfortunately, we will not bring up Wynton to run jobs today. We are evaluating what, if anything, may be possible to bring up before the long weekend. The reason being that the required metadata resynchronization failed late yesterday. The vendor has provided us with a script to fix the failure. That script is running, and once it’s done, we’ll reattempt to resynchronize. November 9, 10:30 PT\nUpdate: We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. At that time, we expect all login, all data-transfer, and most development nodes will be available. A large number of the compute nodes will also be available via the scheduler. November 8, 10:30 PT\nUpdate: The team makes progress on the scheduled downtime activities, which was delayed due to the BeeGFS incident. We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. November 7, 11:20 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Monday October 30 through end of business on Friday November 3, 2023 (2023W44). The operating system will be upgraded system wide (all machines) from CentOS 7.9 to Rocky 8 Linux, the BeeGFS will be upgrade, and old hardware will be replaced. UCSF Facilities will perform scheduled work. After the downtime, there will no longer be any machine running CentOS 7.9. All machines will have their local disks (including /scratch and /tmp) wiped. Anything under /wynton (including /wynton/scratch, /wynton/home, …) should be unaffected, but please note that Wynton does not back anything up, so we recommend you to back up critical data. For more information about the Rocky 8 Linux migration project and how you can prepare for it is available at on the Migration to Rocky 8 Linux from CentOS 7 page. October 13, 11:15 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS metadata resynchronization is complete around 02:30 this morning. November 14, 13:15 PT\nUpdate: The BeeGFS metadata resynchronization is still unresolved. We are looking into other strategies, which we are currently testing. If those tests are successful, we will attempt to deploy the fix in the production. November 13, 13:45 PT\nUpdate: After resynchronization of the BeeGFS metadata kept failing, we identified a possible culprit. We suspect BeeGFS cannot handle the folders with many millions of files, causing the resynchronization to fail. We keep working on stabilizing BeeGFS. November 11, 00:45 PT\nUpdate: The BeeGFS metadata resynchronization that had been running for several hours, failed late yesterday. The vendor has provided us with a script tailored to fix the issue we ran into. That script is running, and once it’s done, we’ll start the resynchronization again. November 9, 10:30 PT\nUpdate: The recovery from the BeeGFS incident goes as planned. We estimate to have resolved this issue by the end of November 9, 2023, when full read-write access to /wynton will be available again. November 8, 10:30 PT\nUpdate: The Wynton team works on fixing and stabilizing the BeeGFS incident. We estimate to have resolved this issue by the end of November 9, 2023. November 7, 11:20 PT\nUpdate: Read-only access to Wynton has been enabled for users to retrieve their files. Login nodes log1 and plog1 are available for this. If going through the Wynton 2FA, make sure to answer “no” (default) when prompted for “Remember connection authentication from 98.153.103.186 for 12 hours? [y/N]”; answering “yes” causes the SSH connection to fail. November 5, 00:30 PT\nUpdate: Wynton admins can retrieve user files under /wynton/ upon requests until 18:00 today, when the UCSF network will go down. We are not able to share the PHI data under /wynton/protected/. Please contact support with all details including full path of the data to be retrieved. October 30, 15:30 PT\nUpdate: The BeeGFS issue is related to a CentOS 7-kernel bug in one of our BeeGFS metadata servers. To minimize the risk of data loss on the /wynton file system, we took the decision to shut down Wynton immediately. At the moment, we do not have an estimate on how long it will take to resolve this problem. It has to be resolved before we can begin the major upgrade scheduled for 2023W44. October 27, 16:30 PT\nNotice: The BeeGFS file system, which hosts /wynton, is experiencing unexpected, major issues. Some or all files on /wynton cannot be accessed, and when attempted, an Communication error on send error is seen. The problem started around 13:45 on Friday 2023-10-27. October 27, 15:10 PT\n\n\n\n\n\nResolve: Login node log2 and data-transfer node dt1 are available again. October 26, 12:15 PT\nUpdate: Development node dev2 is available again. October 24, 12:45 PT\nNotice: Access to login node log2, data-transfer nodes dt1, and development node dev2 will be disabled from Monday-Friday October 23-27, 2023 (2023W43) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node log1, data-transfer nodes dt2, and development nodes dev1 and dev3 are unaffected, so are the Wynton HPC Globus endpoints. October 23, 11:10 PT\n\n\n\nResolved: Login node log1, data-transfer nodes dt2 and pdt2 are available again and are now running Rocky 8. October 20, 17:00 PT\nNotice: Data-transfer nodes dt2 will be disabled this week instead of dt1 as previously announced. October 16, 14:30 PT\nNotice: Access to login node log1, data-transfer nodes dt1, and pdt2 will be disabled from Monday-Friday October 16-20, 2023 (2023W42) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node log2, data-transfer nodes dt2, and pdt1 are unaffected, so are the Wynton HPC Globus endpoints. October 13, 11:20 PT\n\n\n\n\n\nResolved: All corrupted and orphaned files have now been deleted. There might be orphaned directories remaining, which we leave to each user to remove, if they exist. April 3, 2024, 11:15 PT\nUpdate: Reading files whose data was lost on the unrecovered storage targets back in May no longer results in an error message. Instead, the portion of the file that was lost will be replaced by null bytes. Obviously, this results in a file with corrupt content. The admins will be going through and deleting all the corrupted files as soon as possible. It’s a big task and will take some time. July 13, 14:15 PT\nUpdate: The remaining two ZFS storage targets (22004 and 22006) are back online again. July 11, 10:30 PT\nUpdate: Four out of the six ZFS storage targets have been brought back online. Two targets (22004 and 22006) remain offline. If you encounter a “Communication error on send” error, please do not delete or move the affected file. July 6, 17:00 PT\nUpdate: Six ZFS storage targets (22001-22006) are down, because one of the recovered storage targets encountered latent damage that had gone undetected since the recovery in May. This locked up the server and thus all six targets on that server. July 6, 08:30 PT\nUpdate: The final two ZFS storage targets are now serving the BeeGFS file system (/wynton) again. June 30, 11:00 PT\nUpdate: We will be reintroducing the final two ZFS storage targets back into the BeeGFS file system (/wynton) on Friday June 30. The work will start at 10 am and should take an hour or so. During that time, there will be a couple of brief “blips” as we reconfigure the storage. June 29, 23:55 PT\nUpdate: Organizing the data recovered from ZFS storage target 22004 into a form suitable for BeeGFS is taking long than expected. Thus far, we’ve properly replaced 10,354,873 of the 11,351,926 recovered files. Approximately one million files remain. We now hope to complete the work this week. The automatic clean up of old files on /wynton/scratch and /wynton/protected/scratch have been resumed. June 27, 17:00 PT\nUpdate: There are two broken ZFS storage targets (22004 and 21002). We expect to recover most files on target 22004 (approximately 14 TB). The reason it takes this long to recover that storage target is that the file chunks are there, but we have to puzzle them together to reconstruct the original files, which is a slow process. We estimate this process to complete by the end of the week. The files on the other target, target 21002, are unfortunately not recoverable. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 21, 23:30 PT\nNotice: There are two ZFS storage targets that are still failing and offline. We have hopes to be able to recover files from one of them. As of June 9, about 12 TB of low-level, raw file data (out of ~15 TB) was recovered. When that is completed, we will start the tedious work on reconstructing the actual files lost. The consultants are less optimistic about recovering data from second storage target, because it was much more damaged. They will give us the final verdict by the end of the week. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 12, 16:00 PT\n\n\n\n\n\n\nResolved: The job scheduler is now available. Access to /wynton/group, /wynton/protected/group, and /wynton/protected/project has been restored. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 1, 16:00 PT\nUpdate: Wynton will be fully available later today, meaning the job scheduler and access to /wynton/group, /wynton/protected/group, and /wynton/protected/project will be re-enabled. Note, two ZFS storage targets are still faulty and offline, but the work of trying to recover them will continue while we go live. This means that any files on the above re-opened /wynton subfolders that are stored, in part or in full, on those two offline storage targets will be inaccessible. Any attempt to read such files will result in a “Communication error on send” error and stall. To exit, press Ctrl-C. Importantly, do not attempt to remove, move, or update such files! That will make it impossible to recover them! June 1, 12:15 PT\nUpdate: In total 22 (92%) out of 24 failed storage targets has been recovered. The consultant hopes to recover the bulk of the data from one of the two remaining damaged targets. The final damage target is heavily damaged, work on it will continue a few more days, but it is likely it cannot be recovered. The plan is to open up /wynton/group tomorrow Thursday with instructions what to expect for files on the damaged targets. The compute nodes and the job scheduler will also be enabled during the day tomorrow. May 31, 22:45 PT\nUpdate: In total 22 (92%) out of 24 failed storage targets has been recovered. The remaining two targets are unlikely to be fully recovered. We’re hoping to restore the bulk of the files from them, but there is a risk that we will get none back. Then plan is to bring back /wynton/group, /wynton/protected/group, and /wynton/protected/project, and re-enable the job queue, on Thursday. May 31, 01:00 PT\nUpdate: The login, data-transfer, and development nodes (except gpudev1) are now online an available for use. The job scheduler and compute nodes are kept offline, to allow for continued recovery of the failed ZFS storage pools. For the same reason, folders under /wynton/group, /wynton/protected/group, and /wynton/protected/project are locked down, except for groups who have mirrored storage. /wynton/home and /wynton/scratch are fully available. We have suspended the automatic cleanup of old files under /wynton/scratch and /wynton/protected/scratch. The ZFS consultant recovered 3 of the 6 remaining storage targets. We have now recovered in total 21 (88%) out of 24 failed targets. The recovery work will continue on Monday (sic!). May 26, 17:00 PT\nUpdate: All 12 ZFS storage targets on one server pair have been recovered and are undergoing final verification, after which that server pair is back in production. On the remaining server pair with also 12 failed ZFS storage targets, 4 targets have been recovered, 4 possibly have been, and 4 are holding out. We’re continuing our work with the consultant on those targets. These storage servers were installed on 2023-03-28, so it is only files written after that date that may be affected. We are tentatively planning on bringing up the login, data transfer and development nodes tomorrow Friday, prior to the long weekend, but access to directories in /wynton/group, /wynton/protected/group, or /wynton/protected/project will be blocked with the exception for a few groups with mirrored storage. /wynton/home and /wynton/scratch would be fully accessible. May 25, 17:00 PT\nUpdate: 8 more ZFS storage targets were recovered today. We have now recovered in total 17 (71%) out of 24 failed targets. The content of the recovered targets is now being verified. We will continue working with the consultant tomorrow on the remaining 7 storage targets. May 24, 17:00 PT\nUpdate: The maintenance and upgrade of the Wynton network switch was successful and is now completed. We also made progress of recovering the failed ZFS storage targets - 9 (38%) out of 24 failed targets have been recovered. To maximize our chances at a full recovery, Wynton will be kept down until the consultant completes their initial assessment. Details: The contracted ZFS consultant started to work on recovering the failed ZFS storage targets that we have on four servers. During the two hours of work, they quickly recovered another three targets on on the first server, leaving us with only one failed target on that server. Attempts of the same recovery method on the second and third servers were not successful. There was no time today to work on the fourth server. The work to recover the remaining targets will resume tomorrow. After the initial recovery attempt has been attempted on all targets, the consultant, who is one of the lead ZFS developers, plans to load a development version of ZFS on the servers in order to perform more thorough and deep-reaching recovery attempts. May 23, 17:00 PT\nUpdate: Wynton will be kept down until the ZFS-recovery consultant has completed their initial assessment. If they get everything back quickly, Wynton will come back up swiftly. If recovery takes longer, or is less certain, we will look at coming back up without the problematic storage targets. As the purchase is being finalized, we hope that the consultant can start their work either on Tuesday or Wednesday. The UCSF Networking Team is performing more maintenance on the switch tonight. May 22, 23:30 PT\nUpdate: The cluster will be kept offline until at least Tuesday May 23. The BeeGFS file-system failure is because 24 out of 144 ZFS storage targets got corrupted. These 24 storage targets served our “group” storage, which means only files written to /wynton/group, /wynton/protected/group, and /wynton/protected/project within the past couple of months are affected. Files under /wynton/home and /wynton/scratch are not affected. We are scanning the BeeGFS file system to identify exactly which files are affected. Thus far, we have managed to recover 6 (25%) out of the 24 failed targets. The remaining 18 targets are more complicated and we are working with a vendor to start helping us recover them next week. May 19, 10:15 PT\nUpdate: Automatic cleanup of /wynton/scratch has been disabled. May 18, 23:00 PT\nUpdate: Several ZFS storage targets that are used by BeeGFS experienced failures during the scheduled maintenance window. There is a very high risk of partial data loss, but we will do everything possible to minimize the loss. In addition, the Wynton core network switch failed and needs to be replaced. The UCSF IT Infrastructure Network Services Team works with the vendor to get a rapid replacement. May 17, 16:30 PT\nUpdate: The cluster is down and unavailable because of maintenance. May 16, 21:00 PT\nUpdate: There will be a one-day downtime starting at 21:00 on Tuesday May 16 and ending at 17:00 on Wednesday May 17. This is aligned with a planned PG&E power-outage maintenance on May 17. Starting May 2, the maximum job runtime will be decreased on a daily basis from the maximum 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will only be started after the downtime. The default run time is 14 days, so make sure to specify qsub -l h_rt=&lt;run-time&gt; ... if you want something shorter. May 3, 10:00 PT\nUpdate: The updated plan is to only have a 24-hour downtime starting the evening of Tuesday May 16 and end by the end of Wednesday May 17. This is aligned with a planned PG&E power-outage maintenance on May 17. April 24, 11:00 PT\nUpdate: The updated plan is to have the downtime during the week of May 15, 2023 (2023W20). This is aligned with a planned PG&E power-outage maintenance during the same week. March 27, 11:00 PT\nNotice: We will performing a full-week major update to the cluster during late Spring 2023. Current plan is to do this during either the week of May 8, 2023 (2023W19) or the week of May 15, 2023 (2023W20). February 27, 11:00 PT\n\n\n\n\n\n\n\n\nResolved: The cluster maintenance has completed and the cluster is now fully operational again. February 23, 14:00 PT\nUpdate: The cluster has been shut down for maintenance. February 22, 21:00 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Wednesday February 22 until 5:00 pm on Thursday February 23, 2023. This is done to avoid possible file-system and hardware failures when the UCSF Facilities performs power-system maintenance. During this downtime, we will perform cluster maintenance. Starting February 8, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. February 9, 09:00 PT\n\n\n\n\n\n\n\nResolve: Network issues has been resolved and access to all login and data-transfer has been re-established. The problem was physical (a cable was disconnected). January 24, 16:00 PT\nNotice: There is no access to non-PHI login and data-transfer hosts (log[1-2], dt[1-2]). We suspect a physical issue (e.g. somebody kicked a cable), which means we need to send someone onsite to fix the problem. January 24, 14:45 PT\n\n\n\n\n\n\nResolved: The network issue for the proxy servers has been fixed. All development nodes now have working internet access. January 11, 16:00 PT\nWorkarounds: Until this issue has been resolved, and depending on needs, you might try to use a data-transfer node.Some of the software tools on the development nodes are also available on the data-transfer nodes, e.g. curl, wget, and git. January 11, 09:50 PT\nNotice: The development nodes have no internet access, because the network used by out proxy servers is down for unknown reasons. The problem most likely started on January 10 around 15:45. January 11, 09:00 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 94.0 hours = 3.9 days = 1.1%\nUnscheduled: 220.0 hours = 9.2 days = 2.5%\nTotal: 314.0 hours = 13.1 days = 3.6%\nExternal factors: 36% of the above downtime, corresponding to 114 hours (= 4.8 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2022-02-08 (53.5 hours)\n2022-09-27 (40.5 hours)\n\nTotal downtime: 94.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2022-08-05 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2022-09-06 (66 hours)\n\nTotal downtime: 66 hours of which 66 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2022-03-28 (1 hours): Major BeeGFS issues\n2022-03-26 (5 hours): Major BeeGFS issues\n2022-03-18 (100 hours): Major BeeGFS issues\n\nTotal downtime: 106.0 hours of which 0 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2022-03-26 (48 hours): Data-center cooling issues\n\nTotal downtime: 48 hours of which 48 hours were due to external factors\n\n\n\n\n\nNumber of user account: 1,643 (change: +369 during the year)\n\n\n\n\n\n\n\nResolved: The BeeGFS issues have been resolved. At 05:29 this morning, a local file system hosting one of our 12 BeeGFS meta daemons crashed. Normally, BeeGFS detects this and redirects processing to a secondary, backup daemon. In this incident, this failback did not get activated and a manual intervention was needed. November 2, 09:30 PT\nNotice: The BeeGFS file system started to experience issues early morning on Tuesday 2022-11-02. The symptoms are missing files and folders. November 2, 08:15 PT\n\n\n\n\n\n\n\nResolved: The job scheduler is responsive again, but we are not certain what caused the problem. We will keep monitoring the issue. November 1, 16:30 PT\nNotice: The job scheduler, SGE, does not respond to user requests, e.g. qstat and qsub. No new jobs can be submitted at this time. The first reports on problems came in around 09:00 this morning. We are troubleshooting the problem. November 1, 10:25 PT\n\n\n\n\n\n\n\nResolved: The cluster maintenance has completed and the cluster is now fully operational again. September 29, 13:30 PT\nUpdate: The cluster has been shut down for maintenance. September 27, 21:00 PT\nNotice: Wynton will be shut down on Tuesday September 27, 2022 at 21:00. We expect the cluster to be back up by the end of the workday on Thursday September 29. This is done to avoid file-system and hardware failures that otherwise may occur when the UCSF Facilities performs maintenance to the power system in Byers Hall. We will take the opportunity to perform cluster maintenance after the completion of the power-system maintenance. September 14, 17:00 PT\n\n\n\n\n\n\n\nResolved: As of 09:20 on 2022-09-09, the cluster is back in full operation. The queues are enabled, jobs are running, and the development nodes are accepting logins. September 9, 09:35 PT\nUpdate: Login and data-transfer nodes are disabled to minimize the risk for file corruption. September 7, 12:45 PT\nNotice: The Wynton system experiencing system-wide issues, including the file system, due to a campus power glitch. To minimize the risk of corrupting the file system, it was decided to shut down the job scheduler and terminate all running jobs. The power outage at Mission Bay campus happened at 15:13. Despite diesel-generated backup power started up momentarily, it was enough to affect some of our servers. The job scheduler will be offline until the impact on Wynton is fully investigated. September 6, 16:20 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. Aug 9, 12:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Monday August 8 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~14,500 cores) in the graph above. Aug 5, 10:30 PT\n\n\n\n\n\n\nResolved: The Sali lab software module repository is back. Aug 4, 12:00 PT\nNotice: The Sali lab software module repository is back will be unavailable from around 10:30-11:30 today August 4 for maintenance. Aug 4, 03:30 PT\n\n\n\n\n\n\nResolved: The patch of the BeeGFS servers were successfully deployed by 14:30 and went without disruptions. As a side effect, rudimentary benchmarking shows that this patch also improves the overall performance. Since the troubleshooting, bug fixing, and testing started on 2022-03-28, we managed to keep the impact of the bugs to a minimum resulting in only one hour of BeeGFS stall. April 6, 17:00 PT\nUpdate: The BeeGFS servers will be updated tomorrow April 6 at 14:00. The cluster should work as usual during the update. April 5, 17:00 PT\nUpdate: Our load tests over the weekend went well. Next, we will do discrepancy validation tests between our current version and the patch versions. When those pass, we will do a final confirmation with the BeeGFS vendor. We hope to deploy the patch to Wynton in a few days. April 4, 10:30 PT\nUpdate: After a few rounds, we now have a patch that we have confirmed work on our test BeeGFS system. The plan is to do additional high-load testing today and over the weekend. April 1, 10:30 PT\nUpdate: The BeeGFS vendors will send us a patch by tomorrow Tuesday, which we will test on our separate BeeGFS test system. After being validated there, will will deploy it to the main system. We hope to have a patch deploy by the end of the week. March 28, 11:30 PT\nUpdate: We have re-enabled the job scheduler after manually having resolved the BeeGFS meta server issues. We will keep monitoring the problem and send more debug data to the BeeGFS vendors. March 28, 11:00 PT\nNotice: On Monday 2022-03-28 morning at 10:30 the BeeGFS hung again. We put a hold on the job scheduler for now. March 28, 10:30 PT\n\n\n\n\n\n\n\nResolved: The compute nodes and the job scheduler are up and running again. March 26, 11:00 PT\nNotice: The job scheduler as disabled and running jobs where terminated on Saturday 2022-03-26 around 09:00. This was done due to an emergency shutdown because the ambient temperature in the data center started to rise around 08:00 and at 09:00 it hit the critical level, where our monitoring system automatically shuts down compute nodes to prevent further damage. This resulted in the room temperature coming down to normal levels again. We are waiting on UCSF Facilities to restore cooling in the data center. March 26, 10:30 PT\n\n\n\n\n\n\n\nResolved: Just after 03:00 on Saturday 2022-03-26 morning BeeGFS hung. Recover actions were taken at 07:30 and the problem was resolved before 08:00. We have tracked down the problem occur when a user runs more than one rm -r /wynton/path/to/folder concurrently on the same folder. This is a bug in BeeGFS that vendors is aware of. March 26, 10:30 PT\n\n\n\n\n\n\n\nResolved: We have re-enabled the job scheduler, which now processes all queued jobs. We will keep working with the BeeGFS vendor to find a solution to avoid this issue from happening again. March 22, 16:30 PT\nUpdate: The BeeGFS issue has been identified. We identified a job that appears to trigger a bug in BeeGFS, which we can reproduce. The BeeGFS vendor will work on a bug fix. The good news is that the job script that triggers the problem can be tweaked to avoid hitting the bug. This means we can enable the job scheduler as soon as all BeeGFS metadata servers have synchronized, which we expect to take a few hours. March 22, 12:00 PT\nUpdate: The BeeGFS file system troubleshooting continues. The job queue is still disabled. You might experience login and non-responsive prompt issues while we troubleshoot this. We have met with the BeeGFS vendors this morning and we are collecting debug information to allow them to troubleshoot the problem on their end. At the same time, we hope to narrow in on the problem further on our end by trying to identify whether there is a particular job or software running on the queue that might cause this. Currently, we have no estimate when this problem will be fixed. We have another call scheduled with the vendor tomorrow morning. March 21, 11:45 PT\nUpdate: The BeeGFS file system is back online and the cluster can be accessed again. However, we had to put SGE in maintenance mode, which means no jobs will be started until the underlying problem, which is still unknown, has been identified and resolved. The plan is to talk to the BeeGFS vendor as soon as possible after the weekend. Unfortunately, in order to stabilize BeeGFS, we had to kill, at 16:30 today, all running jobs and requeue them on the SGE job scheduler. They are now listed as status ‘Rq’. For troubleshooting purposes, please do not delete any of your ‘Rq’ jobs. March 18, 17:05 PT\nNotification: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues. The problem, which started around 11:45 today, is being investigated. March 18, 11:55 PT\n\n\n\n\n\n\n\nNoticed: UCSF Network IT will be performing maintenance on several network switches in the evening and overnight on Monday March 14. This will not affect jobs running on the cluster. One of the switches is the one which provides Wynton with external network access. When that switch is rebooted, Wynton will be inaccessible for about 15 minutes. This is likely to happen somewhere between 22:00 and 23:00 that evening, but the outage window extends from 21:00 to 05:00 the following morning, so it could take place anywhere in that window. March 11, 10:15 PT\n\n\n\n\n\n\nResolved: Wynton is available again. March 2, 15:30 PT\nUpdate: The Wynton environment is now offline for maintenance work. February 28, 10:00 PT\nClarification: The shutdown will take place early Monday morning February 28, 2022. Also, this is on a Monday and not on a Tuesday (as previously written below). February 22, 11:45 PT\nUpdate: We confirm that this downtime will take place as scheduled. February 14, 15:45 PT\nNotice: We are planning a full file-system maintenance starting on Tuesday Monday February 28, 2022. As this requires a full shutdown of the cluster environment, we will start decreasing the job queue, on February 14, two weeks prior to the shutdown. On February 14, jobs that requires 14 days or less to run will be launched. On February 15, only jobs that requires 13 days or less will be launched, and so on until the day of the downtime. Submitted jobs that would go into the downtime window if launched, will only be launched after the downtime window. November 22, 11:45 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 64 hours = 2.7 days = 0.73%\nUnscheduled: 58 hours = 2.4 days = 0.66%\nTotal: 122 hours = 5.1 days = 1.4%\nExternal factors: 39% of the above downtime, corresponding to 47 hours (=2.0 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2021-05-25 (64 hours)\n\nTotal downtime: 64 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2021-01-29 (up to 14 days)\n2021-07-23 (up to 14 days)\n2021-12-08 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2021-08-26 (28 hours) - Planned Byers Hall power shutdown failed\n2021-11-09 (10 hours) - Unplanned PG&E power outage\n\nTotal downtime: 38 hours of which 38 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2021-03-26 (9 hours) - Campus networks issues causing significant BeeGFS slowness\n2021-07-23 (8 hours) - BeeGFS silently failed disks\n2021-11-05 (3 hours) - BeeGFS non-responsive\n\nTotal downtime: 20 hours of which 9 hours were due to external factors\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2021-04-28 (210 hours) - GPU taken down due to server room cooling issues\n\nTotal downtime: 0 hours\n\n\n\n\n\nNumber of user account: 1,274 (change: +410 during the year)\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. Dec 23, 12:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted tomorrow Thursday December 9 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~12,500 cores) in the graph above.\nDec 8, 16:30 PT\n\n\n\n\n\n\nResolved: Data-transfer node dt1 and Globus file transfers are working again. Dec 21, 13:20 PT\nUpdate: Globus file transfers to and from Wynton are not working. This is because Globus relies on the data-transfer node dt1, which is currently down. Dec 20, 15:30 PT\nNotice: Data-transfer node dt1 has issues. Please use dt2 until resolved. The first report on this problem came yesterday at 21:30. Dec 20, 09:30 PT\n\n\n\n\n\n\nResolved: All hosts have been rebooted and are now up and running. November 9, 11:00 PT\nNotice: There was a brief PG&E power outage early Tuesday November 9 around 01:20. This affected the power on the Mission Bay campus, including the data center housing Wynton. The parts of our system with redundant power were fine, but many of the compute nodes are on PG&E-power only and, therefore, went down. As a result, lots of jobs crashed. We will restart the nodes that crashed manually during the day today. November 9, 09:10 PT\n\n\n\n\n\n\n\nResolved: Resynchronization of all file-system meta servers is complete, which concludes the maintenance. October 26, 09:45 PT\nUpdate: The maintenance work has started. October 25, 14:00 PT\nNotice: We will perform BeeGFS maintenance work starting Monday October 25 at 2:00 pm. During this work, the filesystem might be less performant. We don’t anticipate any downtime. October 21, 12:10 PT\n\n\n\n\n\n\nResolved: The corrupted filesystem has been recovered. September 10, 17:20 PT\nUpdate: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on /wynton/group/. The file recovery tools are still running. August 27, 13:05 PT\nPartially resolved: Wynton is back online but the problematic BeeGFS filesystem is kept offline, which affects access to some of the folders and files hosted on /wynton/group/. The file recovery tools are still running. August 27, 13:05 PT\nUpdate: The BeeGFS filesystem recovering attempt keeps running. The current plan is to bring Wynton back online while keeping the problematic BeeGFS filesystem offline. August 26, 23:05 PT\nUpdate: All of the BeeGFS servers are up and running, but one of the 108 filesystems that make up BeeGFS was corrupted by the sudden power outage. The bad filesystem is part of /wynton/group/. We estimate that 70 TB of data is affected. We are making every possible effort to restore this filesystem, which will take time. While we do so, Wynton will remain down. August 26, 14:05 PT\nNotice: The cluster is down after an unplanned power outage in the main data center. The power is back online but several of our systems, including BeeGFS servers, did not come back up automatically and will require on-site, manual actions. August 26, 09:15 PT\n\n\n\n\n\n\n\nResolved: The majority of the compute nodes have been rebooted after only four days, which was quicker than the maximum of 14 days. July 28, 08:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted at 13:00 on Friday July 23 at 1:00 pm. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above.\nJuly 23, 07:40 PT\n\n\n\n\n\n\nResolved: Wynton and BeeGFS is back online. The problem was due to failed disks. Unfortunately, about 10% of the space in /wynton/scratch/ went bad, meaning some files are missing or corrupted. It is neither possible to recover them nor identify which files or folders are affected. In other words, expect some oddness if you had data under /wynton/scratch/. There will also be some hiccups over the next several days as we get everything in ZFS and BeeGFS back into an as stable state as possible. June 24, 14:55 PT\nUpdate: We’re working hard on getting BeeGFS back up. We were not able to recover the bad storage target, so it looks like there will be some data loss on /wynton/scratch/. More updates soon. June 24, 13:45 PT\nNotification: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues since early this morning. The problem is being investigated. June 24, 07:00 PT\n\n\n\n\n\n\n\nResolved: All remaining issues from the downtime have been resolved.  June 7, 17:00 PT\nUpdate: Login node log2 can now be reached from the UCSF Housing WiFi network.  June 7, 17:00 PT\nUpdate: dt2 can now be reached from outside the Wynton cluster.  June 7, 13:15 PT\nUpdate: Login node log2 cannot be reached from the UCSF Housing WiFi network. If you are on that network, use log1 until this has been resolved.  June 2, 07:00 PT\nUpdate: Both data transfer nodes are back online since a while, but dt2 can only be reached from within the Wynton cluster.  June 1, 13:45 PT\nUpdate: A large number of of the remaining compute nodes have been booted up. There are now ~8,600 cores serving jobs.  June 1, 10:15 PT\nUpdate: The development nodes are now back too. For the PHI pilot project, development node pgpudev1 is back up, but pdev1 is still down.  May 28, 10:00 PT\nUpdate: Wynton is partially back up and running. Both login hosts are up (log1 and log2). The job scheduler, SGE, accepts new jobs and and launches queued jobs. Two thirds of the compute node slots are back up serving jobs. Work is done to bring up the the development nodes and the data transfer hosts (dt1 and dt2).  May 27, 10:30 PT\nUpdate: We hit more than a few snags today. Our filesystem, BeeGFS, is up and running, but it still needs some work. The login hosts are up, but SGE is not and neither are the dev nodes. We will continue the work early tomorrow Thursday.  May 26, 21:40 PT\nNotice: The Wynton HPC environment will be shut down late afternoon on Tuesday May 25, 2021, for maintenance. We expect the cluster to be back online late Wednesday May 26. To allow for an orderly shutdown of Wynton, the queues have been disabled starting at 3:30 pm on May 25. Between now and then, only jobs whose runtimes end before that time will be able to start. Jobs whose runtimes would run into the maintenance window will remain in the queue.  May 10, 16:40 PT\nPreliminary notice: The Wynton HPC cluster will be undergoing a major upgrade on Wednesday May 26, 2021. As usual, starting 15 days prior to this day, on May 11, the maximum job run-time will be decreased on a daily basis so that all jobs finishes in time, e.g. if you submit a job on May 16 with a run-time longer than nine days, it will not be able to scheduled and it will be queued until after the downtime.  May 3, 11:00 PT\n\n\n\n\n\n\n\nResolved: Password updates works again.  June 2, 10:30 PT\nNotice: Due to technical issues, it is currently not possible to change your Wynton password. If attempted from the web interface, you will get an error on “Password change not successful! (kadmin: Communication failure with server while initializing kadmin interface )”. If attempted using ‘passwd’, you will get “passwd: Authentication token manipulation error”.  June 1, 10:30 PT\n\n\n\n\n\n\nResolved: Cooling has been restored and all GPU nodes are back online again.  May 7, 11:10 PT\nUpdate: Half of the GPU nodes that was taken down are back online. Hopefully, the remaining ones can be brought back up tomorrow when the cooling in the server room should be fully functioning again.  May 6, 14:30 PT\nNotification: One of Wynton’s ancillary server rooms is having cooling issues. To reduce the heat load in the room, we had to turn off all the Wynton nodes in the room around 09:45 this morning. This affects GPU nodes named msg*gpu* and a few other regular nodes. We estimate that the UCSF Facilities to fix the cooling problem by early next week.  April 28, 16:30 PT\n\n\n\n\n\n\n\nResolved: The malfunctioning network link between two of Wynton’s data centers, which affected our BeeGFS file system and Wynton HPC as a whole, has been restored. March 26, 21:30 PT\nNotification: Campus network issues causing major Wynton HPC issues including extremely slow access to our BeeGFS file system. This was first reported around 11:30 today. A ticket has been filed with the UCSF Network. ETA is unknown. March 26, 12:30 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. A few compute nodes remain offline that has to be rebooted manually, which will be done as opportunity is given. February 13, 09:00 PT\nNotice: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Login, data-transfer, and development nodes will be rebooted at 13:00 on Monday February 1. January 31, 17:00 PT\n\n\n\n\n\n\nResolved: Development node dev2 is available again. February 3, 15:00 PT\nNotice: Development node dev2 is down. It failed to come back up after the kernel upgrade on 2021-02-01. An on-site reboot is planned for Wednesday February 3. February 2, 11:45 PT\n\n\n\n\n\n\nNotice: The air conditioning system in one of our server rooms will be upgraded on January 28. The compute nodes in this room will be powered down during the upgrade resulting in fewer compute nodes being available on the cluster. Starting 14 days prior to this date, compute nodes in this room will only accept jobs that will finish in time. January 13, 10:00 PT\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 123 hours = 5.1 days = 1.4%\nUnscheduled: 91.5 hours = 3.8 days = 1.0%\nTotal: 214.5 hours = 8.9 days = 2.4%\nExternal factors: 12% of the above downtime, corresponding to 26.5 hours (=1.1 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2020-08-10 (93 hours)\n2020-12-07 (30 hours)\n\nTotal downtime: 123 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2020-06-11 (up to 14 days)\n2020-12-11 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nNone\n\nTotal downtime: 0 hours\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2020-01-22 (2.5 hours) - BeeGFS failure to failed upgrade\n2020-01-29 (1.0 hours) - BeeGFS non-responsive\n2020-02-05 (51.5 hours) - Legacy NetApp file system failed\n2020-05-22 (0.5 hours) - BeeGFS non-responsive to failed upgrade\n2020-08-19 (1.5 hours) - BeeGFS non-responsive\n2020-10-21 (3 hours) - BeeGFS non-responsive\n2020-11-05 (3 hours) - BeeGFS non-responsive\n\nTotal downtime: 63.0 hours\n\n\n\n\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2020-05-28 (26.5 hours) - MSG Data Center outage affecting many GPU compute nodes\n2020-07-04 (2 hours) - SGE scheduler failed\n2020-11-04 (288 hours) - ~80 compute nodes lost due to network switch failure\n\nTotal downtime: 28.5 hours of which 26.5 hours were due to external factors\n\n\n\n\n\nNumber of user account: 864 (change: +386 during the year)\n\n\n\n\n\n\n\nResolved: Login node ‘log1.wynton.ucsf.edu’ can again be accessed from outside of the UCSF network. December 17, 14:20 PT\nNotice: Login node ‘log1.wynton.ucsf.edu’ is only accessible from within UCSF network. This is a side effect of the recent network upgrades. We are waiting for The UCSF IT Network to resolve this for us. Until resolved, please use the alternative ‘log2.wynton.ucsf.edu’ login node when connecting from outside of the UCSF network. December 8, 23:00 PT\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. December 16, 05:00 PT\nNotice: The new BeeGFS setting introduced during the upgrades earlier this week caused problems throughout the system and we need to roll them back. The compute nodes will no longer take on new jobs until they have been rebooted. A compute node will be automatically rebooted as soon as all of its running jobs have completed. Unfortunately, we have to kill jobs that run on compute nodes that are stalled and suffer from the BeeGFS issues. December 11, 13:50 PT\n\n\n\n\n\n\nResolved: All login and development nodes have been rebooted. December 12, 17:00 PT\nNotice: Login node ‘log1.wynton.ucsf.edu’ and all the development nodes will be rebooted at 4:30 PM today Friday. This is needed in order to roll back the new BeeGFS setting introduced during the upgrades earlier this week. December 11, 13:50 PT\n\n\n\n\n\n\nResolved: The upgrade has been completed. The cluster back online, including all of the login, data-transfer, and development nodes, as well as the majority of the compute nodes. The scheduler is processing jobs again. All hosts now run CentOS 7.9. December 8, 16:30 PT\nUpdate: The upgrade is paused and will resume tomorrow. We hope to be bring all of the cluster back online by the end of tomorrow. For now, login node ‘log2’ (but not ‘log1’), and data-transfer nodes ‘dt1’, and ‘dt2’ are back online and can be used for accessing files. Development nodes ‘dev1’ and ‘dev3’ are also available (please make sure to leave room for others). The scheduler remains down, i.e. it is is not be possible to submit jobs. December 7, 17:00 PT\nUpdate: The upgrades have started. Access to Wynton HPC has been disable as of 10:30 this morning. The schedulers stopped launching queued jobs as of 23:30 last night. December 7, 10:30 PT\nRevised notice: We have decided to hold back on upgrading BeeGFS during the downtime and only focus on the remain parts including operating system and network upgrades. The scope of the work is still non-trivial. There is a risk that the downtime will extend into Thursday December 10. However, if everything go smoothly, we hope that Wynton HPC will be back up by the end of Monday or during the Tuesday. There will only be one continuous downtime, that is, when the cluster comes back up, it will stay up. December 3, 09:00 PT\nNotice: Starting early Monday December 7, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the operating system, the network, and the BeeGFS file system. We anticipate that the cluster will be available again by the end of Tuesday December 8, when load testing of the upgraded BeeGFS file system will start. If these tests fail, we will have to unroll the BeeGFS upgrade, which in case we anticipate that the cluster is back online by the end of Wednesday December 9. November 23, 16:50 PT\n\n\n\n\n\n\n\nResolved: All 74 compute nodes that were taken off the job scheduler on 2020-11-04 are back up and running November 16, 12:00 PT\nNotice: 74 compute nodes, including several GPU nodes, were taken off the job scheduler around 14:00 on 2020-11-04 due to a faulty network switch. The network switch needs to be replaced in order to resolve this. November 4, 16:10 PT\n\n\n\n\n\n\n\nResolved: Our BeeGFS file system was non-responsive during 01:20-04:00 on 2020-11-05 because one of the meta servers hung. November 5, 08:55 PT\n\n\n\n\n\n\n\nResolved: Our BeeGFS file system was non-responsive because one of its meta servers hung, which now has been restarted. October 21, 11:15 PT\nNotice: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 09:30 today. October 21, 10:45 PT\n\n\n\n\n\n\n\nResolved: Our BeeGFS file system was non-responsive between 17:22 and 18:52 today because one of its meta servers hung while the other attempted to synchronize to it. August 19, 19:00 PT\nNotice: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 17:30 today. August 19, 18:15 PT\n\n\n\n\n\n\n\nResolved: The cluster is fully back up and running. Several compute nodes still need to be rebooted but we consider this upgrade cycle completed. The network upgrade took longer than expected, which delayed the processes. We hope to bring the new lab storage online during the next week. August 13, 21:00 PT\nUpdate: All login, data-transfer, and development nodes are online. Additional compute nodes are being upgraded and are soon part of the pool serving jobs. August 13, 14:50 PT\nUpdate: Login node log1, data-transfer node dt2, and the development nodes are available again. Compute nodes are going through an upgrade cycle and will soon start serving jobs again. The upgrade work is taking longer than expected and will continue tomorrow Thursday August 13. August 12, 16:10 PT\nNotice: All of the Wynton HPC environment is down for maintenance and upgrades. August 10, 00:00 PT\nNotice: Starting early Monday August 10, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the network and adding lab storage purchased by several groups. We anticipate that the cluster will be available again by the end of Wednesday August 12. July 24, 15:45 PT\n\n\n\n\n\n\n\nResolved: All three development nodes have been rebooted. July 6, 15:20 PT\nNotice: The three regular development nodes have all gotten themselves hung up on one particular process. This affects basic system operations and preventing such basic commands as ps and w. To clear this state, we’ll be doing an emergency reboot of the dev nodes at about 15:15. July 6, 15:05 PT\n\n\n\n\n\n\nResolved: The SGE scheduler produced errors when queried or when jobs were submitted or launched. The problem started 00:30 and lasted until 02:45 early Sunday 2020-07-05. July 6, 22:00 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. June 26, 10:45 PT\nUpdate: Development node dev3 is back online. June 15, 11:15 PT\nUpdate: Development node dev3 is not available. It failed to reboot and requires on-site attention, which might not be possible for several days. All other log-in, data-transfer, and development nodes were rebooted successfully. June 11, 15:45 PT\nNotice: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Thursday June 11. June 11, 10:45 PT\n\n\n\n\n\n\nResolved: Internet access from the development nodes is available again. A new web-proxy server had to be built and deploy. June 9, 09:15 PT\nNotice: Internet access from the development nodes is not available. This is because the proxy server providing them with internet access had a critical hardware failure around 08-09 this morning. At the most, we cannot provide an estimate when we get to restore this server. June 5, 16:45 PT\n\n\n\n\n\n\nUpdate: The upgrade of the BeeGFS filesystem introduced new issues. We decided to rollback the upgrade and we are working with the vendor. There is no upgrade planned for the near term. June 8, 09:00 PT\nUpdate: The BeeGFS filesystem has been upgraded using a patch from the vendor. The patch was designed to lower the amount of resynchronization needed between the two metadata servers. Unfortunately, after the upgrade we observe an increase of resynchronization. We will keep monitoring the status. If the problem remains, we will consider a rollback to the BeeGFS version used prior to May 18. May 22, 01:25 PT\nUpdate: For a short moment around 01:00 early Friday, both of our BeeGFS metadata servers were down. This may have lead to some applications experiencing I/O errors around this time. May 22, 01:25 PT\nNotice: Work to improve the stability of the BeeGFS filesystem (/wynton) will be conducted during the week of May 18-22. This involves restarting the eight pairs of metadata server processes, which may result in several brief stalls of the file system. Each should last less than 5 minutes and operations will continue normally after each one. May 6, 15:10 PT\n\n\n\n\n\n\n\nResolved: The GPU compute nodes are now fully available to serve jobs. May 29, 12:00 PT\nUpdate: The GPU compute nodes that went down yesterday have been rebooted. May 29, 11:10 PT\nInvestigating: A large number of GPU compute nodes in the MSG data center are currently down for unknown reasons. We are investigating the cause. May 28, 09:35 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC system is considered fully functional again. The legacy, deprecated NetApp storage was lost. February 10, 10:55 PT\nUpdate: The majority of the compute nodes have been rebooted and are now online and running jobs. We will actively monitor the system and assess the how everything works before we considered this incident resolved. February 7, 13:40 PT\nUpdate: The login, development and data transfer nodes will be rebooted at 01:00 today Friday February 7. February 7, 12:00 PT\nUpdate: The failed legacy NetApp server is the cause to the problems, e.g. compute nodes not being responsive causing problems for SGE etc. Because of this, all of the cluster - login, development, transfer, and computes nodes - will be rebooted tomorrow Friday 2020-02-07. February 6, 10:00 PT\nNotice: Wynton HPC is experience major issues due to NetApp file-system failure, despite this is being deprecated and not used much these days. The first user report on this came in around 09:00 and the job-queue logs suggests the problem began around 02:00. It will take a while for everything to come back up and there will be brief BeeGFS outage while we reboot the BeeGFS management node. February 5, 10:15 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file-system issue has been resolved by rebooting two meta servers. January 29, 17:00 PT\nNotice: There’s currently an issue with the BeeGFS file system. Users reporting that they cannot log in. January 29, 16:00 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS upgrade issue has been resolved. Jan 22, 14:30 PT\nUpdate: The planned upgrade caused unexpected problems to the BeeGFS file system resulting in /wynton/group becoming unstable. Jan 22, 13:35 PT\nNotice: One of the BeeGFS servers, which serve our cluster-wide file system, will be swapped out starting at noon (11:59am) on Wednesday January 22, 2020 and the work is expected to last one hour. We don’t anticipate any downtime because the BeeGFS servers are mirrored for availability. Jan 16, 14:40 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been updated and rebooted. Jan 4, 11:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. December 20, 10:20 PT\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 96 hours = 4.0 days = 1.1%\nUnscheduled: 83.5 hours = 3.5 days = 1.0%\nTotal: 179.5 hours = 7.5 days = 2.0%\nExternal factors: 15% of the above downtime, corresponding to 26 hours (=1.1 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2021-01-09 (1.0 hours) - job scheduler updates\n2021-07-08 (95 hours)\n\nTotal downtime: 96.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2019-01-22 (up to 14 days)\n2019-03-21 (up to 14 days)\n2019-10-29 (up to 14 days)\n2019-12-22 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2019-07-30 (6.5 hour) - Byers Hall power outage\n2019-08-15 (5.5 hour) - Diller power outage\n2019-10-25 (1.0 hour) - Byers Hall power outage\n2019-10-22 (13.0 hour) - Diller power backup failed during power maintenance\n\nTotal downtime: 26.0 hours of which 26.0 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2019-01-08 (2.0 hours) - BeeGFS server non-responsive\n2019-01-14 (1.5 hours) - BeeGFS non-responsive\n2019-05-15 (24.5 hours) - BeeGFS non-responsive\n2019-05-17 (5.0 hours) - BeeGFS slowdown\n2019-06-17 (10.5 hours) - BeeGFS non-responsive\n2019-08-23 (4.0 hours) - BeeGFS server non-responsive\n2019-09-24 (3.0 hours) - BeeGFS server non-responsive\n2019-12-18 (3.5 hours) - Network switch upgrade\n2019-12-22 (5.5 hours) - BeeGFS server non-responsive\n\nTotal downtime: 58.5 hours of which 0 hours were due to external factors\n\n\n\n\n\nNumber of user account: 478 (change: +280 during the year)\n\n\n\n\n\n\n\nResolved: All compute nodes have been updated and rebooted. Jan 4, 11:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. December 20, 10:20 PT\n\n\n\n\n\n\nResolved: No further hiccups were needed during the BeeGFS resynchronization. Everything is working as expected. December 23, 10:00 PT\nUpdate: The issues with login was because the responsiveness of one of the BeeGFS file servers became unreliable around 04:20. Rebooting that server resolved the problem. The cluster is fully functional again although slower than usual until the file system have been resynced. After this, there might be a need for one more, brief, reboot. December 22, 14:40 PT\nNotice: It is not possible to log in to the Wynton HPC environment. The reason is currently not known. December 22, 09:15 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is fully functional again. The BeeGFS filesystem was not working properly during 18:30-22:10 on December 18 resulting in no login access to the cluster and job file I/O being backed up. December 19, 08:50 PT\nUpdate: The BeeGFS filesystem is non-responsive, which we believe is due to the network switch upgrade. December 18, 21:00 PT\nNotice: One of two network switches will be upgraded on Wednesday December 18 starting at 18:00 and lasting a few hours. We do not expect this to impact the Wynton HPC environment other than slowing down the network performance to 50%. December 17, 10:00 PT\n\n\n\n\n\n\n\nResolved: All compute nodes have been updated and rebooted. Nov 11, 01:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). GPU nodes will be rebooted as soon as all GPU jobs complete. During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,000 cores) in the graph above. Oct 29, 16:30 PT\n\n\n\n\n\n\nResolved: Development node qb3-dev2 was rebooted. Data-transfer node dt1.wynton.ucsf.edu is kept offline because it is scheduled to be upgraded next week. October 28, 15:00 PT\nUpdate: Most compute nodes that went down due to the power glitch has been rebooted. Data-transfer node dt1.wynton.ucsf.edu and development node qb3-dev2 are still down - they will be brought back online on Monday October 28. October 25, 14:00 PT\nNotice: A very brief power outage in the Byers Hall building caused several compute nodes in its Data Center to go down. Jobs that were running on those compute nodes at the time of the power failure did unfortunately fail. Log-in, data-transfer, and development nodes were also affected. All these hosts are currently being rebooted. October 25, 13:00 PT\n\n\n\n\n\n\n\nResolved: Log in works again. October 24, 09:45 PT\nNotice: It is not possible to log in to the Wynton HPC environment. This is due to a recent misconfiguration of the LDAP server. October 24, 09:30 PT\n\n\n\n\n\n\nResolved: The Wynton HPC BeeGFS file system is fully functional again. During the outage, /wynton/group and /wynton/scratch was not working properly, whereas /wynton/home was unaffected. October 23, 10:35 PT\nNotice: The Wynton HPC BeeGFS file system is non-functional. It is expected to be resolved by noon on October 23. The underlying problem is that the power backup at the Diller data center did not work as expected during a planned power maintenance. October 22, 21:45 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. September 24, 20:25 PT\nNotice: The Wynton HPC environment is unresponsive. Problem is being investigated. September 24, 17:30 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. The reason for this downtime was the BeeGFS file server became unresponsive. August 23, 20:45 PT\nNotice: The Wynton HPC environment is unresponsive. August 23, 16:45 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. August 15, 21:00 PT\nNotice: The Wynton HPC environment is down due to a non-planned power outage at the Diller data center. Jobs running on compute nodes located in that data center, were terminated. Jobs running elsewhere may also have been affected because /wynton/home went down as well (despite it being mirrored). August 15, 15:45 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment is up and running again. July 30, 14:40 PT\nNotice: The Wynton HPC environment is down due to a non-planned power outage at the main data center. July 30, 08:20 PT\n\n\n\n\n\n\n\nResolved: The Wynton HPC environment and the BeeGFS file system are fully functional after updates and upgrades. July 12, 11:15 PT\nNotice: The Wynton HPC environment is down for maintenance. July 8, 12:00 PT\nNotice: Updates to the BeeGFS file system and the operating system that require to bring down all of Wynton HPC will start on the morning of Monday July 8. Please make sure to log out before then. The downtime might last the full week. July 1, 14:15 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file system is fully functional again. June 18, 01:30 PT\nInvestigating: Parts of /wynton/scratch and /wynton/group are currently unavailable. The /wynton/home space should be unaffected. June 17, 15:05 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file system and the cluster is functional again. May 17, 16:00 PT\nInvestigating: There is a major slowdown of the BeeGFS file system (/wynton), which in turn causes significant problems throughout the Wynton HPC environment. May 17, 10:45 PT\n\n\n\n\n\n\n\nResolved: The BeeGFS file system, and thereby also the cluster itself, is functional again. May 16, 10:30 PT\nInvestigating: The BeeGFS file system (/wynton) is experiencing major issues. This caused all on Wynton HPC to become non-functional. May 15, 10:00 PT\n\n\n\n\n\n\n\nResolved: The UCSF-wide network issue that affected access to Wynton HPC has been resolved. May 15, 15:30 PT\nUpdate: The login issue is related to UCSF-wide network issues. May 15, 13:30 PT\nInvestigating: There are issues logging in to Wynton HPC. May 15, 10:15 PT\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. April 5, 12:00 PT\nUpdate: Nearly all compute nodes have been rebooted (~5,200 cores are now available). Mar 29, 12:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 5,424 cores) in the graph above. Mar 21, 15:30 PT\n\n\n\n\n\n\nResolved: The login, development and transfer hosts have been rebooted. March 22, 10:35 PT\nNotice: On Friday March 22 at 10:30am, all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. These hosts should be offline for less than 5 minutes. Mar 21, 15:30 PT\n\n\n\n\n\n\nResolved: All compute nodes have been rebooted. Feb 5, 11:30 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 1,944 cores) in the graph above. Jan 22, 16:45 PT\n\n\n\n\n\n\nResolved: The login, development and transfer hosts have been rebooted. Jan 23, 13:00 PT\nNotice: On Wednesday January 23 at 12:00 (noon), all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. The hosts should be offline for less than 5 minutes. Jan 22, 16:45 PT\n\n\n\n\n\n\nResolved: The file system under /wynton/ is back up again. We are looking into the cause and taking steps to prevent this from happening again. Jan 9, 12:45 PT\nInvestigating: The file system under /wynton/ went down around 11:30 resulting is several critical failures including the scheduler failing. Jan 14, 11:55 PT\n\n\n\n\n\n\n\nResolved: The SGE job scheduler is now back online and accepts new job submission again. Jan 9, 12:45 PT\nUpdate: The downtime of the job scheduler will begin on Wednesday January 9 @ noon and is expected to be completed by 1:00pm. Jan 8, 16:00 PT\nNotice: There will be a short job-scheduler downtime on Wednesday January 9 due to SGE maintenance. During this downtime, already running jobs will keep running and queued jobs will remain in the queue, but no new jobs can be submitted. Dec 20, 12:00 PT\n\n\n\n\n\n\n\nInvestigating: One of the parallel file-system servers (BeeGFS) appears to have crashed on Monday January 7 at 07:30 and was recovered on 9:20pm. Right now we are monitoring its stability, and investigating the cause and what impact it might have had. Currently, we believe users might have experienced I/O errors on /wynton/scratch/ whereas /wynton/home/ was not affected. Jan 8, 10:15 PT\n\n\n\n\n\n\n\n\n\n\nFull downtime:\n\nScheduled: 0 hours = 0.0%\nUnscheduled: 84 hours = 3.5 days = 1.9%\nTotal: 84 hours = 3.5 days = 1.9%\nExternal factors: 100% of the above downtime, corresponding to 84 hours (=3.5 days), were due to external factors\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nNone\n\nTotal downtime: 0.0 hours\n\n\n\n\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2018-09-28 (up to 14 days)\n\n\n\n\n\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2018-06-17 (23 hours) - Campus power outage\n2018-11-08 (19 hours) - Byers Hall power maintenance without notice\n2018-12-14 (42 hours) - Sandler Building power outage\n\nTotal downtime: 84 hours of which 84 hours were due to external factors\n\n\n\n\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nNone.\n\nTotal downtime: 0.0 hours\n\n\n\n\n\nNumber of user account: 198 (change: +103 during the year)\n\n\n\n\n\n\n\nResolved: Parts of the new BeeGFS file system was non-functional for approx. 1.5 hours during Friday December 21 when a brief maintenance task failed. Dec 21, 20:50 PT\n\n\n\n\n\n\n\nResolved: All of the `msg-* compute nodes but one are operational. Dec 20, 16:40 PT\nNotice: Starting Wednesday December 12 around 11:00, several msg-* compute nodes went down (~200 cores in total). The cause of this is unknown. Because it might be related to the BeeGFS migration project, the troubleshooting of this incident will most likely not start until the BeeGFS project is completed, which is projected to be done on Wednesday December 19. Dec 17, 17:00 PT\n\n\n\n\n\n\n\nResolved: Development node qb3-dev1 is functional. Dec 18, 20:50 PT\nInvestigating: Development node qb3-dev1 does not respond to SSH. This will be investigated the first thing tomorrow morning (Wednesday December 19). In the meanwhile, development node qb3-gpudev1, which is “under construction”, may be used. Dec 18, 16:30 PT\n\n\n\n\n\n\nResolved: /wynton/scratch is now back online and ready to be used. Dec 19, 14:20 PT\nUpdate: The plan is to bring /wynton/scratch back online before the end of the day tomorrow (Wednesday December 19). The planned SGE downtime has been rescheduled to Wednesday January 9. Moreover, we will start providing the new 500-GiB /wynton/home/ storage to users who explicitly request it (before Friday December 21) and who also promise to move the content under their current /netapp/home/ to the new location. Sorry, users on both QB3 and Wynton HPC will not be able to migrate until the QB3 cluster has been incorporated into Wynton HPC (see Roadmap) or they giving up their QB3 account. Dec 18, 16:45 PT\nUpdate: The installation and migration to the new BeeGFS parallel file servers is on track and we expect to go live as planned on Wednesday December 19. We are working on fine tuning the configuration, running performance tests, and resilience tests. Dec 17, 10:15 PT\nUpdate: /wynton/scratch has been taken offline. Dec 12, 10:20 PT\nReminder: All of /wynton/scratch will be taken offline and completely wiped starting Wednesday December 12 at 8:00am. Dec 11, 14:45 PT\nNotice: On Wednesday December 12, 2018, the global scratch space /wynton/scratch will be taken offline and completely erased. Over the week following this, we will be adding to and reconfiguring the storage system in order to provide all users with new, larger, and faster (home) storage space. The new storage will served using BeeGFS, which is a new much faster file system - a system we have been prototyping and tested via /wynton/scratch. Once migrated to the new storage, a user’s home directory quota will be increased from 200 GiB to 500 GiB. In order to do this, the following upgrade schedule is planned:\n\nWednesday November 28-December 19 (21 days): To all users, please refrain from using /wynton/scratch - use local, node-specific /scratch if possible (see below). The sooner we can take it down, the higher the chance is that we can get everything in place before December 19.\nWednesday December 12-19 (8 days): /wynton/scratch will be unavailable and completely wiped. For computational scratch space, please use local /scratch unique to each compute node. For global scratch needs, the old and much slower /scrapp and /scrapp2 may also be used.\nWednesday December 19, 2018 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline. No jobs will be able to be submitted until it is restarted.\nWednesday December 19, 2018: The upgraded Wynton HPC with the new storage will be available including /wynton/scratch.\nWednesday January 9, 2019 (1/2 day): The Wynton HPC scheduler (SGE) will be taken offline temporarily. No jobs will be able to be submitted until it is restarted.\n\nIt is our hope to be able to keep the user’s home accounts, login nodes, the transfer nodes, and the development nodes available throughout this upgrade period.\nNOTE: If our new setup proves more challenging than anticipated, then we will postpone the SGE downtime to after the holidays, on Wednesday January 9, 2019. Wynton HPC will remain operational over the holidays, though without /wynton/scratch. Dec 6, 14:30 PT [edited Dec 18, 17:15 PT]\n\n\n\n\n\n\n\nResolved: All mac-* compute nodes are up and functional. Dec 14, 12:00 PT\nInvestigating: The compute nodes named mac-* (in the Sandler building) went down due to power failure on Wednesday December 12 starting around 05:50. Nodes are being rebooted. Dec 12, 09:05 PT\n\n\n\n\n\n\n\nResolved: The cluster is full functional. It turns out that none of the compute nodes, and therefore none of the running jobs, were affected by the power outage. Nov 8, 11:00 PT\nUpdate: The queue-metric graphs are being updated again. Nov 8, 11:00 PT\nUpdate: The login nodes, the development nodes and the data transfer node are now functional. Nov 8, 10:10 PT\nUpdate: Login node wynlog1 is also affected by the power outage. Use wynlog2 instead. Nov 8, 09:10 PT\nNotice: Parts of the Wynton HPC cluster will be shut down on November 8 at 4:00am. This shutdown takes place due to the UCSF Facilities shutting down power in the Byers Hall. Jobs running on affected compute nodes will be terminated abruptly. Compute nodes with battery backup or in other buildings will not be affected. Nodes will be rebooted as soon as the power comes back. To follow the reboot progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. Unfortunately, the above queue-metric graphs cannot be updated during the power outage. Nov 7, 15:45 PT\n\n\n\n\n\n\n\nResolved: The compute nodes has been rebooted and are accepting new jobs. For the record, on day 5 approx. 300 cores were back online, on day 7 approx. 600 cores were back online, on day 8 approx. 1,500 cores were back online, and on day 9 the majority of the 1,832 cores were back online. Oct 11, 09:00 PT\nNotice: On September 28, a kernel update was applied to all compute nodes. To begin running the new kernel, each node must be rebooted. To achieve this as quickly as possible and without any loss of running jobs, the queues on the nodes were all disabled (i.e., they stopped accepting new jobs). Each node will reboot itself and re-enable its own queues as soon as all of its running jobs have completed. Since the maximum allowed run time for a job is two weeks, it may take until October 11 before all nodes have been rebooted and accepting new jobs. In the meanwhile, there will be fewer available slots on the queue than usual. To follow the progress, see the ‘Available CPU cores’ curve (target 1,832 cores) in the graph above. Sept 28, 16:30 PT\n\n\n\n\n\n\nResolved: The login, development, and data transfer hosts have been rebooted. Oct 1, 13:30 PT\nNotice: On Monday October 1 at 01:00, all of the login, development, and data transfer hosts will be rebooted. Sept 28, 16:30 PT\n\n\n\n\n\n\nResolved: Around 11:00 on Wednesday September 12, the SGE scheduler (“qmaster”) became unreachable such that the scheduler could not be queried and no new jobs could be submitted. Jobs that relied on run-time access to the scheduler may have failed. The problem, which was due to a misconfiguration being introduced, was resolved early morning on Thursday September 13. Sept 13, 09:50 PT\n\n\n\n\n\n\n\nResolved: Nodes were rebooted on August 1 shortly after the power came back. Aug 2, 08:15 PT\nNotice: On Wednesday August 1 at 6:45am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. Jul 30, 20:45 PT\n\n\n\n\n\n\n\nResolved: The nodes brought down during the July 30 partial shutdown has been rebooted. Unfortunately, the same partial shutdown has to be repeated within a few days because the work in server room was not completed. Exact date for the next shutdown is not known at this point. Jul 30, 09:55 PT\nNotice: On Monday July 30 at 7:00am, parts of the compute nodes (msg-io{1-10} + msg-*gpu) will be powered down. They will be brought back online within 1-2 hours. The reason is a planned power shutdown affecting one of Wynton HPC’s server rooms. Jul 29, 21:20 PT\n\n\n\n\n\n\nResolved: The Nvidia-driver issue occurring on some of the GPU compute nodes has been fixed. Jun 26, 11:55 PT\nUpdate: Some of the compute nodes with GPUs are still down due to issues with the Nvidia drivers. Jun 19, 13:50 PT\nUpdate: The login nodes and and the development nodes are functional. Some compute nodes that went down are back up, but not all. Jun 18, 10:45 PT\nInvestigating: The UCSF Mission Bay Campus experienced a power outage on Saturday June 16 causing parts of Wynton HPC to go down. One of the login nodes (wynlog1), the development node (qb3-dev1), and parts of the compute nodes are currently non-functional. Jun 17, 15:00 PT\n\n\n\n\n\n\n\n\nloading… Reload"
  },
  {
    "objectID": "hpc/status/incidents-2028.html",
    "href": "hpc/status/incidents-2028.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2028 (this far)\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 0.0 hours (= 0.0 days)\nTotal: 0.0 hours (= 0.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\nNo incidents thus far :)"
  },
  {
    "objectID": "hpc/status/incidents-current.html",
    "href": "hpc/status/incidents-current.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "November 16-ongoing, 2023\n\nSporadic job failure\nUpdate: There was another burst of “can’t get password entry for user” errors starting on 2025-01-26 around 15:30, causing jobs to fail immediately. We are restarting the SSSD service on the ~140 compute nodes we have identified suffer from this problem. January 27, 11:45 PT\nUpdate: To lower the risk for this problem to occur, the SSSD timeout limit was increased from 10 seconds to 30 seconds. November 20, 2023, 10:00 PT\nUpdate: The “can’t get password entry for user” error happens on some compute nodes where the System Security Services Daemon (SSSD) has failed. Until the cause for failed SSSD has been identified and resolved, the only solution is to resubmit the job. November 17, 2023, 09:30 PT\nNotice: Some jobs end up in an error state (Eqw) with an error “can’t get password entry for user”alice”. Either user does not exist or error with NIS/LDAP etc.” November 16, 2023, 17:00 PT\n\n\n\n\nNovember 5-ongoing, 2023\n\nPasswords cannot be reset\nNotice: Passwords can be changed via the web interface. It is still not possible to change it via the command-line while logged in to Wynton. November 13, 11:00 PT\nNotice: It is not possible to change or reset passwords since 2023-11-05. This problem was introduced while doing cluster-wide upgrades to Rocky 8. November 11, 09:00 PT"
  },
  {
    "objectID": "hpc/status/incidents-upcoming.html",
    "href": "hpc/status/incidents-upcoming.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "November 12-13, 2025\n\nFull downtime\nNotice: The cluster will down for maintenance from 3:00 pm on Wednesday November 12 until 6:00 pm on Thursday November 13, 2025. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Jobs with runtimes that go into the maintenance window will be started after the downtime. Starting October 29 at 4:00pm, jobs relying on the default 14-day runtime will not be launched until after the downtime. UCSF Facilities will perform annual fire inspection activities to remain compliant with regulations. The network team will update a core switch. The Wynton team will take the opportunity to implement kernel updates during this period. October 29, 12:00 PT"
  },
  {
    "objectID": "hpc/status/incidents-2025.html",
    "href": "hpc/status/incidents-2025.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2025 (this far)\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 505 hours (= 21.0 days)\nTotal: 505 hours (= 21.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2025-01-09 – 2025-01-09 ( 1.25 hours)\n2025-01-17 – 2025-01-22 (81.75 hours)\n2025-02-21 – 2025-03-07 (61.0 hours)\n2025-03-31 – 2025-04-01 (17.0 hours)\n2025-04-11 – 2025-04-14 (62.0 hours)\n2025-05-29 – 2025-06-10 (282.0 hours)\n\nTotal downtime: 505 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\nMay 29-June 10, 2025\n\nMajor file system failures\nResolved: Wynton logins are available as of noon today. At that time we will start unsuspending jobs. We lost about 50 TiBs (0.7%) of compressed data from 6550 TiBs with the group storage pool for files in /wynton/group, /wynton/protected/group, and /wynton/protected/project. See Wynton Announcement email for further details. June 10, 12:00 PT\nUpdate: We plan to resume operations by the weekend, given that the current backup and the necessary, manual one-at-the-time replacement of multiple drives completes in time. Files that lived on the failed storage pool are broken and cannot not be fully read, but possible partially. Home directories are unaffected. The affected files live under /wynton/group, /wynton/protected/group, and /wynton/protected/project. We are scanning the file system to identify exactly which files are affected - this is a slow processes. We will share file lists with affected groups. Eventually, any broken files have to be deleted. June 4, 14:00 PT\nUpdate: Wynton jobs and logins are still paused until further notice. Our team is working on determining all of the files that may be corrupt/unavailable and will work with the vendor on the best course of action. We do not yet have an estimate on when we will be back up. May 30, 10:40 PT\nNotice: Jobs and logins have been paused until further notice. Our team is actively troubleshooting and coordinating with the vendor. A drive was replaced today and was in the process of resilvering when two more drives failed, totally three failed drives, which causes significant problems. Data corruption is expected. May 29, 18:20 PT\n\n\n\n\nApril 11-14, 2025\n\nFile system timeouts\nResolved: All cluster jobs and queues were unsuspended at 02:00 this night. April 14, 08:15 PT\nNotice: All cluster jobs have been suspended in order to allow multiple metadata mirror resyncing processes to complete. These processes are what led to the hanging episodes that we have been seeing. Interactive nodes remain available. Resyncing processes are estimated to complete by Monday. April 11, 12:00 PT\n\n\n\n\nMarch 31-April 1, 2025\n\nFile system timeouts\nResolved: Queues and jobs are re-enabled. April 1, 12:00 PT\nUpdate: Login is re-enabled. Queues and jobs remains suspended. March 31, 20:15 PT\nNotice: BeeGFS metadata servers are experiencing issues. We have suspended all queues and jobs and disabled logins. We will work with the file system vendor to resolve the issue. March 31, 19:00 PT\n\n\n\n\nFebruary 21-March 7, 2025\n\nFile system timeouts\nResolved: We have resumed the scheduler and jobs are being processed again. We identified several problems related to the BeeGFS file system that could have contributed to the recent, severe performance degradation. Specifically, the process that automatically removes files older than 14 days from /wynton/scratch/ failed to complete, which resulted in close to 100% full storage servers. We believe this issues started in November 2024 and has gone unnoticed until now. We do not understand why these cleanup processes had failed, but one hypothesis is that there are corrupt files or folders where the cleanup process gets stuck, preventing it from cleaning up elsewhere. It might be that these problems have caused our metadata servers resynchronizing over and over - resynchronization itself is an indication that something is wrong. We are in the process of robustifying our cleanup process, putting in monitoring systems to detect these issues before system degradation takes place. March 7, 11:30 PT\nNotice: We have decided to again suspending all running jobs and disable the queue from taking on new jobs. March 5, 15:00 PT\nNotice: Resynchronization of BeeGFS metadata server pair (42,52) finished after 23 hours. March 4, 14:00 PT\nNotice: Resynchronization of BeeGFS metadata server pairs (32,22) and (23,33) started 2025-03-03, and (42,52) on 2025-03-04. March 4, 09:00 PT\nNotice: The job queue has been re-enabled and all suspended jobs have been released. February 28, 09:00 PT\nNotice: Login and file transfers to Wynton has been re-enabled. February 28, 09:00 PT\nNotice: Resynchronization of BeeGFS metadata server pair (41,51) completed after 24 hours, and pair (63,73) completed after 18 hours. February 28, 09:00 PT\nNotice: In order to speed up resynchronization of metadata servers, we have decided to minimize the load on the file system by suspending all running jobs, disable login to Wynton, and disable all file transfers to and from Wynton. February 27, 16:30 PT\nNotice: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (51,73). February 27, 16:15 PT\nNotice: Resynchronization of BeeGFS metadata server pair meta22 and meta32 completed after 30 hours. February 27, 06:00 PT\nNotice: The file system latency is extremely high, resulting in the cluster being unusable and attempts to log in via SSH failing. This is due to the resynchronization of BeeGFS metadata server pair (22,32). February 26, 19:30 PT\nNotice: We are working with the vendor to try to resolve this problem. February 26, 09:00 PT\nNotice: The file system is again very slow. delays when working interactively and jobs to slow down. February 25, 15:15 PT\nNotice: The file system is again very slow. February 25, 10:00 PT\nNotice: The file system is very slow, which result in long delays when working interactively and jobs to take longer than usual. February 21, 16:00 PT\n\n\n\n\nFebruary 21-24, 2025\n\nKernel maintenance\nResolved: Login node plog1 respects SSH keys again. February 24, 2025, 11:15 PT\nUpdate: Login node plog1 is available again, but does not respect SSH keys. February 24, 2025, 10:30 PT\nUpdate: Data-transfer node dt1 is available again. February 24, 2025, 10:30 PT\nUpdate: With the exception for plog1 and dt1, all login, data-transfer, and development nodes have been rebooted. Until plog1 is available, PHI-users may use pdt1 and pdt2 to login into the cluster. February 22, 2025, 13:30 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Saturday, February 22, 2025 at 13:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. February 21, 2025, 12:15 PT\n\n\n\nFebruary 22-24, 2025\n\nGlobus issues\nResolved: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is available again after data-transfer node dt1 coming online. February 24, 2025, 10:30 PT\nNotice: The ‘Wynton HPC’ Globus endpoint used by non-PHI users is unavailable, because data-transfer node dt1 is unavailable. February 22, 2025, 13:30 PT\n\n\n\nFebruary 18-24, 2025\n\nGlobus issues\nResolved: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are functional again. February 24, 2025, 09:30 PT\nUpdate: The vendor has escalated our support ticket. February 19, 2025, 13:30 PT\nNotice: The ‘Wynton HPC UCSF Box Connector’ for Globus and the ‘Wynton HPC PHI Compatible’ Globus endpoint are currently unavailable. The former gives an error on “Unknown user or wrong password”, and the latter “Authentication Required - Identity set contains an identity from an allowed domain, but it does not map to a valid username for this connector”. The regular ‘Wynton HPC’ Globus endpoint is unaffected and available. The problem has been there since at least 2025-02-14 at 22:36, when I user reported it. February 19, 2025, 12:00 PT\n\n\n\nJanuary 17-22, 2025\n\nCluster unavailable\nResolved: Wynton is fully operational again. The BeeGFS file system issue has been resolved. All data consistency has been verified. Working with the vendor, we have identified a potential bug in the BeeGFS quota system that caused the BeeGFS outage. That part is still under investigation in order to minimize and remove the risk of reoccurrence. January 22, 12:15 PT\nUpdate: The login and data-transfer nodes are available again. January 22, 11:00 PT\nUpdate: The third resynchronization completed successfully. January 21, 18:30 PT\nUpdate: Further investigation of the failed resynchronization this morning indicated that the resynchronization did indeed keep running while it stopped producing any output and the underlying BeeGFS service was unresponsive. Because of this, we decided to not restart the resynchronization, but instead let it continue in the hope it will finish. But, by not restarting, Wynton will remain inaccessible. Our first objective is to not jeopardize the cluster, the second objective is to bring the system back online. January 21, 15:15 PT\nUpdate: The cluster is unavailable again. The past resynchronization of the problematic BeeGFS metadata server failed again, which triggers the problem. We are communicating with the vendor for their support. January 21, 09:45 PT\nUpdate: The cluster is available again, but the scheduler has been paused. No queued jobs are launched and running jobs have been suspended, but will resume when the pause of scheduler is removed. This is done to minimize the load on BeeGFS, which will simplify troubleshooting and increase the chances to stabilize BeeGFS. It is the same BeeGFS metadata server as before that is experiencing problems. January 19, 13:45 PT\nUpdate: The cluster is unavailable again. January 19, 12:45 PT\nUpdate: The cluster is working again. We have started a resynchronization of the problematic BeeGFS metadata server pair meta22 and meta32. January 18, 13:45 PT\nUpdate: First signs of the cluster coming back online again, e.g. queued jobs are launched, and it is possible to access the cluster via SSH. January 18, 06:00 PT\nUpdate: Identifies a specific BeeGFS metadata server that is unresponsive. The BeeGFS vendor has been contacted. January 18, 01:00 PT\nUpdate: The underlying problem appears to be BeeGFS. The storage servers are okay, but one or more metadata servers are unresponsive. January 17, 21:30 PT\nNotice: The cluster is unavailable, e.g. i is not possible to access the login or the data-transfer nodes. January 17, 19:45 PT\n\n\n\n\nJanuary 9, 2025\n\nFile-system emergency shutdown\nResolved: The cluster full operational again. Suspended jobs have been resumed. The BeeGFS issue has been resolved. Checked hardware and cables. Rebooted affected BeeGFS server. January 9, 16:20 PT\nNotice: An issue with BeeGFS was detected. All Wynton jobs have been paused until further notice. January 9, 15:10 PT"
  },
  {
    "objectID": "hpc/status/incidents-2024.html",
    "href": "hpc/status/incidents-2024.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2024\n\nFull downtime:\n\nScheduled: 137.0 hours (= 5.7 days) = 1.6%\nUnscheduled: 142.3 hours (= 5.9 days) = 1.6%\nTotal: 279.3 hours (= 11.6 days) = 3.2%\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2024-06-17 – 2024-06-18 (32.0 hours)\n2024-10-14 – 2024-10-18 (105.0 hours)\n\nTotal downtime: 137.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2024-04-03 (~500 hours)\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2024-03-14 (13.0 hours)\n2024-03-17 (15.0 hours)\n2024-05-31 (2.3 hours)\n2024-06-15 – 2024-06-21 (112.0 hours; excluding 32 hours scheduled maintenance)\n\nTotal downtime: 142.3 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\nOctober 14-18, 2024\n\nFull downtime\nResolved: The cluster is back online. October 18, 17:00 PT\nUpdate: The cluster including all its storage is offline undergoing a scheduled maintenance. October 14, 11:00 PT\nNotice: The cluster will be shut down for maintenance from 8:00 am on Monday October 14 until 5:00 pm on Friday October 18, 2024. This is a full downtime, including no access to login, development, data-transfer, and app nodes. Compute nodes will be shutdown as well. Starting 14 days before, the maximum job runtime will be decreased on a daily basis from the current 14 days down to one day so that jobs finish in time before the shutdown. Jobs with runtimes that go into the maintenance window will be started after the downtime. The reason for the downtime is that UCSF Facilities will perform maintenance affecting cooling in our data center. We will take this opportunity to perform system updates and BeeGFS maintenance. September 20, 16:45 PT\n\n\n\n\nSeptember 12, 2024\n\nKernel maintenance\nResolved: All interactive nodes have been updated and deployed with the new CGroups limits. September 13, 13:00 PT\nNotice: All interactive nodes will be shutdown and rebooted on Thursday September 12 at 12:30 to update Linux kernels and deploy CGroups-controlled CPU and memory user limits. To avoid data loss, please save your work and logout before. Queued and running jobs are not affected. September 11, 09:15 PT\n\n\n\nJune 15-25, 2024\n\nFile-system unreliable\nResolved: 14,000 compute slots are now available, which corresponds to the majority of compute nodes. June 25, 00:30 PT\nUpdate: We will go ahead and re-enable the remaining compute nodes. June 24, 13:00 PT\nUpdate: Development nodes are available. We have also opened up 100 compute nodes. We will keep monitoring BeeGFS over the weekend with the plan to re-enable the remaining compute nodes if all go well. June 21, 19:15 PT\nUpdate: The login and data-transfer nodes are available. We will continue to validate BeeGFS during the day with the intent to open up the development nodes and a portion of the compute nodes before the weekend. June 21, 12:45 PT\nUpdate: We decided to replace the problematic chassis with a spare. The RAID file system has two failing drives, which are currently being restored. We expect this to finish up in the morning. Then, we will replace those two failing drives and proceed with another restore. If that succeeds, we plan to open up the login nodes to make files available again. After that, the goal is to slowly open up the queue and compute nodes over the weekend. June 20, 23:30 PT\nUpdate: We had folks onsite today to complete some preventative maintenance on all of the disk chassis (and, in a fit of optimism, bring up all of the nodes to prepare for a return to production). As this maintenance involved new firmware, we had some hope that it might sort out our issues with the problematic chassis. Unfortunately, our testing was still able to cause an issue (read: crash). We’ve sent details from this latest crash to the vendor and we’ll be pushing hard to work with them tomorrow Thursday to sort things out. June 20, 00:15 PT\nUpdate: The vendor is still working on diagnosing our disk chassis issue. That work will resume after Wednesday’s holiday. So, unfortunately, we will not be able to bring Wynton up on Wednesday. We hope to come up on Thursday, but it all depends on our testing and the vendor’s investigation. June 19, 01:00 PT\nUpdate: We are working with both the system and chassis vendors to diagnose this and determine what the problem is and how to fix it. This process is taking much longer than we’d like, and it is looking increasingly unlikely that we’ll be in a position to bring Wynton back online today. June 18, 14:00 PT\nUpdate: A disk chassis that hosts part of /wynton/home appears to be failing. It works for a while and then fails, which brings down /wynton. We are trying to keep it running as much as possible, but can’t make any promises. June 16, 00:15 PT\nNotice: Wynton is currently down due to an unknown issue. The problem started around 15:00 on Saturday 2024-06-15. June 15, 23:15 PT\n\n\n\n\nJune 17-18, 2024\n\nFull downtime\nUpdate: All but one of the planned maintenance upgrades were completed during this scheduled maintenance. The remain upgrade does not require a downtime and will be done in a near future without disrupting the cluster. June 18, 17:00 PT\nUpdate: Wynton is down for maintenance as of 09:00 on Monday 2024-06-17. June 17, 09:00 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Monday June 17 until 5:00 pm on Tuesday June 18, 2024. Starting June 3, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. June 5, 09:00 PT\n\n\n\n\nJune 7-June 10, 2024\n\nDevelopment nodes are inaccessible\nResolved: Development nodes are available again. June 10, 10:25 PT\nNotice: Development nodes are inaccessible since Friday June 7 at 17:00. We will investigate the problem on Monday. June 8, 05:45 PT\n\n\n\n\nMay 31, 2024\n\nFile-system failures\nResolved: The BeeGFS issue has been resolved. Wynton is operational again. May 31, 09:20 PT\nNotice: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started around 06:00. We’re working on it and will post updates as we know more. May 31, 08:45 PT\n\n\n\n\nApril 3-25, 2024\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. April 25, 09:00 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. April 4, 11:15 PT\nUpdate: A new set of kernel updates will be rolled out. Login, data-transfer, and development nodes will be rebooted briefly on Thursday April 11 at 11:00. All compute nodes will also have to be drained and rebooted, which might take up to two weeks. Some of the compute have been draining since last week, meaning that will only have been drain for at most another week. April 10, 16:00 PT\nUpdate: Hosts dt1 and plog1 are now also available. April 4, 12:15 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. It will take some more time before dt1 and plog1 are available again, because they did not come back as expected after the reboot. April 4, 11:15 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Thursday April 4 at 11:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. April 3, 17:30 PT\n\n\n\nMarch 17-18, 2024\n\nFile-system failures\nResolved: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. March 18, 10:30 PT\nNotice: Wynton is currently down due to an unknown BeeGFS issues. The problem started around 19:30 on 2024-03-17. We’re working on it and will post updates as we know more. March 18, 09:00 PT\n\n\n\n\nMarch 14, 2024\n\nFile-system failures\nResolved: Wynton and BeeGFS is back up and running again after a full reboot of the BeeGFS servers. Root cause is still unknown. March 14, 15:15 PT\nNotice: Wynton is currently down due to an unknown issue with the BeeGFS filesystem. The problem started at 02:11 this morning. We’re working on it and will post updates as we know more. March 14, 09:15 PT\n\n\n\n\nFebruary 2-3, 2024\n\nKernel maintenance\nResolved: All hosts are available. February 3, 17:00 PT\nUpdate: Login, data-transfer, and development nodes have been rebooted. It will take some more time before plog1, dt1, and dev2 are available again, because they did not come back as expected after the reboot. PHI users may use pdt1 and pdt2 to access the cluster. February 2, 14:45 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Friday February 2 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. February 1, 23:30 PT\n\n\n\nJanuary 25-August, 2024\n\nEmergency shutdown due to cooling issue\nResolved: UCSF Facilities has resolved the cooling issue and there are again two working chillers. As a fallback backup, the building is now connected to the campus chilled water loop. This was confirmed by UCSF Facilities on 2024-12-10. July-August, 2024\nUpdate: UCSF Facilities performed testing for rerouting of updated chilled-water piping the building where the Wynton data center is hosted between 07-12 on 2024-05-08. May 9, 12:30 PT\nUpdate: The compute and development nodes are available again. Jobs that were running when we did the emergency shutdown should be considered lost and need to be resubmitted. UCSF Facilities has re-established cooling, but there is currently no redundancy cooling system available, meaning there is a higher-than-usual risk for another failure. January 25, 15:45 PT\nNotice: We are shutting down all Wynton compute and development nodes as an emergency action. This is due to a serious issue with the chilled-water system that feeds the cooling in the Wynton data center. By shutting down all of the compute nodes, we hope to slow the current temperature rise, while keeping the storage system, login and data-transfer nodes up. The will come back up again as soon as the UCSF Facilities has resolved the chilled-water system. ETA is currently unknown. January 25, 11:25 PT"
  },
  {
    "objectID": "hpc/status/incidents-2027.html",
    "href": "hpc/status/incidents-2027.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2027 (this far)\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 0.0 hours (= 0.0 days)\nTotal: 0.0 hours (= 0.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\nNo incidents thus far :)"
  },
  {
    "objectID": "hpc/status/templates/incident-kernel-update.html",
    "href": "hpc/status/templates/incident-kernel-update.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "MONTH, DATE-current, YEAR\n\nKernel maintenance\n\n\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on WEEKDAY MONTH DAY, YEAR at HOUR:MINUTE. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. MONTH DAY, YEAR, HOUR:MINUTE PT"
  },
  {
    "objectID": "hpc/scheduler/kill-jobs.html",
    "href": "hpc/scheduler/kill-jobs.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "To kill a specific job, use:\nqdel &lt;jobid&gt;\nIf the node, where the job is running, is non-responsive, you can force the scheduler to remove the job by using:\nqdel -f &lt;jobid&gt;\nNote that this command can take a long time to complete (up to a couple of minutes) if the node is non-responsive.\nTo kill all of your jobs, use:\nqdel -u $USER\nTechnical details: If the job was submitted with SGE option -notify, then the scheduler send signal SIGUSR2 twice to the job notifying that it is about to be terminated. The second signal is sent momentarily after the first. This is also the case when the job exhausts its maximum runtime. The job process can choose to capture this signal to perform a clean-up task before exiting. If the job process is still running 60 seconds after these notification signals, then it will be terminated with signal SIGKILL, which cannot be caught. If -notify was not specified, then only SIGKILL is signaled."
  },
  {
    "objectID": "hpc/scheduler/kill-jobs.html#kill-jobs",
    "href": "hpc/scheduler/kill-jobs.html#kill-jobs",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "To kill a specific job, use:\nqdel &lt;jobid&gt;\nIf the node, where the job is running, is non-responsive, you can force the scheduler to remove the job by using:\nqdel -f &lt;jobid&gt;\nNote that this command can take a long time to complete (up to a couple of minutes) if the node is non-responsive.\nTo kill all of your jobs, use:\nqdel -u $USER\nTechnical details: If the job was submitted with SGE option -notify, then the scheduler send signal SIGUSR2 twice to the job notifying that it is about to be terminated. The second signal is sent momentarily after the first. This is also the case when the job exhausts its maximum runtime. The job process can choose to capture this signal to perform a clean-up task before exiting. If the job process is still running 60 seconds after these notification signals, then it will be terminated with signal SIGKILL, which cannot be caught. If -notify was not specified, then only SIGKILL is signaled."
  },
  {
    "objectID": "hpc/scheduler/email-notifications.html",
    "href": "hpc/scheduler/email-notifications.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Do not request email notifications for large array jobs! If done, there will be email messages sent for every single task in the job array."
  },
  {
    "objectID": "hpc/scheduler/email-notifications.html#job-email-notifications",
    "href": "hpc/scheduler/email-notifications.html#job-email-notifications",
    "title": "UCSF Wynton HPC Cluster",
    "section": "Job Email Notifications",
    "text": "Job Email Notifications\nInstead of polling qstat to check whether submitted jobs are queued, running, or finished, one can tell the job scheduler to send email notifications as jobs are started or completed. This is done by specifying qsub option -m &lt;when&gt;, where &lt;when&gt; specifies under what circumstances an email notification should be sent.\nTo send an email when the job (b)egins, (e)nds, or (a)borts, submit the job as:\n$ qsub -m bea myscript.sh\nTo send an email only when the job completed, successfully or not, skip (b)egin notifications by using only:\n$ qsub -m ea myscript.sh\nThe email message sent when a job starts (-m b), will look like:\nFrom: root &lt;root@wynton.ucsf.edu&gt;\nTo: alice@log1.wynton.ucsf.edu\nSubject: Job 8968283 (myscript.sh) Started\n\nJob 8968283 (myscript.sh) Started\n User       = alice\n Queue      = long.q\n Host       = qb3-ad4\n Start Time = 11/14/2019 00:07:00\nand the one sent when a job ends successfully (-m e), will look like:\nFrom: root &lt;root@wynton.ucsf.edu&gt;\nTo: alice@log1.wynton.ucsf.edu\nSubject: Job 8968283 (myscript.sh) Complete\n\nJob 8968283 (myscript.sh) Complete\n User             = alice\n Queue            = long.q@qb3-ad4\n Host             = qb3-ad4\n Start Time       = 11/14/2019 00:07:00\n End Time         = 11/14/2019 00:07:01\n User Time        = 00:00:00\n System Time      = 00:00:00\n Wallclock Time   = 00:00:01\n CPU              = 00:00:00\n Max vmem         = 5.410M\n Exit Status      = 0\nThe message sent when a job is aborted (-m a), for instance via qdel, will look like:\nFrom: root &lt;root@wynton.ucsf.edu&gt;\nTo: alice@log1.wynton.ucsf.edu\nSubject: Job 8974017 (myscript.sh) Aborted\n\nJob 8974017 (myscript.sh) Aborted\n Exit Status      = 137\n Signal           = KILL\n User             = alice\n Queue            = long.q@msg-id7\n Host             = msg-id7\n Start Time       = 11/14/2019 08:07:02\n End Time         = 11/14/2019 08:07:07\n CPU              = 00:00:00\n Max vmem         = 1.965M\nfailed assumedly after job because:\njob 8974017.1 died through signal KILL (9)\n\nConfigure the default email address\nNormally, you would not set the email address for where SGE notifications are sent to. By default, the email notifications are sent to the email address you have associated with your Wynton account. To find out which address this is, run:\n$ ldapsearch -LLL -x -H ldap://m1,ldap://m2 uidNumber=\"$(id -u)\" mail\ndn: uid=alice,ou=Accounts,dc=cgl,dc=ucsf,dc=edu\nmail: alice.bobson@ucsf.edu\nBut, if you have to send to email another address than your Wynton-associated email address, you can set a new default in the ~/.sge_request file (create if missing) as:\n## Default recipient of job notifications\n-M alice2@bobson.org\nTechnically, you could also specify -M &lt;email-address&gt; as an SGE directive comment in the job script, but we recommend using ~/.sge_request for this. The advantage is that your job scripts won’t carry your email address, making them more generic and easy to share. If you have the address specified part of the script, anyone who copies your script must remember to update the email address. If not, you will be the one getting email notifications when the other person runs it on the cluster (or on other SGE clusters in the world). It is also possible to specify -M &lt;email-address&gt; a command-line option to qsub. When specifying -M &lt;email-address&gt; in multiple ways, e.g. in ~/.sge_request and at the command line, then notifications will be sent to all email addresses specified combined.\nTo send to multiple email addresses, use semicolon (;) to separate the addresses, e.g. -M alice.aliceson@ucsf.edu,alice2@bobson.org.\nTechnical details: When not specifying -M, the To: address used for these notifications will be &lt;username&gt;@&lt;hostname&gt;, where &lt;hostname&gt; is a Wynton hostname, e.g. alice@log1.wynton.ucsf.edu. That is a local address that only exists on the Wynton cluster. Any message sent to this address is redirected to your Wynton-associated email address. If you configure SGE to use another email address (e.g. -M alice2@bobson.org), then that will become the To: address.\n\nPlease do not specify -m bea in ~/.sge_request to make it the default for all of your jobs. If done, you might end up producing thousands of email messages when you submit array jobs.\n\n\n\nEmail notifications for array jobs\nDo not request email notifications for array jobs! If done, there will be email messages sent for every single task of the job array. Instead, to get an email notification when a job array completes, submit a “dummy” job that depend on the job array such that it will only launch when the job array completes. The sole purpose of this dummy job is to trigger an email notification. For instance, if the job array has job ID 9156754, then submit a job:\n$ job_id=9156754\n$ echo 'date' | qsub -N \"Array_job_${job_id}_done\" -m b  -l h_rt=00:00:05 -hold_jid \"${job_id}\"\nThis will send an email with ‘Array_job_9156754_done’ in the subject line as soon as the dummy job launches."
  },
  {
    "objectID": "hpc/scheduler/using-local-scratch.html",
    "href": "hpc/scheduler/using-local-scratch.html",
    "title": "Using Local /scratch (TMPDIR) on Compute Nodes",
    "section": "",
    "text": "All nodes (compute and development) have their own locally storage mounted as /scratch. The /scratch storage is fast - faster than system-wide storage such as home folders but also /wynton/scratch/wynton/protected/scratch - which make it ideal for holding intermediate data files. This will also lower the load on the system-wide storage and the local network. Using local /scratch is a win-win for everyone.\n\n\nHere is how to use /scratch:\n\nUse environment variable TMPDIR - on compute nodes, it points to an already created job-specific folder under local /scratch. On other machines, you need to set it.\nWrite intermediate files to folder $TMPDIR. Bonus: most software already acknowledges TMPDIR for their internal temporary files.\nWhen done, make sure to copy TMPDIR files to be kept to your persistent storage, e.g. to $HOME.\nThe job-specific TMPDIR folder (e.g. /scratch/8327141.1.long.q) will be deleted automatically when the job terminates.\nSpecify how much local scratch (TMPDIR) storage your job will need. Local storage is limited to 0.1-1.8 TiB/node. If your job will use up to 200 GiB of disk space, you can specify this resource as -l scratch=200G (in units of GiB) when submitting the job. A node with 800 GiB of scratch space can support up to four -l scratch=200G jobs running at the same time.\n\n\n\nHere is a script called ex-scratch.sh that illustrates how to copy input files over from the $HOME folder to the local scratch folder (TMPDIR) of whatever node the job ends up running on. After processing of the input files is complete, the output files are moved from the local scratch (TMPDIR) to HOME.\n#!/bin/env bash\n#$ -S /bin/bash     # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd             # use current working directory\n#$ -l scratch=200G  # needs 200 GiB of /scratch space\n\n## 0. In case TMPDIR is not set, set it to local /scratch, \n##    if it exists, otherwise to /tmp\nif [[ -z \"$TMPDIR\" ]]; then\n  if [[ -d /scratch ]]; then TMPDIR=/scratch/$USER; else TMPDIR=/tmp/$USER; fi\n  mkdir -p \"$TMPDIR\"\n  export TMPDIR\nfi\n\n## 1. Use a temporary working directory\ncd \"$TMPDIR\"\n\n## 2. Copy input files from global disk to local scratch\ncp ~/sample.fq .\ncp ~/reference.fa .\n\n## 3. Process input files\n/path/to/my_pipeline --cores=\"${NSLOTS:-1}\" reference.fa sample.fq &gt; output.bam\n\n## 4. Move output files back to global disk\nmv output.bam ~\n\n## 5. End-of-job summary\n[[ -n \"$JOB_ID\" ]] && qstat -j \"$JOB_ID\"\nAssume that the total amount of local scratch you need for your input files and your output files and whatever intermediate files my_pipeline needs is 100 GiB, and assume that the process requires up to 8 GiB of RAM (=4 GiB per core) to complete. Moreover, let’s say you wish to run in parallel using two cores. Then you should submit this job script as:\n$ qsub -l scratch=100G -l mem_free=4G -pe smp 2 ex-scratch.sh\nTo understand the purpose of qstat -j at the end, see the Job Summary page."
  },
  {
    "objectID": "hpc/scheduler/using-local-scratch.html#instructions",
    "href": "hpc/scheduler/using-local-scratch.html#instructions",
    "title": "Using Local /scratch (TMPDIR) on Compute Nodes",
    "section": "",
    "text": "Here is how to use /scratch:\n\nUse environment variable TMPDIR - on compute nodes, it points to an already created job-specific folder under local /scratch. On other machines, you need to set it.\nWrite intermediate files to folder $TMPDIR. Bonus: most software already acknowledges TMPDIR for their internal temporary files.\nWhen done, make sure to copy TMPDIR files to be kept to your persistent storage, e.g. to $HOME.\nThe job-specific TMPDIR folder (e.g. /scratch/8327141.1.long.q) will be deleted automatically when the job terminates.\nSpecify how much local scratch (TMPDIR) storage your job will need. Local storage is limited to 0.1-1.8 TiB/node. If your job will use up to 200 GiB of disk space, you can specify this resource as -l scratch=200G (in units of GiB) when submitting the job. A node with 800 GiB of scratch space can support up to four -l scratch=200G jobs running at the same time.\n\n\n\nHere is a script called ex-scratch.sh that illustrates how to copy input files over from the $HOME folder to the local scratch folder (TMPDIR) of whatever node the job ends up running on. After processing of the input files is complete, the output files are moved from the local scratch (TMPDIR) to HOME.\n#!/bin/env bash\n#$ -S /bin/bash     # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd             # use current working directory\n#$ -l scratch=200G  # needs 200 GiB of /scratch space\n\n## 0. In case TMPDIR is not set, set it to local /scratch, \n##    if it exists, otherwise to /tmp\nif [[ -z \"$TMPDIR\" ]]; then\n  if [[ -d /scratch ]]; then TMPDIR=/scratch/$USER; else TMPDIR=/tmp/$USER; fi\n  mkdir -p \"$TMPDIR\"\n  export TMPDIR\nfi\n\n## 1. Use a temporary working directory\ncd \"$TMPDIR\"\n\n## 2. Copy input files from global disk to local scratch\ncp ~/sample.fq .\ncp ~/reference.fa .\n\n## 3. Process input files\n/path/to/my_pipeline --cores=\"${NSLOTS:-1}\" reference.fa sample.fq &gt; output.bam\n\n## 4. Move output files back to global disk\nmv output.bam ~\n\n## 5. End-of-job summary\n[[ -n \"$JOB_ID\" ]] && qstat -j \"$JOB_ID\"\nAssume that the total amount of local scratch you need for your input files and your output files and whatever intermediate files my_pipeline needs is 100 GiB, and assume that the process requires up to 8 GiB of RAM (=4 GiB per core) to complete. Moreover, let’s say you wish to run in parallel using two cores. Then you should submit this job script as:\n$ qsub -l scratch=100G -l mem_free=4G -pe smp 2 ex-scratch.sh\nTo understand the purpose of qstat -j at the end, see the Job Summary page."
  },
  {
    "objectID": "hpc/scheduler/gpu.html",
    "href": "hpc/scheduler/gpu.html",
    "title": "GPU Scheduling",
    "section": "",
    "text": "Wynton HPC has 61 GPU nodes with a total of 235 GPUs available to all users. Among these, 39 GPU nodes, with a total of 147 GPUs, were contributed by different research groups. On these nodes, GPU jobs from Wynton users not in the contributing lab are limited to 2 hours. In contrast, contributors are not limited to 2-hour GPU jobs on nodes they contributed. On the institutional GPU nodes (i.e. those not contributed by any particular research group), the standard Wynton job length limit of 2 weeks applies for all users. There is also one GPU development node that is available to all users."
  },
  {
    "objectID": "hpc/scheduler/gpu.html#compiling-gpu-applications",
    "href": "hpc/scheduler/gpu.html#compiling-gpu-applications",
    "title": "GPU Scheduling",
    "section": "Compiling GPU applications",
    "text": "Compiling GPU applications\nThe CUDA Toolkit is installed on the development nodes. Several versions of CUDA are available via software modules. To see the currently available versions, run the command:\nmodule avail cuda"
  },
  {
    "objectID": "hpc/scheduler/gpu.html#submitting-gpu-jobs",
    "href": "hpc/scheduler/gpu.html#submitting-gpu-jobs",
    "title": "GPU Scheduling",
    "section": "Submitting GPU jobs",
    "text": "Submitting GPU jobs\nGPU jobs run in a dedicated queue which must be requested in the job submission. Each slot in this queue represents a GPU that the job will use. Therefore, users must ensure that every job’s GPU use matches its submission request. For a job using a single GPU, the submission should look like:\nqsub -q gpu.q ...\nJobs requiring more than one GPU must be submitted like this:\nqsub -q gpu.q -pe smp N ...\nwhere N is the number of GPUs the job will use.\nIf your application requires MPI, you should still use the proper parallel environment regardless of how many GPUs you’ll be using:\nqsub -q gpu.q -pe mpi_onehost N ...\nmpirun -np M --oversubscribe ...\nwhere N is the number of GPUs your job will use and M is the number of MPI processes your job will launch. M does not have to equal N (see below). Please note that, at the moment, each GPU job must limit itself to a single host.\nNOTE: GPU jobs must include a runtime request, i.e. -l h_rt=HH:MM:SS. This allows for proper scheduling of GPU jobs on member and institutional nodes. If your job does not include a runtime request, it may be removed from the queue. Runtime requests are hard limits, so your job will be killed by SGE when it hits this limit. Be sure to request enough time for you job to finish."
  },
  {
    "objectID": "hpc/scheduler/gpu.html#submitting-gpu-jobs-to-the-msg-4-gpu-nodes",
    "href": "hpc/scheduler/gpu.html#submitting-gpu-jobs-to-the-msg-4-gpu-nodes",
    "title": "GPU Scheduling",
    "section": "Submitting GPU jobs to the MSG 4-GPU nodes",
    "text": "Submitting GPU jobs to the MSG 4-GPU nodes\nThe 4gpu.q has {{ site.data.specs.msg_4gpus }} GPUs on {{ site.data.specs.msg_4gpu_nodes }} nodes. These GPUs are reserved such that all 4 on the node are reserved when a job is submitted to the queue.\nTo submit a 4-GPU job to a host a dedicated 4-GPU host, do this:\nqsub -q 4gpu.q ...\nNOTE: The GPU servers in the 4gpu.q were all contributed by MSG Labs. If you are not a member of a MSG lab, for your job to run in the 4gpu.q, your job must have a run time (h_rt) of less than 2 hours.\n\nDo not use a parallel environment (i.e. -pe smp 4 or -pe mpi_onehost 4) for 4gpu.q as you would for gpu.q. If you do, the job will never start."
  },
  {
    "objectID": "hpc/scheduler/gpu.html#submitting-multi-node-multi-gpu-jobs",
    "href": "hpc/scheduler/gpu.html#submitting-multi-node-multi-gpu-jobs",
    "title": "GPU Scheduling",
    "section": "Submitting Multi-Node Multi-GPU jobs",
    "text": "Submitting Multi-Node Multi-GPU jobs\n\n2022-12-20: We are still testing this feature. Although basic use-case testing has been done, Multi-Node Multi-GPU jobs still haven’t been thoroughly validated by end users. If you run into issues or have suggestions to make the documentation clearer, please reach out to wynton-support@ucsf.edu.\n\nWe have setup a trial of multi-node GPU jobs on Wynton. This enables you to run jobs on all the GPUs of more than one server. To run a multi-node GPU, job, you must do the following:\n\nSubmit to the MPI PE and request a number of slots equal to the number of GPUs you want to use. Assuming you want to use all A40s, the number should be a multiple of 4. Example: \"-pe mpi 8\".\nRequest the “exclusive” resource: \"-l exclusive=true\"\n\nAnd that’s all that’s required. You’ll need to use ‘mpirun’ to launch your applications on the hosts assigned to the job. And you’ll likely want to tailor the rest of your qsub flags to make sure you run on hosts with the same GPU types. Also, you’ll really want to set “-R y” to turn on reservations. As with all parallel jobs, keep in mind that the more resources you request, the longer your wait time will be.\nMore information about using OpenMPI is on our website here: Using OpenMPI on Wynton\nPlease let us know how your testing goes. We’ve done limited testing of our own (only the scheduling piece), but some things are sure to come up that we didn’t see there. We’ll be happy to tweak as we go along.\nSuggestions:\n\nSome applications are finicky about the version of CUDA, load the correct CUDA module your program has been compiled against.\nMake sure you are using the version of OpenMPI which your application has been compiled against.\nYou’ll want to be sure you use servers with matching models of GPUs and matching numbers of GPUs.\n\nIn addition, a discussion group has been started on the Wynton Slack by the Wynton Users who are currently testing the feature:\n#multinode-multigpu"
  },
  {
    "objectID": "hpc/scheduler/gpu.html#gpu-relevant-resource-requests",
    "href": "hpc/scheduler/gpu.html#gpu-relevant-resource-requests",
    "title": "GPU Scheduling",
    "section": "GPU relevant resource requests",
    "text": "GPU relevant resource requests\nThe GPU nodes in Wynton HPC contain many different generations and models of NVIDIA GPUs. In order to ensure that your GPU jobs run on GPUs with the proper capabilities, there are two SGE resource complexes assigned to each GPU node:\n\ncompute_cap - describes the Compute Capability (or SM version) of the GPUs in the node (see NVIDIA’s CUDA GPU page for more details). compute_cap is an integer in keeping with the relevant flags to nvcc. For example, a Compute Capability of 6.1 (e.g. GeForce GTX 1080) is requested by -l compute_cap=61.\ngpu_mem - describes how much GPU memory the GPUs in the node have. It’s defined in units of MiB.\n\nSpecifying either of these resources is not required. If you do specify one, your job will be scheduled on a GPU node with resources &gt;= those that you requested. As an example, if you wanted to only run on at least GeForce GTX 1080 generation nodes with more than 10 GiB of GPU memory, you would specify:\n-l compute_cap=61,gpu_mem=10G"
  },
  {
    "objectID": "hpc/scheduler/gpu.html#a40-gpu-node-cuda-compatibility",
    "href": "hpc/scheduler/gpu.html#a40-gpu-node-cuda-compatibility",
    "title": "GPU Scheduling",
    "section": "A40 GPU Node CUDA Compatibility",
    "text": "A40 GPU Node CUDA Compatibility\nIf you see an error similar to RuntimeError: CUDA error: no kernel image is available for execution on the device this means your software has been compiled against a CUDA version which is not compatible with the GPU in the node your job is trying to run on. CUDA versions are usually backwards compatible. However, newer GPUs, like the A40 GPUs in the qb3-atgpu nodes, may require a minimum CUDA version. If your software is compiled against a version of CUDA less than CUDA-11.1, it will not run on the qb3-atgpu GPU nodes. To fix, recompile your software against the Wynton cuda/11.5 module (preferable) or avoid the qb3-atgpu nodes (not recommended, as these are the fastest and most numerous institutional GPUs in the Wynton cluster). To tell the scheduler to skip the qb3-atgpu nodes, use limitation -l hostname='!qb3-atgpu*' on the command line or in your job script."
  },
  {
    "objectID": "hpc/scheduler/gpu.html#running-gpu-applications",
    "href": "hpc/scheduler/gpu.html#running-gpu-applications",
    "title": "GPU Scheduling",
    "section": "Running GPU applications",
    "text": "Running GPU applications\nSeveral CUDA runtimes are installed on the GPU nodes. They can be loaded via modules just as above on the development nodes, e.g. module load cuda and module load cuda/7.5.\n\nGPU selection\nWhen your job is assigned to a node, it will also be assigned specific GPUs on that node. The GPU assignment will be contained in the environment variable SGE_GPU as a comma-delimited set of one or more non-negative integers where then number of integers corresponds to the number of GPU cores requested. For example, a 3-core GPU job (-q gpu.q -pe smp 3) may get assigned GPU cores SGE_GPU=2,0,6 whereas a 1-core GPU job (-q gpu.q) may get assigned GPU core SGE_GPU=5. Be sure to send this GPU-core assignment to your application using the proper format for your application.\nFor example, if your application uses CUDA, you should limit which GPUs are used with:\nexport CUDA_VISIBLE_DEVICES=$SGE_GPU\n\nTo avoid overloading GPUs, it is important that each job use only the GPUs it was assigned, which is given by environment variable SGE_GPU.\n\n\n\nCPU core usage\nSince we are using gpu.q slots to represent GPUs rather than the usual CPU cores, there is no way to ensure that a GPU node’s CPU cores don’t get oversubscribed. For this reason, please limit your CPU core usage to 4 CPU cores per GPU requested. This will prevent CPU core overloading on all the GPU node types."
  },
  {
    "objectID": "hpc/scheduler/gpu.html#monitor-gpu-usage",
    "href": "hpc/scheduler/gpu.html#monitor-gpu-usage",
    "title": "GPU Scheduling",
    "section": "Monitor GPU usage",
    "text": "Monitor GPU usage\n\nSGE summaries on GPU usage for a specific machine\nIf you know the name of the compute node where your job is running, you can ask SGE to summarize the GPU usage on that machine, including use from other jobs than yours. This can be done from one the login and development hosts, e.g.\n[alice@dev2 ~]$ qconf -se msg-iogpu3\nhostname              msg-iogpu3\nload_scaling          NONE\ncomplex_values        mem_free=128000M\nload_values           arch=lx-amd64,num_proc=32,mem_total=128739.226562M, \\\n                      swap_total=4095.996094M,virtual_total=132835.222656M, \\\n                      m_topology=SCTTCTTCTTCTTCTTCTTCTTCTTSCTTCTTCTTCTTCTTCTTCTTCTT, \\\n                      m_socket=2,m_core=16,m_thread=32,load_avg=5.020000, \\\n                      load_short=4.640000,load_medium=5.020000, \\\n                      load_long=5.110000,mem_free=124798.726562M, \\\n                      swap_free=4095.996094M,virtual_free=128894.722656M, \\\n                      mem_used=3940.500000M,swap_used=0.000000M, \\\n                      virtual_used=3940.500000M,cpu=17.700000, \\\n                      m_topology_inuse=SCTTCTTCTTCTTCTTCTTCTTCTTSCTTCTTCTTCTTCTTCTTCTTCTT, \\\n                      np_load_avg=0.156875,np_load_short=0.145000, \\\n                      np_load_medium=0.156875,np_load_long=0.159688, \\\n                      gpu.ncuda=2,gpu.ndev=2,gpu.cuda.0.mem_free=758054912, \\\n                      gpu.cuda.0.procs=1,gpu.cuda.0.clock=2025, \\\n                      gpu.cuda.0.util=57,gpu.cuda.1.mem_free=758054912, \\\n                      gpu.cuda.1.procs=1,gpu.cuda.1.clock=2025, \\\n                      gpu.cuda.1.util=54,gpu.names=GeForce GTX 1080;GeForce \\\n                      GTX 1080;\nprocessors            32\nuser_lists            NONE\nxuser_lists           NONE\nprojects              NONE\nxprojects             NONE\nusage_scaling         NONE\nreport_variables      NONE\nThis tells us that host msg-iogpu3 has two GeForce GTX 1080 GPUs. Each GPU is running one process, each is just over 50% utilized, and each has approximately 722 MiB (758,054,912 bytes) of free memory.\n\n\nGPU profiling from within job\nOne can also use NVIDIA’s Data Center GPU Manager to get detailed profiling of GPU jobs. To use it, add the following at the beginning of your job script before you launch the software tools that use the GPUs:\ngpuprof=$(dcgmi group --create mygpus --add \"$SGE_GPU\" | awk '{print $10}')\ndcgmi stats --group \"$gpuprof\" --enable\ndcgmi stats --group \"$gpuprof\" --jstart \"$JOB_ID\"\nMake sure to stop the GPU profile when the job finishes, by putting the following at the end of your job script:\ndcgmi stats --group \"$gpuprof\" --jstop \"$JOB_ID\"\ndcgmi stats --group \"$gpuprof\" --verbose --job \"$JOB_ID\"\ndcgmi group --delete \"$gpuprof\"\nThe GPU stats will be written to the job’s output file."
  },
  {
    "objectID": "hpc/scheduler/queues.html",
    "href": "hpc/scheduler/queues.html",
    "title": "Available Queues",
    "section": "",
    "text": "The cluster provides different queues (“running areas”) that each is optimized for a different purpose.\n\nshort.q:\n\nMaximum runtime: 30 minutes\nProcess priority: 10 (medium)\nAvailability: all compute nodes\nQuota: 100 (queued or active) jobs per user (all users)\nPurpose: Low-latency needs, e.g. pipeline prototyping and quick turn-around analysis\n\nlong.q:\n\nMaximum runtime: 2 weeks (336 hours)\nProcess priority: 19 (lowest)\nAvailability: all compute nodes\nQuota: Unlimited (all users)\nPurpose: General needs\n\nmember.q:\n\nMaximum runtime: 2 weeks (336 hours)\nProcess priority: 0 (highest)\nAvailability: all compute nodes except GPU and institutionally purchased nodes\nCompute power: {{ site.data.specs.pu_total }} processing units\nNumber of slots: {{ site.data.specs.member_q_total }}\nQuota: Proportional to your lab’s contributed share on the cluster. When a lab has exhausted all its available member.q slots, additional jobs scheduled by lab members will spill over to the long.q queue\nPurpose: Research groups who need more computational resources than the above communal queues can contribute resources to the Wynton HPC cluster and gain priority access corresponding to the contribution\n\ngpu.q:\n\nMaximum runtime on communal GPU nodes: 2 weeks (336 hours)\nMaximum runtime on contributed GPU nodes: 2 weeks (336 hours) if you are the contributor, otherwise 2 hours\nProcess priority: 0 (highest)\nAvailability: 235 GPUs on 61 GPU nodes (88/22 GPUs/nodes are communal and 147/39 GPUs/nodes are contributed)\nNumber of GPU slots: 235\nQuota: Unlimited (all users)\nPurpose: For software that utilize Graphics Processing Units (GPUs)\n\n4gpu.q:\n\nMaximum runtime on contributed “All-4-GPU” nodes: 2 weeks (336 hours) if you are the contributor, otherwise 2 hours\nProcess priority: 0 (highest)\nAvailability: {{ site.data.specs.msg_4gpus }} GPUs on {{ site.data.specs.msg_4gpu_nodes }} GPU nodes (all are contributed nodes)\nNumber of GPU slots: {{ site.data.specs.msg_4gpu_nodes }}\nQuota: Unlimited (all users)\nPurpose: For software that need to exclusively, utilize all four Graphics Processing Units (GPUs) on the node\nComment: Only MSG members are contributors as of May 2022\n\nondemand.q:\n\nMaximum runtime: 2 weeks (336 hours)\nProcess priority: 0 (highest)\nAvailability: Institutionally purchased nodes only\nQuota: Available upon application and approval by the Wynton Steering Committee\nPurpose: Intended for scheduled, high-priority computing needs and / or temporary paid priority access\n\n\nComment: Here “runtime” means “walltime”, i.e. the runtime of a job is how long it runs according to the clock on the wall, not the amount of CPU time.\n\n\nExcept for the gpu.q and 4gpu.q queues, there is often no need to explicitly specify what queue your job should be submitted to. Instead, it is sufficient to specify the resources that your jobs need, e.g. the maximum processing time (e.g. -l h_rt=00:10:00 for ten minutes), the maximum memory usage (e.g. -l mem_free=1G for 1 GiB of RAM), and the number of cores (e.g. -pe smp 2 for two cores). When the scheduler knows about your job’s resource need, it will allocate your job to a compute node that better fits your needs and your job is likely to finish sooner.\nOnly in rare cases there should be a need to specify through what queue your job should run. To do this, you can use the -q &lt;name&gt; option of qsub, e.g. qsub -q long.q my_script."
  },
  {
    "objectID": "hpc/scheduler/queues.html#usage",
    "href": "hpc/scheduler/queues.html#usage",
    "title": "Available Queues",
    "section": "",
    "text": "Except for the gpu.q and 4gpu.q queues, there is often no need to explicitly specify what queue your job should be submitted to. Instead, it is sufficient to specify the resources that your jobs need, e.g. the maximum processing time (e.g. -l h_rt=00:10:00 for ten minutes), the maximum memory usage (e.g. -l mem_free=1G for 1 GiB of RAM), and the number of cores (e.g. -pe smp 2 for two cores). When the scheduler knows about your job’s resource need, it will allocate your job to a compute node that better fits your needs and your job is likely to finish sooner.\nOnly in rare cases there should be a need to specify through what queue your job should run. To do this, you can use the -q &lt;name&gt; option of qsub, e.g. qsub -q long.q my_script."
  },
  {
    "objectID": "hpc/transfers/dos2unix.html",
    "href": "hpc/transfers/dos2unix.html",
    "title": "Dos2Unix: Windows-Unix File Transfers",
    "section": "",
    "text": "Do you get obscure errors like \\r: command not found when trying to run your scripts on Wynton HPC? If so, the most likely explanation is that the problematic script file was created in an editor on MS Windows and then copied to Wynton HPC. If so, there is a simple solution - just run dos2unix on the file, e.g.\n[alice@dev2 ~]$ dos2unix script.sh\nSimilar problems may occur when you try to run Matlab, Python, and R scripts.\n\n\n\nWhen you hit ENTER in text editor, the editor will add a so-called invisible newline. When you edit on Linux or macOS, the newline comprise the \\n symbol, which is also referred to as the LF (line-feed) symbol. However, if you edit on MS Windows, the newline sequence comprise two symbols - \\r\\n, referred to as CR (carriage-return) followed by LF (line-feed).\nNow, most software tools on Linux and macOS assumes LF line endings and will not handle CR+LF line endings. So, say we create the following shell script file script.sh on an MS Windows machine:\n#! /usr/bin/env bash\n\nhostname\ntransfer it to Wynton HPC, and then try to run it there, we get an obscure error:\n[alice@dev2 ~]$ source script.sh \n\n: command not found\nThe problem is that Unix-like systems gets confused by that extra, invisible \\r (CR) at the end of each line. Now, since these symbols are invisible to you, we cannot really tell when looking at the file in the editor whether the line endings are LF (line feed) or CR+LF.\n\n\n\nWe can use file to inspect a file and report on what type of file it is, including type of line endings;\n[alice@dev2 ~]$ file script.sh\nscript.sh: Bourne-Again shell script, ASCII text executable,\nwith CRLF line terminators\nNote the mentioning of “CRLF line terminators”. Another way to check whether a file has CR symbols, is to use dos2unix with the --info=d option;\n[alice@dev2 ~]$ dos2unix --info=d script.sh\n       3  script.sh\nThe ‘3’ is the number of CR symbols found in file script.sh. When running on Wynton HPC, we want this count to be zero.\nWe can also “visualize” odd symbols, including the CR and LF symbols, by using cat with option -A;\n[alice@dev2 ~]$ source script.sh\n$ cat -A script.sh\n#! /usr/bin/env bash^M$\n^M$ \nhostname^M$ \nThe problematic CR symbols are displayed as ^M and the LF symbols as $.\n\n\n\nTo fix this problem, we can use dos2unix. As its name suggests, this tool converts a file from a DOS format to Unix format. DOS is the origin of MS Windows. By running:\n[alice@dev2 ~]$ dos2unix script.sh\ndos2unix: converting file script.sh to Unix format...\nall CR+LF line endings will be replaced with LF line endings. We can confirm this as:\n[alice@dev2 ~]$ file script.sh\nscript.sh: Bourne-Again shell script, ASCII text executable\nNote that there is no longer a mentioning of “CRLF line terminators”. We can also ask dos2unix to confirm there are zero CRLF line endings;\n[alice@dev2 ~]$ dos2unix --info=d script.sh\n       0  script.sh\nFinally, we can use cat -A to visually confirm this:\n[alice@dev2 ~]$ cat -A script.sh\n#! /usr/bin/env bash$\n$ \nhostname$ \nThere are no ^M displayed. The $ symbols are LF, which is what we want for line endings on Unix. If we try to run this script again, it’ll now work:\n[alice@dev2 ~]$ source script.sh\ndev2"
  },
  {
    "objectID": "hpc/transfers/dos2unix.html#tldr",
    "href": "hpc/transfers/dos2unix.html#tldr",
    "title": "Dos2Unix: Windows-Unix File Transfers",
    "section": "",
    "text": "Do you get obscure errors like \\r: command not found when trying to run your scripts on Wynton HPC? If so, the most likely explanation is that the problematic script file was created in an editor on MS Windows and then copied to Wynton HPC. If so, there is a simple solution - just run dos2unix on the file, e.g.\n[alice@dev2 ~]$ dos2unix script.sh\nSimilar problems may occur when you try to run Matlab, Python, and R scripts."
  },
  {
    "objectID": "hpc/transfers/dos2unix.html#the-reason-for-the-problem",
    "href": "hpc/transfers/dos2unix.html#the-reason-for-the-problem",
    "title": "Dos2Unix: Windows-Unix File Transfers",
    "section": "",
    "text": "When you hit ENTER in text editor, the editor will add a so-called invisible newline. When you edit on Linux or macOS, the newline comprise the \\n symbol, which is also referred to as the LF (line-feed) symbol. However, if you edit on MS Windows, the newline sequence comprise two symbols - \\r\\n, referred to as CR (carriage-return) followed by LF (line-feed).\nNow, most software tools on Linux and macOS assumes LF line endings and will not handle CR+LF line endings. So, say we create the following shell script file script.sh on an MS Windows machine:\n#! /usr/bin/env bash\n\nhostname\ntransfer it to Wynton HPC, and then try to run it there, we get an obscure error:\n[alice@dev2 ~]$ source script.sh \n\n: command not found\nThe problem is that Unix-like systems gets confused by that extra, invisible \\r (CR) at the end of each line. Now, since these symbols are invisible to you, we cannot really tell when looking at the file in the editor whether the line endings are LF (line feed) or CR+LF."
  },
  {
    "objectID": "hpc/transfers/dos2unix.html#identifying-the-problem",
    "href": "hpc/transfers/dos2unix.html#identifying-the-problem",
    "title": "Dos2Unix: Windows-Unix File Transfers",
    "section": "",
    "text": "We can use file to inspect a file and report on what type of file it is, including type of line endings;\n[alice@dev2 ~]$ file script.sh\nscript.sh: Bourne-Again shell script, ASCII text executable,\nwith CRLF line terminators\nNote the mentioning of “CRLF line terminators”. Another way to check whether a file has CR symbols, is to use dos2unix with the --info=d option;\n[alice@dev2 ~]$ dos2unix --info=d script.sh\n       3  script.sh\nThe ‘3’ is the number of CR symbols found in file script.sh. When running on Wynton HPC, we want this count to be zero.\nWe can also “visualize” odd symbols, including the CR and LF symbols, by using cat with option -A;\n[alice@dev2 ~]$ source script.sh\n$ cat -A script.sh\n#! /usr/bin/env bash^M$\n^M$ \nhostname^M$ \nThe problematic CR symbols are displayed as ^M and the LF symbols as $."
  },
  {
    "objectID": "hpc/transfers/dos2unix.html#fixing-the-problem",
    "href": "hpc/transfers/dos2unix.html#fixing-the-problem",
    "title": "Dos2Unix: Windows-Unix File Transfers",
    "section": "",
    "text": "To fix this problem, we can use dos2unix. As its name suggests, this tool converts a file from a DOS format to Unix format. DOS is the origin of MS Windows. By running:\n[alice@dev2 ~]$ dos2unix script.sh\ndos2unix: converting file script.sh to Unix format...\nall CR+LF line endings will be replaced with LF line endings. We can confirm this as:\n[alice@dev2 ~]$ file script.sh\nscript.sh: Bourne-Again shell script, ASCII text executable\nNote that there is no longer a mentioning of “CRLF line terminators”. We can also ask dos2unix to confirm there are zero CRLF line endings;\n[alice@dev2 ~]$ dos2unix --info=d script.sh\n       0  script.sh\nFinally, we can use cat -A to visually confirm this:\n[alice@dev2 ~]$ cat -A script.sh\n#! /usr/bin/env bash$\n$ \nhostname$ \nThere are no ^M displayed. The $ symbols are LF, which is what we want for line endings on Unix. If we try to run this script again, it’ll now work:\n[alice@dev2 ~]$ source script.sh\ndev2"
  },
  {
    "objectID": "hpc/transfers/rclone.html",
    "href": "hpc/transfers/rclone.html",
    "title": "Using rclone to mount /wynton filesystem",
    "section": "",
    "text": "Using rclone to mount /wynton filesystem\n\n2024-07-26: It appears that rclone over SFTP to Wynton is broken. If tried, an error ‘Failed to create file system for “log1:”: NewFs: couldn’t connect SSH: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none], no supported methods remain’ is produced.\n\nThe rclone application is available for macOS, Windows and Linux. Check the rclone website for the most up-to-date installation instructions for your operating system.\n\ninstall FUSE\n\nmacOS: install macFUSE\nfor Windows, follow instructions for installing on Windows\n\ninstall rclone from the Downloads page\nset up a new ssh key\n\ngenerate key with ssh-keygen command with​ a password/passphrase ~/.ssh % ssh-keygen -m PEM -f ~/.ssh/rclone_to_wynton\n2 files should be generated, in this case named:\n\nrclone_to_wynton\nrclone_to_wynton.pub\n\n\nstore the new key rclone_to_wynton in ~/.ssh/ on laptop/workstation\npush the .pub file to wynton using: $ ssh-copy-id -i ~/.ssh/rclone_to_wynton.pub alice@log2.wynton.ucsf.edu\n\nnote: replace alice with your Wynton username\n\n\nset up a new “remote” with: rclone config\n\nTo create a new remote: run rclone config, in this example named log1 and select sftp as the “Type of Storage”:\nname&gt; log1\nstorage&gt; sftp\nhost&gt; log1.wynton.ucsf.edu\nuser&gt; alice                         &lt;-- enter *your* Wynton username\nport&gt; 22\nkey_file&gt; ~/.ssh/rclone_to_wynton\nOption key_file_pass.\nThe passphrase to decrypt the PEM-encoded private key file.\nOnly PEM encrypted key files (old OpenSSH format) are supported. Encrypted keys\nin the new OpenSSH format can't be used.\nChoose an alternative below. Press Enter for the default (n).\ny) Yes, type in my own password\nEnter the password:\npassword:\nTo mount /wynton you will need to be on the UCSF network or VPN, or have authenticated to Duo already that day and selected “Remember me for 12hrs” so rclone doesn’t get prompted for the Duo password.\n\nin one terminal:\n\nrclone mount by default runs in the foreground (there’s a background option). But using the default in foreground, in one terminal window on laptop (log1 is the name of the “remote” in this example):\nlaptop ~ $ ./rclone mount log1:/ ./wynton\n\nin a separate terminal, now that Wynton’s / directory is mounted:\n\n$ ls -l wynton\ndrwxr-xr-x.   3 root root         1 Oct 22  2020 globus\ndrwxr-xr-x. 130 root root       130 Sep 20 14:05 group\ndrwxr-xr-x. 563 root root       561 Oct  5 17:22 home\ndrwxrwx---.   2 root root         0 Jul 23  2019 lost+found\ndrwxr-x---.   6 root wynton-phi   4 Aug 17  2022 protected\ndrwxrwxrwt. 105 root root       103 Oct  9 03:15 scratch\nshould now show the contents from the /wynton and look like a local directory on your laptop/workstation.\nDepending what work you are doing you might want to just mount a directory from /wynton/scratch/ or your home directory.\nYou should also then be able to run other rclone commands like rclone lsd, rclone copy, etc."
  },
  {
    "objectID": "hpc/transfers/ucsf-box.html",
    "href": "hpc/transfers/ucsf-box.html",
    "title": "Accessing UCSF Box",
    "section": "",
    "text": "Although there is no hard quota in place on UCSF Box, the storage is limited and not really meant to store large data sets. There are reports that the UCSF Box maintainers have started to reach out to users who misuse it to store huge amounts of data. Please use it responsibly.\n\n\nBelow instructions work on data transfer nodes only. Attempts to use them on development nodes will result in “Access failed: 401 Unauthorized” errors.\n\nIt is possible to access UCSF Box using FTP over a secure SSL connection (FTPS). The below instructions works from the Wynton HPC data-transfer nodes as well as your local computer.\nIt is also possible for UCSF Users to access UCSF Box via the UCSF Wynton HPC Box Connector. For details on how to utilize the UCSF Wynton HPC Box Connector see the Globus File Transfers page on our website.\n\n\nIn order to do access UCSF Box as describe below, you need to:\n\nSetup a UCSF Box-specific password in Box under Account Settings -&gt; Account as explained in https://support.box.com/hc/en-us/articles/360043697414-Using-Box-with-FTP-or-FTPS.\n\n\n\n\nWith a UCSF Box-specific password (see above), you can use, for instance, the lftp or curl tools to access to your UCSF Box account. Start by logging in to one of the data-transfer nodes, either directly from outside or via a login node, e.g.\n[alice@log2 ~]$ ssh dt2\nalice1@dt2:s password: XXXXXXXXXXXXXXXXXXX\n[alice@dt2 ~]$ \nThen, verify that your UCSF Box setup is correct by logging into the root of your UCSF Box folder using your UCSF Box-specific password (not your Wynton HPC password):\n[alice@dt2 ~]$ lftp --user alice.aliceson@ucsf.edu ftps://ftp.box.com\nPassword: XXXXXXXX  &lt;== UCSF Box password here!\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; ls\ndrwx------  1 owner group     0 Jun 12  2014 Grant_R01.pdf\ndrwx------  1 owner group     0 Sep 30  2016 Secure-alice.aliceson@ucsf.edu\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; exit\n[alice@dt2 ~]$ \n\n🛑 Never specify your password via a command-line argument! If you do, it will be visible to all other users via commands such as ps and htop.\n\n\n\n\nWhen starting lftp as above, you need to manually enter your password, which can be tedious or even prevent automatic file transfers in batch scripts. A solution to this is to set up the FTPS credentials in ~/.netrc. Here is what it could look like:\n[alice@dt2 ~]$ cat ~/.netrc\nmachine ftp.box.com\n        login alice.aliceson@ucsf.edu\n        password AliceSecretPwd2017\n\n🛑 The ~/.netrc file must be kept private, otherwise its content could be readable to other users.\n\nSince the password is fully visible in plain text, make sure to keep this file private at all times, otherwise users on the system can see all your credentials, i.e.\n[alice@dt2 ~]$ chmod 600 ~/.netrc\n[alice@dt2 ~]$ ls -l ~/.netrc\n-rw------- 1 alice alice 72 Jul  3 15:10 /wynton/home/boblab/alice/.netrc\nTo verify that the automatic authentication works, try to log in again. You should no longer be prompted for your password - instead lftp gets it automatically from ~/.netrc. For example:\n[alice@dt2 ~]$ lftp --user alice.aliceson@ucsf.edu ftps://ftp.box.com\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; ls\ndrwx------  1 owner group     0 Jun 12  2014 Grant_R01.pdf\ndrwx------  1 owner group     0 Sep 30  2016 Secure-alice.aliceson@ucsf.edu\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; exit\n$ \nNote that curl also recognizes ~/.netrc credentials, e.g.\n[alice@dt2 ~]$ curl --netrc -O ftps://ftp.box.com/Grant_R01.pdf\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 15.6M  100 15.6M    0     0  1561k      0  0:00:10  0:00:10 --:--:-- 3918k\n[alice@dt2 ~]$ ls -la Grant_R01.pdf\n-rw-r--r-- 1 alice cluster 16453180 Jul 10 21:13 Grant_R01.pdf\nTo upload a file, we can do:\n[alice@dt2 ~]$ curl --netrc --upload-file notes.txt ftps://ftp.box.com/"
  },
  {
    "objectID": "hpc/transfers/ucsf-box.html#prerequisites",
    "href": "hpc/transfers/ucsf-box.html#prerequisites",
    "title": "Accessing UCSF Box",
    "section": "",
    "text": "In order to do access UCSF Box as describe below, you need to:\n\nSetup a UCSF Box-specific password in Box under Account Settings -&gt; Account as explained in https://support.box.com/hc/en-us/articles/360043697414-Using-Box-with-FTP-or-FTPS."
  },
  {
    "objectID": "hpc/transfers/ucsf-box.html#accessing-ucsf-box-over-ftps",
    "href": "hpc/transfers/ucsf-box.html#accessing-ucsf-box-over-ftps",
    "title": "Accessing UCSF Box",
    "section": "",
    "text": "With a UCSF Box-specific password (see above), you can use, for instance, the lftp or curl tools to access to your UCSF Box account. Start by logging in to one of the data-transfer nodes, either directly from outside or via a login node, e.g.\n[alice@log2 ~]$ ssh dt2\nalice1@dt2:s password: XXXXXXXXXXXXXXXXXXX\n[alice@dt2 ~]$ \nThen, verify that your UCSF Box setup is correct by logging into the root of your UCSF Box folder using your UCSF Box-specific password (not your Wynton HPC password):\n[alice@dt2 ~]$ lftp --user alice.aliceson@ucsf.edu ftps://ftp.box.com\nPassword: XXXXXXXX  &lt;== UCSF Box password here!\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; ls\ndrwx------  1 owner group     0 Jun 12  2014 Grant_R01.pdf\ndrwx------  1 owner group     0 Sep 30  2016 Secure-alice.aliceson@ucsf.edu\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; exit\n[alice@dt2 ~]$ \n\n🛑 Never specify your password via a command-line argument! If you do, it will be visible to all other users via commands such as ps and htop."
  },
  {
    "objectID": "hpc/transfers/ucsf-box.html#automatic-authentication",
    "href": "hpc/transfers/ucsf-box.html#automatic-authentication",
    "title": "Accessing UCSF Box",
    "section": "",
    "text": "When starting lftp as above, you need to manually enter your password, which can be tedious or even prevent automatic file transfers in batch scripts. A solution to this is to set up the FTPS credentials in ~/.netrc. Here is what it could look like:\n[alice@dt2 ~]$ cat ~/.netrc\nmachine ftp.box.com\n        login alice.aliceson@ucsf.edu\n        password AliceSecretPwd2017\n\n🛑 The ~/.netrc file must be kept private, otherwise its content could be readable to other users.\n\nSince the password is fully visible in plain text, make sure to keep this file private at all times, otherwise users on the system can see all your credentials, i.e.\n[alice@dt2 ~]$ chmod 600 ~/.netrc\n[alice@dt2 ~]$ ls -l ~/.netrc\n-rw------- 1 alice alice 72 Jul  3 15:10 /wynton/home/boblab/alice/.netrc\nTo verify that the automatic authentication works, try to log in again. You should no longer be prompted for your password - instead lftp gets it automatically from ~/.netrc. For example:\n[alice@dt2 ~]$ lftp --user alice.aliceson@ucsf.edu ftps://ftp.box.com\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; ls\ndrwx------  1 owner group     0 Jun 12  2014 Grant_R01.pdf\ndrwx------  1 owner group     0 Sep 30  2016 Secure-alice.aliceson@ucsf.edu\nlftp alice.aliceson@ucsf.edu@ftp.box.com:~&gt; exit\n$ \nNote that curl also recognizes ~/.netrc credentials, e.g.\n[alice@dt2 ~]$ curl --netrc -O ftps://ftp.box.com/Grant_R01.pdf\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 15.6M  100 15.6M    0     0  1561k      0  0:00:10  0:00:10 --:--:-- 3918k\n[alice@dt2 ~]$ ls -la Grant_R01.pdf\n-rw-r--r-- 1 alice cluster 16453180 Jul 10 21:13 Grant_R01.pdf\nTo upload a file, we can do:\n[alice@dt2 ~]$ curl --netrc --upload-file notes.txt ftps://ftp.box.com/"
  },
  {
    "objectID": "hpc/openings/index.html",
    "href": "hpc/openings/index.html",
    "title": "Job Openings at Wynton",
    "section": "",
    "text": "Job Openings at Wynton\nWe currently do not have any opening.\nLast updated: June 12, 2023"
  },
  {
    "objectID": "hpc/get-started/phi-quickstart.html",
    "href": "hpc/get-started/phi-quickstart.html",
    "title": "Wynton PHI Quickstart",
    "section": "",
    "text": "2025-07-08: Termination of Wynton Support for P4/PHI data\n\nStarting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton.\n\n\n\nWynton PHI Quickstart\nIt is as of 2025-07-08 forbidden to process Protected Health Information (PHI) data, including P4 data, on the Wynton HPC environment."
  },
  {
    "objectID": "hpc/get-started/protected-quickstart.html",
    "href": "hpc/get-started/protected-quickstart.html",
    "title": "Wynton Protected Quickstart",
    "section": "",
    "text": "2025-07-08: Termination of Wynton Support for P4/PHI data\n\nStarting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton."
  },
  {
    "objectID": "hpc/get-started/protected-quickstart.html#ucsf-data-classification-standard-restricted-data-types",
    "href": "hpc/get-started/protected-quickstart.html#ucsf-data-classification-standard-restricted-data-types",
    "title": "Wynton Protected Quickstart",
    "section": "UCSF Data Classification Standard & Restricted Data Types",
    "text": "UCSF Data Classification Standard & Restricted Data Types\n\nPlease see UCSF Policy 650-16 Addendum F, UCSF Data Classification Standard for more information regarding UCSF’s data classification standard:\n\nPersonally Identifiable Information (PII)\nProtected Health Information (PHI)"
  },
  {
    "objectID": "hpc/get-started/protected-quickstart.html#first-step-wynton-accounts",
    "href": "hpc/get-started/protected-quickstart.html#first-step-wynton-accounts",
    "title": "Wynton Protected Quickstart",
    "section": "First step: Wynton accounts",
    "text": "First step: Wynton accounts\n\nIf you either don’t have a Wynton HPC account or if you already have a Wynton account, but it is not authorized to use protected data:\n\nlink to learn more about the Wynton cluster\nlink to actually request a Wynton Protected account\n\nA link will be emailed to a Statement of Responsibility form that all users applying for Wynton Protected access must sign agreeing to the responsibilities of handling data.\n\nImportant: The email address associated with a new Statement of Responsibility must exactly match the user’s original email address."
  },
  {
    "objectID": "hpc/get-started/protected-quickstart.html#principal-investigator-responsibilities-for-wynton-protected",
    "href": "hpc/get-started/protected-quickstart.html#principal-investigator-responsibilities-for-wynton-protected",
    "title": "Wynton Protected Quickstart",
    "section": "Principal Investigator responsibilities for Wynton Protected",
    "text": "Principal Investigator responsibilities for Wynton Protected\n\nThe Principal Investigator (PI) is responsible for all protected data\nAdditionally, the following rules apply to PIs using protected data on Wynton or approving users that use Wynton Protected:\n\nThe PI must notify Wynton of any approved users whose access needs to be removed or is no longer required\nThe PI must notify Wynton of any users who have transferred departments and no longer require access to study data\nThe PI must notify Wynton when departing UCSF and transfer to another UCSF owner or archive their projects and data\nThe PI is responsible for ensuring that any user added to a Wynton Protected project that requires IRB approval, is listed on the IRB\nThe PI is responsible for classifying and taking inventory of data within their Wynton Protected project\nThe PI must notify Wynton of any change in security requirements in research agreements to Wynton admins"
  },
  {
    "objectID": "hpc/get-started/protected-quickstart.html#user-responsibilities-for-wynton-protected",
    "href": "hpc/get-started/protected-quickstart.html#user-responsibilities-for-wynton-protected",
    "title": "Wynton Protected Quickstart",
    "section": "User responsibilities for Wynton Protected",
    "text": "User responsibilities for Wynton Protected\n\nRead and comply with the Wynton HPC User Agreement and Disclaimer\nAbide by the statement of Wynton HPC Purpose, Principles and Governance\nUser end points (e.g. laptops and desktops) connecting to Wynton must meet UCSF Minimum Security Standards for Electronic Information Resources\nWynton Protected users must use Wynton Protected-specific nodes on Wynton;\n\nlogin nodes: plog1.wynton.ucsf.edu\ndevelopment nodes: pdev1 and pgpudev1\ndata-transfer nodes: pdt1.wynton.ucsf.edu and pdt2.wynton.ucsf.edu\n\nWynton Protected users must not use any of the Wynton Regular nodes on Wynton, including log1, log2, dev1, dev2, dev3, gpudev1, dt1, and dt2.\nData containing P3 must not be transferred to, mounted on, or processed with any Wynton HPC cluster resources outside of the /wynton/protected/ location. P4 data is prohibited.\nWynton Protected users must use data-transfer nodes pdt1 and pdt2 for all file transfers to and from the cluster, including when using Globus\nIf you have questions regarding the security status of your data, please contact the UCSF Privacy Office"
  },
  {
    "objectID": "hpc/get-started/protected-quickstart.html#frequently-asked-questions-faq",
    "href": "hpc/get-started/protected-quickstart.html#frequently-asked-questions-faq",
    "title": "Wynton Protected Quickstart",
    "section": "Frequently Asked Questions (FAQ)",
    "text": "Frequently Asked Questions (FAQ)\nQ. What if I want to share data between /wynton/protected/group/ (Wynton Protected) and /wynton/group/ (Wynton Regular) directories?\nA. Users with Wynton Protected access still have access to /wynton/group/, as do Wynton Regular users. However, protected data should never be stored under /wynton/group/ and protected data should never be shared with a user who does not have Wynton Protected access."
  },
  {
    "objectID": "hpc/get-started/access-cluster.html",
    "href": "hpc/get-started/access-cluster.html",
    "title": "Wynton HPC Login & Logout",
    "section": "",
    "text": "⚠️ Warning: You appear to be connected to one of the UCSF WiFi:s - ‘UCSFguest’, ‘UCSFhousing’ or UCSF ‘eduroam’. If you are on ‘UCSFguest’ or UCSF ‘eduroam’, you will not be able to reach Wynton by SSH. The symptom is an “ssh: connect to host log2.wynton.ucsf.edu port 22: Connection timed out” error. If you get that error, make sure to switch to another WiFi such as ‘UCSFwpa’ or ‘UCSFhousing’, or connect to the UCSF VPN, before trying again."
  },
  {
    "objectID": "hpc/get-started/access-cluster.html#prerequisites",
    "href": "hpc/get-started/access-cluster.html#prerequisites",
    "title": "Wynton HPC Login & Logout",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe instructions below requires that you:\n\nhave an SSH client available on your local computer\nhave a Wynton HPC account (not the same as your UCSF account)\nknow your Wynton HPC username (not the same as your UCSF password)\nknow your Wynton HPC password (not the same as your UCSF password)\nknow whether you have a Regular or Protected Wynton HPC account (you only have a Protected account if you requested it)\n\nUsers on Linux, macOS, and MS Windows have an SSH client built-in, which is available from the terminal as ssh. We will use that one in all our examples. You can also use other SSH clients if you prefer, include PuTTY on MS Windows. If you choose to use another SSH client, consult that program’s documentation for further instructions. Another alternative, which some might find easier, is to connect to the cluster using the X2Go software.\n\nProblem logging in? It might be that you have a Protected account, but you try to log into a regular login node. Set the following switch to tailor the instructions on the website to Protected users:\n\n👉  Regular     Protected"
  },
  {
    "objectID": "hpc/get-started/access-cluster.html#instructions",
    "href": "hpc/get-started/access-cluster.html#instructions",
    "title": "Wynton HPC Login & Logout",
    "section": "Instructions",
    "text": "Instructions\n\nIf this is the first time you access Wynton HPC and you are outside of the UCSF network, we recommend that you first log onto the UCSF VPN before continuing below. This avoids having to deal with Wynton’s two-factor authentication that is otherwise required when accessing the cluster from outside of the UCSF. Note that this advice is just to get you started. In the long run, you do not want to be on the very bandwidth-limited UCSF VPN if you are transferring large amounts of files to and from Wynton to your local machine.\n\nTo log into the cluster, assuming your user name on Wynton HPC is alice (case sensitive), do the following:\n\nFor first-timers outside of the UCSF network, log onto the UCSF VPN,\nopen a terminal (‘Terminal’ on macOS and most Linux distributions, and ‘Windows Console’ [old] or ‘Windows Terminal’ [modern] on MS Windows),\nat the terminal prompt, type ssh alice@log2.wynton.ucsf.edu and press ENTER, and\nwhen prompted, enter your Wynton HPC password.\n\n\nExample\nWhen logging in from your local computer, you should see something like:\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu:s password: \n[alice@log2 ~]$ \nNote, when typing your password, there will be no visual feedback at all, but the system will still receive what you type, so just press ENTER afterward.\nIf you get Permission denied, please try again. when you enter your password, make sure you use the correct Wynton HPC username (case sensitive) and the correct password.\n\nIt is possible to set up password-less authentication via a public-private SSH key pair. For details, see the how-to page Log in without Password."
  },
  {
    "objectID": "hpc/about/warning-hold-on-storage-requests.html",
    "href": "hpc/about/warning-hold-on-storage-requests.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "2025-01-15: Temporary Hold on Storage and Compute Requests\nAs Wynton continues to expand, we are approaching the limits of space, cooling, and power capacity in the Byers Hall server room. In addition, as we work to move the administration of the cluster to the Academic Research Systems (ARS) Team, we are currently reprioritizing our workload with the support of Wynton faculty leadership to ensure that we continue to meet our most critical objectives. We understand that this may cause some delays and appreciate your patience and understanding during this period.\n\nWaitlist for New Storage Brick: If you are already on the waitlist for the new storage brick, please be assured that you will be notified when this service is brought online.\nPending Requests: If you have recently submitted a request and have not yet received a response, we will evaluate whether we can accommodate some or all of your requests at this time. The Wynton Project Manager will reach out to you with more information.\nUrgent Needs: If you have an urgent need, please contact {{ site.cluster.email_support }} with the details of your request and its urgency. We will do our best to accommodate your needs.\n\nSee also the Wynton-announcement email titled ‘Important Updates on Wynton Storage and Compute Requests’ sent to all users on 2024-11-25."
  },
  {
    "objectID": "hpc/about/pricing.html",
    "href": "hpc/about/pricing.html",
    "title": "Wynton HPC Compute and Storage Pricing",
    "section": "",
    "text": "2018-10-06: This is a mockup page with mockup information."
  },
  {
    "objectID": "hpc/about/pricing.html#compute-pricing",
    "href": "hpc/about/pricing.html#compute-pricing",
    "title": "Wynton HPC Compute and Storage Pricing",
    "section": "Compute pricing",
    "text": "Compute pricing\nFree members, and other members who have not contributed toward compute, will be limited by the number of concurrent cores and will have lower priority on the job queue. Participating co-op members that contribute compute to the cluster will get priority on the job queue and will be able to utilize a large number of concurrent cores (proportionate to their contribution). For further details on scheduling and compute priorities, see Available queues.\nContributions toward compute can be done either in cash (“buy in”) or by integrating existing hardware (“bring your own”).\n\nBuy new compute\nAs of October 2018, a compute unit with four nodes (112 cores), 384 GiB RAM, ~0.8 TiB local scratch, 10 Gbps network card, is ~37,000 USD. It is possible buy into a partial compute unit - please let us known and we will combine your contributions with others before purchasing the new hardware.\n\n\nBring your own compute\nAs of October 2018, the minimal requirement for hardware contributions is 8 cores, 16 GiB RAM, and 1 Gbps networking. Please note that contributed hardware will be placed on Wynton’s private network, wiped, and reinstalled with the standard Wynton HPC image."
  },
  {
    "objectID": "hpc/about/pricing.html#storage-pricing",
    "href": "hpc/about/pricing.html#storage-pricing",
    "title": "Wynton HPC Compute and Storage Pricing",
    "section": "Storage pricing",
    "text": "Storage pricing\nWe are currently in the process of defining the storage pricing model, which will be available as soon as we have identified all the costs involved."
  },
  {
    "objectID": "hpc/about/contact.html",
    "href": "hpc/about/contact.html",
    "title": "Contact Information",
    "section": "",
    "text": "Contact Information\nTo reach Wynton support staff by email and for other support options, please refer to the Support page."
  },
  {
    "objectID": "hpc/about/news.html",
    "href": "hpc/about/news.html",
    "title": "News",
    "section": "",
    "text": "News\n2025-07-08: Termination of Wynton Support for P4/PHI data. Starting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton.\n2025-04-22: PHI Freeze: Effective immediately, we are pausing requests for new projects that process P4 data, specifically Protected Health Information (PHI).\n2025-01-16: The memory limit on development nodes was decreased from 68 GiB to 48 GiB, in order to further lower the risk for these machines to run low on memory resulting in non-responsiveness.\n2024-12-14: The memory limit on development nodes was decreased from 96 GiB to 68 GiB, in order to lower the risk for these machines to run low on memory resulting in non-responsiveness.\n2024-12-04: The CPU quota limit on development nodes was disabled, because it resulting in sever load on the operating system from context switching.\n2024-11-11: The CPU quota limit on development nodes were bumped up from 200% (“2 cores”) to 400% (“4 cores”), with the hope to lower the overall overhead from context switching.\n2024-09-12: CPU and memory usage is now limited on interactive nodes for each user. On development and data-transfer nodes, each user is limited to 200% (“2 cores”) and 96 GiB of memory. On login nodes, the limits are 100% (“1 core”) and 32 GiB of memory.\n2023-11-30: The JupyterHub (JHub) server has been discontinued.\n2023-11-15: Wynton is now running Rocky 8 Linux.\n2023-11-06: Wynton has been upgrade to Rocky 8 Linux from CentOS 7.\n2023-10-26: Rocky 8: Login node log2, data-transfer nodes dt1, and development node dev2 are now running Rocky 8.\n(Rocky 8 Linux) 2023-10-20: Rocky 8: Login node log1 and data-transfer nodes dt2 and ptd2 are now running Rocky 8.\n2023-10-13: Rocky 8: 6% of Wynton compute slots are now running on Rocky 8 compute nodes. 41 compute nodes with a total of 1,104 CPU slots run Rocky 8.\n2023-09-28: Rocky 8: 5% of Wynton compute slots are now running on Rocky 8 compute nodes. 33 compute nodes with a total of 880 CPU slots run Rocky 8.\n2023-09-13: Rocky 8: Development node dev3 is now running Rocky 8.\n2023-07-21: Rocky 8: Wynton will migrate from CentOS 7 to Rocky 8 at the end of October 2023. To prepare for this, we have made one non-PHI and one PHI development node available for all users, together with six compute nodes.\n2022-06-01: Added support for transferring files between Wynton and UCSF Box via Globus.\n2022-05-02: Added a PHI-compliant endpoint to Globus.\n2021-12-17: Wynton PHI is now available to all UCSF researchers and affiliates to process computing jobs involving PHI.\n2021-10-01: Added four communal GPU nodes each with four Nvidia A40 GPUs (48 GB GPU RAM) and a 32-core CPU and 512 GiB RAM\n2021-09-22: Added 2,048 cores (+20%) via 16 huge-memory nodes (128 cores and 1 TiB of RAM)\n2021-05-28: GPU development node gpudev1 has been upgraded to a machine with 32 CPU cores, 128 GiB RAM, and Nvidia Tesla K80 (was 12 cores, 48 GiB RAM, and GeForce GTX 980 Ti)\n2021-05-28: Development node dev1 has been upgraded to a machine with 72 CPU cores and 384 GiB RAM (was 8 cores and 16 GiB RAM)\n2021-04-05: We have improved the purchasing and accounting workflow for lab storage orders resulting in shorter waiting times.\n2021-03-12: Zsh is now also available in addition to the officially supported Bash, our default shell, Csh, and Tcsh.\n2021-01-25: We now offer weekly Office Hours.\n2020-10-29: Access to Wynton’s login and data-transfer nodes requires 2FA authentication.\n2020-08-21: Deployed four BeeGFS storage bricks for the group storage /wynton/group to 5.6 PB (was 3.8 PB). Groups who purchased storage have had their new group quotas updated.\n2020-08-21: Increased global /wynton/scratch to 615 TiB (was 492 TiB).\n2020-08-21: Prefix qb3- has been dropped from the development node names, which are now named dev1, dev2, dev3, and gpudev1.\n2020-05-22: Added 2,016 cores (+27%) via 48 standard nodes (36 cores and 384 GiB of RAM), 4 high-memory nodes (36 cores and 756 GiB of RAM), and 4 huge-memory nodes (36 cores and 1.5 TiB of RAM).\n2020-04-14: Progress until the next round of storage purchase can be found on About -&gt; Storage Pricing.\n2020-04-03: Added support for file transfers via the Globus service.\n2020-02-26: Website moved to &lt;https://wynton.ucsf.edu//hpc/&gt;.\n2020-02-05: The QB3-legacy NetApp storage (/netapp/), deprecated with a deadline on December 2019, crashed on 2020-02-05 and was declared non-recoverable without further resource/funding.\n2019-12-13: Status page now include GPU queue metrics.\n2019-11-12: Added support for email notifications when a job starts running or ends.\n2019-09-20: New GPU policy in place. All GPU nodes, communal and contributed, are now available to all users. Run time is two weeks, unless for jobs running on a contributed nodes that was not contributed by you in which case the run time is limited to 2 hours.\n2019-09-13: Added another three communal GPU nodes with a total of 12 GPUs.\n2019-09-12 Added a second data transfer node.\n2019-08-15 Legacy NetApp storage locations /scrapp and /scrapp2 used for global scratch have been removed - use /wynton/scratch instead.\n2019-07-27 Legacy NetApp storage locations /scrapp and /scrapp2 used for global scratch are now deprecated - use /wynton/scratch instead. The deprecated mounts will become read-only on 2019-08-01 and removed on 2019-08-09.\n2019-06-13: Contributed Software Repositories are now available and documented.\n2019-04-30: Contributing Member Shares are now explained and listed online.\n2019-04-15: Added a dedicated GPU development node (after having been in beta testing for several months).\n2019-04-12: Added another two communal GPU nodes available to all users. There are now four communal GPU nodes with a total of 12 GPUs.\n2019-04-12: Migrated another 24 nodes (396 cores) from QB3 to Wynton HPC.\n2019-04-09: Added the first two communal GPU nodes available to all users.\n2019-03-21: To decrease the number of stray shells, any shell session that has been idle for more than eight hours will timeout and exit automatically.\n2019-03-17: The majority (136 nodes; 3680 cores) of the QB3 nodes has now been migrated to Wynton HPC.\n2019-03-15: Migrated another 8 nodes (224 cores) from QB3 to Wynton HPC.\n2019-03-12: Migrated another 19 nodes (532 cores) from QB3 to Wynton HPC.\n2019-03-08: Migrated another 10 nodes (264 cores) from QB3 to Wynton HPC.\n2019-03-07: Migrated another 11 nodes (292 cores) from QB3 to Wynton HPC.\n2019-03-06: Migrated another 15 nodes (392 cores) from QB3 to Wynton HPC.\n2019-03-05: Migrated another 15 nodes (392 cores) from QB3 to Wynton HPC.\n2019-03-01: Migrated 48 nodes (1344 cores) from QB3 to Wynton HPC.\n2019-02-14: Added SGE resource eth_speed for requesting minimum network speeds.\n2019-01-31: Added two more development nodes.\n2019-01-18: It is now possible to purchase additional storage.\n2018-11-05: Nightly cleanup of scratch spaces now respects also when files were “added” - not just when they were last modified. This fixes the problem where files with old timestamps were extracted from an archive just to be wiped by the next nightly cleanup.\n2018-10-02: Added a Roadmap.\n2018-08-20: Global scratch storage on BeeGFS Parallel File System is live - validation and testing completed.\n2018-08-02: Added a dedicate 10 Gbps transfer node for faster file transfers in to and out from Wynton.\n2018-07-25: BeeGFS Parallel File System/Storage: Validation completed - user beta testing started.\n2017-09-07: Wynton HPC is live.\nFor upcoming features and improvements, please see the Roadmap."
  },
  {
    "objectID": "hpc/about/wynton-phi.html",
    "href": "hpc/about/wynton-phi.html",
    "title": "Wynton PHI",
    "section": "",
    "text": "2025-07-08: Termination of Wynton Support for P4/PHI data\n\nStarting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton.\n\n\n\nWynton PHI\nIt is as of 2025-07-08 forbidden to process Protected Health Information (PHI) data, including P4 data, on the Wynton HPC environment."
  },
  {
    "objectID": "hpc/about/specs.html",
    "href": "hpc/about/specs.html",
    "title": "Cluster Specifications",
    "section": "",
    "text": "Compute nodes\n\n\n482\n\n\nPhysical cores\n\n\n17496\n\n\nGPUs\n\n\n235 GPUs on 61 GPU nodes (88/22 GPUs/nodes are communal and 147/39 GPUs/nodes are prioritized for GPU contributors)\n\n\nRAM\n\n\n48-1512 GiB/node\n\n\nLocal scratch\n\n\n0.1-1.8 TiB/node\n\n\nGlobal scratch\n\n\n703 TiB\n\n\nUser home storage\n\n\n500 GiB/user (770 TiB in total)\n\n\nGroup storage\n\n\n6.5 PB\n\n\nNumber of accounts\n\n\n1312 of which 365 are Wynton Protected accounts\n\n\nNumber of projects\n\n\n761\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nLogin Nodes\nTransfer Nodes\nDevelopment Nodes\nCompute Nodes\n\n\n\n\nHostname\nlog[1-2].wynton.ucsf.edu, plog1.wynton.ucsf.edu\ndt[1-2].wynton.ucsf.edu, pdt[1-2].wynton.ucsf.edu\ndev[1-3], gpudev1, pdev1, pgpudev1\n…\n\n\nAccessible via SSH from outside of cluster\n✓ (2FA if outside of UCSF)\n✓ (2FA if outside of UCSF)\nno\nno\n\n\nAccessible via SSH from within cluster\n✓\n✓\n✓\nno\n\n\nOutbound access\nWithin UCSF only: SSH and SFTP\nHTTP/HTTPS, FTP/FTPS, SSH, SFTP, Globus\nVia proxy: HTTP/HTTPS, GIT+SSH(*)\nno\n\n\nNetwork speed\n10 Gbps\n10 Gbps\n10 Gbps\n1,10,40 Gbps\n\n\nCore software\nMinimal\nMinimal\nSame as compute nodes + compilers and source-code packages\nRocky 8 packages\n\n\nmodules (software stacks)\nno\nno\n✓\n✓\n\n\nGlobal file system\n✓\n✓\n✓\n✓\n\n\nJob submission\n✓\nno\n✓\n✓\n\n\nCPU quota per user(**)\n100% (“1 core”)\n200% (“2 cores”)\nnot limited\nnot limited\n\n\nMemory limit per user(**)\n32 GiB\n96 GiB\n48 GiB\nper job request\n\n\nPurpose\nSubmit and query jobs. SSH to development nodes. File management.\nFast in- & outbound file transfers. File management.\nCompile and install software. Prototype and test job scripts. Submit and query jobs. Version control (clone, pull, push). File management.\nRunning short and long-running job scripts.\n\n\n\n(*) GIT+SSH access on development nodes is restricted to git.bioconductor.org, bitbucket.org, gitea.com, github.com / gist.github.com, gitlab.com, cci.lbl.gov, and git.ucsf.edu.\n(**) CPU is throttled and memory is limited by Linux Control Groups (CGroups). If a process overuses the memory, it will be killed by the operating system.\nAll nodes on the cluster run Rocky Linux 8.10 which is updated on a regular basis. The job scheduler is SGE 8.1.9 (Son of Grid Engine) which provides queues for both communal and lab-priority tasks.\n\n\n\n\n\nThe cluster can be accessed via SSH to one of the login nodes:\n\nlog1.wynton.ucsf.edu\nlog2.wynton.ucsf.edu\nplog1.wynton.ucsf.edu (for Wynton Protected users)\n\n\n\n\nFor transferring large data files, it is recommended to use one of the dedicate data transfer nodes:\n\ndt1.wynton.ucsf.edu\ndt2.wynton.ucsf.edu\npdt1.wynton.ucsf.edu (for Wynton Protected users)\npdt2.wynton.ucsf.edu (for Wynton Protected users)\n\nwhich have a 10 Gbps connection - providing a file transfer speed of up to (theoretical) 1.25 GB/s = 4.5 TB/h. As for the login nodes, the transfer nodes can be accessed via SSH.\nComment: You can also transfer data via the login nodes, but since those only have 1 Gbps connections, you will see much lower transfer rates.\n\n\n\nThe cluster has development nodes for the purpose of validating scripts, prototyping pipelines, compiling software, and more. Development nodes can be accessed from the login nodes.\n\n\n\n\n\n\n\n\n\n\n\n\nNode\nPhysical Cores\nRAM\nLocal /scratch\nCPU x86-64 level\nCPU\nGPU\n\n\n\n\ndev1.wynton.ucsf.edu\n72\n384 GiB\n0.93 TiB\nx86-64-v4\nIntel Gold 6240 2.60GHz\n\n\n\ndev2.wynton.ucsf.edu\n48\n512 GiB\n0.73 TiB\nx86-64-v3\nIntel Xeon E5-2680 v3 2.50GHz\n\n\n\ndev3.wynton.ucsf.edu\n48\n256 GiB\n0.73 TiB\nx86-64-v3\nIntel Xeon E5-2680 v3 2.50GHz\n\n\n\ngpudev1.wynton.ucsf.edu\n56\n256 GiB\n0.82 TiB\nx86-64-v3\nIntel Xeon E5-2660 v4 2.00GHz\nNVIDIA GeForce GTX 1080\n\n\npdev1.wynton.ucsf.edu (for Wynton Protected users)\n32\n256 GiB\n1.1 TiB\nx86-64-v3\nIntel E5-2640 v3\n\n\n\npgpudev1.wynton.ucsf.edu (for Wynton Protected users)\n56\n256 GiB\n0.82 TiB\nx86-64-v3\nIntel Xeon E5-2660 v4 2.00GHz\nNVIDIA GeForce GTX 1080\n\n\n\nComment: Please use the GPU development node only if you need to build or prototype GPU software. The CPU x86-64 level is the x86-64 microarchitecture levels supported by the nodes CPU. \n\n\n\nThe majority of the compute nodes have Intel processors, while a few have AMD processors. Each compute node has a local /scratch drive (see above for size), which is either a hard disk drive (HDD), a solid state drive (SSD), or even a Non-Volatile Memory Express (NVMe) drive. Each node has a tiny /tmp drive (4-8 GiB).\nThe compute nodes can only be utilized by submitting jobs via the scheduler - it is not possible to explicitly log in to compute nodes.\n\n\n\n\n\n\nThe Wynton HPC cluster provides two types of scratch storage:\n\nLocal /scratch/ - 0.1-1.8 TiB/node storage unique to each compute node (can only be accessed from the specific compute node).\nGlobal /wynton/scratch/ and /wynton/protected/scratch/ (for Wynton Protected users) - 703 TiB storage (BeeGFS) accessible from everywhere.\n\nThere are no per-user quotas in these scratch spaces. Files not added or modified during the last two weeks will be automatically deleted on a nightly basis. Note, files with old timestamps that were “added” to the scratch place during this period will not be deleted, which covers the use case where files with old timestamps are extracted from a tar.gz file. (Details: tmpwatch --ctime --dirmtime --all --force is used for the cleanup.)\n\n\n\n\n/wynton/home/ and /wynton/protected/home/ (for Wynton Protected users): 770 TiB storage space\n/wynton/group/, /wynton/protected/group/ (for Wynton Protected users), and /wynton/protected/projects/ (for Wynton Protected users): 6500 TB (= 6.5 PB) storage space\n\nEach user may use up to 500 GiB disk space in the home directory. It is not possible to expand user’s home directory. Research groups can add additional storage space under /wynton/group/, /wynton/protected/group/, and /wynton/protected/projects/ by purchasing additional storage.\n\nWhile waiting to receive purchased storage, users may use the global scratch space, which is “unlimited” in size with the important limitation that files older than two weeks will be deleted automatically.\n\n\nImportantly, note that the Wynton HPC storage is not backed up. Users and labs are responsible for backing up their own data outside of Wynton HPC.\n\n\n\n\n\nThe majority of the compute nodes are connected to the local network with 1 Gbps and 10 Gbps network cards while a few got 40 Gbps cards.\nThe cluster itself connects to NSF’s Pacific Research Platform at a speed of 100 Gbps - providing a file transfer speed of up to (theoretical) 12.5 GB/s = 45 TB/h."
  },
  {
    "objectID": "hpc/about/specs.html#overview",
    "href": "hpc/about/specs.html#overview",
    "title": "Cluster Specifications",
    "section": "",
    "text": "Compute nodes\n\n\n482\n\n\nPhysical cores\n\n\n17496\n\n\nGPUs\n\n\n235 GPUs on 61 GPU nodes (88/22 GPUs/nodes are communal and 147/39 GPUs/nodes are prioritized for GPU contributors)\n\n\nRAM\n\n\n48-1512 GiB/node\n\n\nLocal scratch\n\n\n0.1-1.8 TiB/node\n\n\nGlobal scratch\n\n\n703 TiB\n\n\nUser home storage\n\n\n500 GiB/user (770 TiB in total)\n\n\nGroup storage\n\n\n6.5 PB\n\n\nNumber of accounts\n\n\n1312 of which 365 are Wynton Protected accounts\n\n\nNumber of projects\n\n\n761"
  },
  {
    "objectID": "hpc/about/specs.html#summary-of-compute-environment",
    "href": "hpc/about/specs.html#summary-of-compute-environment",
    "title": "Cluster Specifications",
    "section": "",
    "text": "Feature\nLogin Nodes\nTransfer Nodes\nDevelopment Nodes\nCompute Nodes\n\n\n\n\nHostname\nlog[1-2].wynton.ucsf.edu, plog1.wynton.ucsf.edu\ndt[1-2].wynton.ucsf.edu, pdt[1-2].wynton.ucsf.edu\ndev[1-3], gpudev1, pdev1, pgpudev1\n…\n\n\nAccessible via SSH from outside of cluster\n✓ (2FA if outside of UCSF)\n✓ (2FA if outside of UCSF)\nno\nno\n\n\nAccessible via SSH from within cluster\n✓\n✓\n✓\nno\n\n\nOutbound access\nWithin UCSF only: SSH and SFTP\nHTTP/HTTPS, FTP/FTPS, SSH, SFTP, Globus\nVia proxy: HTTP/HTTPS, GIT+SSH(*)\nno\n\n\nNetwork speed\n10 Gbps\n10 Gbps\n10 Gbps\n1,10,40 Gbps\n\n\nCore software\nMinimal\nMinimal\nSame as compute nodes + compilers and source-code packages\nRocky 8 packages\n\n\nmodules (software stacks)\nno\nno\n✓\n✓\n\n\nGlobal file system\n✓\n✓\n✓\n✓\n\n\nJob submission\n✓\nno\n✓\n✓\n\n\nCPU quota per user(**)\n100% (“1 core”)\n200% (“2 cores”)\nnot limited\nnot limited\n\n\nMemory limit per user(**)\n32 GiB\n96 GiB\n48 GiB\nper job request\n\n\nPurpose\nSubmit and query jobs. SSH to development nodes. File management.\nFast in- & outbound file transfers. File management.\nCompile and install software. Prototype and test job scripts. Submit and query jobs. Version control (clone, pull, push). File management.\nRunning short and long-running job scripts.\n\n\n\n(*) GIT+SSH access on development nodes is restricted to git.bioconductor.org, bitbucket.org, gitea.com, github.com / gist.github.com, gitlab.com, cci.lbl.gov, and git.ucsf.edu.\n(**) CPU is throttled and memory is limited by Linux Control Groups (CGroups). If a process overuses the memory, it will be killed by the operating system.\nAll nodes on the cluster run Rocky Linux 8.10 which is updated on a regular basis. The job scheduler is SGE 8.1.9 (Son of Grid Engine) which provides queues for both communal and lab-priority tasks."
  },
  {
    "objectID": "hpc/about/specs.html#details",
    "href": "hpc/about/specs.html#details",
    "title": "Cluster Specifications",
    "section": "",
    "text": "The cluster can be accessed via SSH to one of the login nodes:\n\nlog1.wynton.ucsf.edu\nlog2.wynton.ucsf.edu\nplog1.wynton.ucsf.edu (for Wynton Protected users)\n\n\n\n\nFor transferring large data files, it is recommended to use one of the dedicate data transfer nodes:\n\ndt1.wynton.ucsf.edu\ndt2.wynton.ucsf.edu\npdt1.wynton.ucsf.edu (for Wynton Protected users)\npdt2.wynton.ucsf.edu (for Wynton Protected users)\n\nwhich have a 10 Gbps connection - providing a file transfer speed of up to (theoretical) 1.25 GB/s = 4.5 TB/h. As for the login nodes, the transfer nodes can be accessed via SSH.\nComment: You can also transfer data via the login nodes, but since those only have 1 Gbps connections, you will see much lower transfer rates.\n\n\n\nThe cluster has development nodes for the purpose of validating scripts, prototyping pipelines, compiling software, and more. Development nodes can be accessed from the login nodes.\n\n\n\n\n\n\n\n\n\n\n\n\nNode\nPhysical Cores\nRAM\nLocal /scratch\nCPU x86-64 level\nCPU\nGPU\n\n\n\n\ndev1.wynton.ucsf.edu\n72\n384 GiB\n0.93 TiB\nx86-64-v4\nIntel Gold 6240 2.60GHz\n\n\n\ndev2.wynton.ucsf.edu\n48\n512 GiB\n0.73 TiB\nx86-64-v3\nIntel Xeon E5-2680 v3 2.50GHz\n\n\n\ndev3.wynton.ucsf.edu\n48\n256 GiB\n0.73 TiB\nx86-64-v3\nIntel Xeon E5-2680 v3 2.50GHz\n\n\n\ngpudev1.wynton.ucsf.edu\n56\n256 GiB\n0.82 TiB\nx86-64-v3\nIntel Xeon E5-2660 v4 2.00GHz\nNVIDIA GeForce GTX 1080\n\n\npdev1.wynton.ucsf.edu (for Wynton Protected users)\n32\n256 GiB\n1.1 TiB\nx86-64-v3\nIntel E5-2640 v3\n\n\n\npgpudev1.wynton.ucsf.edu (for Wynton Protected users)\n56\n256 GiB\n0.82 TiB\nx86-64-v3\nIntel Xeon E5-2660 v4 2.00GHz\nNVIDIA GeForce GTX 1080\n\n\n\nComment: Please use the GPU development node only if you need to build or prototype GPU software. The CPU x86-64 level is the x86-64 microarchitecture levels supported by the nodes CPU. \n\n\n\nThe majority of the compute nodes have Intel processors, while a few have AMD processors. Each compute node has a local /scratch drive (see above for size), which is either a hard disk drive (HDD), a solid state drive (SSD), or even a Non-Volatile Memory Express (NVMe) drive. Each node has a tiny /tmp drive (4-8 GiB).\nThe compute nodes can only be utilized by submitting jobs via the scheduler - it is not possible to explicitly log in to compute nodes."
  },
  {
    "objectID": "hpc/about/specs.html#file-system",
    "href": "hpc/about/specs.html#file-system",
    "title": "Cluster Specifications",
    "section": "",
    "text": "The Wynton HPC cluster provides two types of scratch storage:\n\nLocal /scratch/ - 0.1-1.8 TiB/node storage unique to each compute node (can only be accessed from the specific compute node).\nGlobal /wynton/scratch/ and /wynton/protected/scratch/ (for Wynton Protected users) - 703 TiB storage (BeeGFS) accessible from everywhere.\n\nThere are no per-user quotas in these scratch spaces. Files not added or modified during the last two weeks will be automatically deleted on a nightly basis. Note, files with old timestamps that were “added” to the scratch place during this period will not be deleted, which covers the use case where files with old timestamps are extracted from a tar.gz file. (Details: tmpwatch --ctime --dirmtime --all --force is used for the cleanup.)\n\n\n\n\n/wynton/home/ and /wynton/protected/home/ (for Wynton Protected users): 770 TiB storage space\n/wynton/group/, /wynton/protected/group/ (for Wynton Protected users), and /wynton/protected/projects/ (for Wynton Protected users): 6500 TB (= 6.5 PB) storage space\n\nEach user may use up to 500 GiB disk space in the home directory. It is not possible to expand user’s home directory. Research groups can add additional storage space under /wynton/group/, /wynton/protected/group/, and /wynton/protected/projects/ by purchasing additional storage.\n\nWhile waiting to receive purchased storage, users may use the global scratch space, which is “unlimited” in size with the important limitation that files older than two weeks will be deleted automatically.\n\n\nImportantly, note that the Wynton HPC storage is not backed up. Users and labs are responsible for backing up their own data outside of Wynton HPC."
  },
  {
    "objectID": "hpc/about/specs.html#network",
    "href": "hpc/about/specs.html#network",
    "title": "Cluster Specifications",
    "section": "",
    "text": "The majority of the compute nodes are connected to the local network with 1 Gbps and 10 Gbps network cards while a few got 40 Gbps cards.\nThe cluster itself connects to NSF’s Pacific Research Platform at a speed of 100 Gbps - providing a file transfer speed of up to (theoretical) 12.5 GB/s = 45 TB/h."
  },
  {
    "objectID": "hpc/about/roadmap.html",
    "href": "hpc/about/roadmap.html",
    "title": "Roadmap & Accomplishments",
    "section": "",
    "text": "The below “timeline” is a rough outline on when we can expect different milestones to be accomplished - the order does not reflect their relative priorities or the amount of effort spent.\n\n\n\nNew job scheduler: Upgrade from SGE to Altair’s AGE to improve scheduling performance, job observability, and GPU resource use experience. AGE also has paid support, which will allow us to make the best use of AGE, unlike the lack of support with SGE.\nHardware refresh: Complete a thorough assessment of Wynton hardware assets, replace outdated hardware where possible, and develop a sustainable plan for future hardware replacement.\nArchitecture and Data Assessment: Conduct a thorough review of the use cases, projects, and types of data stored within Wynton and review the architecture to ensure compliance and identify areas for improvement.\n\n\n\n\n\nSlurm pilot: Slurm was evaluated as a job scheduler instead of SGE. The result of the test phase was that we cannot migrate to Slurm, because Slurm does not support our co-op model.\nMigration: The UCSF Memory and Aging Center (MAC) have joined Wynton HPC by contributing new nodes\nMigration: The UCSF Macromolecular Structure Group (MSG) compute cluster (~1,300 physical cores) is incorporated into Wynton HPC\nDevelopment: Dedicated 10 Gbps transfer node for faster file transfers in to and out from Wynton HPC (2018-08-02)\nStorage: Global scratch storage on BeeGFS parallel file system (2018-08-20)\nPricing model: Description on how to purchase (amount in USD) additional storage space beyond what is available for free (2019-01-15)\nDevelopment: Additional development nodes for building software to take load off the current development node (2019-01-31)\nGPU compute: Four communal GPU nodes with a total of 12 GPUs from the QB3 cluster (2019-04-12)\nDevelopment: A dedicated Graphics Processing Unit (GPU) development node for building GPU software (2019-04-15)\nSoftware stacks: Support for sharing software installations between groups via environment modules, e.g. module load CBI (2019-06-13)\nStorage: The QB3-legacy NetApp scratch storage /scrapp and /scrapp2 has been removed in favor of the BeeGFS-based scratch storage /wynton/scratch (2019-08-15)\nScheduler: Support interactive jobs (qlogin). The Wynton HPC Team decided against this because it increases the risk of having stale jobs occupying the scheduler and overall underutilizing the HPC environment (Fall 2019)\nNetwork: A second data transfer node for faster file transfers in to and out from Wynton HPC (2019-09-12)\nGPU compute: An additional 12 communal GPUs on three GPU nodes (2019-09-13)\nGPU policy: All GPU nodes, communal and contributed, are available to all users. When running on another contributors GPU node, run time is limited to 2 hours. (2019-09-20)\nScheduler: Support for email notification upon job completion, e.g. qsub -m bea (2019-11-12)\nStorage: The QB3-legacy NetApp storage (including /netapp/home) was deprecated during 2019 with a deadline on December 2019. The server was kept up after this deadline and, unfortunately, the server went down on 2020-02-05 and was declared non-recoverable without further resource/funding (2020-02-05)\nStorage: Migrate home directories from the NetApp server (/netapp/home/) to the new, faster BeeGFS parallel file system (/wynton/home/). The migration effort ended on 2020-02-05 when the NetApp server failed (2020-02-05)\nWebsite: Move website from GitHub Pages (https://ucsf-hpc.github.io/wynton/) to on-premise &lt;https://wynton.ucsf.edu//hpc/&gt; (2020-02-26)\nNetwork: Add a Globus ‘Wynton’ Endpoint for efficient, secure data transfers (ingress and egress) (2020-04-03)\nAccess: Two-factor authentication (2FA) is required for accessing the cluster (2020-10-29)\nCompute: QB3 hardware migration to Wynton HPC considered complete (approximately 5,500 out of 7,000 physical cores migrated) (2021-09-27)\nCompliance: Add support for restricted data analysis (“Wynton PHI”) (2021-12-17)\nNetwork: Add a PHI-approved Globus endpoint (2022-05-02)\nNetwork: Support for using Globus transfers to and from UCSF Box via Wynton (2022-06-01)\nWebsite: Host website on GitHub Pages (2022-10-31)\nOperating system: Upgrade from CentOS 7 to Rocky 8 (2023-11-15).\n\nSee also News."
  },
  {
    "objectID": "hpc/about/roadmap.html#near-future",
    "href": "hpc/about/roadmap.html#near-future",
    "title": "Roadmap & Accomplishments",
    "section": "",
    "text": "New job scheduler: Upgrade from SGE to Altair’s AGE to improve scheduling performance, job observability, and GPU resource use experience. AGE also has paid support, which will allow us to make the best use of AGE, unlike the lack of support with SGE.\nHardware refresh: Complete a thorough assessment of Wynton hardware assets, replace outdated hardware where possible, and develop a sustainable plan for future hardware replacement.\nArchitecture and Data Assessment: Conduct a thorough review of the use cases, projects, and types of data stored within Wynton and review the architecture to ensure compliance and identify areas for improvement."
  },
  {
    "objectID": "hpc/about/roadmap.html#accomplished-milestones",
    "href": "hpc/about/roadmap.html#accomplished-milestones",
    "title": "Roadmap & Accomplishments",
    "section": "",
    "text": "Slurm pilot: Slurm was evaluated as a job scheduler instead of SGE. The result of the test phase was that we cannot migrate to Slurm, because Slurm does not support our co-op model.\nMigration: The UCSF Memory and Aging Center (MAC) have joined Wynton HPC by contributing new nodes\nMigration: The UCSF Macromolecular Structure Group (MSG) compute cluster (~1,300 physical cores) is incorporated into Wynton HPC\nDevelopment: Dedicated 10 Gbps transfer node for faster file transfers in to and out from Wynton HPC (2018-08-02)\nStorage: Global scratch storage on BeeGFS parallel file system (2018-08-20)\nPricing model: Description on how to purchase (amount in USD) additional storage space beyond what is available for free (2019-01-15)\nDevelopment: Additional development nodes for building software to take load off the current development node (2019-01-31)\nGPU compute: Four communal GPU nodes with a total of 12 GPUs from the QB3 cluster (2019-04-12)\nDevelopment: A dedicated Graphics Processing Unit (GPU) development node for building GPU software (2019-04-15)\nSoftware stacks: Support for sharing software installations between groups via environment modules, e.g. module load CBI (2019-06-13)\nStorage: The QB3-legacy NetApp scratch storage /scrapp and /scrapp2 has been removed in favor of the BeeGFS-based scratch storage /wynton/scratch (2019-08-15)\nScheduler: Support interactive jobs (qlogin). The Wynton HPC Team decided against this because it increases the risk of having stale jobs occupying the scheduler and overall underutilizing the HPC environment (Fall 2019)\nNetwork: A second data transfer node for faster file transfers in to and out from Wynton HPC (2019-09-12)\nGPU compute: An additional 12 communal GPUs on three GPU nodes (2019-09-13)\nGPU policy: All GPU nodes, communal and contributed, are available to all users. When running on another contributors GPU node, run time is limited to 2 hours. (2019-09-20)\nScheduler: Support for email notification upon job completion, e.g. qsub -m bea (2019-11-12)\nStorage: The QB3-legacy NetApp storage (including /netapp/home) was deprecated during 2019 with a deadline on December 2019. The server was kept up after this deadline and, unfortunately, the server went down on 2020-02-05 and was declared non-recoverable without further resource/funding (2020-02-05)\nStorage: Migrate home directories from the NetApp server (/netapp/home/) to the new, faster BeeGFS parallel file system (/wynton/home/). The migration effort ended on 2020-02-05 when the NetApp server failed (2020-02-05)\nWebsite: Move website from GitHub Pages (https://ucsf-hpc.github.io/wynton/) to on-premise &lt;https://wynton.ucsf.edu//hpc/&gt; (2020-02-26)\nNetwork: Add a Globus ‘Wynton’ Endpoint for efficient, secure data transfers (ingress and egress) (2020-04-03)\nAccess: Two-factor authentication (2FA) is required for accessing the cluster (2020-10-29)\nCompute: QB3 hardware migration to Wynton HPC considered complete (approximately 5,500 out of 7,000 physical cores migrated) (2021-09-27)\nCompliance: Add support for restricted data analysis (“Wynton PHI”) (2021-12-17)\nNetwork: Add a PHI-approved Globus endpoint (2022-05-02)\nNetwork: Support for using Globus transfers to and from UCSF Box via Wynton (2022-06-01)\nWebsite: Host website on GitHub Pages (2022-10-31)\nOperating system: Upgrade from CentOS 7 to Rocky 8 (2023-11-15).\n\nSee also News."
  },
  {
    "objectID": "hpc/about/gpu-shares.html",
    "href": "hpc/about/gpu-shares.html",
    "title": "Contributing Member GPU Shares",
    "section": "",
    "text": "Wynton HPC has 61 GPU nodes with a total of 235 GPUs available to all users. Among these, 39 GPU nodes, with a total of 147 GPUs, were contributed by different research groups. GPU jobs are limited to 2 hours in length when run on GPUs not contributed by the running user’s lab. In contrast, contributors are not limited to 2-hour GPU jobs on nodes they contributed. There is also one GPU development node that is available to all users.\n\n\nContributing Member GPU Shares\nBelow table shows which labs have access to dedicated Graphical Processing Unit (GPU) compute nodes and how many.\n\n\n\n\n\n\n\nSource: gpu_shares.tsv produced on . These data were compiled from the current SGE configuration (qconf -srqs shared_gpu_limits and qconf -shgrp &lt;project&gt;)."
  },
  {
    "objectID": "hpc/about/citation.html",
    "href": "hpc/about/citation.html",
    "title": "How to Cite",
    "section": "",
    "text": "If you’d like to acknowledge Wynton in your publication, you are welcome to use this sample verbiage:\n\nPortions of this work were performed on the Wynton HPC Co-Op cluster, which is supported by UCSF research faculty and UCSF institutional funds. The authors wish to thank the UCSF Wynton team for their ongoing technical support of the Wynton environment.\n\n\n\n\nBelow are a few examples on how to give details on the Wynton HPC environment in a grant application. The node, CPU, and GPU core counts are updated on a regular basis.\n\n\n\nWynton HPC is a distributed high-performance computing cluster with nodes in many different data centers on the UCSF campus. All Wynton HPC sites are connected by multiple 40 Gbps network connections. Wynton HPC currently contains 482 nodes with over 17496 CPU cores. RAM in the nodes ranges from 48 to 1512 GiB with an average RAM-to-core ratio of over 10 GiB. There are also 61 nodes containing a total of 235 GPUs. Storage is provided by a parallel filesystem providing 400 TB of mirrored home space and 500 TB of global scratch space.\nWynton is shared by many research groups at UCSF. Load balancing on the cluster is achieved through SGE (originally Sun Grid Engine). The computing power of the cluster guaranteed to any given research group is directly proportional to the funds that the group contributed to the cluster. The balance of the computing power not used by the contributors is available to other registered groups (including other contributors) on a first-come, first-served basis.\n\n\n\n\n\nWynton HPC is a computational research cluster shared by investigators at UCSF. The cluster is run as a co-op, with access for all and priority given to those who have contributed funds or hardware. Support is also provided by UCSF Research IT. The cluster currently consists of 482 nodes with 17496 cores. Each node has at least 48 GiB of RAM and total home storage is 770 TiB."
  },
  {
    "objectID": "hpc/about/citation.html#publication-acknowledgment",
    "href": "hpc/about/citation.html#publication-acknowledgment",
    "title": "How to Cite",
    "section": "",
    "text": "If you’d like to acknowledge Wynton in your publication, you are welcome to use this sample verbiage:\n\nPortions of this work were performed on the Wynton HPC Co-Op cluster, which is supported by UCSF research faculty and UCSF institutional funds. The authors wish to thank the UCSF Wynton team for their ongoing technical support of the Wynton environment."
  },
  {
    "objectID": "hpc/about/citation.html#grant-applications",
    "href": "hpc/about/citation.html#grant-applications",
    "title": "How to Cite",
    "section": "",
    "text": "Below are a few examples on how to give details on the Wynton HPC environment in a grant application. The node, CPU, and GPU core counts are updated on a regular basis.\n\n\n\nWynton HPC is a distributed high-performance computing cluster with nodes in many different data centers on the UCSF campus. All Wynton HPC sites are connected by multiple 40 Gbps network connections. Wynton HPC currently contains 482 nodes with over 17496 CPU cores. RAM in the nodes ranges from 48 to 1512 GiB with an average RAM-to-core ratio of over 10 GiB. There are also 61 nodes containing a total of 235 GPUs. Storage is provided by a parallel filesystem providing 400 TB of mirrored home space and 500 TB of global scratch space.\nWynton is shared by many research groups at UCSF. Load balancing on the cluster is achieved through SGE (originally Sun Grid Engine). The computing power of the cluster guaranteed to any given research group is directly proportional to the funds that the group contributed to the cluster. The balance of the computing power not used by the contributors is available to other registered groups (including other contributors) on a first-come, first-served basis.\n\n\n\n\n\nWynton HPC is a computational research cluster shared by investigators at UCSF. The cluster is run as a co-op, with access for all and priority given to those who have contributed funds or hardware. Support is also provided by UCSF Research IT. The cluster currently consists of 482 nodes with 17496 cores. Each node has at least 48 GiB of RAM and total home storage is 770 TiB."
  },
  {
    "objectID": "hpc/about/wynton-protected.html",
    "href": "hpc/about/wynton-protected.html",
    "title": "Wynton Protected",
    "section": "",
    "text": "2025-07-08: Termination of Wynton Support for P4/PHI data\n\nStarting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton."
  },
  {
    "objectID": "hpc/about/wynton-protected.html#getting-a-wynton-protected-account",
    "href": "hpc/about/wynton-protected.html#getting-a-wynton-protected-account",
    "title": "Wynton Protected",
    "section": "Getting a Wynton Protected account",
    "text": "Getting a Wynton Protected account\nTo apply for a new Wynton HPC account with access to Wynton Protected, or to modify an existing account to Wynton Protected, please follow the instructions for requesting or updating an account. This will initiate an automated onboarding process to check that you meet the following prerequisites:\n\nValid email address\nPI approval to work on Wynton Protected\nSigned attestation to the UCSF Statement of Responsibility\n\nOnce these are completed, you will be notified of your account credentials to access Wynton Wynton. If you already have a regular Wynton HPC account, your account will be reconfigured to access Wynton Protected."
  },
  {
    "objectID": "hpc/about/wynton-protected.html#other-options-at-ucsf-to-work-with-phi-data",
    "href": "hpc/about/wynton-protected.html#other-options-at-ucsf-to-work-with-phi-data",
    "title": "Wynton Protected",
    "section": "Other options at UCSF to work with PHI data",
    "text": "Other options at UCSF to work with PHI data\nDepending on your project, Wynton Protected may or may not be the best fit for your P3 project. We encourage you to also evaluate the following alternatives at UCSF that are approved for both P3 and P4/PHI data:\n\nUCSF Research Analysis Environment (RAE)\nUCSF Data Center Services\nUCSF Central IT Cloud Services"
  },
  {
    "objectID": "hpc/software/scl.html",
    "href": "hpc/software/scl.html",
    "title": "Software Collections (SCL)",
    "section": "",
    "text": "Software Collections (SCL) provides software versions that are […] more recent &gt; than their equivalent versions included in the base Rocky 8 distribution.\n\n\nTo list all Software Collections installed on the development nodes, use:\n\n[alice@dev2 ~]$ scl list-collections\ngcc-toolset-10\ngcc-toolset-11\ngcc-toolset-12\ngcc-toolset-13\ngcc-toolset-9\nImportantly, the above SCLs are available only on the development nodes.\nTo list all the packages that are part of one or more of these SCLs, use:\n\n[alice@dev2 ~]$ scl list-packages gcc-toolset-11\ngcc-toolset-11-toolchain-11.1-1.el8.x86_64\ngcc-toolset-11-gdb-10.2-5.el8.x86_64\ngcc-toolset-11-dwz-0.14-2.el8.x86_64\ngcc-toolset-11-gcc-gfortran-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-runtime-11.1-1.el8.x86_64\ngcc-toolset-11-valgrind-1:3.17.0-6.el8.x86_64\ngcc-toolset-11-systemtap-runtime-4.5-6.el8.x86_64\ngcc-toolset-11-libquadmath-devel-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-elfutils-0.185-5.el8.x86_64\ngcc-toolset-11-libstdc++-devel-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-gcc-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-gcc-c++-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-11.1-1.el8.x86_64\ngcc-toolset-11-elfutils-libelf-0.185-5.el8.x86_64\ngcc-toolset-11-strace-5.13-7.el8.x86_64\ngcc-toolset-11-systemtap-client-4.5-6.el8.x86_64\ngcc-toolset-11-binutils-2.36.1-4.el8_9.x86_64\ngcc-toolset-11-make-1:4.3-2.el8.x86_64\ngcc-toolset-11-systemtap-4.5-6.el8.x86_64\ngcc-toolset-11-systemtap-devel-4.5-6.el8.x86_64\ngcc-toolset-11-elfutils-libs-0.185-5.el8.x86_64\ngcc-toolset-11-perftools-11.1-1.el8.x86_64\ngcc-toolset-11-elfutils-debuginfod-client-0.185-5.el8.x86_64\ngcc-toolset-11-annobin-docs-10.23-1.el8.noarch\ngcc-toolset-11-ltrace-0.7.91-1.el8.x86_64\ngcc-toolset-11-gcc-gdb-plugin-11.2.1-9.2.el8_9.x86_64\n\n\n\nRocky 8 comes with GCC 8.5.0 (2021-05-14). Never versions are available via the gcc-toolset-NN SCLs. Here is an example how to check the version of one of the newer version:\n\n[alice@dev2 ~]$ scl enable gcc-toolset-12 \"gcc --version\"\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nImportantly, this approach of prefixing the original command call works also in job scripts.\n\n\n\n\nIf you work interactively on one of the development nodes, you can also launch a new shell (typically Bash) with one or more SCLs enabled:\n\n[alice@dev2 ~]$ scl enable gcc-toolset-12 \"$SHELL\"\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nTo “unload” an SCLs, just return to the previous shell by exiting new SCL-enabled shell, i.e.\n[alice@dev2 ~]$ exit\n\n[alice@dev2 ~]$ \n\n\n\nAn alternative approach to using scl enable for activating an SCL, is to module load to achieve the same, e.g.\n\n[alice@dev2 ~]$ module load CBI scl-gcc-toolset/12\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nTo go back to the built-in version of GCC, unload the module, i.e.\n\n[alice@dev2 ~]$ module unload CBI scl-gcc-toolset/12\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22)\nCopyright (C) 2018 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n\n\n\n\nProduct Documentation for Red Hat Developer Toolset"
  },
  {
    "objectID": "hpc/software/scl.html#available-scls-and-their-packages",
    "href": "hpc/software/scl.html#available-scls-and-their-packages",
    "title": "Software Collections (SCL)",
    "section": "",
    "text": "To list all Software Collections installed on the development nodes, use:\n\n[alice@dev2 ~]$ scl list-collections\ngcc-toolset-10\ngcc-toolset-11\ngcc-toolset-12\ngcc-toolset-13\ngcc-toolset-9\nImportantly, the above SCLs are available only on the development nodes.\nTo list all the packages that are part of one or more of these SCLs, use:\n\n[alice@dev2 ~]$ scl list-packages gcc-toolset-11\ngcc-toolset-11-toolchain-11.1-1.el8.x86_64\ngcc-toolset-11-gdb-10.2-5.el8.x86_64\ngcc-toolset-11-dwz-0.14-2.el8.x86_64\ngcc-toolset-11-gcc-gfortran-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-runtime-11.1-1.el8.x86_64\ngcc-toolset-11-valgrind-1:3.17.0-6.el8.x86_64\ngcc-toolset-11-systemtap-runtime-4.5-6.el8.x86_64\ngcc-toolset-11-libquadmath-devel-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-elfutils-0.185-5.el8.x86_64\ngcc-toolset-11-libstdc++-devel-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-gcc-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-gcc-c++-11.2.1-9.2.el8_9.x86_64\ngcc-toolset-11-11.1-1.el8.x86_64\ngcc-toolset-11-elfutils-libelf-0.185-5.el8.x86_64\ngcc-toolset-11-strace-5.13-7.el8.x86_64\ngcc-toolset-11-systemtap-client-4.5-6.el8.x86_64\ngcc-toolset-11-binutils-2.36.1-4.el8_9.x86_64\ngcc-toolset-11-make-1:4.3-2.el8.x86_64\ngcc-toolset-11-systemtap-4.5-6.el8.x86_64\ngcc-toolset-11-systemtap-devel-4.5-6.el8.x86_64\ngcc-toolset-11-elfutils-libs-0.185-5.el8.x86_64\ngcc-toolset-11-perftools-11.1-1.el8.x86_64\ngcc-toolset-11-elfutils-debuginfod-client-0.185-5.el8.x86_64\ngcc-toolset-11-annobin-docs-10.23-1.el8.noarch\ngcc-toolset-11-ltrace-0.7.91-1.el8.x86_64\ngcc-toolset-11-gcc-gdb-plugin-11.2.1-9.2.el8_9.x86_64"
  },
  {
    "objectID": "hpc/software/scl.html#using-scls",
    "href": "hpc/software/scl.html#using-scls",
    "title": "Software Collections (SCL)",
    "section": "",
    "text": "Rocky 8 comes with GCC 8.5.0 (2021-05-14). Never versions are available via the gcc-toolset-NN SCLs. Here is an example how to check the version of one of the newer version:\n\n[alice@dev2 ~]$ scl enable gcc-toolset-12 \"gcc --version\"\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nImportantly, this approach of prefixing the original command call works also in job scripts.\n\n\n\n\nIf you work interactively on one of the development nodes, you can also launch a new shell (typically Bash) with one or more SCLs enabled:\n\n[alice@dev2 ~]$ scl enable gcc-toolset-12 \"$SHELL\"\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nTo “unload” an SCLs, just return to the previous shell by exiting new SCL-enabled shell, i.e.\n[alice@dev2 ~]$ exit\n\n[alice@dev2 ~]$ \n\n\n\nAn alternative approach to using scl enable for activating an SCL, is to module load to achieve the same, e.g.\n\n[alice@dev2 ~]$ module load CBI scl-gcc-toolset/12\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nTo go back to the built-in version of GCC, unload the module, i.e.\n\n[alice@dev2 ~]$ module unload CBI scl-gcc-toolset/12\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22)\nCopyright (C) 2018 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE."
  },
  {
    "objectID": "hpc/software/scl.html#see-also",
    "href": "hpc/software/scl.html#see-also",
    "title": "Software Collections (SCL)",
    "section": "",
    "text": "Product Documentation for Red Hat Developer Toolset"
  },
  {
    "objectID": "hpc/software/core-software.html",
    "href": "hpc/software/core-software.html",
    "title": "Core Software",
    "section": "",
    "text": "Core Software\nA large number of common Linux software tools are available on the cluster as part of the core setup. Below follows a small excerpt of what tools are available development nodes and compute nodes. The login and data-transfer nodes have a much smaller selection of software.\n\n\nFile Editing\n\n\nemacs, nano, vim\n\n\nFile Transferring\n\n\nftp, lftp, scp, sftp, rsync curl, wget rclone dos2unix\n\n\nFile Compression and Archiving\n\n\n7z, bzip2, gzip, lz4, zip, xz ar, cpio, tar\n\n\nCommunication\n\n\nssh, telnet\n\n\nVersion Control\n\n\ngit, svn\n\n\nText, Documents, and Reports\n\n\nhunspell pandoc LaTeX, PDF and Postscript tools\n\n\nProgramming Languages, Compilers, and Tools\n\n\nmake, cmake gcc / g++ (C, C++, and Fortran) java, javac lua node, npm perl python rustc (Rust)\n\n\nSystem Utilities\n\n\ntop screen, tmux\n\n\nLinux Containers\n\n\napptainer (formerly singularity)\n\n\n\nNote: These core tools will be updated without notice as the operating system is updated on a regular basis. Many of the tools originate from the (stable) Rocky 8 repositories. To see what software and system libraries that are installed, run dnf --cacheonly list. Please note that, compared to compute nodes, the development nodes have additional software installed (typically named -devel) needed to build and compile from source."
  },
  {
    "objectID": "hpc/software/sbgrid.html",
    "href": "hpc/software/sbgrid.html",
    "title": "SBGrid Software Collection",
    "section": "",
    "text": "Access to SBGrid software on Wynton HPC is limited to paying members of the SBGrid Consortium.\n\n\nFor bug reports related to SBGrid software, please use the SBGrid’s Report Software Bug form The Wynton sysadmins do not fix problems in SBGrid software.\n\nSBGrid is a collection of hundred of programs for structural biology. It is kept up to date by the SBGrid team without needing intervention by the Wynton admins. Each lab using SBGrid must pay a yearly fee. This covers usage by anyone in the lab, both on Wynton HPC and any other Linux or macOS computer they own. The annual price depends on how many other labs from UCSF join. Many UCSF labs are already members. See the SBGrid website for information on how to join.\n\n\nThe SBGrid software stack is installed on Wynton HPC, but is available only to paying members, which are listed on the SBGrid Member Labs webpage. As a Wynton HPC user part of a paying lab, you will have access to SBGrid on the Wynton HPC cluster from being part of the sbgrid Unix group. You can verify that this is get case by running:\n\n[alice@dev2 ~]$ groups\nboblab sbgrid\nIf sbgrid is not part of your output, then you currently do not have access to the SBGrid software on Wynton HPC. If you think this is a mistake, please contact us so we can add you.\n\n\n\nSBGrid is available for interactive use on the development nodes and for use in job scripts on compute nodes. It is not available on the login or data-transfer nodes.\nIn order to use SBGrid software, the SBGrid environment must be enabled. To enable the SBGrid, in the shell or in a job script, call source /programs/sbgrid.shrc, e.g.\n\n[alice@dev2 ~]$ source /programs/sbgrid.shrc\n********************************************************************************\n                  Software Support by SBGrid (www.sbgrid.org)\n********************************************************************************\n Your use of the applications contained in the /programs  directory constitutes\n acceptance of  the terms of the SBGrid License Agreement included  in the file\n /programs/share/LICENSE.  The applications  distributed by SBGrid are licensed\n exclusively to member laboratories of the SBGrid Consortium.\n              Run sbgrid-accept-license to remove the above message.  \n********************************************************************************\n SBGrid was developed with support from its members, Harvard Medical School,    \n HHMI, and NSF. If use of SBGrid compiled software was an important element     \n in your publication, please include the following reference in your work:      \n                                                                                      \n Software used in the project was installed and configured by SBGrid.                   \n cite: eLife 2013;2:e01456, Collaboration gets the most out of software.                \n********************************************************************************\n SBGrid installation last updated: 2025-01-21\n Please submit bug reports and help requests to:       &lt;bugs@sbgrid.org&gt;  or\n                                                       &lt;https://sbgrid.org/bugs&gt;\n            For additional information visit https://sbgrid.org/wiki\n********************************************************************************\n********************************************************************************\n\n\n\nAfter having enabled SBGrid, as explained above, SBGrid software tools work like regular tools. For example, the SBGrid version of relion,\n\n[alice@dev2 ~]$ sbgrid-info -l relion\n  Version information for: /programs/x86_64-linux/relion\n\nDefault version:                    4.0.1_cu11.6\nIn-use version:                     4.0.1_cu11.6\n\nInstalled versions:                 4.0.1_cu11.6 5.0-beta_cu12.2 5.0-beta_cu11.6 5.0-beta_cu10.1 4.0.1_cu12.2 4.0.1_cu12.1 4.0.1_cu10.1_legacy 4.0.0_cu11.6 4.0.0_cu10.1_legacy 4.0.0_cu10.1 4.0-beta2_cu11.5 4.0-beta2_cu11.4.1 4.0-beta2_cu10.2 4.0-beta_cu9.2 3.1.4_cu11.8 3.1.4_cu11.6 3.1.4_cu10.1_legacy 3.1.4_cu10.1 3.1.3_cu10.2 3.1.1_cu9.2 3.0.8_cu10.1 2.1_cu8.0 1.4-randomphase3d 1.4b 1.4 1.3\nOther available versions:           5.0-beta_cu12.2 5.0-beta_cu11.6 5.0-beta_cu10.1 4.0.1_cu12.2 4.0.1_cu12.1 4.0.1_cu10.1_legacy 4.0.0_cu11.6 4.0.0_cu10.1_legacy 4.0.0_cu10.1 4.0-beta2_cu11.5 4.0-beta2_cu11.4.1 4.0-beta2_cu10.2 4.0-beta_cu9.2 3.1.4_cu11.8 3.1.4_cu11.6 3.1.4_cu10.1_legacy 3.1.4_cu10.1 3.1.3_cu10.2 3.1.1_cu9.2 3.0.8_cu10.1 2.1_cu8.0 1.4-randomphase3d 1.4b 1.4 1.3 \nOverrides use this shell variable:  RELION_X\ncan be launched as:\n\n[alice@dev2 ~]$ relion --version\nRELION version: 4.0.1 \nPrecision: BASE=double\n\n\n\nMany SBGrid programs run only on “modern” CPUs. Broadly speaking, there are four generations of CPUs, on the cluster - x86-64-v1, x86-64-v2, x86-64-v3, and x86-64-v4 - and SBGrid often requires x86-64-v3 compute nodes or newer. If your job fails with an obscure error such as:\nIllegal instruction (core dumped)\nor\n *** caught illegal operation ***\naddress 0x2b3a8b234ccd, cause 'illegal operand'\nit most likely ended up on a x86-64-v1 or x86-64-v2 compute node, while the program required something newer. To avoid this from happening, specify the x86-64-v=&lt;level&gt; resource to request a compute node with x86-64-v3 or newer. Either specify command-line option -l x86-64-v=3 when you submit the job, or add it is as an SGE declaration in your script:\n#$ -l x86-64-v=3  ## Script requires a CPU compliant with x86-64-v3 or newer\nIn the unlikely case that the SBGrid documentation says a program is compatible with pre-Haswell CPUs, it can run on also x86-64-v2 CPUs, and in some cases even x86-64-v1. If so, you can relax the x86-64 level requirement.\nAll our development nodes, including the GPU ones, have x86-64-v3 or newer CPUs.\n\n\n\nIf you are using SBGrid programs with GPU support, please note that SBGrid programs are compiled for specific versions of CUDA. Sometimes SBGrid provide different builds for multiple CUDA versions. For example, the same RELION version is available for different CUDA versions. Because of this, you have to make sure you load a corresponding CUDA environment module, e.g. module load cuda/10.1.\nAs of 2022-09-16, most default versions of GPU-compatible SBGrid programs are not compiled against CUDA 11.0.2, or greater. However, note the newest NVIDIA A40 GPUs on the qb3-atgpu* compute nodes require the use of programs compiled against CUDA 11.0.2, or greater, which means those compute nodes may not be compatible with the SBGrid program you want to run. You may need to specify a beta version of the SBGrid programs, or avoid the qb3-atgpu* nodes. See the SBGrid documentation for your specific program."
  },
  {
    "objectID": "hpc/software/sbgrid.html#verify-access-to-sbgrid",
    "href": "hpc/software/sbgrid.html#verify-access-to-sbgrid",
    "title": "SBGrid Software Collection",
    "section": "",
    "text": "The SBGrid software stack is installed on Wynton HPC, but is available only to paying members, which are listed on the SBGrid Member Labs webpage. As a Wynton HPC user part of a paying lab, you will have access to SBGrid on the Wynton HPC cluster from being part of the sbgrid Unix group. You can verify that this is get case by running:\n\n[alice@dev2 ~]$ groups\nboblab sbgrid\nIf sbgrid is not part of your output, then you currently do not have access to the SBGrid software on Wynton HPC. If you think this is a mistake, please contact us so we can add you."
  },
  {
    "objectID": "hpc/software/sbgrid.html#enable-sbgrid",
    "href": "hpc/software/sbgrid.html#enable-sbgrid",
    "title": "SBGrid Software Collection",
    "section": "",
    "text": "SBGrid is available for interactive use on the development nodes and for use in job scripts on compute nodes. It is not available on the login or data-transfer nodes.\nIn order to use SBGrid software, the SBGrid environment must be enabled. To enable the SBGrid, in the shell or in a job script, call source /programs/sbgrid.shrc, e.g.\n\n[alice@dev2 ~]$ source /programs/sbgrid.shrc\n********************************************************************************\n                  Software Support by SBGrid (www.sbgrid.org)\n********************************************************************************\n Your use of the applications contained in the /programs  directory constitutes\n acceptance of  the terms of the SBGrid License Agreement included  in the file\n /programs/share/LICENSE.  The applications  distributed by SBGrid are licensed\n exclusively to member laboratories of the SBGrid Consortium.\n              Run sbgrid-accept-license to remove the above message.  \n********************************************************************************\n SBGrid was developed with support from its members, Harvard Medical School,    \n HHMI, and NSF. If use of SBGrid compiled software was an important element     \n in your publication, please include the following reference in your work:      \n                                                                                      \n Software used in the project was installed and configured by SBGrid.                   \n cite: eLife 2013;2:e01456, Collaboration gets the most out of software.                \n********************************************************************************\n SBGrid installation last updated: 2025-01-21\n Please submit bug reports and help requests to:       &lt;bugs@sbgrid.org&gt;  or\n                                                       &lt;https://sbgrid.org/bugs&gt;\n            For additional information visit https://sbgrid.org/wiki\n********************************************************************************\n********************************************************************************"
  },
  {
    "objectID": "hpc/software/sbgrid.html#example-running-an-sbgrid-software",
    "href": "hpc/software/sbgrid.html#example-running-an-sbgrid-software",
    "title": "SBGrid Software Collection",
    "section": "",
    "text": "After having enabled SBGrid, as explained above, SBGrid software tools work like regular tools. For example, the SBGrid version of relion,\n\n[alice@dev2 ~]$ sbgrid-info -l relion\n  Version information for: /programs/x86_64-linux/relion\n\nDefault version:                    4.0.1_cu11.6\nIn-use version:                     4.0.1_cu11.6\n\nInstalled versions:                 4.0.1_cu11.6 5.0-beta_cu12.2 5.0-beta_cu11.6 5.0-beta_cu10.1 4.0.1_cu12.2 4.0.1_cu12.1 4.0.1_cu10.1_legacy 4.0.0_cu11.6 4.0.0_cu10.1_legacy 4.0.0_cu10.1 4.0-beta2_cu11.5 4.0-beta2_cu11.4.1 4.0-beta2_cu10.2 4.0-beta_cu9.2 3.1.4_cu11.8 3.1.4_cu11.6 3.1.4_cu10.1_legacy 3.1.4_cu10.1 3.1.3_cu10.2 3.1.1_cu9.2 3.0.8_cu10.1 2.1_cu8.0 1.4-randomphase3d 1.4b 1.4 1.3\nOther available versions:           5.0-beta_cu12.2 5.0-beta_cu11.6 5.0-beta_cu10.1 4.0.1_cu12.2 4.0.1_cu12.1 4.0.1_cu10.1_legacy 4.0.0_cu11.6 4.0.0_cu10.1_legacy 4.0.0_cu10.1 4.0-beta2_cu11.5 4.0-beta2_cu11.4.1 4.0-beta2_cu10.2 4.0-beta_cu9.2 3.1.4_cu11.8 3.1.4_cu11.6 3.1.4_cu10.1_legacy 3.1.4_cu10.1 3.1.3_cu10.2 3.1.1_cu9.2 3.0.8_cu10.1 2.1_cu8.0 1.4-randomphase3d 1.4b 1.4 1.3 \nOverrides use this shell variable:  RELION_X\ncan be launched as:\n\n[alice@dev2 ~]$ relion --version\nRELION version: 4.0.1 \nPrecision: BASE=double"
  },
  {
    "objectID": "hpc/software/sbgrid.html#some-sbgrid-programs-do-not-run-on-older-compute-nodes",
    "href": "hpc/software/sbgrid.html#some-sbgrid-programs-do-not-run-on-older-compute-nodes",
    "title": "SBGrid Software Collection",
    "section": "",
    "text": "Many SBGrid programs run only on “modern” CPUs. Broadly speaking, there are four generations of CPUs, on the cluster - x86-64-v1, x86-64-v2, x86-64-v3, and x86-64-v4 - and SBGrid often requires x86-64-v3 compute nodes or newer. If your job fails with an obscure error such as:\nIllegal instruction (core dumped)\nor\n *** caught illegal operation ***\naddress 0x2b3a8b234ccd, cause 'illegal operand'\nit most likely ended up on a x86-64-v1 or x86-64-v2 compute node, while the program required something newer. To avoid this from happening, specify the x86-64-v=&lt;level&gt; resource to request a compute node with x86-64-v3 or newer. Either specify command-line option -l x86-64-v=3 when you submit the job, or add it is as an SGE declaration in your script:\n#$ -l x86-64-v=3  ## Script requires a CPU compliant with x86-64-v3 or newer\nIn the unlikely case that the SBGrid documentation says a program is compatible with pre-Haswell CPUs, it can run on also x86-64-v2 CPUs, and in some cases even x86-64-v1. If so, you can relax the x86-64 level requirement.\nAll our development nodes, including the GPU ones, have x86-64-v3 or newer CPUs."
  },
  {
    "objectID": "hpc/software/sbgrid.html#sbgrid-programs-with-gpu-support",
    "href": "hpc/software/sbgrid.html#sbgrid-programs-with-gpu-support",
    "title": "SBGrid Software Collection",
    "section": "",
    "text": "If you are using SBGrid programs with GPU support, please note that SBGrid programs are compiled for specific versions of CUDA. Sometimes SBGrid provide different builds for multiple CUDA versions. For example, the same RELION version is available for different CUDA versions. Because of this, you have to make sure you load a corresponding CUDA environment module, e.g. module load cuda/10.1.\nAs of 2022-09-16, most default versions of GPU-compatible SBGrid programs are not compiled against CUDA 11.0.2, or greater. However, note the newest NVIDIA A40 GPUs on the qb3-atgpu* compute nodes require the use of programs compiled against CUDA 11.0.2, or greater, which means those compute nodes may not be compatible with the SBGrid program you want to run. You may need to specify a beta version of the SBGrid programs, or avoid the qb3-atgpu* nodes. See the SBGrid documentation for your specific program."
  },
  {
    "objectID": "hpc/software/software-repositories.html",
    "href": "hpc/software/software-repositories.html",
    "title": "Software Repositories",
    "section": "",
    "text": "Software Repositories\nIn addition to the core software tools that are available by default, additional software is available via environment modules. For example, although MATLAB is installed on the system, it is not available by default. Instead, we need to “load” its module first, e.g.\n[alice@dev2 ~]$ module load matlab\n[alice@dev2 ~]$ module list\n\nCurrently Loaded Modules:\n  1) matlab/2021a\nThen we can launch MATLAB using:\n[alice@dev2 ~]$ matlab -nosplash -nodesktop\nTo see what other “built-in” modules are available, use:\n[alice@dev2 ~]$ module avail\nIn additional a set of built-in environment modules, there are also modules for software tools that are installed and maintained by other users or research groups. To access these, we first need to load the corresponding “Software Repository” module. For example, to get access to the repository and the software shared by the UCSF Computation Biology and Informatics core (CBI), make sure to load (“enable”) the repository first, e.g.\nmodule load CBI\nThen use module avail to list what modules and versions are available (or see below). Next, to actually get access to one or more of the shared software modules, use module load &lt;name&gt; or module load &lt;name&gt;/&lt;version&gt;. Here are a few examples:\nmodule load bowtie2\nmodule load bowtie2/2.4.2\nmodule load r\nIt is possible to (i) enable a software repository and (ii) load a set of software tools in one call, e.g.\nmodule load CBI r bwa bowtie2/2.4.2\nBelow are 3 software repositories, each providing a set of software tools.\n\n\nbuilt-in (6)\n\n\nCBI (105)\n\n\nSali (96)\n\n\n\n\n\nModule Software Repository: built-in (6)\n\nMaintained by: Wynton Systems Administrators, Wynton HPC Enable repository: this software repository is always enabled\n\namber\n\n\n\nVersions: 20, 22\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Amber: Tools for Molecular Simulations\"\nsetenv        AMBERHOME       /usr/local/amber22\nprepend-path  PATH            /usr/local/amber22/bin\nprepend-path  LD_LIBRARY_PATH /usr/local/amber22/lib\nprepend-path  PERL5LIB        /usr/local/amber22/lib/perl\nprepend-path  PYTHONPATH      /usr/local/amber22/lib/python3.6/site-packages\nsetenv        QUICK_BASIS     /usr/local/amber22/AmberTools/src/quick/basis\n\n\n\n\n\ncuda\n\n\n\nVersions: 7.5, 8.0, 9.1, 9.2, 10.1, 11.0, 11.3, 11.4, 11.5, 11.8, 12.2, 12.5\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"NVIDIA CUDA Toolkit libraries\"\nprepend-path  PATH               /usr/local/cuda-12.5/bin\nprepend-path  LD_LIBRARY_PATH    /usr/local/cuda-12.5/lib64\nsetenv        CUDA_LIB_PATH      /usr/local/cuda-12.5/lib64\n\n\n\n\n\njulia\n\n\n\nVersions: 0.6.4, 1.6.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Julia programming language\"\nprepend-path  PATH      /usr/local/julia-1.6.0/bin\nprepend-path  MANPATH   /usr/local/julia-1.6.0/share/man\n\n\n\n\n\nmatlab\n\n\n\nVersions: 2018b, 2019a, 2019b, 2020a, 2020b, 2021a, 2021b, 2022a, 2022b, 2023a, 2023b, 2024a\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Language for technical computing\"\nprepend-path  PATH                /usr/local/matlab/R2024a/bin\nsetenv        MLM_LICENSE_FILE    27000@matl1.wynton.ucsf.edu:27000@matl2.wynton.ucsf.edu\n\n\n\n\n\nmatlab-runtime\n\n\n\nVersions: 2018b, 2019a, 2019b, 2020a, 2020b, 2021a, 2021b, 2022a, 2022b, 2023a, 2023b, 2024a\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"MATLAB Runtime for making use of MATLAB Compiler applications\"\nprepend-path  LD_LIBRARY_PATH     /usr/local/matlab/R2024a/runtime/glnxa64:/usr/local/matlab/R2024a/bin/glnxa64:/usr/local/matlab/R2024a/sys/os/glnxa64:/usr/local/matlab/R2024a/extern/bin/glnxa64\n\n\n\n\n\nopenjdk\n\n\n\nopenjdk: Open Java Development Kit OpenJDK is a free and open-source implementation of the Java Platform, Standard Edition. It is the result of an effort Sun Microsystems began in 2006. Example: java -version and javac -version (SDK only). Note: This module loads the Software Development Kit (SDK) version, if available, otherwise the Run-Time Environment (JRE). URL: https://openjdk.java.net/, https://openjdk.java.net/projects/jdk/ (changelog), https://github.com/openjdk/jdk (source code) Versions: 1.8.0, 11, 17\n\n\nModule code: view\n\nhelp(\"openjdk: Open Java Development Kit\")\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, java\")\nwhatis(\"URL: https://openjdk.java.net/, https://openjdk.java.net/projects/jdk/ (changelog), https://github.com/openjdk/jdk (source code)\")\nwhatis([[\nDescription: OpenJDK is a free and open-source implementation of the Java Platform, Standard Edition. It is the result of an effort Sun Microsystems began in 2006.\nExamples: `java -version` and `javac -version` (SDK only).\nNote: This module loads the Software Development Kit (SDK) version, if available, otherwise the Run-Time Environment (JRE).\n]])\n\nlocal root = \"/usr/lib/jvm\"\n\n-- Use SDK, if available, otherwise JRE\nlocal home = pathJoin(root, \"java\" .. \"-\" .. version)\nif not isDir(home) then -- isDir() supports symlinked folders\n    home = pathJoin(root, \"jre\" .. \"-\" .. version)\nend\n\n-- Assert that OpenJDK version still exists, because\n-- it happens at times that older versions are removed\nif not isDir(home) then\n  LmodError(\"INTERNAL ERROR: Module \" .. name .. \"/\" .. version .. \" is broken, because folder \" .. home .. \" does not exist on host \" .. os.getenv(\"HOSTNAME\") .. \". Please report this to the sysadms.\")\nend\n\nsetenv(\"JAVA_HOME\", home)\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"CPATH\", pathJoin(home, \"include\"))\n\n\n\n\n\n\n\nModule Software Repository: CBI (105)\n\nMaintained by: Henrik Bengtsson, Computational Biology and Informatics Enable repository: module load CBI\n\nPlease note that this software stack is maintained and contributed by a research group on a voluntary basis. It is not maintained by the Wynton HPC admins. Please reach out to the corresponding maintainer for bug reports, feedback, or questions.\n\n\napache-ant\n\n\n\nApache Ant: A Java Library and Command-Line Tool to Build Software Apache Ant is a Java library and command-line tool that help building software. Example: ant -h URL: https://ant.apache.org/bindownload.cgi, https://ant.apache.org/antnews.html (changelog) Versions: 1.10.12, 1.10.15\n\n\nModule code: view\n\nhelp([[\nApache Ant: A Java Library and Command-Line Tool to Build Software\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming\")\nwhatis(\"URL: https://ant.apache.org/bindownload.cgi, https://ant.apache.org/antnews.html (changelog)\")\nwhatis([[\nDescription: Apache Ant is a Java library and command-line tool that help building software.\nExamples: `ant -h`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\n\nasciigenome\n\n\n\nASCIIGenome: Text Only Genome Viewer ASCIIGenome is a genome browser based on command line interface and designed for running from console terminals. Since ASCIIGenome does not require a graphical interface it is particularly useful for quickly visualizing genomic data on remote servers while offering flexibility similar to popular GUI viewers like IGV. Example: ASCIIGenome –help, and ASCIIGenome bigWigExample.bw. URL: https://github.com/dariober/ASCIIGenome, https://github.com/dariober/ASCIIGenome/blob/master/CHANGELOG.md (changelog), https://asciigenome.readthedocs.io/en/latest/ (documentation) Warning: Only the most recent version of this software will be kept. Versions: 1.18.0\n\n\nModule code: view\n\nhelp([[\nASCIIGenome: Text Only Genome Viewer\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, viewer, cli, tui\")\nwhatis(\"URL: https://github.com/dariober/ASCIIGenome, https://github.com/dariober/ASCIIGenome/blob/master/CHANGELOG.md (changelog), https://asciigenome.readthedocs.io/en/latest/ (documentation)\")\nwhatis([[\nDescription: ASCIIGenome is a genome browser based on command line interface and designed for running from console terminals. Since ASCIIGenome does not require a graphical interface it is particularly useful for quickly visualizing genomic data on remote servers while offering flexibility similar to popular GUI viewers like IGV.\nExamples: `ASCIIGenome --help`, and `ASCIIGenome bigWigExample.bw`.\nWarning: Only the most recent version of this software will be kept.\n]])\n-- too long for small screens: `ASCIIGenome http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/functional_annotation/filtered/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5.20130502.sites.annotation.vcf.gz`\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, \"ASCIIGenome\" .. \"-\" .. version)\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\n\nbamutil\n\n\n\nbamUtil: Programs for Working on SAM/BAM Files bamUtil is a repository that contains several programs that perform operations on SAM/BAM files. All of these programs are built into a single executable, bam. Example: bam help. URL: https://genome.sph.umich.edu/wiki/BamUtil, https://github.com/statgen/bamUtil Versions: 1.0.14\n\n\nModule code: view\n\nhelp([[\nbamUtil: Programs for Working on SAM/BAM Files\n]])\n\nlocal name = \"bamUtil\"\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: version control\")\nwhatis(\"URL: https://genome.sph.umich.edu/wiki/BamUtil, https://github.com/statgen/bamUtil\")\nwhatis(\"Description: bamUtil is a repository that contains several programs that perform operations on SAM/BAM files. All of these programs are built into a single executable, `bam`. Example: `bam help`.\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nbat\n\n\n\nbat: A cat(1) Clone with Syntax Highlighting and Git Integration A cat(1) clone with syntax highlighting and Git integration. Example: bat README.md, bat scripts/.sh, and bat src/.c. URL: https://github.com/sharkdp/bat, https://github.com/sharkdp/bat/blob/master/CHANGELOG.md (changelog) Warning: Only the most recent version of this software will be kept. Versions: 0.25.0, 0.26.0\n\n\nModule code: view\n\nhelp([[\nbat: A cat(1) Clone with Syntax Highlighting and Git Integration\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, file viewer, cli, tui\")\nwhatis(\"URL: https://github.com/sharkdp/bat, https://github.com/sharkdp/bat/blob/master/CHANGELOG.md (changelog)\")\nwhatis([[\nDescription: A cat(1) clone with syntax highlighting and Git integration.\nExamples: `bat README.md`, `bat scripts/*.sh`, and `bat src/*.c`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\", \"man1\"))\n\n\n\n\n\nbats-assert\n\n\n\nbats-assert: Assertion Library for Bats (Bash Automated Testing System) This is a helper library providing common assertions for Bats. Example: In Bats setup() function: load \"\\({BATS_SUPPORT_HOME}/load.bash&quot;&lt;/code&gt; and &lt;code&gt;load &quot;\\){BATS_ASSERT_HOME}/load.bash\" URL: https://github.com/bats-core/bats-assert, https://github.com/bats-core/bats-assert/releases (changelog), https://bats-core.readthedocs.io/en/stable/faq.html?highlight=assert#how-can-i-use-helper-libraries-like-bats-assert (documentation) Versions: 2.1.0, 2.2.0, 2.2.4\n\n\nModule code: view\n\nhelp([[\nbats-assert: Assertion Library for Bats (Bash Automated Testing System)\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: bash, shell, testing\")\nwhatis(\"URL: https://github.com/bats-core/bats-assert, https://github.com/bats-core/bats-assert/releases (changelog), https://bats-core.readthedocs.io/en/stable/faq.html?highlight=assert#how-can-i-use-helper-libraries-like-bats-assert (documentation)\")\nwhatis([[\nDescription: This is a helper library providing common assertions for Bats.\nExamples: In Bats `setup()` function: `load \"${BATS_SUPPORT_HOME}/load.bash\"` and `load \"${BATS_ASSERT_HOME}/load.bash\"`\n]])\n\ndepends_on(\"bats-support\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"BATS_ASSERT_HOME\", home)\n\n\n\n\n\nbats-core\n\n\n\nbats: Bash Automated Testing System Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected. Example: bats –version, bats –help, man bats, man 7 bats, and bats tests/. URL: https://github.com/bats-core/bats-core, https://github.com/bats-core/bats-core/blob/master/docs/CHANGELOG.md (changelog), https://bats-core.readthedocs.io/en/stable/ (documentation) Versions: 1.10.0, 1.11.0, 1.11.1, 1.12.0\n\n\nModule code: view\n\nhelp([[\nbats: Bash Automated Testing System\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: bash, shell, testing\")\nwhatis(\"URL: https://github.com/bats-core/bats-core, https://github.com/bats-core/bats-core/blob/master/docs/CHANGELOG.md (changelog), https://bats-core.readthedocs.io/en/stable/ (documentation)\")\nwhatis([[\nDescription: Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected.\nExamples: `bats --version`, `bats --help`, `man bats`, `man 7 bats`, and `bats tests/`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\n\n\n\n\n\nbats-file\n\n\n\nbats-file: File-System Assertion Library for Bats (Bash Automated Testing System) This is a helper library providing common filesystem related assertions and helpers foor Bats. Example: In Bats setup() function: load \"\\({BATS_SUPPORT_HOME}/load.bash&quot;&lt;/code&gt; and &lt;code&gt;load &quot;\\){BATS_FILE_HOME}/load.bash\" URL: https://github.com/bats-core/bats-file, https://github.com/bats-core/bats-file/releases (changelog), https://bats-core.readthedocs.io/en/stable/faq.html?highlight=assert#how-can-i-use-helper-libraries-like-bats-file (documentation) Versions: 0.4.0\n\n\nModule code: view\n\nhelp([[\nbats-file: File-System Assertion Library for Bats (Bash Automated Testing System)\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: bash, shell, testing\")\nwhatis(\"URL: https://github.com/bats-core/bats-file, https://github.com/bats-core/bats-file/releases (changelog), https://bats-core.readthedocs.io/en/stable/faq.html?highlight=assert#how-can-i-use-helper-libraries-like-bats-file (documentation)\")\nwhatis([[\nDescription: This is a helper library providing common filesystem related assertions and helpers foor Bats.\nExamples: In Bats `setup()` function: `load \"${BATS_SUPPORT_HOME}/load.bash\"` and `load \"${BATS_FILE_HOME}/load.bash\"`\n]])\n\ndepends_on(\"bats-support\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"BATS_FILE_HOME\", home)\n\n\n\n\n\nbats-support\n\n\n\nbats-support: Supporting Library for Bats (Bash Automated Testing System) This is a supporting library providing common functions to test helper libraries written for Bats. Example: In Bats setup() function: load \"${BATS_SUPPORT_HOME}/load.bash\" URL: https://github.com/bats-core/bats-support, https://github.com/bats-core/bats-support/releases (changelog), https://bats-core.readthedocs.io/en/stable/faq.html?highlight=assert#how-can-i-use-helper-libraries-like-bats-assert (documentation) Versions: 0.3.0\n\n\nModule code: view\n\nhelp([[\nbats-support: Supporting Library for Bats (Bash Automated Testing System)\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: bash, shell, testing\")\nwhatis(\"URL: https://github.com/bats-core/bats-support, https://github.com/bats-core/bats-support/releases (changelog), https://bats-core.readthedocs.io/en/stable/faq.html?highlight=assert#how-can-i-use-helper-libraries-like-bats-assert (documentation)\")\nwhatis([[\nDescription: This is a supporting library providing common functions to test helper libraries written for Bats.\nExamples: In Bats `setup()` function: `load \"${BATS_SUPPORT_HOME}/load.bash\"`\n]])\n\ndepends_on(\"bats-core\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"BATS_SUPPORT_HOME\", home)\n\n\n\n\n\nbcftools\n\n\n\nBCFtools: Utilities for Variant Calling and Manipulating VCFs and BCFs BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed. Example: bcftools –version URL: https://www.htslib.org/, https://github.com/samtools/bcftools/blob/develop/NEWS (changelog), https://github.com/samtools/bcftools (source code) Versions: 1.9, 1.10, 1.10.2, 1.11, 1.13, 1.14, 1.15, 1.15.1, 1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22\n\n\nModule code: view\n\nhelp([[\nBCFtools: Utilities for Variant Calling and Manipulating VCFs and BCFs\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://www.htslib.org/, https://github.com/samtools/bcftools/blob/develop/NEWS (changelog), https://github.com/samtools/bcftools (source code)\")\nwhatis([[\nDescription: BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed.\nExamples: `bcftools --version`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\npushenv(\"BCFTOOLS_PLUGINS\", pathJoin(home, \"libexec\", name))\n\n\n-- Warn about bug https://github.com/samtools/htslib/issues/1236\nif (mode() == \"load\" and version == \"1.11\") then\n  LmodMessage(\"MODULE WARNING: \" .. name .. \" \" .. version .. \" has a bug that results in valid but incorrect CIGAR strings. Because of this, it is recommended to use an older or a newer version instead. For details, see https://github.com/samtools/htslib/issues/1236\")\nend\n\n\n\n\n\nbcl2fastq\n\n\n\nbcl2fastq: Illumina Conversion Software bcl2fastq Conversion Software both demultiplexes data and converts BCL files generated by Illumina sequencing systems to standard FASTQ file formats for downstream analysis. Example: bcl2fastq –version URL: https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software.html, https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software/downloads.html (changelog) Versions: 2.20.0\n\n\nModule code: view\n\nhelp([[\nbcl2fastq: Illumina Conversion Software\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, Illumina\")\nwhatis(\"URL: https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software.html, https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software/downloads.html (changelog)\")\nwhatis([[\nDescription: bcl2fastq Conversion Software both demultiplexes data and converts BCL files generated by Illumina sequencing systems to standard FASTQ file formats for downstream analysis.\nExamples: `bcl2fastq --version`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\nbedops\n\n\n\nBEDOPS: The Fast, Highly Scalable and Easily-Parallelizable Genome Analysis Toolkit BEDOPS is an open-source command-line toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data of arbitrary scale. Tasks can be easily split by chromosome for distributing whole-genome analyses across a computational cluster. Example: bedops –version URL: https://bedops.readthedocs.io/, https://bedops.readthedocs.io/en/latest/content/revision-history.html (changelog), https://github.com/bedops/bedops (source code) Versions: 2.4.36, 2.4.37, 2.4.38, 2.4.39, 2.4.40, 2.4.41\n\n\nModule code: view\n\nhelp([[\nBEDOPS: The Fast, Highly Scalable and Easily-Parallelizable Genome Analysis Toolkit\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: \")\nwhatis(\"URL: https://bedops.readthedocs.io/, https://bedops.readthedocs.io/en/latest/content/revision-history.html (changelog), https://github.com/bedops/bedops (source code)\")\nwhatis([[\nDescription: BEDOPS is an open-source command-line toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data of arbitrary scale. Tasks can be easily split by chromosome for distributing whole-genome analyses across a computational cluster.\nExamples: `bedops --version`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\nbedtools2\n\n\n\nbedtools2: The Swiss Army Knife for Genome Arithmetic Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. Example: bedtools –version and ls $BEDTOOLS2_HOME/genomes/. URL: https://github.com/arq5x/bedtools2/, https://bedtools.readthedocs.io/en/latest/content/history.html (changelog), https://bedtools.readthedocs.org (documentation), https://code.google.com/archive/p/bedtools/downloads (legacy) Versions: 2.26.0, 2.28.0, 2.29.1, 2.29.2, 2.30.0, 2.31.1\n\n\nModule code: view\n\nhelp([[\nbedtools2: The Swiss Army Knife for Genome Arithmetic\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://github.com/arq5x/bedtools2/, https://bedtools.readthedocs.io/en/latest/content/history.html (changelog), https://bedtools.readthedocs.org (documentation), https://code.google.com/archive/p/bedtools/downloads (legacy)\")\nwhatis([[\nDescription: Collectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF.\nExamples: `bedtools --version` and `ls $BEDTOOLS2_HOME/genomes/`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n-- custom; helps find $BEDTOOLS2_HOME/genomes\npushenv(\"BEDTOOLS2_HOME\", home)\n\n\n\n\n\n\n\n\n\n\n\nblast\n\n\n\nBLAST+: Basic Local Alignment Search Tool BLAST finds regions of similarity between biological sequences. The program compares nucleotide or protein sequences to sequence databases and calculates the statistical significance. Example: blastx -version URL: https://blast.ncbi.nlm.nih.gov/Blast.cgi, https://www.ncbi.nlm.nih.gov/books/NBK131777/ (changelog) Versions: 2.9.0, 2.10.1, 2.11.0, 2.12.0, 2.13.0, 2.14.0, 2.14.1, 2.15.0, 2.16.0\n\n\nModule code: view\n\nhelp([[\nBLAST+: Basic Local Alignment Search Tool\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, scripting\")\nwhatis(\"URL: https://blast.ncbi.nlm.nih.gov/Blast.cgi, https://www.ncbi.nlm.nih.gov/books/NBK131777/ (changelog)\")\nwhatis([[\nDescription: BLAST finds regions of similarity between biological sequences. The program compares nucleotide or protein sequences to sequence databases and calculates the statistical significance.\nExamples: `blastx -version`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\nblat\n\n\n\nBLAT: Fast Sequence Search Command Line Tool BLAT - client and server combined into a single program, first building the index, then using the index, and then exiting. Example: blat URL: https://genome.ucsc.edu/goldenPath/help/blatSpec.html (documentation), https://genome.ucsc.edu/FAQ/FAQblat.html (faq), https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/ (download), https://github.com/ucscGenomeBrowser/kent/tree/master/src/blat (source code) Versions: 36x4, 37x1, 39x1\n\n\nModule code: view\n\nhelp([[\nBLAT: Fast Sequence Search Command Line Tool\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, alignment\")\nwhatis(\"URL: https://genome.ucsc.edu/goldenPath/help/blatSpec.html (documentation), https://genome.ucsc.edu/FAQ/FAQblat.html (faq), https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/blat/ (download), https://github.com/ucscGenomeBrowser/kent/tree/master/src/blat (source code)\")\nwhatis([[\nDescription: BLAT - client and server combined into a single program, first building the index, then using the index, and then exiting.\nExamples: `blat`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", home)\n\n\n\n\n\nbowtie\n\n\n\nBowtie: A Fast and Sensitive Gapped Read Aligner Bowtie is an ultrafast, memory-efficient short read aligner. Example: bowtie –version and ls $BOWTIE_HOME/{genomes,indexes} Note: This is Bowtie v1 - not v2. URL: https://bowtie-bio.sourceforge.net/index.shtml, https://bowtie-bio.sourceforge.net/index.shtml (changelog), https://github.com/BenLangmead/bowtie (source code) Versions: 1.2.3, 1.3.0, 1.3.1\n\n\nModule code: view\n\nhelp([[\nBowtie: A Fast and Sensitive Gapped Read Aligner\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://bowtie-bio.sourceforge.net/index.shtml, https://bowtie-bio.sourceforge.net/index.shtml (changelog), https://github.com/BenLangmead/bowtie (source code)\")\nwhatis([[\nDescription: Bowtie is an ultrafast, memory-efficient short read aligner.\nExamples: `bowtie --version` and `ls $BOWTIE_HOME/{genomes,indexes}`\nNote: This is Bowtie v1 - _not v2_.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", home)\npushenv(\"BOWTIE_HOME\", home)\n\n\n\n\n\nbowtie2\n\n\n\nBowtie 2: A Fast and Sensitive Gapped Read Aligner Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. Example: bowtie2 –version URL: https://bowtie-bio.sourceforge.net/bowtie2/index.shtml, https://bowtie-bio.sourceforge.net/bowtie2/index.shtml (changelog), https://github.com/BenLangmead/bowtie2 (source code) Versions: 2.3.5, 2.3.5.1, 2.4.1, 2.4.2, 2.4.4, 2.4.5, 2.5.0, 2.5.1, 2.5.2, 2.5.4\n\n\nModule code: view\n\nhelp([[\nBowtie 2: A Fast and Sensitive Gapped Read Aligner\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://bowtie-bio.sourceforge.net/bowtie2/index.shtml, https://bowtie-bio.sourceforge.net/bowtie2/index.shtml (changelog), https://github.com/BenLangmead/bowtie2 (source code)\")\nwhatis([[\nDescription: Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.\nExamples: `bowtie2 --version`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\nbwa\n\n\n\nBWA: Burrows-Wheeler Aligner Burrows-Wheeler Aligner (BWA) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. Example: bwa. URL: http://bio-bwa.sourceforge.net/, https://github.com/lh3/bwa/blob/master/NEWS.md (changelog), https://github.com/lh3/bwa (source code) Versions: 0.7.12, 0.7.17, 0.7.18\n\n\nModule code: view\n\nhelp([[\nBWA: Burrows-Wheeler Aligner\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: http://bio-bwa.sourceforge.net/, https://github.com/lh3/bwa/blob/master/NEWS.md (changelog), https://github.com/lh3/bwa (source code)\")\nwhatis([[\nDescription: Burrows-Wheeler Aligner (BWA) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome.\nExamples: `bwa`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", pathJoin(home, \"man\"))\n\n\n\n\n\n\nbyobu\n\n\n\nbyobu: Elegant Enhancement of the Otherwise Functional, Plain, Practical GNU Screen Byobu is an elegant enhancement of the otherwise functional, plain, practical GNU Screen. Byobu includes an enhanced profile, configuration utilities, and system status notifications for the GNU screen window manager as well as the Tmux terminal multiplexer. Example: byobu –version. URL: https://www.byobu.org/, https://github.com/dustinkirkland/byobu/releases (releases), https://github.com/dustinkirkland/byobu (source code) Warning: Only the most recent version of this software will be kept. Versions: 5.133\n\n\nModule code: view\n\nhelp([[\nbyobu: Elegant Enhancement of the Otherwise Functional, Plain, Practical GNU Screen\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: terminal, SSH, cli, tui\")\nwhatis(\"URL: https://www.byobu.org/, https://github.com/dustinkirkland/byobu/releases (releases), https://github.com/dustinkirkland/byobu (source code)\")\nwhatis([[\nDescription: Byobu is an elegant enhancement of the otherwise functional, plain, practical GNU Screen. Byobu includes an enhanced profile, configuration utilities, and system status notifications for the GNU screen window manager as well as the Tmux terminal multiplexer.\nExamples: `byobu --version`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\",  pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\",  pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\n\ncellranger\n\n\n\nCell Ranger: 10x Genomics Pipeline for Single-Cell Data Analysis Cell Ranger is a set of analysis pipelines that process Chromium Single Cell 3’ RNA-seq output to align reads, generate gene-cell matrices and perform clustering and gene expression analysis. Example: cellranger –help and cellranger –version. URL: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger, https://www.10xgenomics.com/support/software/cell-ranger/latest/release-notes/cr-release-notes (changelog), https://github.com/10XGenomics/cellranger (source code) Warning: To prevent a single Cell Ranger process from hijacking all CPU and RAM by default, this module sets environment variable MROFLAGS=‘–localcores=1 –localmem=8 –limit-loadavg’ making those the default. Versions: 2.1.0, 3.0.2, 3.1.0, 4.0.0, 5.0.1, 6.1.1, 6.1.2, 7.0.0, 7.0.1, 7.1.0, 7.2.0, 8.0.0, 8.0.1, 9.0.0, 9.0.1\n\n\nModule code: view\n\nhelp([[\nCell Ranger: 10x Genomics Pipeline for Single-Cell Data Analysis\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, 10x genomics\")\nwhatis(\"URL: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger, https://www.10xgenomics.com/support/software/cell-ranger/latest/release-notes/cr-release-notes (changelog), https://github.com/10XGenomics/cellranger (source code)\")\nwhatis([[\nDescription: Cell Ranger is a set of analysis pipelines that process Chromium Single Cell 3' RNA-seq output to align reads, generate gene-cell matrices and perform clustering and gene expression analysis.\nExamples: `cellranger --help` and `cellranger --version`.\nWarning: To prevent a single Cell Ranger process from hijacking all CPU and RAM by default, this module sets environment variable `MROFLAGS='--localcores=1 --localmem=8 --limit-loadavg'` making those the default.\n]])\n\nload(\"bcl2fastq\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n-- Make CellRanger run on a single core with little memory by default\n-- This prevents a single Cell Ranger process from hijacking all\n-- available CPU and memory resources.\n-- REFERENCES:\n-- * https://martian-lang.org/advanced-features/#job-management\npushenv(\"MROFLAGS\", \"--localcores=1 --localmem=8 --limit-loadavg\")\n\n-- Assert that the current machine supports x86-64 v2 or greater\nsetenv(\"X86_64_LEVEL_ASSERT\", \"2\")\ntry_load(\"x86-64-level\")\nsetenv(\"X86_64_LEVEL_ASSERT\", \"\")\n\n\n\n\n\n\nchafa\n\n\n\nchafa: Chafa (Character Art Facsimile) Terminal Graphics and Character Art Generator Chafa is a command-line utility that converts image data, including animated GIFs, into graphics formats or ANSI/Unicode character art suitable for display in a terminal. Supports modern terminal graphics formats (Sixels, Kitty, iTerm2, Unicode mosaics) - pick the one that your local terminal supports. Example: chafa image.png, chafa –format=iterm image.png, chafa –format=kitty image.png, chafa –version and chafa –help. URL: https://hpjansson.org/chafa/, https://github.com/hpjansson/chafa/blob/master/NEWS (changelog) https://github.com/hpjansson/chafa/releases (releases), https://github.com/hpjansson/chafa (source code) Versions: 1.14.4-1\n\n\nModule code: view\n\nhelp([[\nchafa: Chafa (Character Art Facsimile) Terminal Graphics and Character Art Generator\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, shell\")\nwhatis(\"URL: https://hpjansson.org/chafa/, https://github.com/hpjansson/chafa/blob/master/NEWS (changelog) https://github.com/hpjansson/chafa/releases (releases), https://github.com/hpjansson/chafa (source code)\")\nwhatis([[\nDescription: Chafa is a command-line utility that converts image data, including animated GIFs, into graphics formats or ANSI/Unicode character art suitable for display in a terminal. Supports modern terminal graphics formats (Sixels, Kitty, iTerm2, Unicode mosaics) - pick the one that your local terminal supports.\nExamples: `chafa image.png`, `chafa --format=iterm image.png`, `chafa --format=kitty image.png`, `chafa --version` and `chafa --help`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\ncmake\n\n\n\nCMake: Open-source, Cross-platform Family of Tools Designed to Build, Test and Package Software CMake is cross-platform free and open-source software for managing the build process of software using a compiler-independent method. It supports directory hierarchies and applications that depend on multiple libraries. Example: cmake –version. URL: https://cmake.org/, https://cmake.org/cmake/help/latest/release/index.html (changelog), https://github.com/Kitware/CMake/releases (download) Versions: 3.29.0, 3.30.2, 3.31.5, 4.0.1\n\n\nModule code: view\n\nhelp([[\nCMake: Open-source, Cross-platform Family of Tools Designed to Build, Test and Package Software\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: development, make\")\nwhatis(\"URL: https://cmake.org/, https://cmake.org/cmake/help/latest/release/index.html (changelog), https://github.com/Kitware/CMake/releases (download)\")\nwhatis([[\nDescription: CMake is cross-platform free and open-source software for managing the build process of software using a compiler-independent method. It supports directory hierarchies and applications that depend on multiple libraries.\nExamples: `cmake --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\",  pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\",  pathJoin(home, \"man\"))\n\n\n\n\n\n\nconda-stage\n\n\n\nconda-stage: Stage Conda Environment on Local Disk The ‘conda-stage’ tool stages a Conda environment to local disk. Working with a Conda environment on local disk can greatly improve the performance as local disk is often much faster than a global, network-based file system, including multi-tenant parallel file systems such as BeeGFS and Lustre often found in high-performance compute (HPC) environments. Example: conda-stage –auto-stage=enable, and conda-stage –help. URL: https://github.com/HenrikBengtsson/conda-stage, https://github.com/HenrikBengtsson/conda-stage/blob/develop/NEWS.md (changelog), https://github.com/HenrikBengtsson/conda-stage/tags (releases) Warning: This is work under construction. Your milage may vary! /HB 2022-04-13 Versions: 0.7.2, 0.8.0, 0.9.0\n\n\nModule code: view\n\nhelp([[\nconda-stage: Stage Conda Environment on Local Disk\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, shell\")\nwhatis(\"URL: https://github.com/HenrikBengtsson/conda-stage, https://github.com/HenrikBengtsson/conda-stage/blob/develop/NEWS.md (changelog), https://github.com/HenrikBengtsson/conda-stage/tags (releases)\")\nwhatis([[\nDescription: The 'conda-stage' tool stages a Conda environment to local disk. Working with a Conda environment on local disk can greatly improve the performance as local disk is often much faster than a global, network-based file system, including multi-tenant parallel file systems such as BeeGFS and Lustre often found in high-performance compute (HPC) environments.\nExamples: `conda-stage --auto-stage=enable`, and `conda-stage --help`.\nWarning: This is work under construction. Your milage may vary! /HB 2022-04-13\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- WORKAROUND: For some reasons, this is required in order for\n-- the 'root' part to be included in the 'home' path below in\n-- *some* cases. For example, if we do 'conda activate base',\n-- 'conda deactivate', and then 'module load conda-stage' we\n-- would, for unknown reasons, end up with an empty 'root'.\n-- The below seems to force the correct value of 'root'.\n-- /HB 2022-09-22\nif not isDir(root) then\n  LmodError(\"Environment variable 'SOFTWARE_ROOT_CBI' does not specify an existing folder: \" .. os.getenv(\"SOFTWARE_ROOT_CBI\"))\nend\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\npushenv(\"CONDA_STAGE_PROLOGUE\", \"module load CBI \" .. name)\n\nlocal script = pathJoin(home, \"bin\", \"conda-stage.\" .. myShellType())\nif not isFile(script) then\n  LmodError(\"The \" .. name .. \" module is not supported for your shell (\" .. myShellType() .. \"; SHELL=\" .. os.getenv(\"SHELL\") .. \"). No such file: \" .. script)\nend\n\n-- Create conda-stage() function, which will overwrite itself after the\n-- first invocation\nlocal body = 'source \"' .. script .. '\"; '\nbody = body .. 'conda-stage \"$@\"'\nset_shell_function(\"conda-stage\", body, '')\n\n\n\n\n\ncontrol-freec\n\n\n\nControl FREEC: Control-FREE Copy Number and Genotype Caller Prediction of copy numbers and allelic content using deep-sequencing data. Example: freec URL: http://boevalab.com/FREEC/, https://github.com/BoevaLab/FREEC/releases (changelog), https://github.com/BoevaLab/FREEC/ (source code) Versions: 11.5, 11.6\n\n\nModule code: view\n\nhelp([[\nControl FREEC: Control-FREE Copy Number and Genotype Caller\n]])\n\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: \")\nwhatis(\"URL: http://boevalab.com/FREEC/, https://github.com/BoevaLab/FREEC/releases (changelog), https://github.com/BoevaLab/FREEC/ (source code)\")\nwhatis([[\nDescription: Prediction of copy numbers and allelic content using deep-sequencing data.\nExamples: `freec`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal name = \"FREEC\"\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\n\ncufflinks\n\n\n\nCufflinks: Transcriptome Assembly and Differential Expression Analysis for RNA-Seq Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. It accepts aligned RNA-Seq reads and assembles the alignments into a parsimonious set of transcripts. Cufflinks then estimates the relative abundances of these transcripts based on how many reads support each one, taking into account biases in library preparation protocols. Example: cufflinks URL: http://cole-trapnell-lab.github.io/cufflinks/, https://github.com/cole-trapnell-lab/cufflinks (source code) Versions: 2.2.1\n\n\nModule code: view\n\nhelp([[\nCufflinks: Transcriptome Assembly and Differential Expression Analysis for RNA-Seq\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: http://cole-trapnell-lab.github.io/cufflinks/, https://github.com/cole-trapnell-lab/cufflinks (source code)\")\nwhatis([[\nDescription: Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples. It accepts aligned RNA-Seq reads and assembles the alignments into a parsimonious set of transcripts. Cufflinks then estimates the relative abundances of these transcripts based on how many reads support each one, taking into account biases in library preparation protocols.\nExamples: `cufflinks`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\ncutadapt\n\n\n\nCutadapt: Remove Adapter Sequences from Sequencing Reads Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads. Example: cutadapt –version and cutadapt –help. URL: https://cutadapt.readthedocs.io/en/stable/, https://github.com/marcelm/cutadapt/blob/main/CHANGES.rst (changelog), https://github.com/marcelm/cutadapt (source code) Requirement: CentOS 7. Versions: 3.7, 4.9, 5.0, 5.1\n\n\nModule code: view\n\nhelp([[\nCutadapt: Remove Adapter Sequences from Sequencing Reads \n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, genome\")\nwhatis(\"URL: https://cutadapt.readthedocs.io/en/stable/, https://github.com/marcelm/cutadapt/blob/main/CHANGES.rst (changelog), https://github.com/marcelm/cutadapt (source code)\")\nwhatis([[\nDescription: Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.\nRequirements: CentOS 7.\nExamples: `cutadapt --version` and `cutadapt --help`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\n-- if os.getenv(\"CBI_LINUX\") ~= \"centos7\" then\n--   LmodError(\"The '\" .. name .. \"' module is not yet supported on \" .. os.getenv(\"CBI_LINUX\") .. \". See https://github.com/HenrikBengtsson/CBI-software/issues/102 for updates on this problem.\")\n-- end\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\n\n\n\ndmtcp\n\n\n\nDMTCP: Distributed MultiThreaded CheckPointing DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Among the applications supported by DMTCP are MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages. DMTCP also supports GNU screen sessions, including vim/cscope and emacs. Example: dmtcp_launch –help and man dmtcp. To run R and create check points every 10 minutes, call dmtcp_launch –interval=600 R. If that process gets terminated, it can then be relaunched using ./dmtcp_restart_script.sh. URL: http://dmtcp.sourceforge.net/, https://docs.nersc.gov/development/checkpoint-restart/dmtcp/ (documentation), https://dmtcp.sourceforge.io/FAQ.html (FAQ), https://github.com/dmtcp/dmtcp/releases (changelog), https://github.com/dmtcp/dmtcp (source code) Versions: 3.1.2\n\n\nModule code: view\n\nhelp([[\nDMTCP: Distributed MultiThreaded CheckPointing\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: hpc, checkpointing\")\nwhatis(\"URL: http://dmtcp.sourceforge.net/, https://docs.nersc.gov/development/checkpoint-restart/dmtcp/ (documentation), https://dmtcp.sourceforge.io/FAQ.html (FAQ), https://github.com/dmtcp/dmtcp/releases (changelog), https://github.com/dmtcp/dmtcp (source code)\")\nwhatis([[\nDescription: DMTCP is a tool to transparently checkpoint the state of multiple simultaneous applications, including multi-threaded and distributed applications. It operates directly on the user binary executable, without any Linux kernel modules or other kernel modifications. Among the applications supported by DMTCP are MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages. DMTCP also supports GNU screen sessions, including vim/cscope and emacs.\nExamples: `dmtcp_launch --help` and `man dmtcp`. To run R and create check points every 10 minutes, call `dmtcp_launch --interval=600 R`. If that process gets terminated, it can then be relaunched using `./dmtcp_restart_script.sh`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\n-- Runtime\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"MANPATH_PATH\", pathJoin(home, \"share\", \"man\"))\n\n-- Build time\n--prepend_path(\"CPATH\", pathJoin(home, \"include\"))\n--prepend_path(\"LDFLAGS\", \"-L\" .. pathJoin(home, \"lib\"), \" \")\n--prepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n\n\n\n\neasycatfs\n\n\n\neasycatfs: Easy Read-Only Mounting of Slow Folders onto a Local Drive This is Linux command-line tool for mounting one or more folders on a network file system on a local disk such that the local-disk folders mirrors everything (read-only) on the network folder. This will result in (i) faster repeated access to files, and (ii) decreased load on the network file system. This is particularly beneficial when working on high-performance compute (HPC) clusters used by hundreds and thousands of processes and users simultaneously. Example: easycatfs –help and easycatfs mount /shared/data. URL: https://github.com/HenrikBengtsson/easycatfs Warning: Only the most recent version of this software will be kept. Versions: 0.1.5\n\n\nModule code: view\n\nhelp([[\neasycatfs: Easy Read-Only Mounting of Slow Folders onto a Local Drive\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, files, hpc\")\nwhatis(\"URL: https://github.com/HenrikBengtsson/easycatfs\")\nwhatis([[\nDescription: This is Linux command-line tool for mounting one or more folders on a network file system on a local disk such that the local-disk folders mirrors everything (read-only) on the network folder. This will result in (i) faster repeated access to files, and (ii) decreased load on the network file system. This is particularly beneficial when working on high-performance compute (HPC) clusters used by hundreds and thousands of processes and users simultaneously.\nExamples: `easycatfs --help` and `easycatfs mount /shared/data`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\nemacs\n\n\n\nGNU Emacs: An Extensible, Customizable, Free/Libre Text Editor At its core is an interpreter for Emacs Lisp, a dialect of the Lisp programming language with extensions to support text editing. Example: emacs –version and emacs -nw. URL: https://www.gnu.org/software/emacs/, https://www.gnu.org/savannah-checkouts/gnu/emacs/emacs.html#Releases (changelog) Warning: Only the most recent version of this software will be kept. Versions: 29.4, 30.1\n\n\nModule code: view\n\nhelp([[\nGNU Emacs: An Extensible, Customizable, Free/Libre Text Editor\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: editor, text, cli, tui\")\nwhatis(\"URL: https://www.gnu.org/software/emacs/, https://www.gnu.org/savannah-checkouts/gnu/emacs/emacs.html#Releases (changelog)\")\nwhatis([[\nDescription: At its core is an interpreter for Emacs Lisp, a dialect of the Lisp programming language with extensions to support text editing.\nExamples: `emacs --version` and `emacs -nw`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\nets\n\n\n\nets: Command Output Timestamper ets is a command output timestamper - it prefixes each line of a command’s output with a timestamp. The purpose of ets is similar to that of moreutils ts(1), but ets differentiates itself from similar offerings by running commands directly within ptys, hence solving thorny issues like pipe buffering and commands disabling color and interactive features when detecting a pipe as output. Example: ets –help, man ets, and ets ping localhost. URL: https://github.com/zmwangx/ets, https://github.com/zmwangx/ets/releases (releases) Warning: Only the most recent version of this software will be kept. Versions: 0.2.2\n\n\nModule code: view\n\nhelp([[\nets: Command Output Timestamper \n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli\")\nwhatis(\"URL: https://github.com/zmwangx/ets, https://github.com/zmwangx/ets/releases (releases)\")\nwhatis([[\nDescription: ets is a command output timestamper - it prefixes each line of a command's output with a timestamp. The purpose of ets is similar to that of moreutils ts(1), but ets differentiates itself from similar offerings by running commands directly within ptys, hence solving thorny issues like pipe buffering and commands disabling color and interactive features when detecting a pipe as output.\nExamples: `ets --help`, `man ets`, and `ets ping localhost`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\", \"man1\"))\n\n\n\n\n\nexpect\n\n\n\nexpect: Programmed Dialogue with Interactive Programs Expect is a tool for automating interactive applications such as telnet, ftp, passwd, fsck, rlogin, tip, etc. Expect really makes this stuff trivial. Expect is also useful for testing these same applications. Example: expect -version, and man expect. URL: https://core.tcl-lang.org/expect/index, https://core.tcl-lang.org/expect/file?name=ChangeLog&ci=tip (changelog), https://core.tcl-lang.org/expect/dir?ci=tip (source code), https://sourceforge.net/projects/expect/files/Expect/ (download) Versions: 5.45.4\n\n\nModule code: view\n\nhelp([[\nexpect: Programmed Dialogue with Interactive Programs\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: scripting, programming\")\nwhatis(\"URL: https://core.tcl-lang.org/expect/index, https://core.tcl-lang.org/expect/file?name=ChangeLog&ci=tip (changelog), https://core.tcl-lang.org/expect/dir?ci=tip (source code), https://sourceforge.net/projects/expect/files/Expect/ (download)\")\nwhatis([[\nDescription: Expect is a tool for automating interactive applications such as telnet, ftp, passwd, fsck, rlogin, tip, etc. Expect really makes this stuff trivial. Expect is also useful for testing these same applications.\nExamples: `expect -version`, and `man expect`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"libs\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\n\nfastqc\n\n\n\nFastQC: A Quality Control Analysis Tool for High Throughput Sequencing Data FastQC is a program designed to spot potential problems in high throughput sequencing datasets. It runs a set of analyses on one or more raw sequence files in fastq or bam format and produces a report which summarises the results. Example: fastqc –version. URL: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/, https://raw.githubusercontent.com/s-andrews/FastQC/master/RELEASE_NOTES.txt (changelog), https://github.com/s-andrews/FastQC/ (source code) Versions: 0.11.8, 0.11.9, 0.12.1\n\n\nModule code: view\n\nhelp([[\nFastQC: A Quality Control Analysis Tool for High Throughput Sequencing Data\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, qc\")\nwhatis(\"URL: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/, https://raw.githubusercontent.com/s-andrews/FastQC/master/RELEASE_NOTES.txt (changelog), https://github.com/s-andrews/FastQC/ (source code)\")\nwhatis([[\nDescription: FastQC is a program designed to spot potential problems in high throughput sequencing datasets. It runs a set of analyses on one or more raw sequence files in fastq or bam format and produces a report which summarises the results.\nExamples: `fastqc --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, \"FastQC\" .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\nfclones\n\n\n\nfclones: Efficient Duplicate File Finder ‘fclones’ is a command line utility that identifies groups of identical files and gets rid of the file copies you no longer need. It comes with plenty of configuration options for controlling the search scope and offers many ways of removing duplicates. Example: fclones –version, fclones –help, fclones group . –threads main:1, and fclones group . –depth 1 –threads main:1. URL: https://github.com/pkolaczk/fclones, https://github.com/pkolaczk/fclones/releases (releases), https://docs.rs/fclones/latest/fclones/ (Rust API documentation) Versions: 0.34.0\n\n\nModule code: view\n\nhelp([[\nfclones: Efficient Duplicate File Finder\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli, files\")\nwhatis(\"URL: https://github.com/pkolaczk/fclones, https://github.com/pkolaczk/fclones/releases (releases), https://docs.rs/fclones/latest/fclones/ (Rust API documentation)\")\nwhatis([[\nDescription: 'fclones' is a command line utility that identifies groups of identical files and gets rid of the file copies you no longer need. It comes with plenty of configuration options for controlling the search scope and offers many ways of removing duplicates.\nExamples: `fclones --version`, `fclones --help`, `fclones group . --threads main:1`, and `fclones group . --depth 1 --threads main:1`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\nfzf\n\n\n\nfzf: A Command-Line Fuzzy Finder fzf is a general-purpose command-line fuzzy finder. It’s an interactive Unix filter for command-line that can be used with any list; files, command history, processes, hostnames, bookmarks, git commits, etc. Example: fzf –version and emacs \"\\((fzf)&quot;&lt;/code&gt;.&lt;/span&gt;&lt;br&gt;\nNote: &lt;span class=\"module-note\"&gt;To install tab completions and key bindinds to your shell, call &lt;code&gt;\\)FZF_HOME/install. To uninstall, use $FZF_HOME/uninstall. URL: https://github.com/junegunn/fzf, https://github.com/junegunn/fzf/wiki (documentation), https://github.com/junegunn/fzf/blob/master/CHANGELOG.md (changelog), https://github.com/junegunn/fzf/releases (download) Warning: Only the most recent version of this software will be kept. Versions: 0.65.2, 0.66.0\n\n\nModule code: view\n\nhelp([[\nfzf: A Command-Line Fuzzy Finder\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: , cli, tui, shell\")\nwhatis(\"URL: https://github.com/junegunn/fzf, https://github.com/junegunn/fzf/wiki (documentation), https://github.com/junegunn/fzf/blob/master/CHANGELOG.md (changelog), https://github.com/junegunn/fzf/releases (download)\")\nwhatis([[\nDescription: fzf is a general-purpose command-line fuzzy finder. It's an interactive Unix filter for command-line that can be used with any list; files, command history, processes, hostnames, bookmarks, git commits, etc.\nExamples: `fzf --version` and `emacs \"$(fzf)\"`. Note: To install tab completions and key bindinds to your shell, call `$FZF_HOME/install`. To uninstall, use `$FZF_HOME/uninstall`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"man\"))\n\n-- Custom\npushenv(\"FZF_HOME\", home)\n\n\n\n\n\ngatk\n\n\n\nGenome Analysis Toolkit (GATK): Variant Discovery in High-Throughput Sequencing Data Developed in the Data Sciences Platform at the Broad Institute, the toolkit offers a wide variety of tools with a primary focus on variant discovery and genotyping. Its powerful processing engine and high-performance computing features make it capable of taking on projects of any size. Example: gatk –help and gatk –list. URL: https://gatk.broadinstitute.org/hc/en-us, https://github.com/broadinstitute/gatk (source code), https://github.com/broadinstitute/gatk/releases (changelog), https://github.com/broadgsa/gatk (legacy), https://console.cloud.google.com/storage/browser/gatk-software/package-archive (legacy), ftp://ftp.broadinstitute.org/pub/gsa/GenomeAnalysisTK/ (legacy) Requirement: Modern GATK versions require Java (&gt;= 17). Versions: 4.1.0.0, 4.1.2.0, 4.1.3.0, 4.1.4.0, 4.1.6.0, 4.1.7.0, 4.1.9.0, 4.2.2.0, 4.2.4.1, 4.2.5.0, 4.2.6.0, 4.2.6.1, 4.3.0.0, 4.4.0.0, 4.5.0.0, 4.6.0.0, 4.6.1.0, 4.6.2.0\n\n\nModule code: view\n\nhelp([[\nGenome Analysis Toolkit (GATK): Variant Discovery in High-Throughput Sequencing Data\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, genome\")\nwhatis(\"URL: https://gatk.broadinstitute.org/hc/en-us, https://github.com/broadinstitute/gatk (source code), https://github.com/broadinstitute/gatk/releases (changelog), https://github.com/broadgsa/gatk (legacy), https://console.cloud.google.com/storage/browser/gatk-software/package-archive (legacy), ftp://ftp.broadinstitute.org/pub/gsa/GenomeAnalysisTK/ (legacy)\")\nwhatis([[\nDescription: Developed in the Data Sciences Platform at the Broad Institute, the toolkit offers a wide variety of tools with a primary focus on variant discovery and genotyping. Its powerful processing engine and high-performance computing features make it capable of taking on projects of any size.\nRequirements: Modern GATK versions require Java (&gt;= 17).\nExamples: `gatk --help` and `gatk --list`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nlocal version_x = string.gsub(version, \"[.].*\", \"\")\nif version_x == \"1\" then\n  -- GATK v1.* requires Java (&lt;= 1.7)\n  local cluster = os.getenv(\"CLUSTER\") or \"\"\n  if (cluster == \"tipcc\") then\n    load(\"jdk/1.7.0\")\n  else\n    depends_on(\"openjdk/1.6.0\")\n  end\n  pushenv(\"GATK_HOME\", home)\nelseif version_x == \"4\" then\n  if mode() == \"load\" then\n    local success=false\n\n    -- try all possible openjdk/(&gt;= 17) versions\n    for version = 17,30 do\n      module=\"openjdk/\" .. version\n      if isAvail(module) then\n        load(module)\n        success=true\n        break\n      end\n    end\n    \n    -- try oraclejdk/(&gt;= 17) versions\n    if not success then\n      for version = 17,30 do\n        module=\"oraclejdk/\" .. version\n        if isAvail(module) then\n          load(module)\n          success=true\n          break\n        end\n      end\n    end\n    \n    if not success then\n      LmodError(name .. \" requires openjdk/17 or newer, but that is not available on \" .. os.getenv(\"CBI_LINUX\") .. \" machine \" .. os.getenv(\"HOSTNAME\"))\n    end\n  end\n  prepend_path(\"PATH\", home)\nelse\n  prepend_path(\"PATH\", home)\nend\n\n-- Tweak Java for the current environment\ndepends_on(\"java-tweaks\")\n\n\n\n\n\ngcta\n\n\n\nGCTA: Genome-wide Complex Trait Analysis A tool for Genome-wide Complex Trait Analysis (GCTA). Example: gcta64. URL: https://yanglab.westlake.edu.cn/software/gcta/#Overview, https://cnsgenomics.com/software/gcta/, https://github.com/jianyangqt/gcta (source code) Versions: 1.26.0, 1.92.3beta3, 1.92.4beta, 1.93.2beta, 1.94.0beta\n\n\nModule code: view\n\nhelp([[\nGCTA: Genome-wide Complex Trait Analysis\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: genome\")\nwhatis(\"URL: https://yanglab.westlake.edu.cn/software/gcta/#Overview, https://cnsgenomics.com/software/gcta/, https://github.com/jianyangqt/gcta (source code)\")\nwhatis([[\nDescription: A tool for Genome-wide Complex Trait Analysis (GCTA).\nExamples: `gcta64`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\ngeos\n\n\n\nGEOS: Geometry Engine, Open Source GEOS (Geometry Engine - Open Source) is a C++ port of the JTS Topology Suite (JTS). It aims to contain the complete functionality of JTS in C++. This includes all the OpenGIS Simple Features for SQL spatial predicate functions and spatial operators, as well as specific JTS enhanced functions. GEOS provides spatial functionality to many other projects and products. Example: geos-config –version. URL: https://libgeos.org/, https://libgeos.org/usage/download/ (changelog), https://github.com/libgeos/geos/issues (bug reports) Versions: 3.5.2, 3.7.3, 3.8.1, 3.9.1, 3.9.3, 3.9.4\n\n\nModule code: view\n\nhelp([[\nGEOS: Geometry Engine, Open Source\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: statistics, spatial\")\nwhatis(\"URL: https://libgeos.org/, https://libgeos.org/usage/download/ (changelog), https://github.com/libgeos/geos/issues (bug reports)\")\nwhatis([[\nDescription: GEOS (Geometry Engine - Open Source) is a C++ port of the JTS Topology Suite (JTS). It aims to contain the complete functionality of JTS in C++. This includes all the OpenGIS Simple Features for SQL spatial predicate functions and spatial operators, as well as specific JTS enhanced functions. GEOS provides spatial functionality to many other projects and products.\nExamples: `geos-config --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\n-- execution\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\n-- linking\nprepend_path(\"LD_RUN_PATH\", pathJoin(home, \"lib\"))\n-- building\nprepend_path(\"CPATH\",  pathJoin(home, \"include\"))\nprepend_path(\"CFLAGS\", \"-I\" .. pathJoin(home, \"include\"), \" \")\nprepend_path(\"LDFLAGS\", \"-L\" .. pathJoin(home, \"lib\"), \" \")\n\n\n\n\n\ngit-extras\n\n\n\ngit-extras: Little Git Extras GIT utilities – repo summary, repl, changelog population, author commit percentages and more. Example: git extras –version and git extras –help. URL: https://github.com/tj/git-extras, https://github.com/tj/git-extras/releases (releases), https://github.com/tj/git-extras/blob/main/History.md (changelog) Warning: Only the most recent version of this software will be kept. Versions: 7.3.0\n\n\nModule code: view\n\nhelp([[\ngit-extras: Little Git Extras\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: version control\")\nwhatis(\"URL: https://github.com/tj/git-extras, https://github.com/tj/git-extras/releases (releases), https://github.com/tj/git-extras/blob/main/History.md (changelog)\")\nwhatis(\n[[Description: GIT utilities -- repo summary, repl, changelog population, author commit percentages and more.\nExamples: `git extras --version` and `git extras --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\ngit-flow\n\n\n\ngit-flow: Git Extension Git Flow (AVH Edition) A collection of Git extensions to provide high-level repository operations for Vincent Driessen’s branching model. Example: git flow. URL: https://github.com/petervanderdoes/gitflow-avh, https://github.com/petervanderdoes/gitflow-avh/blob/develop/CHANGELOG.md (changelog) Warning: Only the most recent version of this software will be kept. Versions: 1.12.3\n\n\nModule code: view\n\nhelp([[\ngit-flow: Git Extension Git Flow (AVH Edition)\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: version control\")\nwhatis(\"URL: https://github.com/petervanderdoes/gitflow-avh, https://github.com/petervanderdoes/gitflow-avh/blob/develop/CHANGELOG.md (changelog)\")\nwhatis(\n[[Description: A collection of Git extensions to provide high-level repository operations for Vincent Driessen's branching model.\nExamples: `git flow`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\ngithub-cli\n\n\n\ngithub-cli: GitHub’s Official Command Line Tool gh is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with git and your code. Example: gh –version and gh –help. Setup: gh auth login (once). CLI query example: gh issue list –repo=futureverse/future. TUI: gh extension install dlvhdr/gh-dash (once), then gh dash. AI: gh extension install github/gh-copilot (once), then gh copilot suggest \"find files older than one year\". URL: https://cli.github.com/, https://cli.github.com/manual/ (documentation), https://github.com/cli/cli/releases (changelog), https://github.com/cli/cli/ (source code), https://github.com/topics/gh-extension (GitHub CLI extensions) Warning: Only the most recent version of this software will be kept. Versions: 2.67.0, 2.82.1\n\n\nModule code: view\n\nhelp([[\ngithub-cli: GitHub's Official Command Line Tool \n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, file viewer, cli, tui\")\nwhatis(\"URL: https://cli.github.com/, https://cli.github.com/manual/ (documentation), https://github.com/cli/cli/releases (changelog), https://github.com/cli/cli/ (source code), https://github.com/topics/gh-extension (GitHub CLI extensions)\")\nwhatis([[\nDescription: `gh` is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with `git` and your code.\nExamples: `gh --version` and `gh --help`. Setup: `gh auth login` (once). CLI query example: `gh issue list --repo=futureverse/future`. TUI: `gh extension install dlvhdr/gh-dash` (once), then `gh dash`. AI: `gh extension install github/gh-copilot` (once), then `gh copilot suggest \"find files older than one year\"`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\", \"man1\"))\n\n\n\n\n\ngitleaks\n\n\n\ngitleaks: Protect and Discover Secrets using Gitleaks Examples: gitleaks –version, and gitleaks –help. URL: https://gitleaks.io/, https://github.com/gitleaks/gitleaks/releases (releases), https://github.com/gitleaks/gitleaks (source code) Warning: Only the most recent version of this software will be kept. Versions: 8.21.2\n\n\nModule code: view\n\nhelp([[\ngitleaks: Protect and Discover Secrets using Gitleaks\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli\")\nwhatis(\"URL: https://gitleaks.io/, https://github.com/gitleaks/gitleaks/releases (releases), https://github.com/gitleaks/gitleaks (source code)\")\nwhatis([[\nDescription: \nExamples: `gitleaks --version`, and `gitleaks --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\ngitui\n\n\n\ngitui: Blazing Fast Terminal-UI for Git Written in Rust GitUI provides you with the comfort of a git GUI but right in your terminal. Example: gitui –help, gitui –version, and gitui. URL: https://github.com/extrawurst/gitui/blob/master/CHANGELOG.md (changelog), https://github.com/extrawurst/gitui (source code) Versions: 0.26.3\n\n\nModule code: view\n\nhelp([[\ngitui: Blazing Fast Terminal-UI for Git Written in Rust\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, git\")\nwhatis(\"URL: https://github.com/extrawurst/gitui/blob/master/CHANGELOG.md (changelog), https://github.com/extrawurst/gitui (source code)\")\nwhatis([[\nDescription: GitUI provides you with the comfort of a git GUI but right in your terminal.\nExamples: `gitui --help`, `gitui --version`, and `gitui`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\n\nglow\n\n\n\nglow: Render Markdown on the CLI, with Pizzazz! Glow is a terminal based markdown reader designed from the ground up to bring out the beauty—and power—of the CLI. Use it to discover markdown files, read documentation directly on the command line and stash markdown files to your own private collection so you can read them anywhere. Glow will find local markdown files in subdirectories or a local Git repository. Example: glow README.md, glow –pager README.md. URL: https://github.com/charmbracelet/glow, https://github.com/charmbracelet/glow/releases (changelog) Warning: Only the most recent version of this software will be kept. Versions: 2.0.0\n\n\nModule code: view\n\nhelp([[\nglow: Render Markdown on the CLI, with Pizzazz!\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, file viewer, pager, markdown, cli, tui\")\nwhatis(\"URL: https://github.com/charmbracelet/glow, https://github.com/charmbracelet/glow/releases (changelog)\")\nwhatis([[\nDescription: Glow is a terminal based markdown reader designed from the ground up to bring out the beauty—and power—of the CLI.  Use it to discover markdown files, read documentation directly on the command line and stash markdown files to your own private collection so you can read them anywhere. Glow will find local markdown files in subdirectories or a local Git repository.\nExamples: `glow README.md`, `glow --pager README.md`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", home)\n\n\n\n\n\ngo\n\n\n\ngo: The Go Programming Language Build simple, secure, scalable systems with Go. Example: go version and go help. URL: https://go.dev/, https://go.dev/doc/devel/release (changelog), https://go.dev/dl/ (releases), https://github.com/golang (source code) Versions: 1.22.5, 1.23.1, 1.23.4, 1.23.5, 1.23.6, 1.24.0, 1.24.4\n\n\nModule code: view\n\nhelp([[\ngo: The Go Programming Language\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, shell\")\nwhatis(\"URL: https://go.dev/, https://go.dev/doc/devel/release (changelog), https://go.dev/dl/ (releases), https://github.com/golang (source code)\")\nwhatis([[\nDescription: Build simple, secure, scalable systems with Go.\nExamples: `go version` and `go help`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nsetenv(\"GOROOT\", home)\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\ngping\n\n\n\ngping: Ping, but with a Graph gping comes with the following super-powers: (i) graph the ping time for multiple hosts, (ii) graph the execution time for commands via the –cmd flag, and (iii) custom colours. Example: gping –version, gping –help, gping 8.8.8.8 9.9.9.9, and gping –cmd \"curl -o /dev/null https://www.github.com\" \"wget -O /dev/null https://github.com\". URL: https://github.com/orf/gping, https://github.com/orf/gping/releases (changelog), https://github.com/orf/gping (source code) Warning: Only the most recent version of this software will be kept. Versions: 1.20.1\n\n\nModule code: view\n\nhelp([[\ngping: Ping, but with a Graph\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: terminal, cli, utility\")\nwhatis(\"URL: https://github.com/orf/gping, https://github.com/orf/gping/releases (changelog), https://github.com/orf/gping (source code)\")\nwhatis([[\nDescription: gping comes with the following super-powers: (i) graph the ping time for multiple hosts, (ii) graph the execution time for commands via the `--cmd` flag, and (iii) custom colours.\nExamples: `gping --version`, `gping --help`, `gping 8.8.8.8 9.9.9.9`, and `gping --cmd \"curl -o /dev/null https://www.github.com\" \"wget -O /dev/null https://github.com\"`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\",  home)\n\n\n\n\n\n\ngsl\n\n\n\nGSL: Gnu Scientific Library The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. It is free software under the GNU General Public License. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. There are over 1000 functions in total with an extensive test suite. Example: gsl-config –version. URL: https://www.gnu.org/software/gsl/, https://git.savannah.gnu.org/cgit/gsl.git/tree/NEWS (changelog), https://mirror.ibcp.fr/pub/gnu/gsl/ (download), https://git.savannah.gnu.org/cgit/gsl.git/ (source code) Versions: 2.6, 2.7, 2.8\n\n\nModule code: view\n\nhelp([[\nGSL: Gnu Scientific Library\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: library\")\nwhatis(\"URL: https://www.gnu.org/software/gsl/, https://git.savannah.gnu.org/cgit/gsl.git/tree/NEWS (changelog), https://mirror.ibcp.fr/pub/gnu/gsl/ (download), https://git.savannah.gnu.org/cgit/gsl.git/ (source code)\")\nwhatis([[\nDescription: The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. It is free software under the GNU General Public License. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting. There are over 1000 functions in total with an extensive test suite.\nExamples: `gsl-config --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n\n\n\n\nhdf5\n\n\n\nhdf5: A General Purpose Library and File Format for Storing Scientific Data Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data. The HDF5 format is designed to address some of the limitations of the HDF4 library, and to address current and anticipated requirements of modern systems and applications. Example: h5stat –version. URL: https://www.hdfgroup.org/download-hdf5/, https://github.com/HDFGroup/hdf5/releases (releases), https://github.com/HDFGroup/hdf5 (source code) Versions: 1.10.6, 1.12.0, 1.12.1, 1.12.2\n\n\nModule code: view\n\nhelp([[\nhdf5: A General Purpose Library and File Format for Storing Scientific Data\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: library\")\nwhatis(\"URL: https://www.hdfgroup.org/download-hdf5/, https://github.com/HDFGroup/hdf5/releases (releases), https://github.com/HDFGroup/hdf5 (source code)\")\nwhatis([[\nDescription: Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data. The HDF5 format is designed to address some of the limitations of the HDF4 library, and to address current and anticipated requirements of modern systems and applications.\nExamples: `h5stat --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\n-- prepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\nprepend_path(\"CPATH\",  pathJoin(home, \"include\"))\n-- prepend_path(\"CFLAGS\", \"-I\" .. pathJoin(home, \"include\"), \" \")\n-- prepend_path(\"LDFLAGS\", \"-L\" .. pathJoin(home, \"lib\"), \" \")\n\n\n\n\n\nhisat2\n\n\n\nHISAT2: Graph-based Alignment of Next Generation Sequencing Reads to a Population of Genomes HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome). Based on an extension of BWT for graphs [Sirén et al. 2014], we designed and implemented a graph FM index (GFM), an original approach and its first implementation to the best of our knowledge. In addition to using one global GFM index that represents a population of human genomes, HISAT2 uses a large set of small GFM indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp, with 55,000 indexes needed to cover the human population). These small indexes (called local indexes), combined with several alignment strategies, enable rapid and accurate alignment of sequencing reads. This new indexing scheme is called a Hierarchical Graph FM index (HGFM). Example: hisat2 –version and hisat2 –help. URL: https://daehwankimlab.github.io/hisat2/, https://github.com/DaehwanKimLab/hisat2/releases (changelog), https://github.com/DaehwanKimLab/hisat2/ (source code) Versions: 2.1.0, 2.2.0\n\n\nModule code: view\n\nhelp([[\nHISAT2: Graph-based Alignment of Next Generation Sequencing Reads to a Population of Genomes\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: Programming, Statistics\")\nwhatis(\"URL: https://daehwankimlab.github.io/hisat2/, https://github.com/DaehwanKimLab/hisat2/releases (changelog), https://github.com/DaehwanKimLab/hisat2/ (source code)\")\nwhatis([[\nDescription: HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome). Based on an extension of BWT for graphs [Sirén et al. 2014], we designed and implemented a graph FM index (GFM), an original approach and its first implementation to the best of our knowledge. In addition to using one global GFM index that represents a population of human genomes, HISAT2 uses a large set of small GFM indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp, with 55,000 indexes needed to cover the human population). These small indexes (called local indexes), combined with several alignment strategies, enable rapid and accurate alignment of sequencing reads. This new indexing scheme is called a Hierarchical Graph FM index (HGFM).\nExamples: `hisat2 --version` and `hisat2 --help`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nhtop\n\n\n\nhtop: An Interactive Process Viewer for Unix htop is an interactive process viewer for Unix systems. It is a text-mode application (for console or X terminals) and requires ncurses. Example: htop. URL: https://htop.dev, https://github.com/htop-dev/htop/blob/main/ChangeLog (changelog), https://github.com/htop-dev/htop (source code) Warning: Only the most recent version of this software will be kept. Versions: 3.4.1\n\n\nModule code: view\n\nhelp([[\nhtop: An Interactive Process Viewer for Unix\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: system, utility, cli, tui\")\nwhatis(\"URL: https://htop.dev, https://github.com/htop-dev/htop/blob/main/ChangeLog (changelog), https://github.com/htop-dev/htop (source code)\")\nwhatis([[\nDescription: `htop` is an interactive process viewer for Unix systems. It is a text-mode application (for console or X terminals) and requires ncurses.\nExamples: `htop`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\nhtslib\n\n\n\nHTSlib: C Library for High-Throughput Sequencing Data Formats HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data, and is the core library used by samtools and bcftools. HTSlib also provides the bgzip, htsfile, and tabix utilities. Example: bgzip –version, htsfile –version, and tabix –version. URL: https://www.htslib.org/, https://github.com/samtools/htslib/blob/develop/NEWS (changelog), https://github.com/samtools/htslib (source code) Versions: 1.9, 1.10.2, 1.11, 1.13, 1.14, 1.15, 1.15.1, 1.16, 1.17, 1.18, 1.19, 1.19.1, 1.20, 1.21, 1.22.1\n\n\nModule code: view\n\nhelp([[\nHTSlib: C Library for High-Throughput Sequencing Data Formats\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, programming\")\nwhatis(\"URL: https://www.htslib.org/, https://github.com/samtools/htslib/blob/develop/NEWS (changelog), https://github.com/samtools/htslib (source code)\")\nwhatis([[\nDescription: HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data, and is the core library used by samtools and bcftools. HTSlib also provides the bgzip, htsfile, and tabix utilities.\nExamples: `bgzip --version`, `htsfile --version`, and `tabix --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n\n-- Warn about bug https://github.com/samtools/htslib/issues/1236\nif (mode() == \"load\" and version == \"1.11\") then\n  LmodMessage(\"MODULE WARNING: \" .. name .. \" \" .. version .. \" has a bug that results in valid but incorrect CIGAR strings. Because of this, it is recommended to use an older or a newer version instead. For details, see https://github.com/samtools/htslib/issues/1236\")\nend\n\n\n\n\n\nhtstools\n\n\n\nhtstools: Tools to Process BAM Files for Downstream Copy-Number Analysis Contains three tools (dnafrags, ppflag-fixer, snp-pileup) written by Alex Studer to process bam files for downstream copy number analysis. Example: snp-pileup –help, dnafrags –help, ppflag-fixer –help. URL: https://github.com/mskcc/htstools, https://github.com/mskcc/htstools/releases (changelog) Versions: 0.1.1\n\n\nModule code: view\n\nhelp([[\nhtstools: Tools to Process BAM Files for Downstream Copy-Number Analysis\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://github.com/mskcc/htstools, https://github.com/mskcc/htstools/releases (changelog)\")\nwhatis([[\nDescription: Contains three tools (dnafrags, ppflag-fixer, snp-pileup) written by Alex Studer to process bam files for downstream copy number analysis.\nExamples: `snp-pileup --help`, `dnafrags --help`, `ppflag-fixer --help`.\n]])\n\ndepends_on(\"htslib\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nigv\n\n\n\nIGV: The Integrative Genomics Viewer The Integrative Genomics Viewer (IGV) is a high-performance visualization tool for interactive exploration of large, integrated genomic datasets. It supports a wide variety of data types, including array-based and next-generation sequence data, and genomic annotations. Example: igv –help, igv –version, and igv. URL: https://software.broadinstitute.org/software/igv/, https://github.com/igvteam/igv/tags (changelog), https://github.com/igvteam/igv/ (source code) Warning: IGV (&gt;= 2.7.0) requires Java 17. Only the most recent version of this software will be kept. Versions: 2.16.2, 2.17.0, 2.17.1, 2.17.3, 2.17.4, 2.18.1, 2.18.3, 2.18.4, 2.19.0\n\n\nModule code: view\n\nhelp([[\nIGV: The Integrative Genomics Viewer\n]])\n\n-- local name = myModuleName()\nlocal name = \"IGV\"\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://software.broadinstitute.org/software/igv/, https://github.com/igvteam/igv/tags (changelog), https://github.com/igvteam/igv/ (source code)\")\nwhatis([[\nDescription: The Integrative Genomics Viewer (IGV) is a high-performance visualization tool for interactive exploration of large, integrated genomic datasets. It supports a wide variety of data types, including array-based and next-generation sequence data, and genomic annotations.\nExamples: `igv --help`, `igv --version`, and `igv`.\nWarning: IGV (&gt;= 2.7.0) requires Java 17. Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", home)\n\n-- Parse version x.y.z into x and y\nversion_x=string.gsub(version, \"[.].*$\", \"\")\nversion_xy=string.gsub(version, version_x .. \"[.]\", \"\")\nversion_y=string.gsub(version_xy, \"[.].*$\", \"\")\nversion_x=tonumber(version_x)\nversion_y=tonumber(version_y)\n\n-- Identify required Java version\nlocal min_java_version=8\nif (version_x &gt;= 2) then\n  if (version_y &gt;= 17) then\n    min_java_version=17\n  elseif (version_y &gt;= 2.5) then\n    min_java_version=11\n  end\nend\n\n\nlocal java_check = 'ver=$(java -version 2&gt;&1 | grep -F \"version\" | sed -E \"s/(.* version |\\\\\")//g\"); &gt;&2 echo \"Java version: ${ver} [IGV ' .. version .. ' requires Java ' .. min_java_version .. ' or newer]\"; ver_x=$(sed -E \"s/^1[.]//\" &lt;&lt;&lt; \"${ver}\" | sed \"s/[.].*//\"); if [[ ${ver_x} -lt ' .. min_java_version .. ' ]]; then &gt;&2 echo \"ERROR: Java ${ver_x} detected, but IGV requires Java ' .. min_java_version .. ' or newer: $(java -version 2&gt;&1 | grep -F \"version\")\"; return 1; fi;'\n\nlocal bash = java_check .. ' ' .. home .. '/igv.sh \"$@\"'\nlocal csh  = home .. '/igv.sh $*'\nset_shell_function(\"igv\", bash, csh)\n\n-- Tweak Java for the current environment\ndepends_on(\"java-tweaks\")\n\n\n\n\n\nigvtools\n\n\n\nIGVTools: Tools for Pre-processing HT-Seq Data Files The igvtools utility provides a set of tools for pre-processing data files. Example: igvtools help. Note: igvtools moved to IGV as of IGV (&gt;= 2.5.0). URL: https://software.broadinstitute.org/software/igv/igvtools, https://software.broadinstitute.org/software/igv/2.12.x#LatestVersion (changelog), https://github.com/igvteam/igv/ (source code) Versions: 2.4.19\n\n\nModule code: view\n\nhelp([[\nIGVTools: Tools for Pre-processing HT-Seq Data Files\n]])\n\n-- local name = myModuleName()\nlocal name = \"IGVTools\"\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://software.broadinstitute.org/software/igv/igvtools, https://software.broadinstitute.org/software/igv/2.12.x#LatestVersion (changelog), https://github.com/igvteam/igv/ (source code)\")\nwhatis([[\nDescription: The igvtools utility provides a set of tools for pre-processing data files.\nExamples: `igvtools help`.\nNote: `igvtools` moved to IGV as of IGV (&gt;= 2.5.0).\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\ninxi\n\n\n\ninxi: A Full Featured System Information Script Command line system information tool for console and IRC. Example: inxi –version, inxi –help, and inxi -v4 -c6. URL: https://smxi.org/docs/inxi.htm, https://smxi.org/docs/inxi-changelog.htm (changelog), https://codeberg.org/smxi/inxi (source code), https://codeberg.org/smxi/inxi/tags (releases) Warning: Only the most recent version of this software will be kept. Versions: 3.3.36-1, 3.3.37-1\n\n\nModule code: view\n\nhelp([[\ninxi: A Full Featured System Information Script\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli\")\nwhatis(\"URL: https://smxi.org/docs/inxi.htm, https://smxi.org/docs/inxi-changelog.htm (changelog), https://codeberg.org/smxi/inxi (source code), https://codeberg.org/smxi/inxi/tags (releases)\")\nwhatis([[\nDescription: Command line system information tool for console and IRC. \nExamples: `inxi --version`, `inxi --help`, and `inxi -v4 -c6`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\", \"man1\"))\n\n\n\n\n\njags\n\n\n\nJAGS: Just Another Gibbs Sampler JAGS is Just Another Gibbs Sampler. It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation not wholly unlike BUGS. Example: jags and man jags. URL: http://mcmc-jags.sourceforge.net/, https://sourceforge.net/p/mcmc-jags/code-0/ci/default/tree/NEWS (changelog), https://sourceforge.net/projects/mcmc-jags/ (source code) Versions: 4.3.1, 4.3.2\n\n\nModule code: view\n\nhelp([[\nJAGS: Just Another Gibbs Sampler\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: statistics\")\nwhatis(\"URL: http://mcmc-jags.sourceforge.net/, https://sourceforge.net/p/mcmc-jags/code-0/ci/default/tree/NEWS (changelog), https://sourceforge.net/projects/mcmc-jags/ (source code)\")\nwhatis([[\nDescription: JAGS is Just Another Gibbs Sampler.  It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation not wholly unlike BUGS.\nExamples: `jags` and `man jags`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\nlocal home = pathJoin(root, \"JAGS\" .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n-- AD HOC:\n-- R package 'rjags' uses 'JAGS_LIBDIR' and 'JAGS_INCLUDEDIR' (INSTALL)\n-- Comment: Appears not to be needed /HB 2020-03-09\n-- pushenv(\"JAGS_INCLUDEDIR\", pathJoin(home, \"include\"))\n-- pushenv(\"JAGS_LIBDIR\", pathJoin(home, \"lib\"))\n\n-- R package 'runjags' uses 'JAGS_LIB' and 'JAGS_INCLUDE' (README)\n-- Comment: Email maintainer about diff to 'rjags' /HB 2020-03-09\n-- pushenv(\"JAGS_INCLUDE\", pathJoin(home, \"include\")) -- Not needed /HB 2020-03-09\npushenv(\"JAGS_LIB\", pathJoin(home, \"lib\"))\n\n\n\n\n\njq\n\n\n\njq: Command-line JSON Processor jq is a lightweight and flexible command-line JSON processor. Example: jq –help, jq –version, cat in.json | jq ., and man jq URL: https://github.com/jqlang/jq, https://github.com/jqlang/jq/blob/master/NEWS.md (changelog), https://jqlang.github.io/jq (documentation) Warning: Only the most recent version of this software will be kept. Versions: 1.7.1\n\n\nModule code: view\n\nhelp([[\njq: Command-line JSON Processor \n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli\")\nwhatis(\"URL: https://github.com/jqlang/jq, https://github.com/jqlang/jq/blob/master/NEWS.md (changelog), https://jqlang.github.io/jq (documentation)\")\nwhatis([[\nDescription: jq is a lightweight and flexible command-line JSON processor.\nExamples: `jq --help`, `jq --version`, `cat in.json | jq .`, and `man jq`\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\n-- Run time\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\n\n-- Compile time\nprepend_path(\"CPATH\", pathJoin(home, \"include\"))\nprepend_path(\"LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n\n\n\n\nkallisto\n\n\n\nkallisto: Near-optimal RNA-Seq Quantification kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. It is based on the novel idea of pseudoalignment for rapidly determining the compatibility of reads with targets, without the need for alignment. Example: kallisto version URL: https://pachterlab.github.io/kallisto/about.html, https://github.com/pachterlab/kallisto/releases (changelog), https://github.com/pachterlab/kallisto (source code) Versions: 0.45.0, 0.45.1, 0.46.0, 0.46.1, 0.46.2, 0.50.0, 0.50.1, 0.51.1\n\n\nModule code: view\n\nhelp([[\nkallisto: Near-optimal RNA-Seq Quantification\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://pachterlab.github.io/kallisto/about.html, https://github.com/pachterlab/kallisto/releases (changelog), https://github.com/pachterlab/kallisto (source code)\")\nwhatis([[\nDescription: kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads. It is based on the novel idea of pseudoalignment for rapidly determining the compatibility of reads with targets, without the need for alignment.\nExamples: `kallisto version`\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nlibgit2\n\n\n\nlibgit2: Cross-platform, Linkable Library Implementation of Git A cross-platform, linkable library implementation of Git that you can use in your application. Example: git2 –version and git2 –help. URL: https://libgit2.org/, https://github.com/libgit2/libgit2 (source code), https://github.com/libgit2/libgit2/releases (release) Versions: 1.9.0\n\n\nModule code: view\n\nhelp(\"libgit2: Cross-platform, Linkable Library Implementation of Git\")\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: spatial, library\")\nwhatis(\"URL: https://libgit2.org/, https://github.com/libgit2/libgit2 (source code), https://github.com/libgit2/libgit2/releases (release)\")\nwhatis([[\nDescription: A cross-platform, linkable library implementation of Git that you can use in your application.\nExamples: `git2 --version` and `git2 --help`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib64\"))\n\n-- building\nprepend_path(\"PKG_CONFIG_PATH\",  pathJoin(home, \"lib64\", \"pkgconfig\"))\n\n-- prepend_path(\"CPATH\",  pathJoin(home, \"include\"))\n-- prepend_path(\"CFLAGS\", \"-I\" .. pathJoin(home, \"include\"), \" \")\n-- prepend_path(\"LDFLAGS\", \"-L\" .. pathJoin(home, \"lib\"), \" \")\n\n\n\n\n\nmarkdown-link-check\n\n\n\nmarkdown-link-check: Checks Links in Markdown Files Checks all of the hyperlinks in a markdown text to determine if they are alive or dead. Example: markdown-link-check –version, markdown-link-check –help, markdown-link-check – *.md. URL: https://github.com/tcort/markdown-link-check, https://github.com/tcort/markdown-link-check/releases (releases) Versions: 3.13.6\n\n\nModule code: view\n\nhelp([[\nmarkdown-link-check: Checks Links in Markdown Files\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, utility\")\nwhatis(\"URL: https://github.com/tcort/markdown-link-check, https://github.com/tcort/markdown-link-check/releases (releases)\")\nwhatis([[\nDescription: Checks all of the hyperlinks in a markdown text to determine if they are alive or dead.\nExamples: `markdown-link-check --version`, `markdown-link-check --help`, `markdown-link-check -- *.md`.\n]]\n)\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"node_modules\", \".bin\"))\n\n\n\n\n\nmarkdownlint-cli\n\n\n\nmarkdownlint-cli: MarkdownLint Command Line Interface Examples: markdownlint –version, markdownlint –help, markdownlint – *.md. URL: https://github.com/igorshubovych/markdownlint-cli (documentation), https://github.com/igorshubovych/markdownlint-cli/releases/ (releases), https://github.com/igorshubovych/markdownlint-cli (source code) Versions: 0.43.0\n\n\nModule code: view\n\nhelp([[\nmarkdownlint-cli: MarkdownLint Command Line Interface \n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, utility\")\nwhatis(\"URL: https://github.com/igorshubovych/markdownlint-cli (documentation), https://github.com/igorshubovych/markdownlint-cli/releases/ (releases), https://github.com/igorshubovych/markdownlint-cli (source code)\")\nwhatis([[\nDescription: \nExamples: `markdownlint --version`, `markdownlint --help`, `markdownlint -- *.md`.\n]]\n)\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"node_modules\", \".bin\"))\n\n\n\n\n\nmc\n\n\n\nmc: Midnight Commander GNU Midnight Commander is a visual file manager. It’s a feature rich full-screen text mode application that allows you to copy, move and delete files and whole directory trees, search for files and run commands in the subshell. Internal viewer and editor are included. Example: mc and mc –version. URL: https://midnight-commander.org/, https://github.com/MidnightCommander/mc/blob/master/doc/NEWS (changelog), https://github.com/MidnightCommander/mc (source code), https://github.com/MidnightCommander/mc/tags (download) Warning: Only the most recent version of this software will be kept. Versions: 4.8.33\n\n\nModule code: view\n\nhelp([[\nmc: Midnight Commander\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, files, cli, tui\")\nwhatis(\"URL: https://midnight-commander.org/, https://github.com/MidnightCommander/mc/blob/master/doc/NEWS (changelog), https://github.com/MidnightCommander/mc (source code), https://github.com/MidnightCommander/mc/tags (download)\")\nwhatis([[\nDescription: GNU Midnight Commander is a visual file manager. It's a feature rich full-screen text mode application that allows you to copy, move and delete files and whole directory trees, search for files and run commands in the subshell. Internal viewer and editor are included.\nExamples: `mc` and `mc --version`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\nminiconda3-py39\n\n\n\n[DEFUNCT] Miniconda: A Free Minimal Installer for Conda Versions: 4.12.0\n\n\nModule code: view\n\nhelp([[\n[DEFUNCT] Miniconda: A Free Minimal Installer for Conda\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nlocal new = \"miniconda3/\" .. version .. \"-py39\"\n\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: deprecated, defunct\")\nwhatis([[\nWarning: This module is defunct and has been superseeded by module 'miniconda3'. Please use that module instead.\n]])\n\ndepends_on(new)\nLmodError(\"[DEFUNCT ERROR] The CBI '\" .. name .. \"/\" .. version .. \"' module has been renamed to '\" .. new .. \"'; please use that module instead\")\n\n\n\n\n\nminiconda3\n\n\n\nMiniconda: A Free Minimal Installer for Conda Miniconda is a free minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages, including pip, zlib and a few others. Example: conda –version, conda create –name=myenv, conda env list, conda activate myenv, conda info, and conda deactive. URL: https://docs.conda.io/en/latest/, https://docs.conda.io/en/latest/miniconda.html (documentation), https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links (releases), https://github.com/conda/conda/blob/master/CHANGELOG.md (changelog), https://github.com/conda/conda (source code) Warning: This module is deprecated as of 2024-08-21, because of Anconda Inc. license issues. Please use the miniforge3 module instead. Also, this module works only in Bash. Also, do not do conda init. If you do this by mistake, please undo by conda init –reverse. Versions: 4.12.0-py39, 22.11.1-1-py310, 23.3.1-0-py39, 23.5.2-0-py39, 23.5.2-0-py310, 23.5.2-0-py311, 23.11.0-2-py311, 24.1.2-0-py312, 24.3.0-0-py312, 24.5.0-0-py312\n\n\nModule code: view\n\nhelp([[\n[DEPRECATED] Miniconda: A Free Minimal Installer for Conda\n]])\n\nlocal warning = \"Use at your own peril! Software tools installed via Conda are known to cause conflicts with other software on the system, including core software provided by the operating system as well as other software from the CBI stack. For example, do not install R packages running R from the CBI stack, while conda is activated.\"\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: files, utility\")\nwhatis(\"URL: https://docs.conda.io/en/latest/, https://docs.conda.io/en/latest/miniconda.html (documentation), https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links (releases), https://github.com/conda/conda/blob/master/CHANGELOG.md (changelog), https://github.com/conda/conda (source code)\")\nwhatis([[\nDescription: Miniconda is a free minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages, including pip, zlib and a few others.\nExamples: `conda --version`, `conda create --name=myenv`, `conda env list`, `conda activate myenv`, `conda info`, and `conda deactive`.\nWarning: _This module is deprecated as of 2024-08-21, because of Anconda Inc. license issues. Please use the `miniforge3` module instead._ Also, this module works only in Bash. Also, do _not_ do `conda init`. If you do this by mistake, please undo by `conda init --reverse`.\n]])\n\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n-- Miniconda (&gt;= 23.11.0) [2023-11-30]:\n-- Prevent conda from add shortcuts to user's Desktop.\npushenv(\"CONDA_SHORTCUTS\", \"false\")\n\nif mode() == \"load\" then\n  -- ASSERT: Make sure there is no other active conda environment\n  if os.getenv(\"CONDA_EXE\") then\n    LmodError(\"Cannot load \" .. name .. \" module, because another conda installation is already enabled (detected environment variable CONDA_EXE='\" .. os.getenv(\"CONDA_EXE\") .. \"'). Have you installed conda on your own? If so, run 'conda config --set auto_activate_base false' and then log out and log back in again.  If that is not sufficient, please run 'conda init --reverse' and log out and back in again.\")\n  end\n\n  pushenv(\"CONDA_EXE\", pathJoin(home, \"bin\", \"conda\"))\n  pushenv(\"CONDA_PYTHON_EXE\", pathJoin(home, \"bin\", \"python\"))\n  pushenv(\"_CE_M\", \"\")\n  pushenv(\"_CE_CONDA\", \"\")\nelseif mode() == \"unload\" then\n  pushenv(\"CONDA_EXE\", \"false\")\n  pushenv(\"CONDA_PYTHON_EXE\", \"false\")\n  pushenv(\"_CE_M\", \"false\")\n  pushenv(\"_CE_CONDA\", \"false\")\nend\n\n\nif mode() == \"load\" then\n  LmodWarning(\"[DEPRECATED] The CBI '\" .. name .. \"/\" .. version .. \"' module is deprecated as of 2024-08-21, and it will eventually become defunct. Please use 'miniforge3' instead. The reason for this is license issues introduced by Anconda Inc.\")\nend\n\n\n\n\n\n\n-- Don't edit! Created using: \n-- /usr/share/lmod/lmod/libexec/sh_to_modulefile /wynton/home/cbi/shared/software/CBI/miniconda3-24.5.0-0-py312/etc/profile.d/conda.sh\npushenv(\"CONDA_EXE\",\"/wynton/home/cbi/shared/software/CBI/miniconda3-24.5.0-0-py312/bin/conda\")\npushenv(\"CONDA_PYTHON_EXE\",\"/wynton/home/cbi/shared/software/CBI/miniconda3-24.5.0-0-py312/bin/python\")\npushenv(\"CONDA_SHLVL\",\"0\")\nprepend_path(\"PATH\",\"/wynton/home/cbi/shared/software/CBI/miniconda3-24.5.0-0-py312/condabin\")\npushenv(\"_CE_CONDA\",\"\")\npushenv(\"_CE_M\",\"\")\nset_shell_function(\"__conda_activate\",\" \\\n    if [ -n \\\"${CONDA_PS1_BACKUP:+x}\\\" ]; then\\\n        PS1=\\\"$CONDA_PS1_BACKUP\\\";\\\n        \\\\unset CONDA_PS1_BACKUP;\\\n    fi;\\\n    \\\\local ask_conda;\\\n    ask_conda=\\\"$(PS1=\\\"${PS1:-}\\\" __conda_exe shell.posix \\\"$@\\\")\\\" || \\\\return;\\\n    \\\\eval \\\"$ask_conda\\\";\\\n    __conda_hashr\\\n\",\"\")\nset_shell_function(\"__conda_exe\",\" \\\n    ( \\\"$CONDA_EXE\\\" $_CE_M $_CE_CONDA \\\"$@\\\" )\\\n\",\"\")\nset_shell_function(\"__conda_hashr\",\" \\\n    if [ -n \\\"${ZSH_VERSION:+x}\\\" ]; then\\\n        \\\\rehash;\\\n    else\\\n        if [ -n \\\"${POSH_VERSION:+x}\\\" ]; then\\\n            :;\\\n        else\\\n            \\\\hash -r;\\\n        fi;\\\n    fi\\\n\",\"\")\nset_shell_function(\"__conda_reactivate\",\" \\\n    \\\\local ask_conda;\\\n    ask_conda=\\\"$(PS1=\\\"${PS1:-}\\\" __conda_exe shell.posix reactivate)\\\" || \\\\return;\\\n    \\\\eval \\\"$ask_conda\\\";\\\n    __conda_hashr\\\n\",\"\")\nset_shell_function(\"conda\",\" \\\n    \\\\local cmd=\\\"${1-__missing__}\\\";\\\n    case \\\"$cmd\\\" in \\\n        activate | deactivate)\\\n            __conda_activate \\\"$@\\\"\\\n        ;;\\\n        install | update | upgrade | remove | uninstall)\\\n            __conda_exe \\\"$@\\\" || \\\\return;\\\n            __conda_reactivate\\\n        ;;\\\n        *)\\\n            __conda_exe \\\"$@\\\"\\\n        ;;\\\n    esac\\\n\",\"\")\n\n\n\n\n\nminiforge3\n\n\n\nMiniforge: A Free Minimal Installer for Conda Miniforge is a free minimal installer for conda. Miniforge is a community effort to provide Miniconda-like installers, with the added feature that conda-forge is the default channel. Example: conda –version, conda create –name=myenv, conda env list, conda activate myenv, conda info, and conda deactive. URL: https://conda-forge.org/, https://conda-forge.org/docs/user/introduction/ (documentation), https://github.com/conda-forge/miniforge/releases (releases), https://github.com/conda/conda/blob/master/CHANGELOG.md (changelog), https://github.com/conda/conda (source code) Warning: For now, this module works only in Bash. Also, do not do conda init. If you do this by mistake, please undo by conda init –reverse. Versions: 24.3.0-0, 24.7.1-0, 24.9.0-0, 24.9.2-0, 24.11.0-0, 24.11.2-1\n\n\nModule code: view\n\nhelp([[\nMiniforge: A Free Minimal Installer for Conda\n]])\n\nlocal warning = \"Use at your own peril! Software tools installed via Conda are known to cause conflicts with other software on the system, including core software provided by the operating system as well as other software from the CBI stack. For example, do not install R packages running R from the CBI stack, while conda is activated.\"\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: files, utility\")\nwhatis(\"URL: https://conda-forge.org/, https://conda-forge.org/docs/user/introduction/ (documentation), https://github.com/conda-forge/miniforge/releases (releases),  https://github.com/conda/conda/blob/master/CHANGELOG.md (changelog), https://github.com/conda/conda (source code)\")\nwhatis([[\nDescription: Miniforge is a free minimal installer for conda. Miniforge is a community effort to provide Miniconda-like installers, with the added feature that conda-forge is the default channel.\nExamples: `conda --version`, `conda create --name=myenv`, `conda env list`, `conda activate myenv`, `conda info`, and `conda deactive`.\nWarning: For now, this module works only in Bash. Also, do _not_ do `conda init`. If you do this by mistake, please undo by `conda init --reverse`.\n]])\n\nconflict(\"miniconda3\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n-- Miniconda (&gt;= 23.11.0) [2023-11-30]:\n-- Prevent conda from add shortcuts to user's Desktop.\npushenv(\"CONDA_SHORTCUTS\", \"false\")\n\nif mode() == \"load\" then\n  -- ASSERT: Make sure there is no other active conda environment\n  if os.getenv(\"CONDA_EXE\") then\n    LmodError(\"Cannot load \" .. name .. \" module, because another conda installation is already enabled (detected environment variable CONDA_EXE='\" .. os.getenv(\"CONDA_EXE\") .. \"'). Have you installed conda on your own? If so, run 'conda config --set auto_activate_base false' and then log out and log back in again.  If that is not sufficient, please run 'conda init --reverse' and log out and back in again.\")\n  end\n\n  pushenv(\"CONDA_EXE\", pathJoin(home, \"bin\", \"conda\"))\n  pushenv(\"CONDA_PYTHON_EXE\", pathJoin(home, \"bin\", \"python\"))\n  pushenv(\"_CE_M\", \"\")\n  pushenv(\"_CE_CONDA\", \"\")\nelseif mode() == \"unload\" then\n  pushenv(\"CONDA_EXE\", \"false\")\n  pushenv(\"CONDA_PYTHON_EXE\", \"false\")\n  pushenv(\"_CE_M\", \"false\")\n  pushenv(\"_CE_CONDA\", \"false\")\nend\n-- Don't edit! Created using: \n-- /usr/share/lmod/lmod/libexec/sh_to_modulefile /wynton/home/cbi/shared/software/CBI/miniforge3-24.11.2-1/etc/profile.d/conda.sh\npushenv(\"CONDA_EXE\",\"/wynton/home/cbi/shared/software/CBI/miniforge3-24.11.2-1/bin/conda\")\npushenv(\"CONDA_PYTHON_EXE\",\"/wynton/home/cbi/shared/software/CBI/miniforge3-24.11.2-1/bin/python\")\npushenv(\"CONDA_SHLVL\",\"0\")\nprepend_path(\"PATH\",\"/wynton/home/cbi/shared/software/CBI/miniforge3-24.11.2-1/condabin\")\npushenv(\"_CE_CONDA\",\"\")\npushenv(\"_CE_M\",\"\")\nset_shell_function(\"__conda_activate\",\" \\\n    if [ -n \\\"${CONDA_PS1_BACKUP:+x}\\\" ]; then\\\n        PS1=\\\"$CONDA_PS1_BACKUP\\\";\\\n        \\\\unset CONDA_PS1_BACKUP;\\\n    fi;\\\n    \\\\local ask_conda;\\\n    ask_conda=\\\"$(PS1=\\\"${PS1:-}\\\" __conda_exe shell.posix \\\"$@\\\")\\\" || \\\\return;\\\n    \\\\eval \\\"$ask_conda\\\";\\\n    __conda_hashr\\\n\",\"\")\nset_shell_function(\"__conda_exe\",\" \\\n    ( \\\"$CONDA_EXE\\\" $_CE_M $_CE_CONDA \\\"$@\\\" )\\\n\",\"\")\nset_shell_function(\"__conda_hashr\",\" \\\n    if [ -n \\\"${ZSH_VERSION:+x}\\\" ]; then\\\n        \\\\rehash;\\\n    else\\\n        if [ -n \\\"${POSH_VERSION:+x}\\\" ]; then\\\n            :;\\\n        else\\\n            \\\\hash -r;\\\n        fi;\\\n    fi\\\n\",\"\")\nset_shell_function(\"__conda_reactivate\",\" \\\n    echo \\\"'__conda_reactivate' is deprecated and will be removed in 25.9. Use '__conda_activate reactivate' instead.\\\" 1&gt;&2;\\\n    __conda_activate reactivate\\\n\",\"\")\nset_shell_function(\"conda\",\" \\\n    \\\\local cmd=\\\"${1-__missing__}\\\";\\\n    case \\\"$cmd\\\" in \\\n        activate | deactivate)\\\n            __conda_activate \\\"$@\\\"\\\n        ;;\\\n        install | update | upgrade | remove | uninstall)\\\n            __conda_exe \\\"$@\\\" || \\\\return;\\\n            __conda_activate reactivate\\\n        ;;\\\n        *)\\\n            __conda_exe \\\"$@\\\"\\\n        ;;\\\n    esac\\\n\",\"\")\n\n\n\n\n\nmutect\n\n\n\nmuTect: Identification of Somatic Point Mutations in Next Generation Sequencing Data of Cancer Genomes MuTect is a method developed at the Broad Institute for the reliable and accurate identification of somatic point mutations in next generation sequencing data of cancer genomes. Example: mutect, which is short for java -Xmx2g -jar \"$MUTECT_JAR\". URL: https://software.broadinstitute.org/cancer/cga/mutect, https://github.com/broadinstitute/mutect (source code) Requirement: MuTect (&lt;= 1.1) requires Java v1.7 and will not work with any other Java versions. Versions: 1.1.1, 1.1.4, 1.1.5\n\n\nModule code: view\n\nhelp([[\nmuTect: Identification of Somatic Point Mutations in Next Generation Sequencing Data of Cancer Genomes\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, genome\")\nwhatis(\"URL: https://software.broadinstitute.org/cancer/cga/mutect, https://github.com/broadinstitute/mutect (source code)\")\nwhatis([[\nDescription: MuTect is a method developed at the Broad Institute for the reliable and accurate identification of somatic point mutations in next generation sequencing data of cancer genomes.\nExamples: `mutect`, which is short for `java -Xmx2g -jar \"$MUTECT_JAR\"`.\nRequirements: MuTect (&lt;= 1.1) requires Java v1.7 and will not work with any other Java versions.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nif (version == \"1.0.27783\") then\n  -- muTect 1.0.27783 requires Java (&lt;= 1.7)\n  local cluster = os.getenv(\"CLUSTER\")\n  if (cluster == \"tipcc\") then\n    load(\"jdk/1.7.0\")\n  else\n--    depends_on(\"openjdk/1.6.0\")\n  end\nend\n\n-- Validate proper Java version for older versions of MuTect\nif string.match(version, \"^1[.][01]\") then\n  local bfr = capture(\"java -version 2&gt;&1\")\n  for line in string.gmatch(bfr, \"[^\\n]+\") do\n    if string.match(line, \"version\") then\n      if not string.match(line, \"1[.]7\") then\n        LmodWarning(myModuleFullName() .. \" requires Java v1.7, but you are running \" .. line)\n      end\n    end\n  end\nend\n\n\nname = \"muTect\"\npushenv(\"MUTECT_HOME\", home)\nlocal jarfile = name .. \"-\" .. version .. \".jar\"\npushenv(\"MUTECT_JAR\", pathJoin(home, jarfile))\n\nlocal bash = 'java -Xmx2g -jar \"$MUTECT_HOME/' .. jarfile .. '\" \"$@\"'\nlocal csh  = 'java -Xmx2g -jar \"$MUTECT_HOME/' .. jarfile .. '\" $*'\nset_shell_function(\"mutect\", bash, csh)\n\n-- Tweak Java for the current environment\ndepends_on(\"java-tweaks\")\n\n\n\n\n\nncdu\n\n\n\nncdu: NCurses Disk Usage Ncdu is a disk usage analyzer with an ncurses interface. It is designed to find space hogs on a remote server where you don’t have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple and easy to use, and should be able to run in any minimal POSIX-like environment with ncurses installed. Example: ncdu –version, ncdu –help, and ncdu. For large cleanup tasks, call ncdu –one-file-system -o ncdu.cache once to scan all files, and then use ncdu –enable-delete -f ncdu.cache to clean them out. URL: https://dev.yorhel.nl/ncdu, https://dev.yorhel.nl/ncdu/man2 (documentation), https://dev.yorhel.nl/ncdu/changes2 (changelog), https://code.blicky.net/yorhel/ncdu/ (source code) Versions: 2.7, 2.9.1\n\n\nModule code: view\n\nhelp([[\nncdu: NCurses Disk Usage\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli, files\")\nwhatis(\"URL: https://dev.yorhel.nl/ncdu, https://dev.yorhel.nl/ncdu/man2 (documentation), https://dev.yorhel.nl/ncdu/changes2 (changelog), https://code.blicky.net/yorhel/ncdu/ (source code)\")\nwhatis([[\nDescription: Ncdu is a disk usage analyzer with an ncurses interface. It is designed to find space hogs on a remote server where you don’t have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple and easy to use, and should be able to run in any minimal POSIX-like environment with ncurses installed.\nExamples: `ncdu --version`, `ncdu --help`, and `ncdu`. For large cleanup tasks, call `ncdu --one-file-system -o ncdu.cache` once to scan all files, and then use `ncdu --enable-delete -f ncdu.cache` to clean them out.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\npandoc\n\n\n\nPandoc: A Universal Document Converter Pandoc is a Haskell library and software tool for converting from one markup format to another, and a command-line tool that uses this library. Example: pandoc –version. URL: https://pandoc.org/, https://pandoc.org/releases.html (changelog), https://github.com/jgm/pandoc (source code) Warning: Only the most recent version of this software will be kept. Versions: 3.6.2, 3.8.2.1\n\n\nModule code: view\n\nhelp([[\nPandoc: A Universal Document Converter\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, shell\")\nwhatis(\"URL: https://pandoc.org/, https://pandoc.org/releases.html (changelog), https://github.com/jgm/pandoc (source code)\")\nwhatis([[\nDescription: Pandoc is a Haskell library and software tool for converting from one markup format to another, and a command-line tool that uses this library.\nExamples: `pandoc --version`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\npdfcrop\n\n\n\nPDFCrop: Crop and Rescale PDFs PDFCrop crops the white margins of PDF pages and rescales them to fit a standard size sheet of paper. It makes printed pages much easier to read. PDFCrop is particularly useful for resizing journal articles and converting between letter-size and A4 paper. Example: pdfcrop –help and pdfcrop A4 input.pdf. URL: https://www.ctan.org/tex-archive/support/pdfcrop, https://github.com/ho-tex/pdfcrop (source code), https://github.com/ho-tex/pdfcrop/tags/ (changelog) Versions: 0.4b, 1.42\n\n\nModule code: view\n\nhelp([[\nPDFCrop: Crop and Rescale PDFs\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, tool\")\nwhatis(\"URL: https://www.ctan.org/tex-archive/support/pdfcrop, https://github.com/ho-tex/pdfcrop (source code), https://github.com/ho-tex/pdfcrop/tags/ (changelog)\")\nwhatis([[\nDescription: PDFCrop crops the white margins of PDF pages and rescales them to fit a standard size sheet of paper. It makes printed pages much easier to read. PDFCrop is particularly useful for resizing journal articles and converting between letter-size and A4 paper.\nExamples: `pdfcrop --help` and `pdfcrop A4 input.pdf`.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\npicard\n\n\n\nPicard: Command-line tools for Manipulating High-throughput Sequencing Data and Formats Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF. Example: PicardCommandLine -h, which is short for java -jar \"$PICARD_HOME/picard.jar\" -h. URL: https://broadinstitute.github.io/picard/, https://github.com/broadinstitute/picard/releases (changelog), https://github.com/broadinstitute/picard (source code) Warning: The old picard alias is deprecated. Use function PicardCommandLine instead. Picard 3 requires Java 17, Picard 2 requires Java 1.8. Versions: 2.21.1, 2.21.4, 2.22.2, 2.23.1, 2.24.0, 2.26.2, 2.26.10, 2.27.1, 2.27.4, 2.27.5, 3.1.1, 3.2.0, 3.3.0\n\n\nModule code: view\n\nhelp([[\nPicard: Command-Line Tools for Manipulating High-throughput Sequencing Data and Formats\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://broadinstitute.github.io/picard/, https://github.com/broadinstitute/picard/releases (changelog), https://github.com/broadinstitute/picard (source code)\")\nwhatis([[\nDescription: Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.\nExamples: `PicardCommandLine -h`, which is short for `java -jar \"$PICARD_HOME/picard.jar\" -h`.\nWarning: The old `picard` alias is deprecated. Use function `PicardCommandLine` instead. Picard 3 requires Java 17, Picard 2 requires Java 1.8.\n]])\n\nlocal version_x = string.gsub(version, \"[.].*\", \"\")\nif (version_x == \"1\") then\n  -- Pindel 1.64 requires Java (&lt;= 1.6)\n  depends_on(\"openjdk/1.6.0\")\nelseif (version_x == \"2\") then\n  -- As of version 2.0.1 (Nov. 2015) Picard requires Java 1.8 (jdk8u66)\n  depends_on(\"openjdk/1.8.0\")\nelseif (version_x == \"3\") then\n  -- As of version 3.0.0 (Feb. 2023) Picard requires Java 17\n  depends_on(\"openjdk/17\")\nend\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\npushenv(\"PICARD_HOME\", home)\n\n-- Functions\nlocal bash = 'java -jar \"$PICARD_HOME/picard.jar\" \"$@\"'\nlocal csh  = 'java -jar \"$PICARD_HOME/picard.jar\" $*'\nset_shell_function(\"PicardCommandLine\", bash, csh)\n\n-- Aliases (deprecated)\nset_alias(\"picard\", \"java -jar \\\"$PICARD_HOME/picard.jar\\\"\")\n\n-- Tweak Java for the current environment\ndepends_on(\"java-tweaks\")\n\n\n\n\n\npindel\n\n\n\npindel: Detection of Indels and Structural Variations Pindel can detect breakpoints of large deletions, medium sized insertions, inversions, tandem duplications and other structural variants at single-based resolution from next-gen sequence data. It uses a pattern growth approach to identify the breakpoints of these variants from paired-end short reads. Example: pindel and pindel –help. URL: https://www.sanger.ac.uk/science/tools/pindel, https://github.com/genome/pindel/tags (changelog), https://github.com/genome/pindel (source code) Versions: 0.2.5b8\n\n\nModule code: view\n\nhelp([[\npindel: Detection of Indels and Structural Variations\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://www.sanger.ac.uk/science/tools/pindel, https://github.com/genome/pindel/tags (changelog), https://github.com/genome/pindel (source code)\")\nwhatis([[\nDescription: Pindel can detect breakpoints of large deletions, medium sized insertions, inversions, tandem duplications and other structural variants at single-based resolution from next-gen sequence data. It uses a pattern growth approach to identify the breakpoints of these variants from paired-end short reads.\nExamples: `pindel` and `pindel --help`.\n]])\n\nif (version == \"0.2.4t\") then\n  load(\"samtools/0.1.18\")\nelse\n  load(\"htslib\")\nend\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\nplink\n\n\n\nPLINK: Whole Genome Association Analysis Toolset PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data). Example: plink –help and plink –version. URL: https://www.cog-genomics.org/plink/ Warning: This tool runs on all available CPU cores by default, which is bad practice. Please specify option –threads ncores to avoid this, e.g. –threads 1. Versions: 1.07, 1.90b6.10, 1.90b6.16, 1.90b6.18, 1.90b6.21, 1.90b6.24, 1.90b6.25, 1.90b6.26\n\n\nModule code: view\n\nhelp([[\nPLINK: Whole Genome Association Analysis Toolset\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: genomics\")\nwhatis(\"URL: https://www.cog-genomics.org/plink/\")\nwhatis([[\nDescription: PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data).\nExamples: `plink --help` and `plink --version`.\nWarning: This tool runs on all available CPU cores by default, which is bad practice. Please specify option `--threads ncores` to avoid this, e.g. `--threads 1`.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nplink2\n\n\n\nPLINK2: Whole Genome Association Analysis Toolset PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data). Example: plink2 –help, plink2 –version, and plink2 –threads 1 …. URL: https://www.cog-genomics.org/plink/2.0/, https://www.cog-genomics.org/plink/2.0/general_usage (documentation), https://www.cog-genomics.org/plink/2.0/#recent (changelog), https://github.com/chrchang/plink-ng (source code) Warning: This tool runs on all available CPU cores by default, which is bad practice. Please specify option –threads ncores to avoid this, e.g. –threads 1. Versions: 2.00a2LM, 2.00a3LM, 2.00a5.14, 2.0.0-a.5.15, 2.0.0-a.6.1, 2.0.0-a.6.4, 2.0.0-a.6.9, 2.0.0-a.6.21\n\n\nModule code: view\n\nhelp([[\nPLINK2: Whole Genome Association Analysis Toolset\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: genomics\")\nwhatis(\"URL: https://www.cog-genomics.org/plink/2.0/, https://www.cog-genomics.org/plink/2.0/general_usage (documentation), https://www.cog-genomics.org/plink/2.0/#recent (changelog), https://github.com/chrchang/plink-ng (source code)\")\nwhatis([[\nDescription: PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data).\nExamples: `plink2 --help`, `plink2 --version`, and `plink2 --threads 1 ...`.\nWarning: This tool runs on all available CPU cores by default, which is bad practice. Please specify option `--threads ncores` to avoid this, e.g. `--threads 1`.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nport4me\n\n\n\nport4me: Get the Same, Personal, Free TCP Port over and over ‘port4me’ attempts, with high probability, to provide the user with the same, free port each time, even when used on different days. Example: port4me –help, port4me, port4me –tool=jupyter). URL: https://github.com/HenrikBengtsson/port4me, https://github.com/HenrikBengtsson/port4me/blob/develop/bash/NEWS.md (changelog) Warning: This tool is under development. Versions: 0.7.1\n\n\nModule code: view\n\nhelp([[\nport4me:  Get the Same, Personal, Free TCP Port over and over\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli\")\nwhatis(\"URL: https://github.com/HenrikBengtsson/port4me, https://github.com/HenrikBengtsson/port4me/blob/develop/bash/NEWS.md (changelog)\")\nwhatis([[\nDescription: 'port4me' attempts, with high probability, to provide the user with the same, free port each time, even when used on different days.\nExamples: `port4me --help`, `port4me`, `port4me --tool=jupyter`).\nWarning: This tool is under development.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\nprocs\n\n\n\nprocs: A Modern Replacement for ‘ps’ written in Rust An alternative to ps, pstree, and top. Example: procs –version, procs –help, procs, procs –tree, and procs –watch –nand root. URL: https://github.com/dalance/procs/, https://github.com/dalance/procs/blob/master/CHANGELOG.md (changelog) Warning: Only the most recent version of this software will be kept. Versions: 0.14.9\n\n\nModule code: view\n\nhelp([[\nprocs: A Modern Replacement for 'ps' written in Rust\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli, tui\")\nwhatis(\"URL: https://github.com/dalance/procs/, https://github.com/dalance/procs/blob/master/CHANGELOG.md (changelog)\")\nwhatis([[\nDescription: An alternative to `ps`, `pstree`, and `top`.\nExamples: `procs --version`, `procs --help`, `procs`, `procs --tree`, and `procs --watch --nand root`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nproj\n\n\n\nPROJ: PROJ Coordinate Transformation Software Library PROJ is a generic coordinate transformation software that transforms geospatial coordinates from one coordinate reference system (CRS) to another. This includes cartographic projections as well as geodetic transformations. PROJ includes command line applications for easy conversion of coordinates from text files or directly from user input. In addition to the command line utilities PROJ also exposes an application programming interface, or API in short. The API lets developers use the functionality of PROJ in their own software without having to implement similar functionality themselves. Example: geod, proj and man proj. URL: https://proj.org/ Versions: 4.9.3\n\n\nModule code: view\n\nhelp([[\nPROJ: PROJ Coordinate Transformation Software Library\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: statistics, spatial\")\nwhatis(\"URL: https://proj.org/\")\nwhatis(\"Description: PROJ is a generic coordinate transformation software that transforms geospatial coordinates from one coordinate reference system (CRS) to another. This includes cartographic projections as well as geodetic transformations. PROJ includes command line applications for easy conversion of coordinates from text files or directly from user input. In addition to the command line utilities PROJ also exposes an application programming interface, or API in short. The API lets developers use the functionality of PROJ in their own software without having to implement similar functionality themselves. Example: `geod`, `proj` and `man proj`.\")\n\nif (version &gt;= \"7.2.0\") then\n  depends_on(\"sqlite\")\nend\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n\n-- From 'make install':\n-- If you ever happen to want to link against installed libraries\n-- in a given directory, LIBDIR, you must either use libtool, and\n-- specify the full pathname of the library, or use the '-LLIBDIR'\n-- flag during linking and do at least one of the following:\n--    - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable\n--      during execution\n--    - add LIBDIR to the 'LD_RUN_PATH' environment variable\n--      during linking\n--    - use the '-Wl,-rpath -Wl,LIBDIR' linker flag\n--    - have your system administrator add LIBDIR to '/etc/ld.so.conf'\n\n\n\n\n\nqtop\n\n\n\nqtop: Monitor the State of Queueing Systems, Along with Related Information Relevant on HPC & Grid Clusters qtop (pronounced queue-top) is a tool written in order to monitor the state of Queueing Systems, along with related information relevant on HPC & grid clusters. At present it supports PBS, SGE & OAR families. Please help to increase that list in the Python version of the tool, qtop.py! Example: qtop, qtop –help, qtop –version, and qtop -FGW. URL: https://github.com/qtop/qtop, https://github.com/qtop/qtop/blob/master/CHANGELOG.rst (changelog) Warning: Only the most recent version of this software will be kept. Versions: 0.9.20161222\n\n\nModule code: view\n\nhelp([[\nqtop: Monitor the State of Queueing Systems, Along with Related Information Relevant on HPC & Grid Clusters\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: system, utility, cli, tui\")\nwhatis(\"URL: https://github.com/qtop/qtop, https://github.com/qtop/qtop/blob/master/CHANGELOG.rst (changelog)\")\nwhatis([[\nDescription: `qtop` (pronounced queue-top) is a tool written in order to monitor the state of Queueing Systems, along with related information relevant on HPC & grid clusters. At present it supports PBS, SGE & OAR families. Please help to increase that list in the Python version of the tool, qtop.py!\nExamples: `qtop`, `qtop --help`, `qtop --version`, and `qtop -FGW`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nappend_path(\"PYTHONPATH\", home)\n\n\n\n\n\nquarto\n\n\n\nquarto-cli: Open-Source Scientific and Technical Publishing System Built on Pandoc Quarto is an open-source scientific and technical publishing system built on Pandoc; (i) Create dynamic content with Python, R, Julia, and Observable, (ii) Author documents as plain text markdown or Jupyter notebooks, (iii) Publish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more, (iv) Author with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more. Example: quarto –version and quarto –help. URL: https://quarto.org/, https://quarto.org/docs/guide/ (documentation), https://github.com/quarto-dev/quarto-cli/releases (changelog), https://github.com/quarto-dev/quarto-cli/ (source code) Warning: Only the most recent version of this software will be kept. Versions: 1.6.42, 1.8.25\n\n\nModule code: view\n\nhelp([[\nquarto-cli: Open-Source Scientific and Technical Publishing System Built on Pandoc\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: markdown\")\nwhatis(\"URL: https://quarto.org/, https://quarto.org/docs/guide/ (documentation), https://github.com/quarto-dev/quarto-cli/releases (changelog), https://github.com/quarto-dev/quarto-cli/ (source code)\")\nwhatis([[\nDescription: Quarto is an open-source scientific and technical publishing system built on Pandoc; (i) Create dynamic content with Python, R, Julia, and Observable, (ii) Author documents as plain text markdown or Jupyter notebooks, (iii) Publish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more, (iv) Author with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\nExamples: `quarto --version` and `quarto --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\ndepends_on(\"pandoc\")\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\nr\n\n\n\nR: The R Programming Language The R programming language. Example: R, R –version, and Rscript –version. URL: https://www.r-project.org/, https://cran.r-project.org/doc/manuals/r-release/NEWS.html (changelog) Warning: Because Conda negatively affects this software tool, Conda must be deactived in order to load this module. Versions: 3.0.0, 3.0.3, 3.1.0, 3.1.3, 3.2.0, 3.2.5, 3.3.0, 3.3.3, 3.4.0, 3.4.4, 3.5.0, 3.5.3, 3.6.0, 3.6.3, 4.0.0, 4.0.5, 4.1.0, 4.1.3, 4.2.0-gcc10, 4.2.3-gcc10, 4.3.0-gcc10, 4.3.1-gcc10, 4.3.2-gcc10, 4.3.3-gcc10, 4.4.0-gcc13, 4.4.1-gcc13, 4.4.2-gcc13, 4.4.3-gcc13, 4.5.0-gcc13, 4.5.1-gcc13, 4.5.2-gcc13\n\n\nModule code: view\n\nhelp([[\nR: The R Programming Language\n]])\n\nlocal name = myModuleName()\nlocal version = \"4.4.0-gcc13\"\nversion = string.gsub(version, \"^[.]\", \"\") -- for hidden modules\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: Programming, Statistics\")\nwhatis(\"URL: https://www.r-project.org/, https://cran.r-project.org/doc/manuals/r-release/NEWS.html (changelog)\")\nwhatis([[\nDescription: The R programming language.\nExamples: `R`, `R --version`, and `Rscript --version`.\n]])\n\nhas_devtoolset = function(version)\n  local path = pathJoin(\"/opt\", \"rh\", \"devtoolset-\" .. version)\n  return(isDir(path))\nend\n\nhas_gcc_toolset = function(version)\n  local path = pathJoin(\"/opt\", \"rh\", \"gcc-toolset-\" .. version)\n  return(isDir(path))\nend\n\nlocal name = \"R\"\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\nlocal path = pathJoin(home, \"lib\")\nif not isDir(path) then\n  path = pathJoin(home, \"lib64\")\nend\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(path, \"R\", \"lib\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\nlocal v = version\nv = string.gsub(v, \"-.*\", \"\")\n\n-- WORKAROUND: R 3.6.0 is not compatible with BeeGFS\nif v == \"3.6.0\" then\n  pushenv(\"R_INSTALL_STAGED\", \"false\")\nelse\n  pushenv(\"R_INSTALL_STAGED\", \"true\")\nend\n\nlocal r_libs_user\nif os.getenv(\"CBI_LINUX\") == \"centos7\" then\n  r_libs_user=\"~/R/%p-library/%v-CBI\"\nelse\n  r_libs_user=\"~/R/\" .. os.getenv(\"CBI_LINUX\") .. \"-\" .. \"%p-library/%v-CBI\"\nend\n\nif (v &gt;= \"4.1.0\") then\n  local gv = string.gsub(version, v, \"\")\n  gv = string.gsub(gv, \"-alpha\", \"\")\n  gv = string.gsub(gv, \"-beta\", \"\")\n  gv = string.gsub(gv, \"-rc\", \"\")\n  gv = string.gsub(gv, \"-gcc\", \"\")\n  if (gv ~= \"\") then                                                                                           \n    gv = tonumber(gv)\n    if (gv &gt; 4) then\n      r_libs_user = r_libs_user .. \"-gcc\" .. gv\n      if has_devtoolset(gv) then\n        depends_on(\"scl-devtoolset/\" .. gv)\n      elseif has_gcc_toolset(gv) then\n        depends_on(\"scl-gcc-toolset/\" .. gv)\n      end\n    end\n  end\nend\n\n-- Avoid R CMD build warning on \"invalid uid value replaced by that for user 'nobody'\"\n-- https://stackoverflow.com/questions/30599326\npushenv(\"R_BUILD_TAR\", \"tar\")\n\n-- In-house env var for R repositories mirrored locally\nlocal r_repos_root = os.getenv(\"CBI_SHARED_ROOT\")\nif (r_repos_root) then\n  LmodMessage(\"r_repos_root=\" .. r_repos_root)\n  r_repos_root = pathJoin(r_repos_root, \"mirrors\", \"r-mirrors\")\n  pushenv(\"R_REPOS_ROOT\", r_repos_root)\n  pushenv(\"R_REPOS_CRAN\", \"file://\" .. pathJoin(r_repos_root, \"cran\"))\n  pushenv(\"R_LOCAL_CRAN\", \"file://\" .. pathJoin(r_repos_root, \"cran\"))\nend\n\n-- R packages built from native code and installed using R from EPEL is *not*\n-- always compatible with ditto installed using R from the CBI software stack.\n-- Because of this, we will use R_LIBS_USER specific to the CBI stack.\n-- However, since some users has already installed to the built-in R_LIBS_USER\n-- we will not change this for such users.  The heuristic is to check if the\n-- built-in R_LIBS_USER folder already exists. If not, then it's safe to use\n-- one specific to the CBI stack.\npushenv(\"R_LIBS_USER\", r_libs_user)\n\n-- The R package 'renv' (https://cran.r-project.org/package=renv) is used to create\n-- folder-specific R package library folder that help with reproducibility and long-term\n-- stability.  By setting RENV_PATHS_PREFIX_AUTO=TRUE, these folders are also specific\n-- for the current Linux distribution, which avoids problems occurring when updating\n-- from, say, CentOS 7 to Rocky 8.  This is likely to become the default behavior in\n-- 'renv' (https://github.com/rstudio/renv/issues/1211)\npushenv(\"RENV_PATHS_PREFIX_AUTO\", \"TRUE\")\n\n-- WORKAROUND: utils::download.file(), which is for instance used by install.packages()\n-- have a built-in timeout at 60 seconds.  This might be too short for some larger\n-- Bioconductor annotation packages, e.g.\n--  * 'SNPlocs.Hsapiens.dbSNP150.GRCh38' (2.10 GB)\n--  * 'MafDb.gnomAD.r2.1.GRCh38' (6.04 GB) =&gt; 6 GB/10 min = 600 MB/min = 10 MB/s = 80 Mb/s\n-- Use 20 minutes timeout instead of 1 minute, i.e. enought with 40 Mb/s for a 6 GB file\npushenv(\"R_DEFAULT_INTERNET_TIMEOUT\", \"1200\")\n\n-- WORKAROUND: gert 1.1.0 (2021-01-25) installs toward a static libgit2 that\n-- gives 'Illegal instruction' on some hosts (with older CPUs?)\n-- See https://github.com/r-lib/gert/issues/117\npushenv(\"USE_SYSTEM_LIBGIT2\", \"true\")\n\n-- WORKAROUND: Package udunits2 does not install out of the box and requires\n-- manually specifying 'configure.args' during install unless we set the\n-- following environment variable\nlocal path = \"/usr/include/udunits2\"\nif (isDir(path)) then\n  pushenv(\"UDUNITS2_INCLUDE\", path)\nend\n\n-- WORKAROUND: nloptr 2.0.0 requires CMake (&gt;= 3.15)\n-- See https://github.com/astamm/nloptr/issues/104#issuecomment-1111498876\npushenv(\"CMAKE_BIN\", \"cmake3\")\n\n\n\n-- Assert that there is no active Conda environment\nassert_no_conda_environment = function()  \n  local conda_env = os.getenv(\"CONDA_DEFAULT_ENV\")\n  if conda_env ~= nil then\n    local action = os.getenv(\"CBI_ON_CONDA\") or \"warning\"\n    local msg = \"Using the \" .. \"'\" .. myModuleName() .. \"'\" .. \" module when a Conda environment is active risks resulting in hard-to-troubleshoot errors due to library conflicts. Make sure to deactivate the currently active Conda \" .. \"'\" .. conda_env .. \"'\" .. \" environment before loading this module, e.g. 'conda deactivate'.\"\n    if action == \"error\" then\n      LmodError(msg)\n    elseif action == \"warning\" then\n      LmodWarning(msg)\n    end\n  end\nend\n\n\n-- Protect against a conflicting Conda stack\nif (mode() == \"load\") then\n  assert_no_conda_environment()\nend\n\n\n\n\n\nr-siteconfig\n\n\n\nR Site Configuration: Tweaks to R for the Current Compute Environment Sets R options and environment variables customized for the current compute environment. Notably, it configures R to install packages from local CRAN and Bioconductor mirrors without the need for internet access. Example: In R, install.packages(&quot;ggplot2&quot;). Versions: 0.3\n\n\nModule code: view\n\nhelp([[\nR Site Configuration: Tweaks to R for the Current Compute Environment\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: R, configuration\")\nwhatis([[\nDescription: Sets R options and environment variables customized for the current compute environment. Notably, it configures R to install packages from local CRAN and Bioconductor mirrors without the need for internet access.\nExamples: In R, `install.packages(\\\"ggplot2\\\")`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\n-- Set site-wide (sic!) Renviron and Rprofile files\npushenv(\"R_ENVIRON\", pathJoin(home, \"Renviron.site\"))\npushenv(\"R_PROFILE\", pathJoin(home, \"Rprofile.site\"))\n\n\n\n\n\nrclone\n\n\n\nrclone: Rsync for Cloud Storage and More Rclone is a command line program to sync files and directories to and from a large number of end points on the local file system, or remote file systems, and in the cloud. Example: rclone –version, rclone –help, rclone config, and man rclone. URL: https://rclone.org/, https://rclone.org/changelog/ (changelog), https://github.com/rclone/rclone (source code) Warning: Only the most recent version of this software will be kept. Versions: 1.71.0, 1.71.2\n\n\nModule code: view\n\nhelp(\"rclone: Rsync for Cloud Storage and More\")\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: files, transfers\")\nwhatis(\"URL: https://rclone.org/, https://rclone.org/changelog/ (changelog), https://github.com/rclone/rclone (source code)\")\nwhatis([[\nDescription: Rclone is a command line program to sync files and directories to and from a large number of end points on the local file system, or remote file systems, and in the cloud.\nExamples: `rclone --version`, `rclone --help`, `rclone config`, and `man rclone`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\nredis\n\n\n\nredis: Remote Dictionary Server Redis is an in-memory database that persists on disk. The data model is key-value, but many different kind of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, Bitmaps. Example: redis-cli –version, redis-cli –help, redis-server –version, and redis-server –help. URL: https://redis.io/, https://redis.io/docs/ (docs), https://github.com/redis/redis/releases (changelog), https://github.com/redis/redis (source code) Warning: Only the most recent version of this software will be kept. Versions: 7.0.12, 7.0.13, 7.2.1, 7.2.2, 7.2.3, 7.2.4, 7.2.5\n\n\nModule code: view\n\nhelp([[\nredis: Remote Dictionary Server\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: database\")\nwhatis(\"URL: https://redis.io/, https://redis.io/docs/ (docs), https://github.com/redis/redis/releases (changelog), https://github.com/redis/redis (source code)\")\nwhatis([[\nDescription: Redis is an in-memory database that persists on disk. The data model is key-value, but many different kind of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, Bitmaps.\nExamples: `redis-cli --version`, `redis-cli --help`, `redis-server --version`, and `redis-server --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\nrestic\n\n\n\nrestic: Fast, Secure, Efficient Backup Program restic is a backup program that is fast, efficient and secure. It supports the three major operating systems (Linux, macOS, Windows) and a few smaller ones (FreeBSD, OpenBSD). Example: restic –help and restic version. URL: https://restic.net, https://restic.readthedocs.io/en/latest/ (documentation), https://github.com/restic/restic/releases (changelog), https://github.com/restic/restic (source code) Versions: 0.17.3, 0.18.1\n\n\nModule code: view\n\nhelp([[\nrestic: Fast, Secure, Efficient Backup Program\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: cli, backup, files\")\nwhatis(\"URL: https://restic.net, https://restic.readthedocs.io/en/latest/ (documentation), https://github.com/restic/restic/releases (changelog), https://github.com/restic/restic (source code)\")\nwhatis([[\nDescription: restic is a backup program that is fast, efficient and secure. It supports the three major operating systems (Linux, macOS, Windows) and a few smaller ones (FreeBSD, OpenBSD).\nExamples: `restic --help` and `restic version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", pathJoin(home, \"man\"))\n\n\n\n\n\nripgrep\n\n\n\nripgrep: Recursively Searches Directories for a Regex Pattern ripgrep is a line-oriented search tool that recursively searches your current directory for a regex pattern. By default, ripgrep will respect your .gitignore and automatically skip hidden files/directories and binary files. ripgrep is similar to other popular search tools like The Silver Searcher, ack and grep. Example: rg –help, man rg, and rg –threads=2 -i ‘lorem ipsum’. URL: https://github.com/BurntSushi/ripgrep, https://github.com/BurntSushi/ripgrep/blob/master/GUIDE.md (documentation), https://github.com/BurntSushi/ripgrep/blob/master/FAQ.md (FAQ), https://github.com/BurntSushi/ripgrep/blob/master/CHANGELOG.md (changelog) Warning: This tool uses 12 parallel threads by default; please specify –threads=2. Only the most recent version of this software will be kept. Versions: 14.1.1, 15.1.0\n\n\nModule code: view\n\nhelp([[\nripgrep: Recursively Searches Directories for a Regex Pattern\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, files, search\")\nwhatis(\"URL: https://github.com/BurntSushi/ripgrep, https://github.com/BurntSushi/ripgrep/blob/master/GUIDE.md (documentation), https://github.com/BurntSushi/ripgrep/blob/master/FAQ.md (FAQ), https://github.com/BurntSushi/ripgrep/blob/master/CHANGELOG.md (changelog)\")\nwhatis([[\nDescription: ripgrep is a line-oriented search tool that recursively searches your current directory for a regex pattern. By default, ripgrep will respect your .gitignore and automatically skip hidden files/directories and binary files. ripgrep is similar to other popular search tools like The Silver Searcher, ack and grep.\nExamples: `rg --help`, `man rg`, and `rg --threads=2 -i 'lorem ipsum'`.\nWarning: This tool uses 12 parallel threads by default; please specify `--threads=2`. Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\nprepend_path(\"MANPATH\", pathJoin(home, \"doc\"))\n\n\n\n\n\nrstudio\n\n\n\nrstudio: RStudio Desktop The RStudio Desktop is an integrated development environment (IDE) for R, a programming language for statistical computing and graphics. Example: rstudio. If you get a blank window, retry with QMLSCENE_DEVICE=softwarecontext rstudio. URL: https://posit.co/products/open-source/rstudio/#rstudio-desktop, https://www.rstudio.com/products/rstudio/release-notes/ (changelog), https://github.com/rstudio/rstudio/ (source code) Warning: This module is deprecates as of 2024-08-30 and no longer maintained; please use the ‘rstudio-server’ tool instead. This software works only on the development nodes. It requires an SSH connection with X11 Forwarding enabled. It does not work with X2Go (gives error \"GLX 1.3 or later is required\"). For best performance, use SSH compression when using X11 Forwarding, i.e. ssh -X -C …. Versions: 1.4.1103, 1.4.1717, 2021.09.0+351, 2021.09.1-372, 2021.09.2-382, 2022.02.1-461, 2022.07.2-576, 2022.12.0-353, 2023.06.1-524, 2024.04.2-764\n\n\nModule code: view\n\n\nhelp([[\n[DEPRECATED] RStudio Desktop: The RStudio Desktop IDE for R\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, R, GUI\")\nwhatis(\"URL: https://posit.co/products/open-source/rstudio/#rstudio-desktop, https://www.rstudio.com/products/rstudio/release-notes/ (changelog), https://github.com/rstudio/rstudio/ (source code)\")\nwhatis([[\nDescription: The RStudio Desktop is an integrated development environment (IDE) for R, a programming language for statistical computing and graphics.\nExamples: `rstudio`.  If you get a blank window, retry with `QMLSCENE_DEVICE=softwarecontext rstudio`.\nWarning: _This module is deprecates as of 2024-08-30 and no longer maintained; please use the 'rstudio-server' tool instead._ This software works only on the development nodes. It requires an SSH connection with X11 Forwarding enabled. It does _not_ work with X2Go (gives error \\\"GLX 1.3 or later is required\\\"). For best performance, use SSH compression when using X11 Forwarding, i.e. `ssh -X -C ...`.\n]])\n\ndepends_on(\"r\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\nif mode() == \"load\" then\n    LmodWarning(\"[DEPRECATION WARNING] The CBI '\" .. name .. \"/\" .. version .. \"' module is deprecated as of 2024-08-30 in favor of 'rstudio-server'; please use that module instead\")\nend\n\n\n\n\n\nrstudio-server\n\n\n\nRStudio Server: The RStudio Server The RStudio Server is an integrated development environment (IDE) for R that can be used from the web browser. Example: rserver. URL: https://posit.co/products/open-source/rstudio/#rstudio-server, https://docs.posit.co/ide/news/ (changelog), https://github.com/rstudio/rstudio/ (source code) Versions: 2024.04.2-764, 2024.09.1-394, 2024.12.1-563, 2025.05.1-513\n\n\nModule code: view\n\nhelp([[\nRStudio Server: The RStudio Server\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, R, GUI\")\nwhatis(\"URL: https://posit.co/products/open-source/rstudio/#rstudio-server, https://docs.posit.co/ide/news/ (changelog), https://github.com/rstudio/rstudio/ (source code)\")\nwhatis([[\nDescription: The RStudio Server is an integrated development environment (IDE) for R that can be used from the web browser.\nExamples: `rserver`.\n]])\n\ndepends_on(\"r\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\n\nrstudio-server-controller\n\n\n\nRSC: An RStudio Server Controller The RStudio Server Controller (RSC) is a tool for launching a personal instance of the RStudio Server on a Linux machine, which then can be access via the web browser, either directly or via SSH tunneling. Example: rsc –help, rsc start, and rsc stop. URL: https://github.com/UCSF-CBI/rstudio-server-controller, https://github.com/UCSF-CBI/rstudio-server-controller/blob/main/NEWS.md (changelog) Versions: 0.20.0\n\n\nModule code: view\n\nhelp([[\nRSC: An RStudio Server Controller\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, R, RStudio Server, GUI\")\nwhatis(\"URL: https://github.com/UCSF-CBI/rstudio-server-controller, https://github.com/UCSF-CBI/rstudio-server-controller/blob/main/NEWS.md (changelog)\")\nwhatis([[\nDescription: The RStudio Server Controller (RSC) is a tool for launching a personal instance of the RStudio Server on a Linux machine, which then can be access via the web browser, either directly or via SSH tunneling.\nExamples: `rsc --help`, `rsc start`, and `rsc stop`.\n]])\n\ndepends_on(\"r\")\ndepends_on(\"rstudio-server\")\n\nlocal home = os.getenv(\"HOME\")\n\n-- System-specific settings\nif isDir(\"/wynton\") then\n  -- Update default to: rsc start --auth=auth-via-env --random-password\n  pushenv(\"RSC_AUTH\", \"auth-via-env\")\n  pushenv(\"RSC_PASSWORD\", \"random\")\n  if home and string.find(home, \"/protected/\") then\n    pushenv(\"RSC_SSH_LOGIN_HOSTNAME\", \"plog1.wynton.ucsf.edu\")\n  else\n    pushenv(\"RSC_SSH_LOGIN_HOSTNAME\", \"log1.wynton.ucsf.edu\")\n  end\n  pushenv(\"RSC_HELP_URL\", \"https://wynton.ucsf.edu/hpc/howto/rstudio.html\")\nelseif isDir(\"/c4\") then\n    pushenv(\"RSC_SSH_LOGIN_HOSTNAME\", \"c4-log1.ucsf.edu\")\n    pushenv(\"RSC_HELP_URL\", \"https://www.c4.ucsf.edu/howto/rstudio.html\")\nelse\n  try_load(\"expect\")\n  pushenv(\"RSC_HELP_URL\", \"https://github.com/UCSF-CBI/rstudio-server-controller#readme\")\nend\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n-- Temporary workaround for https://github.com/UCSF-CBI/rstudio-server-controller/issues/91 /2023-12-15\nsetenv(\"PORT4ME_PORT_COMMAND\", \"netstat\")\n\n\n\n\n\nsalmon\n\n\n\nsalmon: Salmon Provides Fast and Bias-Aware Quantification of Transcript Expression Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment. Example: salmon –version, salmon –help, and salmon quant –threads=1 …. URL: https://combine-lab.github.io/salmon/, https://salmon.readthedocs.io/en/latest/ (documentation), https://github.com/COMBINE-lab/salmon/releases (changelog), https://github.com/COMBINE-lab/salmon (source code) Warning: This tool runs on all available CPU cores by default, which is bad practice. Please specify option –threads=ncores to avoid this, e.g. –threads=1. Versions: 1.3.0, 1.4.0, 1.5.2, 1.6.0, 1.8.0, 1.9.0, 1.10.0\n\n\nModule code: view\n\nhelp([[\nsalmon: Salmon Provides Fast and Bias-Aware Quantification of Transcript Expression\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://combine-lab.github.io/salmon/, https://salmon.readthedocs.io/en/latest/ (documentation), https://github.com/COMBINE-lab/salmon/releases (changelog), https://github.com/COMBINE-lab/salmon (source code)\")\nwhatis([[\nDescription: Highly-accurate & wicked fast transcript-level quantification from RNA-seq reads using selective alignment.\nExamples: `salmon --version`, `salmon --help`, and `salmon quant --threads=1 ...`.\nWarning: This tool runs on all available CPU cores by default, which is bad practice. Please specify option `--threads=ncores` to avoid this, e.g. `--threads=1`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\n\n\n\n\n\nsamtools\n\n\n\nSAMtools: Tools (written in C using htslib) for Manipulating Next-Generation Sequencing Data SAMtools is a suite of programs for interacting with high-throughput sequencing data. Example: samtools –version. URL: https://www.htslib.org/, https://github.com/samtools/samtools/blob/develop/NEWS.md (changelog), https://github.com/samtools/samtools (source code) Versions: 1.9, 1.10, 1.11, 1.13, 1.14, 1.15, 1.15.1, 1.16, 1.16.1, 1.17, 1.18, 1.19, 1.19.2, 1.20, 1.21, 1.22.1\n\n\nModule code: view\n\nhelp([[\nSAMtools: Tools (written in C using htslib) for Manipulating Next-Generation Sequencing Data\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://www.htslib.org/, https://github.com/samtools/samtools/blob/develop/NEWS.md (changelog), https://github.com/samtools/samtools (source code)\")\nwhatis([[\nDescription: SAMtools is a suite of programs for interacting with high-throughput sequencing data.\nExamples: `samtools --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nlocal version_x = string.gsub(version, \"[.].*\", \"\")\nif (version_x == \"0\") then\n  prepend_path(\"PATH\", home)\n  prepend_path(\"PATH\", pathJoin(home, \"bcftools\"))\n  prepend_path(\"PATH\", pathJoin(home, \"misc\"))\nelse\n  prepend_path(\"PATH\", pathJoin(home, \"bin\"))\nend\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n-- Warn about bug https://github.com/samtools/htslib/issues/1236\nif (mode() == \"load\" and version == \"1.11\") then\n  LmodMessage(\"MODULE WARNING: \" .. name .. \" \" .. version .. \" has a bug that results in valid but incorrect CIGAR strings. Because of this, it is recommended to use an older or a newer version instead. For details, see https://github.com/samtools/htslib/issues/1236\")\nend\n\n\n\n\n\nscl-gcc-toolset\n\n\n\nSCL GCC Toolset: GNU Compiler Collection, GNU Debugger, etc. These Developer Toolset provides modern versions of the GNU Compiler Collection, GNU Debugger, and other development, debugging, and performance monitoring tools. Loading these modules enables the corresponding RedHat Software Collection (SCL) gcc-toolset-&lt;version&gt; in the current environment. This is an alternative to calling source scl_source enable gcc-toolset-&lt;version&gt;, which is an approach that is not officially supported by RedHat. Example: gcc –version. URL: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/developing_c_and_cpp_applications_in_rhel_8/additional-toolsets-for-development_developing-applications#gcc-toolset_assembly_additional-toolsets-for-development, https://gcc.gnu.org/develop.html#timeline (GCC release schedule) Warning: Older versions may be removed in the future. Requirement: Rocky 8. Versions: 9, 10, 11, 12, 13\n\n\nModule code: view\n\nhelp([[\nSCL GCC Toolset: GNU Compiler Collection, GNU Debugger, etc.\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nlocal scl_name = \"gcc-toolset\" .. \"-\" .. version\n\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, gcc\")\nwhatis(\"URL: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/developing_c_and_cpp_applications_in_rhel_8/additional-toolsets-for-development_developing-applications#gcc-toolset_assembly_additional-toolsets-for-development, https://gcc.gnu.org/develop.html#timeline (GCC release schedule)\")\nwhatis([[\nDescription: These Developer Toolset provides modern versions of the GNU Compiler Collection, GNU Debugger, and other development, debugging, and performance monitoring tools. Loading these modules enables the corresponding RedHat Software Collection (SCL) `gcc-toolset-&lt;version&gt;` in the current environment.  This is an alternative to calling `source scl_source enable gcc-toolset-&lt;version&gt;`, which is an approach that is not officially supported by RedHat.\nExamples: `gcc --version`.  Warning: Older versions may be removed in the future.\nRequirement: Rocky 8.\n]])\n\n-- This module is only available on Rocky 8\nif os.getenv(\"CBI_LINUX\") ~= \"rocky8\" then\n  LmodError(\"Module '\" .. myModuleFullName() .. \"' is only available on Rocky 8 machines, but not on host '\" .. os.getenv(\"HOSTNAME\") .. \"', which runs '\" .. os.getenv(\"CBI_LINUX\") .. \"'\")\nend\n\n\nlocal home = pathJoin(\"/opt\", \"rh\", scl_name)\n\nif not isDir(home) then\n  LmodError(\"Module '\" .. myModuleFullName() .. \"' is not supported because this host '\" .. os.getenv(\"HOSTNAME\") .. \"' does not have path '\" .. home .. \"'\")\nend\n\n\n-- Don't edit! Created using: \n-- /usr/share/lmod/lmod/libexec/sh_to_modulefile /opt/rh/gcc-toolset-13/enable\nsetenv(\"INFOPATH\",\"/opt/rh/gcc-toolset-13/root/usr/share/info\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/opt/rh/gcc-toolset-13/root/usr/lib64\")\nprepend_path(\"MANPATH\",\"/opt/rh/gcc-toolset-13/root/usr/share/man\")\nprepend_path(\"PATH\",\"/opt/rh/gcc-toolset-13/root/usr/bin\")\nsetenv(\"PCP_DIR\",\"/opt/rh/gcc-toolset-13/root\")\nsetenv(\"PKG_CONFIG_PATH\",\"/opt/rh/gcc-toolset-13/root/usr/lib64/pkgconfig\")\n\n\n\n\n\nshellcheck\n\n\n\nShellCheck: A Shell Script Static Analysis Tool ShellCheck finds bugs in your shell scripts. Example: shellcheck –version and shellcheck -x ~/.bashrc. URL: https://www.shellcheck.net/, https://github.com/koalaman/shellcheck/blob/master/CHANGELOG.md (changelog), https://github.com/koalaman/shellcheck/ (source code) Warning: Only the most recent version of this software will be kept. Versions: 0.10.0\n\n\nModule code: view\n\nhelp([[\nShellCheck: A Shell Script Static Analysis Tool\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, shell, sh, bash, dash, ksh\")\nwhatis(\"URL: https://www.shellcheck.net/, https://github.com/koalaman/shellcheck/blob/master/CHANGELOG.md (changelog), https://github.com/koalaman/shellcheck/ (source code)\")\nwhatis([[\nDescription: ShellCheck finds bugs in your shell scripts.\nExamples: `shellcheck --version` and `shellcheck -x ~/.bashrc`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\nshellcheck-repl\n\n\n\nShellCheck REPL: Validation of Shell Commands Before Evaluation ShellCheck is a great tool for validating your Unix shell scripts. It will parse the scripts and warn about mistakes, errors, and potential problems. This tool - shellcheck-repl - brings ShellCheck validation to the Bash read-eval-print loop (REPL), i.e. the Bash prompt. Getting this type of validation and feedback at the prompt lowers the risk of damaging mistakes and will help you become a better Bash user and developer. Example: Try echo $HOME before and after loading this module. To deactive, unload module, or start a new shell. URL: https://github.com/HenrikBengtsson/shellcheck-repl, https://github.com/HenrikBengtsson/shellcheck-repl/blob/master/README.md (documentation), https://github.com/HenrikBengtsson/shellcheck-repl/blob/master/NEWS.md (changelog) Warning: Only the most recent version of this software will be kept. Versions: 0.4.4, 0.5.0\n\n\nModule code: view\n\nhelp([[\nShellCheck REPL: Validation of Shell Commands Before Evaluation \n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\")\n\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: programming, shell, bash\")\nwhatis(\"URL: https://github.com/HenrikBengtsson/shellcheck-repl, https://github.com/HenrikBengtsson/shellcheck-repl/blob/master/README.md (documentation), https://github.com/HenrikBengtsson/shellcheck-repl/blob/master/NEWS.md (changelog)\")\nwhatis([[\nDescription: ShellCheck is a great tool for validating your Unix shell scripts. It will parse the scripts and warn about mistakes, errors, and potential problems. This tool - shellcheck-repl - brings ShellCheck validation to the Bash read-eval-print loop (REPL), i.e. the Bash prompt. Getting this type of validation and feedback at the prompt lowers the risk of damaging mistakes and will help you become a better Bash user and developer.\nExamples: Try `echo $HOME` before and after loading this module. To deactive, unload module, or start a new shell.\nWarning: Only the most recent version of this software will be kept.\n]])\n\ndepends_on(\"shellcheck\")\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nlocal shell = os.getenv(\"SHELL\")\nlocal script = pathJoin(home, \"shellcheck-repl.bash\")\n\n\n-- Shellcheck REPL can only be activated in interactive mode\nif os.getenv(\"PS1\") ~= nil then\n  -- Enable on load\n  execute{cmd = \"source \" .. script, modeA = {\"load\"}}\n  -- Disable on unload\n  execute{cmd = \"sc_repl_disable\", modeA = {\"unload\"}}\nend\n\n\n\n\n\nsnpeff\n\n\n\nSnpEff: Genetic Variant Annotation and Effect Prediction Toolbox SnpEff is a variant annotation and effect prediction tool. It annotates and predicts the effects of variants on genes (such as amino acid changes). Example: snpEff -help and SnpSift -help, which are short for java -jar $SNPEFF_HOME/snpEff/snpEff.jar -help and java -jar $SNPEFF_HOME/snpEff/SnpSift.jar -help. In SnpEff (&lt; 5.0), there is also ClinEff -help, which is short for java -jar $SNPEFF_HOME/ClinEff/ClinEff.jar -help. URL: https://pcingola.github.io/SnpEff/, https://github.com/pcingola/SnpEff/tags (changelog), https://github.com/pcingola/SnpEff (source code) Warning: SnpEff (&gt;= 5.1) requires Java (&gt;= 12). Versions: 4.3t, 5.0c, 5.0e, 5.1e, 5.1, 5.2a, 5.2c\n\n\nModule code: view\n\nhelp([[\nSnpEff: Genetic Variant Annotation and Effect Prediction Toolbox\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: high-throughput sequencing\")\nwhatis(\"URL: https://pcingola.github.io/SnpEff/, https://github.com/pcingola/SnpEff/tags (changelog), https://github.com/pcingola/SnpEff (source code)\")\nwhatis([[\nDescription: SnpEff is a variant annotation and effect prediction tool. It annotates and predicts the effects of variants on genes (such as amino acid changes).\nExamples: `snpEff -help` and `SnpSift -help`, which are short for `java -jar $SNPEFF_HOME/snpEff/snpEff.jar -help` and `java -jar $SNPEFF_HOME/snpEff/SnpSift.jar -help`.  In SnpEff (&lt; 5.0), there is also `ClinEff -help`, which is short for `java -jar $SNPEFF_HOME/ClinEff/ClinEff.jar -help`.\nWarning: SnpEff (&gt;= 5.1) requires Java (&gt;= 12).\n]])\n\nlocal name = \"snpEff\"\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nif (version &lt; \"5.1\") then\n  try_load(\"openjdk/11\")\nelseif (version &gt;= \"5.1\") then\n  try_load(\"openjdk/17\")\nend\n\npushenv(\"SNPEFF_HOME\", home)\n\nlocal jarfile = pathJoin(home, \"snpEff\", \"snpEff.jar\")\npushenv(\"SNPEFF\", jarfile)\nlocal bash = 'java -jar \"$SNPEFF_HOME/snpEff/snpEff.jar\" \"$@\"'\nlocal csh  = 'java -jar \"$SNPEFF_HOME/snpEff/snpEff.jar\" $*'\nset_shell_function(\"snpEff\", bash, csh)\n\nlocal jarfile = pathJoin(home, \"snpEff\", \"SnpSift.jar\")\npushenv(\"SNPSIFT\", jarfile)\nlocal bash = 'java -jar \"$SNPEFF_HOME/snpEff/SnpSift.jar\" \"$@\"'\nlocal csh  = 'java -jar \"$SNPEFF_HOME/snpEff/SnpSift.jar\" $*'\nset_shell_function(\"SnpSift\", bash, csh)\n\nlocal jarfile = pathJoin(home, \"clinEff\", \"ClinEff.jar\")\nif isFile(jarfile) then\n  pushenv(\"CLINEFF\", jarfile)\n  local bash = 'java -jar \"$SNPEFF_HOME/ClinEff/ClinEff.jar\" \"$@\"'\n  local csh  = 'java -jar \"$SNPEFF_HOME/ClinEff/ClinEff.jar\" $*'\n  set_shell_function(\"ClinEff\", bash, csh)\nend\n\n-- Tweak Java for the current environment\ndepends_on(\"java-tweaks\")\n\n\n\n\n\nspacer\n\n\n\nspacer: Insert Spacers whenever Command Output Stalls If you’re the type of person that habitually presses enter a few times in your log tail to know where the last request ended and the new one begins, this tool is for you! Example: spacer –help, tail -f file.log | spacer. URL: https://github.com/samwho/spacer, https://github.com/samwho/spacer/releases (releases) Warning: Only the most recent version of this software will be kept. Versions: 0.3.8, 0.5.0\n\n\nModule code: view\n\nhelp([[\nspacer: Insert Spacers whenever Command Output Stalls\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utility, cli\")\nwhatis(\"URL: https://github.com/samwho/spacer, https://github.com/samwho/spacer/releases (releases)\")\nwhatis([[\nDescription: If you're the type of person that habitually presses enter a few times in your log tail to know where the last request ended and the new one begins, this tool is for you!\nExamples: `spacer --help`, `tail -f file.log | spacer`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\n-- Local variables\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\nsqlite\n\n\n\nsqlite: SQLite Database Engine & Library SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client–server database engine. Rather, it is embedded into the end program. Example: sqlite3 –version. URL: https://sqlite.org/, https://sqlite.org/docs.html (docs), https://github.com/sqlite/sqlite/tags (changelog), https://github.com/sqlite/sqlite (source code) Versions: 3.43.0, 3.43.2, 3.45.2, 3.46.1, 3.47.0\n\n\nModule code: view\n\nhelp([[\nsqlite: SQLite Database Engine & Library\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: database, utilize\")\nwhatis(\"URL: https://sqlite.org/, https://sqlite.org/docs.html (docs), https://github.com/sqlite/sqlite/tags (changelog), https://github.com/sqlite/sqlite (source code)\")\nwhatis([[\nDescription: SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client–server database engine. Rather, it is embedded into the end program.\nExample: `sqlite3 --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\n\n-- Specific to the Linux distribution?\nif string.match(myFileName(), \"/_\" .. os.getenv(\"CBI_LINUX\") .. \"/\") then\n  root = pathJoin(root, \"_\" .. os.getenv(\"CBI_LINUX\"))\nend\n\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\nprepend_path(\"LD_LIBRARY_PATH\", pathJoin(home, \"lib\"))\nprepend_path(\"PKG_CONFIG_PATH\", pathJoin(home, \"lib\", \"pkgconfig\"))\n\n\n\n\n\nsratoolkit\n\n\n\nSRA Toolkit: Tools and Libraries for Using Data in the INSDC Sequence Read Archives The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives. Example: fastq-dump –help. URL: https://github.com/ncbi/sra-tools/wiki (documentation), https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=toolkit_doc (documentation), https://github.com/ncbi/sra-tools/blob/master/CHANGES.md (changelog), https://github.com/ncbi/sra-tools (source code) Warning: To work around a bug where fasterq-dump crashes the local machine, it has been tweaked such that it uses \\(TMPDIR&lt;/code&gt; rather than &lt;code&gt;\\)PWD as the default temporary folder and it will only use two threads instead of six by default. Versions: 2.10.0, 2.10.4, 2.10.5, 2.10.7, 2.10.8, 2.10.9, 2.11.0, 2.11.1, 2.11.2, 2.11.3, 3.0.0, 3.0.1, 3.0.2, 3.0.5, 3.0.6, 3.0.7, 3.1.0, 3.1.1, 3.2.0\n\n\nModule code: view\n\nhelp([[\nSRA Toolkit: Tools and Libraries for Using Data in the INSDC Sequence Read Archives\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://github.com/ncbi/sra-tools/wiki (documentation), https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=toolkit_doc (documentation), https://github.com/ncbi/sra-tools/blob/master/CHANGES.md (changelog), https://github.com/ncbi/sra-tools (source code)\")\nwhatis([[\nDescription: The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives.\nExamples: `fastq-dump --help`.\nWarning: To work around a bug where `fasterq-dump` crashes the local machine, it has been tweaked such that it uses `$TMPDIR` rather than `$PWD` as the default temporary folder and it will only use two threads instead of six by default.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n-- WORKAROUND: fasterq-dump crashes machines!\n-- Not sure exactly why, but one hypothesis is that the file system is\n-- being hit too hard.  The workaround forces 'fasterq-dump' to use\n-- 'TMPDIR' for temporary files rather than the current directory [2]\n-- [1] https://github.com/ncbi/sra-tools/issues/463#issuecomment-824321890\n-- [2] https://github.com/ncbi/sra-tools/issues/161#issuecomment-808294889\n-- In-house tests with sratoolkit 2.11.0 shows that it's *not* sufficient\n-- to control TMPDIR but also the number of parallel threads [Harry Putnam,\n-- 2021-08-20].  Ideally, we would limit it to a single thread, but the\n-- tool will ignore '--threads 1' and use the default six threads. [3]\n-- [3] https://github.com/ncbi/sra-tools/issues/494\n-- In sratoolkit (&gt;= 2.11.2) it might be that we no longer need to use\n-- '--threads 2' [4]. As soon as we have verified that in a safe\n-- environment, we'll drop it\n-- [4] https://github.com/ncbi/sra-tools/issues/463#issuecomment-942410725\n\n-- Workaround only works in shells that support function, i.e. not in csh and tcsh\nif myShellType() == \"sh\" then\n  set_shell_function(\"fasterq-dump\", 'command fasterq-dump --threads 2 --temp \"$(mktemp -d)\" \"$@\"', '')\nend\n\n\n\n\n\n\nstar\n\n\n\nSTAR: Spliced Transcripts Alignment to a Reference STAR (Spliced Transcripts Alignment to a Reference) is a fast NGS read aligner for RNA-seq data. Example: STAR –help and STAR –version. URL: https://github.com/alexdobin/STAR, https://raw.githubusercontent.com/alexdobin/STAR/master/doc/STARmanual.pdf (docs), https://github.com/alexdobin/STAR/blob/master/CHANGES.md (changelog) Versions: 2.7.0e, 2.7.0f, 2.7.1a, 2.7.2b, 2.7.3a, 2.7.5a, 2.7.5c, 2.7.7a, 2.7.9a, 2.7.10a, 2.7.10b, 2.7.11a, 2.7.11b\n\n\nModule code: view\n\nhelp(\"STAR: Spliced Transcripts Alignment to a Reference\")\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://github.com/alexdobin/STAR, https://raw.githubusercontent.com/alexdobin/STAR/master/doc/STARmanual.pdf (docs), https://github.com/alexdobin/STAR/blob/master/CHANGES.md (changelog)\")\nwhatis([[\nDescription: STAR (Spliced Transcripts Alignment to a Reference) is a fast NGS read aligner for RNA-seq data.\nExamples: `STAR --help` and `STAR --version`.\n]])\n\nlocal name = \"STAR\"\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\ntmux\n\n\n\ntmux: A Terminal Multiplexer tmux is a terminal multiplexer. It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal. And do a lot more. Example: tmux and man tmux. URL: https://github.com/tmux/tmux/wiki, https://github.com/tmux/tmux/blob/master/CHANGES (changelog), https://github.com/tmux/tmux (source code) Warning: Only the most recent version of this software will be kept. Versions: 2.8\n\n\nModule code: view\n\nhelp([[\ntmux: A Terminal Multiplexer\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: screen, tmux\")\nwhatis(\"URL: https://github.com/tmux/tmux/wiki, https://github.com/tmux/tmux/blob/master/CHANGES (changelog), https://github.com/tmux/tmux (source code)\")\nwhatis([[\nDescription: tmux is a terminal multiplexer. It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal. And do a lot more.\nExamples: `tmux` and `man tmux`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\ntophat\n\n\n\nTopHat: A Spliced Read Mapper for RNA-Seq TopHat is a fast splice junction mapper for RNA-Seq reads. It aligns RNA-Seq reads to mammalian-sized genomes using the ultra high-throughput short read aligner Bowtie, and then analyzes the mapping results to identify splice junctions between exons. Example: tophat –version. URL: https://ccb.jhu.edu/software/tophat/index.shtml, https://ccb.jhu.edu/software/tophat/index.shtml (changelog), https://github.com/infphilo/tophat (source code) Versions: 2.1.1\n\n\nModule code: view\n\nhelp([[\nTopHat: A Spliced Read Mapper for RNA-Seq\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: Programming, Statistics\")\nwhatis(\"URL: https://ccb.jhu.edu/software/tophat/index.shtml, https://ccb.jhu.edu/software/tophat/index.shtml (changelog), https://github.com/infphilo/tophat (source code)\")\nwhatis([[\nDescription: TopHat is a fast splice junction mapper for RNA-Seq reads. It aligns RNA-Seq reads to mammalian-sized genomes using the ultra high-throughput short read aligner Bowtie, and then analyzes the mapping results to identify splice junctions between exons.\nExamples: `tophat --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", home)\n\n\n\n\n\n\ntree\n\n\n\ntree: List Content of Directories in a Tree-like Format Tree is a recursive directory listing command that produces a depth indented listing of files, which is colorized ala dircolors if the LS_COLORS environment variable is set and output is to tty. Example: tree –help. URL: https://mama.indstate.edu/users/ice/tree/, https://mama.indstate.edu/users/ice/tree/changes.html (changelog) Warning: Only the most recent version of this software will be kept. Versions: 2.1.1\n\n\nModule code: view\n\nhelp([[\ntree: List Content of Directories in a Tree-like Format\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: files, utility, cli\")\nwhatis(\"URL: https://mama.indstate.edu/users/ice/tree/, https://mama.indstate.edu/users/ice/tree/changes.html (changelog)\")\nwhatis([[\nDescription: Tree is a recursive directory listing command that produces a depth indented listing of files, which is colorized ala dircolors if the `LS_COLORS` environment variable is set and output is to tty.\nExamples: `tree --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"man\"))\n\n\n\n\n\ntrimgalore\n\n\n\nTrimGalore: Taking Appropriate QC Measures for RRBS-Type or Other -Seq Applications with Trim Galore! A wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data. Example: trim_galore –version, trim_galore –help, and more \"$TRIMGALORE_HOME/Docs/Trim_Galore_User_Guide.md\". URL: https://github.com/FelixKrueger/TrimGalore, https://github.com/FelixKrueger/TrimGalore/blob/master/CHANGELOG.md (changelog) Versions: 0.6.7, 0.6.10\n\n\nModule code: view\n\nhelp([[\nTrimGalore: Taking Appropriate QC Measures for RRBS-Type or Other -Seq Applications with Trim Galore!\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing, genome\")\nwhatis(\"URL: https://github.com/FelixKrueger/TrimGalore, https://github.com/FelixKrueger/TrimGalore/blob/master/CHANGELOG.md (changelog)\")\nwhatis([[\nDescription: A wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data.\nExamples: `trim_galore --version`, `trim_galore --help`, and `more \"$TRIMGALORE_HOME/Docs/Trim_Galore_User_Guide.md\"`.\n]])\n\ndepends_on(\"cutadapt\")\ndepends_on(\"fastqc\")\n\nname = \"TrimGalore\"\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", home)\npushenv(\"TRIMGALORE_HOME\", home)\n\n\n\n\n\nvarscan\n\n\n\nVarScan: Variant Detection in Massively Parallel Sequencing Data VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. Example: varscan, which is short for java -jar $VARSCAN_HOME/VarScan.jar. URL: https://dkoboldt.github.io/varscan/, https://github.com/dkoboldt/varscan/releases (changelog), https://github.com/dkoboldt/varscan (source code) Versions: 2.4.2, 2.4.6\n\n\nModule code: view\n\nhelp([[\nVarScan: Variant Detection in Massively Parallel Sequencing Data\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: high-throughput sequencing\")\nwhatis(\"URL: https://dkoboldt.github.io/varscan/, https://github.com/dkoboldt/varscan/releases (changelog), https://github.com/dkoboldt/varscan (source code)\")\nwhatis([[\nDescription: VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments.\nExamples: `varscan`, which is short for `java -jar $VARSCAN_HOME/VarScan.jar`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nname = \"VarScan\"\nlocal home = pathJoin(root, name .. \"-\" .. version)\npushenv(\"VARSCAN_HOME\", home)\n\nlocal bash = 'java -jar \"$VARSCAN_HOME/VarScan.jar\" \"$@\"'\nlocal csh  = 'java -jar \"$VARSCAN_HOME/VarScan.jar\" $*'\nset_shell_function(\"varscan\", bash, csh)\n\n-- Tweak Java for the current environment\ndepends_on(\"java-tweaks\")\n\n\n\n\n\nvcf-validator\n\n\n\nvcf-validator: Validation Suite for Variant Call Format (VCF) Files Validator for the Variant Call Format (VCF) implemented using C++11. It includes all the checks from the vcftools suite, and some more that involve lexical, syntactic and semantic analysis of the VCF input. Example: vcf_validator –help, vcf-debugulator –help, and vcf-assembly-checker –help. URL: https://github.com/EBIvariation/vcf-validator, https://github.com/EBIvariation/vcf-validator/releases (changelog) Versions: 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7\n\n\nModule code: view\n\nhelp([[\nvcf-validator: Validation Suite for Variant Call Format (VCF) Files\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://github.com/EBIvariation/vcf-validator, https://github.com/EBIvariation/vcf-validator/releases (changelog)\")\nwhatis([[\nDescription: Validator for the Variant Call Format (VCF) implemented using C++11. It includes all the checks from the vcftools suite, and some more that involve lexical, syntactic and semantic analysis of the VCF input.\nExamples: `vcf_validator --help`, `vcf-debugulator --help`, and `vcf-assembly-checker --help`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", home)\n\n\n\n\n\nvcftools\n\n\n\nVCFtools: Tools Written in Perl and C++ for Working with VCF Files VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provide easily accessible methods for working with complex genetic variation data in the form of VCF files. Example: vcftools –version. URL: https://vcftools.github.io/, https://github.com/vcftools/vcftools/releases (changelog), https://github.com/vcftools/vcftools (source code) Versions: 0.1.16, 0.1.17\n\n\nModule code: view\n\nhelp([[\nVCFtools: Tools Written in Perl and C++ for Working with VCF Files\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: sequencing\")\nwhatis(\"URL: https://vcftools.github.io/, https://github.com/vcftools/vcftools/releases (changelog), https://github.com/vcftools/vcftools (source code)\")\nwhatis([[\nDescription: VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provide easily accessible methods for working with complex genetic variation data in the form of VCF files.\nExamples: `vcftools --version`.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\npushenv(\"PERL5LIB\", pathJoin(home, \"share\", \"perl5\"))\n\n\n\n\n\n\n\nwynton-tools\n\n\n\nwynton-tools: UCSF Wynton HPC Tools The UCSF Wynton HPC Tools is a set of command-line tools that queries different aspects of the UCSF Wynton HPC environment. These tools may be useful for systems administrators as well as end-users. Example: wynton –help and trash –help. URL: https://github.com/ucsf-wynton/wynton-tools, https://github.com/ucsf-wynton/wynton-tools/blob/master/NEWS.md (changelog) Warning: This is work in development, which means that these tools may change, or even be removed, at anytime without notice. Versions: 0.23.3, 0.24.0, 0.25.0, 0.26.0\n\n\nModule code: view\n\nhelp([[\nwynton-tools: UCSF Wynton HPC Tools\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: utilities\")\nwhatis(\"URL: https://github.com/ucsf-wynton/wynton-tools, https://github.com/ucsf-wynton/wynton-tools/blob/master/NEWS.md (changelog)\")\nwhatis([[\nDescription: The UCSF Wynton HPC Tools is a set of command-line tools that queries different aspects of the UCSF Wynton HPC environment.  These tools may be useful for systems administrators as well as end-users.\nWarning: This is work in development, which means that these tools may change, or even be removed, at anytime without notice.\nExamples: `wynton --help` and `trash --help`.\n]])\n\nif not isDir(\"/wynton\") then\n  LmodMessage(\"NOTE: The '\" .. name .. \"' module is designed for the UCSF Wynton HPC environment; not all tools may work on your system\")\nend\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\n\n\n\n\n\nx86-64-level\n\n\n\nx86-64-level: Get the x86-64 Microarchitecture Level on the Current Machine x86-64 is a 64-bit version of the x86 CPU instruction set supported by AMD and Intel CPUs among others. Since the first generations of CPUs, more low-level CPU features have been added over the years. The x86-64 CPU features can be grouped into four CPU microarchitecture levels: x86-64 v1, x86-64 v2, x86-64 v3, and x86-64 v4. This tool checks which CPU level the current machine supports. Example: x86-64-level and x86-64-level –help. URL: https://github.com/HenrikBengtsson/x86-64-level/, https://github.com/HenrikBengtsson/x86-64-level/blob/develop/NEWS.md (changelog) Warning: Only the most recent version of this software will be kept. Versions: 0.2.2\n\n\nModule code: view\n\nhelp([[\nx86-64-level: Get the x86-64 Microarchitecture Level on the Current Machine\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nversion = string.gsub(version, \"^[.]\", \"\")\n\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: tools, shell, bash\")\nwhatis(\"URL: https://github.com/HenrikBengtsson/x86-64-level/, https://github.com/HenrikBengtsson/x86-64-level/blob/develop/NEWS.md (changelog)\")\nwhatis([[\nDescription: x86-64 is a 64-bit version of the x86 CPU instruction set supported by AMD and Intel CPUs among others. Since the first generations of CPUs, more low-level CPU features have been added over the years. The x86-64 CPU features can be grouped into four CPU microarchitecture levels: x86-64 v1, x86-64 v2, x86-64 v3, and x86-64 v4. This tool checks which CPU level the current machine supports.\nExamples: `x86-64-level` and `x86-64-level --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\nprepend_path(\"PATH\", home)\n\n-- Assert x86-64-level on load?\nif (mode() == \"load\") then\n  local level = os.getenv(\"X86_64_LEVEL_ASSERT\")\n  if level ~= nul and level ~= \"\" then\n    local error = capture(\"x86-64-level --assert=\" .. level .. \" 2&gt;&1\")\n    if error ~= \"\" then\n      LmodError(error)\n    end\n  end\nend\n\n\n\n\n\nyq\n\n\n\nyq: Lightweight and Portable Command-Line YAML, JSON and XML Processor. yq is a lightweight and portable command-line YAML, JSON and XML processor. yq uses jq like syntax, but works with YAML files as well as JSON, XML, properties, CSV, and TSV. Example: yq –version, yq –help. URL: https://github.com/mikefarah/yq, https://github.com/mikefarah/yq/releases (changelog), https://github.com/mikefarah/yq (source code) Warning: Only the most recent version of this software will be kept. Versions: 4.45.1, 4.48.1\n\n\nModule code: view\n\nhelp([[\nyq: Lightweight and Portable Command-Line YAML, JSON and XML Processor.\n]])\n\nlocal name = myModuleName()\nlocal version = myModuleVersion()\nwhatis(\"Version: \" .. version)\nwhatis(\"Keywords: terminal, cli, utility\")\nwhatis(\"URL: https://github.com/mikefarah/yq, https://github.com/mikefarah/yq/releases (changelog), https://github.com/mikefarah/yq (source code)\")\nwhatis([[\nDescription: `yq` is a lightweight and portable command-line YAML, JSON and XML processor. `yq` uses `jq` like syntax, but works with YAML files as well as JSON, XML, properties, CSV, and TSV.\nExamples: `yq --version`, `yq --help`.\nWarning: Only the most recent version of this software will be kept.\n]])\n\nlocal root = os.getenv(\"SOFTWARE_ROOT_CBI\")\nlocal home = pathJoin(root, name .. \"-\" .. version)\n\nprepend_path(\"PATH\", pathJoin(home, \"bin\"))\nprepend_path(\"MANPATH\", pathJoin(home, \"share\", \"man\"))\n\n\n\n\n\n\n\n\nModule Software Repository: Sali (96)\n\nMaintained by: Ben Webb, Sali Lab Software Repository Enable repository: module load Sali\n\nPlease note that this software stack is maintained and contributed by a research group on a voluntary basis. It is not maintained by the Wynton HPC admins. Please reach out to the corresponding maintainer for bug reports, feedback, or questions.\n\n\nallosmod\n\n\n\nAllosMod utility library URL: https://github.com/salilab/allosmod-lib/\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: AllosMod utility library\"\nmodule-whatis \"URL: https://github.com/salilab/allosmod-lib/\"\nmodule load modeller\nmodule load dssp\nmodule load profit\nset topdir /salilab/diva1/programs/x86_64linux/allosmod\nprepend-path  PATH            ${topdir}/bin\nprepend-path  PYTHONPATH      ${topdir}/python\n\n\n\n\n\namber\n\n\n\nAmber11, for 64-bit URL: http://ambermd.org/ Versions: 11\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Amber11, for 64-bit\"\nmodule-whatis \"URL: http://ambermd.org/\"\n\nif [ module-info mode load ] {\n  if { ! [file exists /wynton/group ] } {\n    puts stderr \"Sorry, this module only works on the Wynton cluster (or other machine that has /wynton/group mounted)\"\n    break\n  }\n\n  if { ! [file exists /wynton/group/sali/AMBER ] } {\n    puts stderr \"Sorry, this module is only available to members of the Sali lab.\"\n    break\n  }\n}\nsetenv        AMBERHOME       /wynton/group/sali/AMBER/amber11/\nprepend-path  PATH            /wynton/group/sali/AMBER/amber11/bin\nprepend-path  LD_LIBRARY_PATH /wynton/group/sali/AMBER/amber11/deplib\n\n\n\n\n\nanaconda\n\n\n\nAnaconda Python distribution URL: https://www.anaconda.com/ Versions: py310-2023.03, py311-2023.09, py311-2024.02\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Anaconda Python distribution\"\nmodule-whatis \"URL: https://www.anaconda.com/\"\nconflict conda-forge\nprepend-path  PATH       /salilab/diva1/home/anaconda/py311-2024.02/bin/\n\nif [ module-info mode load ] {\n  puts stderr \"!! The Anaconda module is no longer being updated since it is\"\n  puts stderr \"   no longer free; please use the conda-forge module instead.\"\n}\n\n\n\n\n\nblast\n\n\n\nBasic Local Alignment Search Tool URL: https://blast.ncbi.nlm.nih.gov Versions: 2.2.26\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Basic Local Alignment Search Tool\"\nmodule-whatis \"URL: https://blast.ncbi.nlm.nih.gov\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/blast-2.2.26/bin\n\n\n\n\n\nblast+\n\n\n\nBasic Local Alignment Search Tool URL: https://blast.ncbi.nlm.nih.gov/ Versions: 2.2.25, 2.2.28, 2.12.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Basic Local Alignment Search Tool\"\nmodule-whatis \"URL: https://blast.ncbi.nlm.nih.gov/\"\nprepend-path  PATH   /salilab/diva1/programs/x86_64linux/ncbi-blast-2.12.0+/bin\n\n\n\n\n\nboost\n\n\n\nThe free peer-reviewed portable C++ source libraries URL: https://www.boost.org/ Versions: 1.73.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: The free peer-reviewed portable C++ source libraries\"\nmodule-whatis \"URL: https://www.boost.org/\"\nprepend-path LD_LIBRARY_PATH    /salilab/diva1/programs/x86_64linux/boost-1.73.0/lib64\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/boost-1.73.0/include\nprepend-path CMAKE_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/boost-1.73.0/lib64\n\n\n\n\n\ncereal\n\n\n\nA header-only C++11 serialization library URL: http://uscilab.github.io/cereal/ Versions: 1.3.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A header-only C++11 serialization library\"\nmodule-whatis \"URL: http://uscilab.github.io/cereal/\"\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/cereal-1.3.2/include\n\n\n\n\n\ncgal\n\n\n\nComputational Geometry Algorithms Library URL: https://www.cgal.org/ Versions: 4.12.1, 5.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Computational Geometry Algorithms Library\"\nmodule-whatis \"URL: https://www.cgal.org/\"\nif { [file exists /etc/centos-release] || [file exists /etc/rocky-release] || [file exists /etc/almalinux-release] } {\n  module load boost/1.73.0\n  setenv CGAL_DIR /salilab/diva1/programs/x86_64linux/cgal-5.1/share/cmake/CGAL\n} else {\n  set curMod [module-info name]\n  puts stderr \"'$curMod' does not work on Fedora - ask a sysadmin to install the RPM package instead\"\n  break\n}\n\n\n\n\n\ncolabfold\n\n\n\nAlphaFold2 using MMseqs2 URL: https://github.com/sokrypton/ColabFold Versions: 20230808.git79d8c6e\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: AlphaFold2 using MMseqs2\"\nmodule-whatis \"URL: https://github.com/sokrypton/ColabFold\"\nmodule-whatis \"Examples: for example usage on Wynton, see https://www.rbvi.ucsf.edu/chimerax/data/wynton-colabfold-feb2024/colabfold_wynton.html\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/colabfold-20230808.git79d8c6e/localcolabfold/colabfold-conda/bin/\n\n\n\n\n\nconcavity\n\n\n\nLigand binding site prediction from protein sequence and structure URL: https://compbio.cs.princeton.edu/concavity/ Versions: 0.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Ligand binding site prediction from protein sequence and structure\"\nmodule-whatis \"URL: https://compbio.cs.princeton.edu/concavity/\"\nmodule load sali-libraries\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/concavity-0.1/bin\n\n\n\n\n\nconda-forge\n\n\n\nconda-forge Python distribution URL: https://conda-forge.org/ Versions: py312-24.7.1, py312-24.11.0, py312-25.3.0, py312-25.9.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: conda-forge Python distribution\"\nmodule-whatis \"URL: https://conda-forge.org/\"\nconflict anaconda\nprepend-path  PATH       /salilab/diva1/home/anaconda/miniforge/py312-25.9.1/bin/\n\n\n\n\n\ncryptosite\n\n\n\nCryptoSite utility library URL: https://github.com/salilab/cryptosite/\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: CryptoSite utility library\"\nmodule-whatis \"URL: https://github.com/salilab/cryptosite/\"\nmodule load modeller\nmodule load muscle\nmodule load dssp\nmodule load fpocket\nmodule load concavity\nmodule load patch_dock\nmodule load imp\nmodule load blast+\nmodule load usearch\nmodule load python3/scikit/0.21.3\nmodule load python3/biopython\nset topdir /salilab/diva1/programs/linux/cryptosite\nprepend-path  PATH            ${topdir}/bin\nprepend-path  PYTHONPATH      ${topdir}/python\n\n\n\n\n\ncuda\n\n\n\nVersions: 6.0.37, 7.5.18, 8.0.61, 9.0.176, 10.0.130, 11.5.0, 12.4.0, 12.8.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"NVIDIA CUDA Toolkit libraries, plus cuDNN (v9)\"\nif ![file exists /usr/bin/g++] {\n  module load gcc\n}\nprepend-path  PATH               /salilab/diva1/programs/x86_64linux/cuda-12.8.1/lib64/cuda/bin\nprepend-path  LD_LIBRARY_PATH    /salilab/diva1/programs/x86_64linux/cuda-12.8.1/lib64/cuda/lib64\nprepend-path  PKG_CONFIG_PATH    /salilab/diva1/programs/x86_64linux/cuda-12.8.1/lib64/cuda/pkgconfig\nsetenv        CUDA_LIB_PATH      /salilab/diva1/programs/x86_64linux/cuda-12.8.1/lib64/cuda/lib64\nsetenv        CUDA_BIN_PATH      /salilab/diva1/programs/x86_64linux/cuda-12.8.1/lib64/cuda/bin\n\n\n\n\n\nCython\n\n\n\nA language for writing Python extension modules URL: https://cython.org/ Versions: 0.29.36, 3.0.5\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A language for writing Python extension modules\"\nmodule-whatis \"URL: https://cython.org/\"\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/Cython-3.0.5/bin\nprepend-path PYTHONPATH      /salilab/diva1/programs/x86_64linux/Cython-3.0.5/lib64/python\n\n\n\n\n\ndoxygen\n\n\n\nA documentation system for C/C++ URL: http://www.doxygen.org/index.html Versions: 1.8.6, 1.8.15\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A documentation system for C/C++\"\nmodule-whatis \"URL: http://www.doxygen.org/index.html\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/doxygen-1.8.15/bin\n\n\n\n\n\ndssp\n\n\n\nSecondary structure assignment URL: https://swift.cmbi.umcn.nl/gv/dssp/ Versions: 2.0.4, 2.2.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Secondary structure assignment\"\nmodule-whatis \"URL: https://swift.cmbi.umcn.nl/gv/dssp/\"\nmodule load sali-libraries\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/dssp-2.2.1/bin\nprepend-path  MANPATH         /salilab/diva1/programs/x86_64linux/dssp-2.2.1/man\n\n\n\n\n\neigen\n\n\n\nA lightweight C++ template library for vector and matrix math URL: http://eigen.tuxfamily.org Versions: 3.3.5\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A lightweight C++ template library for vector and matrix math\"\nmodule-whatis \"URL: http://eigen.tuxfamily.org\"\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/eigen-3.3.5/include\n\n\n\n\n\nfeaturesketch\n\n\n\nUrsula’s featuresketch script; run feature_sketch.sh then feature_sketch.py Versions: 3.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Ursula's featuresketch script; run feature_sketch.sh then feature_sketch.py\"\nprepend-path  PATH  /salilab/diva1/programs/x86_64linux/featuresketch-3.0/programs/feature_sketch\n\n\n\n\n\nfpocket\n\n\n\nProtein pocket (cavity) detection algorithm URL: https://github.com/Discngine/fpocket Versions: 2.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Protein pocket (cavity) detection algorithm\"\nmodule-whatis \"URL: https://github.com/Discngine/fpocket\"\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/fpocket-2.0/bin\nprepend-path MANPATH         /salilab/diva1/programs/x86_64linux/fpocket-2.0/man\n\n\n\n\n\ngcc\n\n\n\nVarious compilers (C, C++, Objective-C, Java, …) URL: https://gcc.gnu.org/ Versions: 5.1.1, 6.4.1, 7.3.1, 10.2.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Various compilers (C, C++, Objective-C, Java, ...)\"\nmodule-whatis \"URL: https://gcc.gnu.org/\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/gcc-10.2.1/bin\nprepend-path  LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/gcc-10.2.1/lib64\nif { [file exists /etc/centos-release] || [file exists /etc/rocky-release] || [file exists /etc/almalinux-release] } {\n  prepend-path  LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/gcc-10.2.1/lib64/centos\n}\n\nif [ module-info mode load ] {\n  if { [file exists /etc/centos-release] || [file exists /etc/rocky-release] || [file exists /etc/almalinux-release] } {\n    puts stderr \"!! On Wynton, it is recommended to use Software Collections instead to \"\n    puts stderr \"   get new versions of the C++ compiler; see \"\n    puts stderr \"   https://wynton.ucsf.edu/hpc/software/scl.html\"\n  }\n}\n\n\n\n\n\nghostscript\n\n\n\nA PostScript interpreter and renderer URL: https://www.ghostscript.com/ Versions: 8.70\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A PostScript interpreter and renderer\"\nmodule-whatis \"URL: https://www.ghostscript.com/\"\nmodule load sali-libraries\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/ghostscript-8.70/bin\nprepend-path  LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/ghostscript-8.70/lib64\nprepend-path  GS_FONTPATH     /salilab/diva1/programs/x86_64linux/ghostscript-8.70/share/fonts/default/Type1\nprepend-path  GS_FONTPATH     /salilab/diva1/programs/x86_64linux/ghostscript-8.70/share/fonts/default/ghostscript\n\n\n\n\n\ngnuplot\n\n\n\nA program for plotting mathematical expressions and data URL: http://www.gnuplot.info/ Versions: 5.0.5, 5.4.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A program for plotting mathematical expressions and data\"\nmodule-whatis \"URL: http://www.gnuplot.info/\"\nprepend-path  GNUPLOT_FONTPATH /salilab/diva1/programs/linux/fonts\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/gnuplot-5.4.2/bin\n\n\n\n\n\nhdf5\n\n\n\nA general purpose library and file format for storing scientific data URL: https://support.hdfgroup.org/HDF5/ Versions: 1.8.14, 1.8.17, 1.10.1, 1.10.5, 1.10.6, 1.10.7\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A general purpose library and file format for storing scientific data\"\nmodule-whatis \"URL: https://support.hdfgroup.org/HDF5/\"\nprepend-path PATH               /salilab/diva1/programs/x86_64linux/hdf5-1.10.7/bin\nprepend-path LD_LIBRARY_PATH    /salilab/diva1/programs/x86_64linux/hdf5-1.10.7/lib\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/hdf5-1.10.7/include\nprepend-path CMAKE_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/hdf5-1.10.7/lib\n\n\n\n\n\nifort\n\n\n\nIntel Fortran compiler URL: http://software.intel.com/content/www/us/en/develop/tools/oneapi/components/fortran-compiler.html Versions: 10.1.022\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Intel Fortran compiler\"\nmodule-whatis \"URL: http://software.intel.com/content/www/us/en/develop/tools/oneapi/components/fortran-compiler.html\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/ifort-10.1.022/opt/intel/fce/10.1.022/bin\nprepend-path  LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/ifort-10.1.022/opt/intel/fce/10.1.022/lib\nprepend-path  MANPATH         /salilab/diva1/programs/x86_64linux/ifort-10.1.022/usr/share/man\n\n\n\n\n\nImageMagick\n\n\n\nAn X application for displaying and manipulating images URL: https://imagemagick.org/ Versions: 6.8.8.10\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: An X application for displaying and manipulating images\"\nmodule-whatis \"URL: https://imagemagick.org/\"\nmodule load sali-libraries\nmodule load ghostscript\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/ImageMagick-6.8.8.10/bin\nprepend-path  LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/ImageMagick-6.8.8.10/lib64\nprepend-path  PERL5LIB        /salilab/diva1/programs/x86_64linux/ImageMagick-6.8.8.10/lib64/perl5\n\n\n\n\n\nimod\n\n\n\nTomographic reconstruction package URL: http://bio3d.colorado.edu/imod/ Versions: 4.5.7\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Tomographic reconstruction package\"\nmodule-whatis \"URL: http://bio3d.colorado.edu/imod/\"\nmodule load sali-libraries ifort\nsetenv        IMOD_DIR        /salilab/diva1/programs/x86_64linux/imod-4.5.7/IMOD\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/imod-4.5.7/IMOD/bin\nsetenv        IMOD_PLUGIN_DIR /salilab/diva1/programs/x86_64linux/imod-4.5.7/IMOD/lib/imodplug\nsetenv        IMOD_QTLIBDIR   /salilab/diva1/programs/x86_64linux/imod-4.5.7/IMOD/qtlib\nprepend-path  LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/imod-4.5.7/IMOD/lib\nprepend-path  MANPATH         /salilab/diva1/programs/x86_64linux/imod-4.5.7/man\n\n\n\n\n\nimp\n\n\n\nIntegrative Modeling Platform (version 2.23.0, with only usage checks turned on) URL: https://integrativemodeling.org/ Versions: last_ok_build, nightly, 2.7.0, 2.8.0, 2.9.0, 2.10.0, 2.10.1, 2.11.0, 2.11.1, 2.12.0, 2.13.0, 2.14.0, 2.15.0, 2.16.0, 2.17.0, 2.18.0, 2.19.0, 2.20.0, 2.20.1, 2.20.2, 2.21.0, 2.22.0, 2.23.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Integrative Modeling Platform (version 2.23.0, with only usage checks turned on)\"\nmodule-whatis \"URL: https://integrativemodeling.org/\"\nmodule load boost/1.73.0 libtau/1.0.1 opencv/4.3.0 python3/ihm sali-libraries\nset topdir \"/salilab/diva1/home/imp/main\"\nset subdir [file readlink \"${topdir}/2.23.0\"]\nif {[file pathtype ${subdir}] == \"relative\"} {\n  set subdir \"${topdir}/${subdir}\"\n} \nprepend-path  PATH            ${subdir}/bin/release8\nprepend-path  LD_LIBRARY_PATH ${subdir}/lib/release8\nprepend-path  PYTHONPATH      ${subdir}/lib/release8\nsetenv        IMP_DIR         ${subdir}/lib/release8/cmake/IMP\n\n\n\n\n\nimp-fast\n\n\n\nIntegrative Modeling Platform (version 2.23.0, fast build) URL: https://integrativemodeling.org/ Versions: last_ok_build, nightly, 2.7.0, 2.8.0, 2.9.0, 2.10.0, 2.10.1, 2.11.0, 2.11.1, 2.12.0, 2.13.0, 2.14.0, 2.15.0, 2.16.0, 2.17.0, 2.18.0, 2.19.0, 2.20.0, 2.20.1, 2.20.2, 2.21.0, 2.22.0, 2.23.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Integrative Modeling Platform (version 2.23.0, fast build)\"\nmodule-whatis \"URL: https://integrativemodeling.org/\"\nmodule load boost/1.73.0 libtau/1.0.1 opencv/4.3.0 python3/ihm sali-libraries\nset topdir \"/salilab/diva1/home/imp/main\"\nset subdir [file readlink \"${topdir}/2.23.0\"]\nif {[file pathtype ${subdir}] == \"relative\"} {\n  set subdir \"${topdir}/${subdir}\"\n} \nprepend-path  PATH            ${subdir}/bin/fast8\nprepend-path  LD_LIBRARY_PATH ${subdir}/lib/fast8\nprepend-path  PYTHONPATH      ${subdir}/lib/fast8\nsetenv        IMP_DIR         ${subdir}/lib/fast8/cmake/IMP\n\n\n\n\n\nitcell\n\n\n\nIntegrative T-cell epitope prediction URL: https://github.com/salilab/itcell-lib\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Integrative T-cell epitope prediction\"\nmodule-whatis \"URL: https://github.com/salilab/itcell-lib\"\nmodule load imp scwrl\nset topdir /salilab/diva1/programs/x86_64linux/itcell-lib\nprepend-path  PATH            ${topdir}/scripts\n\n\n\n\n\nlibtau\n\n\n\nCode from the SAMBA group at TAU URL: https://integrativemodeling.org/libTAU.html Versions: 1.0.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Code from the SAMBA group at TAU\"\nmodule-whatis \"URL: https://integrativemodeling.org/libTAU.html\"\nprepend-path LD_LIBRARY_PATH    /salilab/diva1/programs/x86_64linux/libtau-1.0.1/lib64\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/libtau-1.0.1/include\nprepend-path CMAKE_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/libtau-1.0.1/lib64\n\n\n\n\n\nmain\n\n\n\nDusan Turk’s MAIN program URL: https://stef.ijs.si/ Versions: 2013\n\n\nModule code: view\n\n#%Module 1.0\nmodule-whatis \"Description: Dusan Turk's MAIN program\"\nmodule-whatis \"URL: https://stef.ijs.si/\"\nset MAIN /salilab/diva1/programs/x86_64linux/MAIN/\nsetenv MAIN ${MAIN}\n\nsetenv MAIN_CMDS \"${MAIN}cmds/\"\nsetenv MAIN_CMDS_NEW \"${MAIN}cmds_new/\"\nsetenv MAIN_UTILS \"${MAIN}utils/\"\nsetenv MAIN_SYMM \"${MAIN}symm/\"\nsetenv MAIN_DOC \"${MAIN}doc/\"\nsetenv MAIN_COM \"${MAIN}doc/com/\"\nsetenv MAIN_MENU \"${MAIN}doc/menu/\"\nsetenv MAIN_CONF \"${MAIN}config/\"\nsetenv MAIN_R3D_FONT \"${MAIN}crke/\"\n\nprepend-path PATH \"${MAIN}:${MAIN}prog:${MAIN}config\"\n\nset-alias mainps \"${MAIN}mainps_2013.exe_LINUX_G95-64\" \nset-alias psmain \"${MAIN}mainps_2013.exe_LINUX_G95-64\"\nset-alias new \"${MAIN}new.exe_LINUX_G95-64\"\nset-alias new_gf \"${MAIN}new.exe_LINUX_GFORT\"\nset-alias l95 \"make -f ${MAIN}source/makefile.LINUX_G95-64\"\nset-alias lgf \"make -f ${MAIN}source/.make_new_LINUX_GFORT\"\n\nset-alias sou \"cd ${MAIN}source\"\nset-alias tes \"cd ${MAIN}test\"\n\n\n\n\n\nmatlab\n\n\n\nLanguage for technical computing URL: https://www.mathworks.com/products/matlab.html Versions: 9.5.0.944444\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Language for technical computing\"\nmodule-whatis \"URL: https://www.mathworks.com/products/matlab.html\"\nprepend-path  PATH               /salilab/diva1/programs/x86_64linux/matlab-r2018b/bin\n\n\n\n\n\nmist\n\n\n\nMiST - Mass spectrometry interaction STatistics URL: https://github.com/salilab/mist/\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: MiST - Mass spectrometry interaction STatistics\"\nmodule-whatis \"URL: https://github.com/salilab/mist/\"\nmodule load python3/scikit\nset topdir /salilab/diva1/programs/linux/mist\nprepend-path  PATH            ${topdir}/bin\n\n\n\n\n\nmodeller\n\n\n\nMODELLER comparative modeling URL: https://salilab.org/modeller/ Versions: 9.10, 9.11, 9.12, 9.13, 9.14, 9.15, 9.16, 9.17, 9.18, 9.19, 9.20, 9.21, 9.22, 9.23, 9.24, 9.25, 10.0, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.8, SVN\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: MODELLER comparative modeling\"\nmodule-whatis \"URL: https://salilab.org/modeller/\"\nset topdir /salilab/diva1/home/modeller/SVN\nprepend-path  PATH            ${topdir}/bin\nprepend-path  LD_LIBRARY_PATH ${topdir}/lib/x86_64-intel8\nprepend-path  PYTHONPATH      ${topdir}/lib/x86_64-intel8\nprepend-path  PYTHONPATH      ${topdir}/modlib\nprepend-path  PKG_CONFIG_PATH ${topdir}/lib/x86_64-intel8/pkgconfig\n\n\n\n\n\nmodpipe\n\n\n\nModPipe (sets $MODPIPE; only works on the cluster) URL: https://salilab.org/modpipe/ Versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.3.0\n\n\nModule code: view\n\n#%Module 1.0\nmodule-whatis \"Description: ModPipe (sets \\$MODPIPE; only works on the cluster)\"\nmodule-whatis \"URL: https://salilab.org/modpipe/\"\nif [file exists /wynton/home] {\n  setenv            MODPIPE /wynton/home/sali/ModPipe/2.3.0\n} else {\n  puts stderr \"Sorry, this module only works on the cluster (or other machine that has /wynton/home mounted)\"\n  break\n}\n\n\n\n\n\nmuscle\n\n\n\nMultiple alignment program for protein sequences URL: http://www.drive5.com/muscle/ Versions: 3.8.31\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Multiple alignment program for protein sequences\"\nmodule-whatis \"URL: http://www.drive5.com/muscle/\"\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/muscle-3.8.31/bin\n\n\n\n\n\nninja\n\n\n\nA small build system with a focus on speed URL: https://ninja-build.org/ Versions: 1.6.0, 1.8.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A small build system with a focus on speed\"\nmodule-whatis \"URL: https://ninja-build.org/\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/ninja-1.8.2/bin\n\n\n\n\n\nopencv\n\n\n\nCollection of algorithms for computer vision URL: https://opencv.org/ Versions: 4.3.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Collection of algorithms for computer vision\"\nmodule-whatis \"URL: https://opencv.org/\"\nmodule load hdf5/1.10.7\nprepend-path PATH               /salilab/diva1/programs/x86_64linux/opencv-4.3.0/bin\nprepend-path LD_LIBRARY_PATH    /salilab/diva1/programs/x86_64linux/opencv-4.3.0/lib64\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/opencv-4.3.0/include/opencv4\nprepend-path CMAKE_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/opencv-4.3.0/lib64\nprepend-path PYTHONPATH         /salilab/diva1/programs/x86_64linux/opencv-4.3.0/lib/python3.6/site-packages\nif { [file exists /etc/centos-release] || [file exists /etc/rocky-release] || [file exists /etc/almalinux-release]} {\n  # RHEL 8 or 9; setup-users was introduced in RHEL 10\n  if { ![file exists /usr/lib/sysusers.d/20-setup-users.conf]} {\n    prepend-path LD_LIBRARY_PATH    /salilab/diva1/programs/x86_64linux/opencv-4.3.0/lib64/rhel\n  }\n}\n\n\n\n\n\nopeneye\n\n\n\nOpenEye products (work on any 64-bit node) URL: https://www.eyesopen.com/ Versions: 2012\n\n\nModule code: view\n\n#%Module 1.0\nmodule-whatis \"Description: OpenEye products (work on any 64-bit node)\"\nmodule-whatis \"URL: https://www.eyesopen.com/\"\nif [file exists /wynton/group] {\n\n  if [ module-info mode load ] {\n    if { ! [file exists /wynton/group/sali/openeye ] } {\n      puts stderr \"Sorry, this module is only available to members of the Sali lab.\"\n      break\n    }\n  }\n\n  prepend-path      PATH        /wynton/group/sali/openeye/bin\n  prepend-path      PYTHONPATH  /wynton/group/sali/openeye/wrappers/v2012.Oct.1/python\n  setenv            OE_LICENSE  /wynton/group/sali/openeye/oe_license.txt\n} else {\n  puts stderr \"Sorry, this module only works on the Wynton cluster (or other machine that has /wynton/group mounted)\"\n  break\n}\n\n\n\n\n\npatch_dock\n\n\n\nMolecular Docking Based on Shape Complementarity Principles URL: https://bioinfo3d.cs.tau.ac.il/PatchDock/ Versions: 1.3\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Molecular Docking Based on Shape Complementarity Principles\"\nmodule-whatis \"URL: https://bioinfo3d.cs.tau.ac.il/PatchDock/\"\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/patch_dock-1.3/bin/cdr\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/patch_dock-1.3/bin\n\n\n\n\n\npcss\n\n\n\nPeptide Classification using Sequence and Structure URL: https://github.com/salilab/pcss\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Peptide Classification using Sequence and Structure\"\nmodule-whatis \"URL: https://github.com/salilab/pcss\"\nif [file exists /wynton/home/sali] {\n  module load svm_light\n  set topdir /wynton/home/sali/peptide\n  prepend-path  PATH      ${topdir}/bin\n  prepend-path  PERL5LIB  ${topdir}/lib\n} else {\n  puts stderr \"Sorry, this module only works on the cluster (or other machine that has /wynton/home/sali mounted)\"\n  break\n}\n\n\n\n\n\nphenix\n\n\n\nPython-based Hierarchical ENvironment for Integrated Xtallography URL: https://www.phenix-online.org/ Versions: 1.10.1.2155, 1.18.2.3874, 1.19.1.4122\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python-based Hierarchical ENvironment for Integrated Xtallography\"\nmodule-whatis \"URL: https://www.phenix-online.org/\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/phenix-1.19.1.4122/phenix-1.19.1-4122/build/bin\nsetenv        PHENIX          /salilab/diva1/programs/x86_64linux/phenix-1.19.1.4122/phenix-1.19.1-4122\nsetenv        PHENIX_VERSION  1.19.1-4122\n\n\n\n\n\nprofit\n\n\n\nProFit, a protein least squares fitting program URL: http://www.bioinf.org.uk/software/profit/ Versions: 3.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: ProFit, a protein least squares fitting program\"\nmodule-whatis \"URL: http://www.bioinf.org.uk/software/profit/\"\nprepend-path  PATH           /salilab/diva1/programs/x86_64linux/profit-3.1\n\n\n\n\n\npsipred\n\n\n\nAccurate protein secondary structure prediction URL: http://bioinf.cs.ucl.ac.uk/psipred/ Versions: 4.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Accurate protein secondary structure prediction\"\nmodule-whatis \"URL: http://bioinf.cs.ucl.ac.uk/psipred/\"\nmodule load blast\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/psipred-4.0/bin\nsetenv        PSIPRED_LIB     /salilab/diva1/programs/x86_64linux/psipred-4.0/lib\n\n\n\n\n\npython3/attrs\n\n\n\nClasses Without Boilerplate URL: https://www.attrs.org/ Versions: 20.3.0, 21.2.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Classes Without Boilerplate\"\nmodule-whatis \"URL: https://www.attrs.org/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/attrs-21.2.0/lib/python3.6/site-packages\n\n\n\n\n\npython3/biopython\n\n\n\nPython tools for computational molecular biology URL: https://biopython.org/ Versions: 1.75\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python tools for computational molecular biology\"\nmodule-whatis \"URL: https://biopython.org/\"\nconflict python2\nmodule load python3/numpy/1.19.5\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/biopython-1.75/lib64/python\n\n\n\n\n\npython3/bottleneck\n\n\n\nCollection of fast NumPy array functions written in Cython URL: https://github.com/pydata/bottleneck Versions: 1.3.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Collection of fast NumPy array functions written in Cython\"\nmodule-whatis \"URL: https://github.com/pydata/bottleneck\"\nconflict python2\nmodule load python3/numpy/1.19.5 python3/scipy/1.3.2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/bottleneck-1.3.1/lib64/python\n\n\n\n\n\npython3/cycler\n\n\n\nComposable style cycles URL: https://github.com/matplotlib/cycler Versions: 0.10.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Composable style cycles\" \nmodule-whatis \"URL: https://github.com/matplotlib/cycler\"\nconflict python2\nmodule load python3/six\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/cycler-0.10.0/lib/python\n\n\n\n\n\npython3/dateutil\n\n\n\nPowerful extensions to the standard datetime module URL: https://dateutil.readthedocs.io/en/stable/ Versions: 2.8.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Powerful extensions to the standard datetime module\"\nmodule-whatis \"URL: https://dateutil.readthedocs.io/en/stable/\"\nconflict python2\nmodule load python3/six\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/dateutil-2.8.1/lib/python3.6/site-packages\n\n\n\n\n\npython3/decorator\n\n\n\nModule to simplify usage of decorators URL: https://github.com/micheles/decorator Versions: 4.4.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Module to simplify usage of decorators\"\nmodule-whatis \"URL: https://github.com/micheles/decorator\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/decorator-4.4.1/lib/python3.6/site-packages\n\n\n\n\n\npython3/flake8\n\n\n\nThe modular source code checker: pep8 pyflakes and co URL: https://github.com/PyCQA/flake8 Versions: 5.0.4\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: The modular source code checker: pep8 pyflakes and co\"\nmodule-whatis \"URL: https://github.com/PyCQA/flake8\"\nconflict python2\nmodule load python3/importlib-metadata\nmodule load python3/mccabe\nmodule load python3/pycodestyle\nmodule load python3/pyflakes\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/flake8-5.0.4/lib/python3.6/site-packages\nprepend-path  PATH   /salilab/diva1/programs/linux/flake8-5.0.4/bin\n\n\n\n\n\npython3/future\n\n\n\nClean single-source support for Python 3 and 2 URL: https://python-future.org/ Versions: 0.18.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Clean single-source support for Python 3 and 2\"\nmodule-whatis \"URL: https://python-future.org/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/future-0.18.2/lib/python3.6/site-packages\nprepend-path  PATH   /salilab/diva1/programs/linux/future-0.18.2/bin\n\n\n\n\n\npython3/h5py\n\n\n\nPython interface to the Hierarchical Data Format library URL: https://www.h5py.org/ Versions: 2.10.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python interface to the Hierarchical Data Format library\"\nmodule-whatis \"URL: https://www.h5py.org/\"\nconflict python2\nmodule load python3/numpy/1.19.5 hdf5/1.10.7 python3/six\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/h5py-2.10.0/lib64/python\n\n\n\n\n\npython3/hdbscan\n\n\n\nA high performance implementation of HDBSCAN clustering URL: https://github.com/scikit-learn-contrib/hdbscan Versions: 0.8.33\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A high performance implementation of HDBSCAN clustering\"\nmodule-whatis \"URL: https://github.com/scikit-learn-contrib/hdbscan\"\nconflict python2\nmodule load python3/numpy/1.19.5 python3/scipy/1.3.2 python3/scikit/0.21.3\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/hdbscan-0.8.33/lib64/python\n\n\n\n\n\npython3/ihm\n\n\n\nPython package for handling IHM mmCIF files URL: https://github.com/ihmwg/python-ihm Versions: 1.8, 2.5, 2.7\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python package for handling IHM mmCIF files\"\nmodule-whatis \"URL: https://github.com/ihmwg/python-ihm\"\nconflict python2\nmodule load python3/msgpack\nprepend-path PYTHONPATH      /salilab/diva1/programs/x86_64linux/ihm-2.7/lib64/python/site-packages\n\n\n\n\n\npython3/importlib-metadata\n\n\n\nRead metadata from Python packages URL: https://github.com/python/importlib_metadata Versions: 3.3.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Read metadata from Python packages\"\nmodule-whatis \"URL: https://github.com/python/importlib_metadata\"\nconflict python2\nmodule load python3/zipp\nmodule load python3/toml\nmodule load python3/typing-extensions\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/importlib-metadata-3.3.0/lib/python3.6/site-packages\n\n\n\n\n\npython3/iniconfig\n\n\n\nBrain-dead simple parsing of ini files URL: https://github.com/RonnyPfannschmidt/iniconfig Versions: 1.1.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Brain-dead simple parsing of ini files\"\nmodule-whatis \"URL: https://github.com/RonnyPfannschmidt/iniconfig\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/iniconfig-1.1.1/lib/python3.6/site-packages\n\n\n\n\n\npython3/joblib\n\n\n\nLightweight pipelining: using Python functions as pipeline jobs URL: https://joblib.readthedocs.io/en/latest/ Versions: 0.17.0, 1.5.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Lightweight pipelining: using Python functions as pipeline jobs\"\nmodule-whatis \"URL: https://joblib.readthedocs.io/en/latest/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/joblib-1.5.2/lib/python\n\n\n\n\n\npython3/kiwisolver\n\n\n\nA fast implementation of the Cassowary constraint solver URL: https://github.com/nucleic/kiwi Versions: 1.1.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A fast implementation of the Cassowary constraint solver\"\nmodule-whatis \"URL: https://github.com/nucleic/kiwi\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/kiwisolver-1.1.0/lib64/python\n\n\n\n\n\npython3/matplotlib\n\n\n\nPython 2D plotting library URL: https://matplotlib.org/ Versions: 3.1.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python 2D plotting library\"\nmodule-whatis \"URL: https://matplotlib.org/\"\nconflict python2\nmodule load python3/cycler\nmodule load python3/numpy/1.19.5\nmodule load python3/pyparsing\nmodule load python3/dateutil\nmodule load python3/pytz\nmodule load python3/kiwisolver\nprepend-path PYTHONPATH   /salilab/diva1/programs/x86_64linux/matplotlib-3.1.2/lib64/python\nprepend-path LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/matplotlib-3.1.2/lib\n\n\n\n\n\npython3/mccabe\n\n\n\nMcCabe checker, plugin for flake8 URL: https://github.com/pycqa/mccabe Versions: 0.6.1, 0.7.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: McCabe checker, plugin for flake8\"\nmodule-whatis \"URL: https://github.com/pycqa/mccabe\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/mccabe-0.7.0/lib/python3.6/site-packages\n\n\n\n\n\npython3/mdp\n\n\n\nModular toolkit for Data Processing URL: http://mdp-toolkit.sourceforge.net/ Versions: 3.6\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Modular toolkit for Data Processing\"\nmodule-whatis \"URL: http://mdp-toolkit.sourceforge.net/\"\nconflict python2\nmodule load python3/scipy python3/future python3/scikit\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/mdp-3.6/lib/python3.6/site-packages\n\n\n\n\n\npython3/modelcif\n\n\n\nPython package for handling ModelCIF mmCIF files URL: https://github.com/ihmwg/python-modelcif Versions: 1.2, 1.3\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python package for handling ModelCIF mmCIF files\"\nmodule-whatis \"URL: https://github.com/ihmwg/python-modelcif\"\nconflict python2\nmodule load python3/ihm\nprepend-path PYTHONPATH      /salilab/diva1/programs/linux/modelcif-1.3/lib/python/site-packages\n\n\n\n\n\npython3/msgpack\n\n\n\nA Python MessagePack (de)serializer URL: https://msgpack.org/ Versions: 0.6.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A Python MessagePack (de)serializer\"\nmodule-whatis \"URL: https://msgpack.org/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/msgpack-0.6.2/lib64/python\n\n\n\n\n\npython3/networkx\n\n\n\nCreates and Manipulates Graphs and Networks URL: https://networkx.github.io/ Versions: 2.4\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Creates and Manipulates Graphs and Networks\"\nmodule-whatis \"URL: https://networkx.github.io/\"\nconflict python2\nmodule load python3/scipy/1.3.2 python3/pyparsing python3/decorator\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/networkx-2.4/lib/python\n\n\n\n\n\npython3/nose\n\n\n\nDiscovery-based unittest extension for Python3 URL: https://nose.readthedocs.io/en/latest/ Versions: 1.3.7\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Discovery-based unittest extension for Python3\"\nmodule-whatis \"URL: https://nose.readthedocs.io/en/latest/\"\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/nose-1.3.7/lib/python3.6/site-packages\nprepend-path  PATH         /salilab/diva1/programs/linux/nose-1.3.7/bin\n\n\n\n\n\npython3/numexpr\n\n\n\nFast numerical array expression evaluator for Python and NumPy URL: https://github.com/pydata/numexpr Versions: 2.8.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Fast numerical array expression evaluator for Python and NumPy\"\nmodule-whatis \"URL: https://github.com/pydata/numexpr\"\nconflict python2\nmodule load python3/numpy/1.19.5 python3/packaging\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/numexpr-2.8.1/lib64/python\n\n\n\n\n\npython3/numpy\n\n\n\nA fast multidimensional array facility for Python URL: https://numpy.org/ Versions: 1.19.5\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A fast multidimensional array facility for Python\"\nmodule-whatis \"URL: https://numpy.org/\"\nconflict python2\nprepend-path PYTHONPATH      /salilab/diva1/programs/x86_64linux/numpy-1.19.5/lib64/python\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/numpy-1.19.5/bin\nprepend-path LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/numpy-1.19.5/lib\n\n\n\n\n\npython3/packaging\n\n\n\nCore utilities for Python packages URL: https://github.com/pypa/packaging Versions: 20.8\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Core utilities for Python packages\"\nmodule-whatis \"URL: https://github.com/pypa/packaging\"\nconflict python2\nmodule load python3/pyparsing\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/packaging-20.8/lib/python3.6/site-packages\n\n\n\n\n\npython3/pandas\n\n\n\nPython Data Analysis Library URL: https://pandas.pydata.org/ Versions: 0.25.3\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python Data Analysis Library\" \nmodule-whatis \"URL: https://pandas.pydata.org/\"\nconflict python2\nmodule load python3/numpy/1.19.5\nmodule load python3/dateutil\nmodule load python3/matplotlib\nmodule load python3/pytz\nmodule load python3/bottleneck\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/pandas-0.25.3/lib64/python\n\n\n\n\n\npython3/pkgconfig\n\n\n\nA Python interface to the pkg-config command line tool URL: https://github.com/matze/pkgconfig Versions: 1.5.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A Python interface to the pkg-config command line tool\"\nmodule-whatis \"URL: https://github.com/matze/pkgconfig\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/pkgconfig-1.5.1/lib/python3.6/site-packages\n\n\n\n\n\npython3/pluggy\n\n\n\nA minimalist production ready plugin system URL: https://github.com/pytest-dev/pluggy Versions: 0.13.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A minimalist production ready plugin system\"\nmodule-whatis \"URL: https://github.com/pytest-dev/pluggy\"\nconflict python2\nmodule load python3/importlib-metadata\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/pluggy-0.13.1/lib/python3.6/site-packages\n\n\n\n\n\npython3/protobuf\n\n\n\nProtocol Buffers - Google’s data interchange format URL: https://developers.google.com/protocol-buffers/ Versions: 3.11.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Protocol Buffers - Google's data interchange format\"\nmodule-whatis \"URL: https://developers.google.com/protocol-buffers/\"\nconflict python2\nmodule load python3/six\nprepend-path PYTHONPATH      /salilab/diva1/programs/x86_64linux/protobuf-py36-3.11.2/lib64/python3.6/site-packages\nprepend-path LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/protobuf-py36-3.11.2/lib64\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/protobuf-py36-3.11.2/bin\nprepend-path EMACSLOADPATH   /salilab/diva1/programs/x86_64linux/protobuf-py36-3.11.2/share/emacs/site-lisp\nprepend-path CMAKE_INCLUDE_PATH /salilab/diva1/programs/x86_64linux/protobuf-py36-3.11.2/include\nprepend-path CMAKE_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/protobuf-py36-3.11.2/lib64\n\n\n\n\n\npython3/py\n\n\n\nCross-python path, ini-parsing, io, code, log facilities URL: https://py.readthedocs.io/en/latest/ Versions: 1.11.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Cross-python path, ini-parsing, io, code, log facilities\"\nmodule-whatis \"URL: https://py.readthedocs.io/en/latest/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/py-1.11.0/lib/python3.6/site-packages\n\n\n\n\n\npython3/pycodestyle\n\n\n\nPython style guide checker URL: https://pycodestyle.readthedocs.io/ Versions: 2.6.0, 2.9.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python style guide checker\"\nmodule-whatis \"URL: https://pycodestyle.readthedocs.io/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/pycodestyle-2.9.1/lib/python3.6/site-packages\nprepend-path  PATH   /salilab/diva1/programs/linux/pycodestyle-2.9.1/bin\n\n\n\n\n\npython3/pyflakes\n\n\n\nPassive checker of Python programs URL: https://github.com/PyCQA/pyflakes Versions: 2.2.0, 2.5.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Passive checker of Python programs\"\nmodule-whatis \"URL: https://github.com/PyCQA/pyflakes\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/pyflakes-2.5.0/lib/python3.6/site-packages\nprepend-path  PATH   /salilab/diva1/programs/linux/pyflakes-2.5.0/bin\n\n\n\n\n\npython3/pyparsing\n\n\n\nAn object-oriented approach to text processing URL: https://github.com/pyparsing/pyparsing/ Versions: 2.4.5\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: An object-oriented approach to text processing\"\nmodule-whatis \"URL: https://github.com/pyparsing/pyparsing/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/pyparsing-py36-2.4.5/lib/python3.6/site-packages\n\n\n\n\n\npython3/pyrmsd\n\n\n\nVersions: 4.3.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Way of performing RMSD calculations of large sets of structures\"\nconflict python2\nmodule load cuda/7.5.18 python3/numpy/1.19.5 python3/scipy/1.3.2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/pyrmsd-4.3.2\n\n\n\n\n\npython3/pytest\n\n\n\nSimple powerful testing with Python URL: https://docs.pytest.org/en/latest/ Versions: 6.2.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Simple powerful testing with Python\"\nmodule-whatis \"URL: https://docs.pytest.org/en/latest/\"\nconflict python2\nmodule load python3/toml\nmodule load python3/importlib-metadata\nmodule load python3/attrs\nmodule load python3/iniconfig\nmodule load python3/packaging\nmodule load python3/pluggy\nmodule load python3/py\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/pytest-6.2.1/lib/python3.6/site-packages\nprepend-path  PATH   /salilab/diva1/programs/linux/pytest-6.2.1/bin\n\n\n\n\n\npython3/pytz\n\n\n\nWorld Timezone Definitions for Python URL: https://pythonhosted.org/pytz/ Versions: 2019.3\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: World Timezone Definitions for Python\"\nmodule-whatis \"URL: https://pythonhosted.org/pytz/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/pytz-2019.3/lib/python3.6/site-packages\n\n\n\n\n\npython3/scikit\n\n\n\nA set of python modules for machine learning and data mining URL: https://scikit-learn.org/stable/index.html Versions: 0.21.3\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: A set of python modules for machine learning and data mining\"\nmodule-whatis \"URL: https://scikit-learn.org/stable/index.html\"\nconflict python2\nmodule load sali-libraries\nmodule load python3/numpy/1.19.5 python3/scipy/1.3.2 python3/joblib\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/scikit-0.21.3/lib64/python\n\n\n\n\n\npython3/scipy\n\n\n\nScipy: Scientific Tools for Python URL: https://www.scipy.org/ Versions: 1.3.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Scipy: Scientific Tools for Python\" \nmodule-whatis \"URL: https://www.scipy.org/\"\nconflict python2\nmodule load python3/numpy/1.19.5\nprepend-path  PYTHONPATH   /salilab/diva1/programs/x86_64linux/scipy-1.3.2/lib64/python\n\n\n\n\n\npython3/six\n\n\n\nPython 2 and 3 compatibility utilities URL: https://github.com/benjaminp/six Versions: 1.16.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python 2 and 3 compatibility utilities\"\nmodule-whatis \"URL: https://github.com/benjaminp/six\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/six-1.16.0/lib/python3.6/site-packages\n\n\n\n\n\npython3/tables\n\n\n\nHierarchical datasets in Python URL: http://www.pytables.org/ Versions: 3.6.1\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Hierarchical datasets in Python\"\nmodule-whatis \"URL: http://www.pytables.org/\"\nconflict python2\nmodule load python3/numpy/1.19.5\nmodule load python3/numexpr/2.8.1\nmodule load hdf5/1.10.7\nmodule load python3/six\nprepend-path PYTHONPATH      /salilab/diva1/programs/x86_64linux/tables-3.6.1/lib64/python\nprepend-path PATH            /salilab/diva1/programs/x86_64linux/tables-3.6.1/bin\n\n\n\n\n\npython3/toml\n\n\n\nPython Library for Tom’s Obvious, Minimal Language URL: https://github.com/uiri/toml Versions: 0.10.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Python Library for Tom's Obvious, Minimal Language\"\nmodule-whatis \"URL: https://github.com/uiri/toml\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/toml-0.10.2/lib/python3.6/site-packages\n\n\n\n\n\npython3/typing-extensions\n\n\n\nBackported and Experimental Type Hints for Python 3.5+ URL: https://pypi.org/project/typing-extensions/ Versions: 3.7.4.3\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Backported and Experimental Type Hints for Python 3.5+\"\nmodule-whatis \"URL: https://pypi.org/project/typing-extensions/\"\nconflict python2\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/typing-extensions-3.7.4.3/lib/python3.6/site-packages\n\n\n\n\n\npython3/zipp\n\n\n\nBackport of pathlib-compatible object wrapper for zip files URL: https://github.com/jaraco/zipp Versions: 3.4.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Backport of pathlib-compatible object wrapper for zip files\"\nmodule-whatis \"URL: https://github.com/jaraco/zipp\"\nconflict python2\nmodule load python3/toml\nprepend-path  PYTHONPATH   /salilab/diva1/programs/linux/zipp-3.4.0/lib/python3.6/site-packages\n\n\n\n\n\nrelion\n\n\n\nElectron cryo-microscopy refinement URL: https://github.com/3dem/relion Versions: 1.4, 2.0.6, 3.0.git9a02562\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Electron cryo-microscopy refinement\"\nmodule-whatis \"URL: https://github.com/3dem/relion\"\nmodule load cuda/7.5.18\n# would use is-avail, but Lmod doesn't support that\nif [file exists /etc/modulefiles/mpi/openmpi-x86_64] { \n  module load mpi/openmpi-x86_64\n} elseif [file exists /usr/share/modulefiles/mpi/openmpi-x86_64] {    \n  module load mpi/openmpi-x86_64\n}\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/relion-3.0.git9a02562/bin\n\n# Provide libX11 if not available on the system\nif [file exists /lib64/libX11.so.6] {\n} else {\n  prepend-path LD_LIBRARY_PATH /salilab/diva1/programs/x86_64linux/relion-3.0.git9a02562/lib\n}\n\n\n\n\n\nrosetta\n\n\n\nComputational modeling and analysis of protein structures URL: https://www.rosettacommons.org/ Versions: 3.5, 3.10\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Computational modeling and analysis of protein structures\"\nmodule-whatis \"URL: https://www.rosettacommons.org/\"\nprepend-path  PATH            /salilab/diva1/programs/x86_64linux/rosetta-3.10/bin\nsetenv        ROSETTA3_DB     /salilab/diva1/programs/x86_64linux/rosetta-3.10/database\n\n\n\n\n\nsali-libraries\n\n\n\nMakes available libraries usually only present on the Sali interactive nodes\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Makes available libraries usually only present on the Sali interactive nodes\"\nprepend-path  LD_LIBRARY_PATH /salilab/diva1/home/libs/x86_64\nif { [file exists /etc/centos-release] || [file exists /etc/rocky-release] || [file exists /etc/almalinux-release]} {\n  if [file exists /etc/dnf/dnf.conf] {\n    prepend-path  LD_LIBRARY_PATH /salilab/diva1/home/libs/x86_64/centos8\n  } elseif [file exists /etc/systemd/system.conf] {\n    prepend-path  LD_LIBRARY_PATH /salilab/diva1/home/libs/x86_64/centos7\n  } else {\n    prepend-path  LD_LIBRARY_PATH /salilab/diva1/home/libs/x86_64/centos\n  }\n} else {\n  if [file exists /usr/lib64/libtiff.so.6] {\n    prepend-path  LD_LIBRARY_PATH /salilab/diva1/home/libs/x86_64/fedora41\n  }\n}\n\n\n\n\n\nscwrl\n\n\n\nProtein side-chain conformation prediction program URL: http://dunbrack.fccc.edu/lab/scwrl Versions: 4.0\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Protein side-chain conformation prediction program\"\nmodule-whatis \"URL: http://dunbrack.fccc.edu/lab/scwrl\"\nprepend-path PATH               /salilab/diva1/programs/x86_64linux/scwrl-4.0/bin\n\n\n\n\n\nsvm_light\n\n\n\nSupport Vector Machine URL: http://svmlight.joachims.org/ Versions: 6.0.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Support Vector Machine\"\nmodule-whatis \"URL: http://svmlight.joachims.org/\"\nprepend-path  PATH /salilab/diva1/programs/x86_64linux/svm_light-6.0.2\n\n\n\n\n\nswig\n\n\n\nConnects C/C++/Objective C to some high-level programming languages URL: http://www.swig.org/ Versions: 3.0.12, 4.0.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Connects C/C++/Objective C to some high-level programming languages\"\nmodule-whatis \"URL: http://www.swig.org/\"\nprepend-path PATH               /salilab/diva1/programs/x86_64linux/swig-4.0.2/bin\n\n\n\n\n\nusearch\n\n\n\nHigh-throughput search and clustering tool URL: http://www.drive5.com/usearch/ Versions: 4.0.43, 10.0.240\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: High-throughput search and clustering tool\"\nmodule-whatis \"URL: http://www.drive5.com/usearch/\"\nprepend-path  PATH            /salilab/diva1/programs/linux/usearch-10.0.240/\n\n\n\n\n\nweb_service\n\n\n\nCommand line interface (web_service.py) to most lab web services URL: https://github.com/salilab/saliweb\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Command line interface (web_service.py) to most lab web services\"\nmodule-whatis \"URL: https://github.com/salilab/saliweb\"\nprepend-path  PATH            /salilab/diva1/programs/linux/web_service\n\n\n\n\n\nzdock\n\n\n\nRigid body docking URL: http://zdock.umassmed.edu/ Versions: 3.0.2\n\n\nModule code: view\n\n#%Module 1.0\n\nmodule-whatis \"Description: Rigid body docking\"\nmodule-whatis \"URL: http://zdock.umassmed.edu/\"\nprepend-path  PATH /salilab/diva1/programs/x86_64linux/zdock-3.0.2\n\n\n\n\n\n\n\n\nbuilt-in (6)\n\n\nCBI (105)\n\n\nSali (96)\n\n\nThe above information is updated automatically once an hour by querying module avail and module spider."
  },
  {
    "objectID": "hpc/assets/debug.html",
    "href": "hpc/assets/debug.html",
    "title": "Debug Information",
    "section": "",
    "text": "Debug Information\n\nsite.baseurl={{ site.baseurl }}\nsite.url={{ site.url }}\nsite.title={{ site.title }}"
  },
  {
    "objectID": "hpc/howto/storage-size.html",
    "href": "hpc/howto/storage-size.html",
    "title": "File Sizes and Disk Quotas",
    "section": "",
    "text": "The /wynton/ storage is on a ZFS file system on top of our BeeGFS parallel storage system. This is automatically compressed (using lz4 compression in ZFS) before anything is written to the physical drives. Because of this, a 1.0 MiB file is likely to occupy less that 1.0 MiB of drive space. Exactly, how much a file is compressed varies greatly with file format but as a rule of thumb plain text files can be compressed more than files in a binary format. Already compressed files such as GZ or ZIP files are unlikely to be compressed further.\nBecause of this underlying disk compression, command-line tools such as ls and du may not report what you expect it to report. For example, consider an Apptainer image file rocker_r-base.sif of size 274,538,527 bytes (= 274,538,527/1024^2 = 261.8 MiB);\n[alice@dev2 ~]$ ls -l rocker_r-base.sif\n-rwxr-xr-x. 1 alice boblab 274538527 May  8  2018 rocker_r-base.sif\nThe actual space consumed on disk by this file is 256,136,704 bytes (93.3%):\n[alice@dev2 ~]$ ls -s --block-size 1 rocker_r-base.sif\n256136704 rocker_r-base.sif\nUsing the disk-usage tool du, we can see the same if we do:\n[alice@dev2 ~]$ du --apparent-size --block-size=1 rocker_r-base.sif\n274538527       rocker_r-base.sif\nand\n[alice@dev2 ~]$ du --block-size=1 rocker_r-base.sif\n256136704       rocker_r-base.sif\nComment: It is the compressed size that counts towards your disk quota.\n\n\n\nThe BeeGFS file system keeps track on how much disk each of us currently consumes in different storage locations, specifically:\n\nUser home folder ($HOME, i.e. /wynton/home/ or /wynton/protected/home/)\nGroup folder under (i.e. /wynton/group/, /wynton/protected/group/, and /wynton/protected/project/)\nUser files and folders under /wynton/scratch/ and /wynton/protected/scratch/ (unlimited quota)\n\nThese different type of locations are formally referred to as storage pools by BeeGFS.\nIf we ever run out of quota in one storage pool, BeeGFS detects this and prevent us from writing additional data. The symptoms of a full quota may vary. You might get a clear “disk full” error, or you might experience obscure issues such as having problems logging in. You can use the BeeGFS tool beegfs-ctl --getquota ... to check how much disk quota you have and how much you currently consume. Below sections give instructions how to check this for the different storage locations available.\n\nPlease, be patient! If you remove files to clean up your quota, it might take up to ten minutes before these changes are seen with beegfs-ctl --getquota .... This is because BeeGFS updates the quota information only once every ten minutes.\n\n\n\nTo check how much storage space you have consumed on /wynton/home/ or /wynton/protected/home/, and the total amount available to you, call:\nbeegfs-ctl --getquota --storagepoolid=11 --uid \"$USER\"\nFor example,\n[alice@dev2 ~]$ beegfs-ctl --getquota --storagepoolid=11 --uid \"$USER\"\n      user/group     ||           size          ||    chunk files    \n     name     |  id  ||    used    |    hard    ||  used   |  hard   \n--------------|------||------------|------------||---------|---------\n         alice| 99002||   88.71 GiB| 1000.00 GiB||   645266|unlimited\ntells us that user alice has 645,266 files that occupy 88.71 GiB (‘size used’) on the BeeGFS file system out of their 1000.00 GiB (‘size hard’). Importantly, because the /wynton/home/ storage is mirrored, the disk usage (‘size used’) and the available quota (‘size hard’) are reported at twice the size of what you would expect for a non-mirrored storage. This is why your 500-GiB home storage space is reported as 1000 GiB by the beegfs-ctl tool.\n\n\n\nIf your group/lab (e.g. boblab) has purchased additional storage, it is available under /wynton/group/ (Wynton Regular), and possibly also under /wynton/protected/group/ (Wynton Protected), or /wynton/protected/project/ (Wynton Protected). To check how much storage space your group/lab has consumed of the total amount available to it, call:\nbeegfs-ctl --getquota --storagepoolid=12 --gid \"$(id --group)\"\nFor example,\n[alice@dev2 ~]$ beegfs-ctl --getquota --storagepoolid=12 --gid \"$(id --group)\"\n      user/group     ||           size          ||    chunk files\n     name     |  id  ||    used    |    hard    ||  used   |  hard\n--------------|------||------------|------------||---------|---------\n        boblab| 34001||   13.43 TiB|   40.00 TiB||        0|unlimited\nsays that the boblab group is using 13.43 TiB out of the 40.00 TiB group storage they have acquired. If the hard limit is 1 Byte (sic!), that is a placeholder that the group does not have purchased any group storage.\nThe group storage is shared among all group members and does not count toward your personal disk quota under $HOME.\nAny group with purchased storage can have a group folder in both the Wynton Regular /wynton/group/ area and the Wynton Protected /wynton/protected/group/ area at the same time, e.g. /wynton/group/boblab/ and /wynton/protected/group/boblab/. In that case, the group quota usage would include group-owned files in both areas. Your lab can also request to create a subgroup (e.g. boblab-phi) to self-manage the quota for Protected-only data storage (e.g. /wynton/protected/group/boblab-phi/), and it could be allocated as part of the parent quota.\nWynton Protected projects under /wynton/protected/projects/ are for controlling access to IRB related data, where the IRB access group does not encompass the whole PI group or where the access group encompasses selected members of more than one PI group. The quota for the directory in /wynton/protected/projects/ could either be a separate group quota purchase or a portion of a purchased group quota dedicated to the project (subgroup).\n\n\nIt could be that you are part of another lab or institute that have purchased their own storage. This is common for users affiliated with Gladstone Institutes. For example, above, user alice is part of the boblab group;\n[alice@dev2 ~]$ id --group\nboblab\nbut they are also part of the carol_inst group;\n[alice@dev2 ~]$ id --groups\nboblab carol_inst\nTo see the disk quota for all groups, we can call:\nfor group in $(id --groups); do beegfs-ctl --getquota --storagepoolid=12 --gid \"$group\"; done\nFor example,\n[alice@dev2 ~]$ for group in $(id --groups); do beegfs-ctl --getquota --storagepoolid=12 --gid \"$group\"; done\n      user/group     ||           size          ||    chunk files\n     name     |  id  ||    used    |    hard    ||  used   |  hard\n--------------|------||------------|------------||---------|---------\n        boblab| 34001||   13.43 TiB|   40.00 TiB||        0|unlimited\n      user/group     ||           size          ||    chunk files    \n     name     |  id  ||    used    |    hard    ||  used   |  hard   \n--------------|------||------------|------------||---------|---------\n    carol_inst| 35000||   75.16 TiB|  100.00 TiB|| 14957560|unlimited\nwhich shows that alice has access to 40 TiB via the boblab group and another 100 TiB via the carol_inst group.\n\n\n\n\nTo check your disk consumption on the global scratch space (/wynton/scratch/ and /wynton/protected/scratch/), use:\nbeegfs-ctl --getquota --storagepoolid=10 --uid \"$USER\"\nComment: There are no user or group quotas on /wynton/scratch/ and /wynton/protected/scratch/, but files on the global scratch that are older than two weeks are deleted automatically."
  },
  {
    "objectID": "hpc/howto/storage-size.html#file-sizes-on-wynton",
    "href": "hpc/howto/storage-size.html#file-sizes-on-wynton",
    "title": "File Sizes and Disk Quotas",
    "section": "",
    "text": "The /wynton/ storage is on a ZFS file system on top of our BeeGFS parallel storage system. This is automatically compressed (using lz4 compression in ZFS) before anything is written to the physical drives. Because of this, a 1.0 MiB file is likely to occupy less that 1.0 MiB of drive space. Exactly, how much a file is compressed varies greatly with file format but as a rule of thumb plain text files can be compressed more than files in a binary format. Already compressed files such as GZ or ZIP files are unlikely to be compressed further.\nBecause of this underlying disk compression, command-line tools such as ls and du may not report what you expect it to report. For example, consider an Apptainer image file rocker_r-base.sif of size 274,538,527 bytes (= 274,538,527/1024^2 = 261.8 MiB);\n[alice@dev2 ~]$ ls -l rocker_r-base.sif\n-rwxr-xr-x. 1 alice boblab 274538527 May  8  2018 rocker_r-base.sif\nThe actual space consumed on disk by this file is 256,136,704 bytes (93.3%):\n[alice@dev2 ~]$ ls -s --block-size 1 rocker_r-base.sif\n256136704 rocker_r-base.sif\nUsing the disk-usage tool du, we can see the same if we do:\n[alice@dev2 ~]$ du --apparent-size --block-size=1 rocker_r-base.sif\n274538527       rocker_r-base.sif\nand\n[alice@dev2 ~]$ du --block-size=1 rocker_r-base.sif\n256136704       rocker_r-base.sif\nComment: It is the compressed size that counts towards your disk quota."
  },
  {
    "objectID": "hpc/howto/storage-size.html#disk-quota",
    "href": "hpc/howto/storage-size.html#disk-quota",
    "title": "File Sizes and Disk Quotas",
    "section": "",
    "text": "The BeeGFS file system keeps track on how much disk each of us currently consumes in different storage locations, specifically:\n\nUser home folder ($HOME, i.e. /wynton/home/ or /wynton/protected/home/)\nGroup folder under (i.e. /wynton/group/, /wynton/protected/group/, and /wynton/protected/project/)\nUser files and folders under /wynton/scratch/ and /wynton/protected/scratch/ (unlimited quota)\n\nThese different type of locations are formally referred to as storage pools by BeeGFS.\nIf we ever run out of quota in one storage pool, BeeGFS detects this and prevent us from writing additional data. The symptoms of a full quota may vary. You might get a clear “disk full” error, or you might experience obscure issues such as having problems logging in. You can use the BeeGFS tool beegfs-ctl --getquota ... to check how much disk quota you have and how much you currently consume. Below sections give instructions how to check this for the different storage locations available.\n\nPlease, be patient! If you remove files to clean up your quota, it might take up to ten minutes before these changes are seen with beegfs-ctl --getquota .... This is because BeeGFS updates the quota information only once every ten minutes.\n\n\n\nTo check how much storage space you have consumed on /wynton/home/ or /wynton/protected/home/, and the total amount available to you, call:\nbeegfs-ctl --getquota --storagepoolid=11 --uid \"$USER\"\nFor example,\n[alice@dev2 ~]$ beegfs-ctl --getquota --storagepoolid=11 --uid \"$USER\"\n      user/group     ||           size          ||    chunk files    \n     name     |  id  ||    used    |    hard    ||  used   |  hard   \n--------------|------||------------|------------||---------|---------\n         alice| 99002||   88.71 GiB| 1000.00 GiB||   645266|unlimited\ntells us that user alice has 645,266 files that occupy 88.71 GiB (‘size used’) on the BeeGFS file system out of their 1000.00 GiB (‘size hard’). Importantly, because the /wynton/home/ storage is mirrored, the disk usage (‘size used’) and the available quota (‘size hard’) are reported at twice the size of what you would expect for a non-mirrored storage. This is why your 500-GiB home storage space is reported as 1000 GiB by the beegfs-ctl tool.\n\n\n\nIf your group/lab (e.g. boblab) has purchased additional storage, it is available under /wynton/group/ (Wynton Regular), and possibly also under /wynton/protected/group/ (Wynton Protected), or /wynton/protected/project/ (Wynton Protected). To check how much storage space your group/lab has consumed of the total amount available to it, call:\nbeegfs-ctl --getquota --storagepoolid=12 --gid \"$(id --group)\"\nFor example,\n[alice@dev2 ~]$ beegfs-ctl --getquota --storagepoolid=12 --gid \"$(id --group)\"\n      user/group     ||           size          ||    chunk files\n     name     |  id  ||    used    |    hard    ||  used   |  hard\n--------------|------||------------|------------||---------|---------\n        boblab| 34001||   13.43 TiB|   40.00 TiB||        0|unlimited\nsays that the boblab group is using 13.43 TiB out of the 40.00 TiB group storage they have acquired. If the hard limit is 1 Byte (sic!), that is a placeholder that the group does not have purchased any group storage.\nThe group storage is shared among all group members and does not count toward your personal disk quota under $HOME.\nAny group with purchased storage can have a group folder in both the Wynton Regular /wynton/group/ area and the Wynton Protected /wynton/protected/group/ area at the same time, e.g. /wynton/group/boblab/ and /wynton/protected/group/boblab/. In that case, the group quota usage would include group-owned files in both areas. Your lab can also request to create a subgroup (e.g. boblab-phi) to self-manage the quota for Protected-only data storage (e.g. /wynton/protected/group/boblab-phi/), and it could be allocated as part of the parent quota.\nWynton Protected projects under /wynton/protected/projects/ are for controlling access to IRB related data, where the IRB access group does not encompass the whole PI group or where the access group encompasses selected members of more than one PI group. The quota for the directory in /wynton/protected/projects/ could either be a separate group quota purchase or a portion of a purchased group quota dedicated to the project (subgroup).\n\n\nIt could be that you are part of another lab or institute that have purchased their own storage. This is common for users affiliated with Gladstone Institutes. For example, above, user alice is part of the boblab group;\n[alice@dev2 ~]$ id --group\nboblab\nbut they are also part of the carol_inst group;\n[alice@dev2 ~]$ id --groups\nboblab carol_inst\nTo see the disk quota for all groups, we can call:\nfor group in $(id --groups); do beegfs-ctl --getquota --storagepoolid=12 --gid \"$group\"; done\nFor example,\n[alice@dev2 ~]$ for group in $(id --groups); do beegfs-ctl --getquota --storagepoolid=12 --gid \"$group\"; done\n      user/group     ||           size          ||    chunk files\n     name     |  id  ||    used    |    hard    ||  used   |  hard\n--------------|------||------------|------------||---------|---------\n        boblab| 34001||   13.43 TiB|   40.00 TiB||        0|unlimited\n      user/group     ||           size          ||    chunk files    \n     name     |  id  ||    used    |    hard    ||  used   |  hard   \n--------------|------||------------|------------||---------|---------\n    carol_inst| 35000||   75.16 TiB|  100.00 TiB|| 14957560|unlimited\nwhich shows that alice has access to 40 TiB via the boblab group and another 100 TiB via the carol_inst group.\n\n\n\n\nTo check your disk consumption on the global scratch space (/wynton/scratch/ and /wynton/protected/scratch/), use:\nbeegfs-ctl --getquota --storagepoolid=10 --uid \"$USER\"\nComment: There are no user or group quotas on /wynton/scratch/ and /wynton/protected/scratch/, but files on the global scratch that are older than two weeks are deleted automatically."
  },
  {
    "objectID": "hpc/howto/jupyterhub.html",
    "href": "hpc/howto/jupyterhub.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "2023-11-30: The JupyterHub (JHub) server has been discontinued."
  },
  {
    "objectID": "hpc/howto/jupyterhub.html#log-in-to-wynton-jupyterhub-jhub",
    "href": "hpc/howto/jupyterhub.html#log-in-to-wynton-jupyterhub-jhub",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "2023-11-30: The JupyterHub (JHub) server has been discontinued."
  },
  {
    "objectID": "hpc/howto/log-in-without-pwd.html",
    "href": "hpc/howto/log-in-without-pwd.html",
    "title": "Log in without Password",
    "section": "",
    "text": "These instructions explains how to set up your local computer such that you can log into the compute cluster, or copy files to and from the cluster, without having to enter your password each time.\n\n\nIn summary, the steps are:\n\nOn your local computer: Generate private-public key pair\nOn the cluster: Add public key\nOn your local computer: Verify password-free access\nOn your local computer: Edit ~/.ssh/config for setting default SSH options per remote host/server\n\nExpected time: &lt; 10 minutes.\n\nThese instructions are for the ssh client available on Linux, macOS, and MS Windows. If you use the PuTTY SSH client, the overall idea is similar. Please consult the PuTTY user forums for further instructions.\n\n\n\nHere, we will generate a private-public SSH key pair (stored in two files) that is unique for accessing the cluster:\n{local}$ mkdir ~/.ssh\n{local}$ chmod u=rwx,go= ~/.ssh\n{local}$ ssh-keygen -m PEM -f ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\nGenerating public/private rsa key pair.\nCreated directory '/home/alice/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/alice/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\nYour public key has been saved in /home/alice/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub.\nhe key fingerprint is:\nSHA256:2MpJL+I6rQbfhvLZAyC6fa6Y40yZhwG+FYOiHCQ94Fw alice@my_laptop\nThe key\\'s randomart image is:\n+---[RSA 2048]----+\n|o+ E             |\n|= =              |\n|o= +             |\n|O . o  o         |\n|+= .  o S        |\n|o O  o +         |\n| @ *. = .        |\n|*oB+*. .         |\n|+B*O+.           |\n+----[SHA256]-----+\n\nIf you specify a passphrase, your local operating system will ask for the passphrase the first time you try to log in to the cluster. All other login attempts will be passphrase (and password) free (until you reboot the machine). This should work out of the box on macOS and most Linux distributions - on MS Windows you need to set up your SSH agent manually (or use an empty passphrase). If you choose to use an empty passphrase, make sure that your machine is safe and uses a highly secure local login password.\n\n\n🛑 The public key you can safely share with the world, but treat your private key as a password; anyone who has access to it will have access to your account if it does not have a passphrase!\n\n\n\n\n\nNext, we will set up the cluster to recognize your public SSH key. Assuming your cluster user name is alice in the cluster, the goal is to append the content of the public key file to ~/.ssh/authorized_keys on the cluster. There are two ways this can be done.\nAlternative 1: If you have the ssh-copy-id tool installed on your local computer, then use:\n{local}$ ssh-copy-id -i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub alice@log2.wynton.ucsf.edu\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/wynton/home/boblab/alice/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub\"\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nalice@log2.wynton.ucsf.edu:s password: \n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'alice@log2.wynton.ucsf.edu'\"\nand check to make sure that only the key(s) you wanted were added.\n\n{local}$ \nDone.\nAlternative 2: If you don’t have ssh-copy-id, you will have to copy the public key file over to the cluster, log in, append it to the target file, and validate file permissions. Assuming you already have a ~/.ssh folder on the cluster, first copy the public key file to ~/.ssh on the cluster:\n{local}$ scp ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub alice@log2.wynton.ucsf.edu:.ssh/\nlaptop_to_{{ site.cluster.nickname | downcase }}.pub           100%  390     0.4KB/s   00:00\nThen, log into the cluster (still using a password) and append the public key to ~/.ssh/authorized_keys:\n{local}$ ssh -o PreferredAuthentications=keyboard-interactive,password alice@log2.wynton.ucsf.edu\nalice1@169.230.79.12\\'s password: XXXXXXXXXXXXXXXXXXX\n[alice@log2 ~]$ cd .ssh\n[alice@log2 .ssh]$ cat laptop_to_{{ site.cluster.nickname | downcase }}.pub &gt;&gt; authorized_keys\nFinally, make sure that ~/.ssh/authorized_keys on Wynton is only accessible to you (otherwise that file will be completely ignored by SSH);\n[alice@log2 .ssh]$ chmod u=rw,go= ~/.ssh/authorized_keys\n[alice@log2 .ssh]$ stat --format=%A ~/.ssh/authorized_keys\n-rw-------\nLastly, log out from the cluster:\n[alice@log2 .ssh]$ exit\n{local}$ \nDone.\n\n\n\nYou should now be able to log into the cluster from your local computer without having to enter the cluster password. Try the following:\n{local}$ ssh -o PreferredAuthentications=publickey,keyboard-interactive -o IdentitiesOnly=yes -i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }} alice@log2.wynton.ucsf.edu\n[alice@log2 ~]$ \nYou will be asked to enter your passphrase, if you chose one above.\nIf you get\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic,keyboard-interactive,password).\nthen make sure you use the correct user name and that the file permissions on ~/.ssh are correct on your local machine. See Appendix Section ‘Fix non-secure file permission on ~/.ssh/’ for instructions. If it still does not work, check the ~/.ssh permissions on the cluster too.\nThe reason why we use -o PreferredAuthentications=publickey,keyboard-interactive -o IdentitiesOnly=yes in the above test, is so that we can make sure no alternative login mechanisms than our SSH keypair are in play. After having validated the above, these options can be dropped and you can now use:\n{local}$ ssh -i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }} alice@log2.wynton.ucsf.edu\n[alice@log2 ~]$ \n\n\n\nIt is rather tedious having to specify what private key file to use (-i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}) each time you use SSH. As a last step, we will set the default options for alice@log2.wynton.ucsf.edu. On your local machine, add the following directive to ~/.ssh/config (if you don’t have the file, create it):\nHost *.wynton.ucsf.edu\n  User alice\n  IdentityFile ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\nWith all of the above, you should now be able to log in to the cluster using:\n{local}$ ssh log2.wynton.ucsf.edu\n[alice@log2 ~]$ \nBecause we use globbing for Host in the above SSH config directive, it applies to SSH connections for all Wynton connections.\n\n\n\n\n\n\nThese instructions are only for Linux and macOS.\n\nOn your local machine, open a terminal, and run\n{local}$ stat --format=%A ~/.ssh\ndrwx------\nThe stat output consists of four parts: d tells us it is a directory, rw- specifies the permission for the user (u), and the following --- and --- specifies the permissions for the group (g), and all others (o), respectively.\nIf the reported permission for group and others are anything but ---, then scp and ssh don’t trust the folder and will silently ignore your SSH key pair. To secure the folder, do:\n{local}$ chmod u=rwx,go= ~/.ssh\n{local}$ stat --format=%A ~/.ssh\ndrwx------\nExplanation: The above chmod settings specify that you as a user (u) have read (r) and write (w) permissions for this directory. In addition, you have executable (x) permission, which also means you can set it as your working directory. Continuing, the settings also specify that other users in your group (g) as well as all other (o) users on the system have no access at all (empty permission).\n\n\n\nIf you often use ssh -X when connecting to the cluster, that is, you often use X11 Forwarding for remote graphics etc., then you can make this the default by adding ForwardX11 yes to the above Host *.wynton.ucsf.edu in ~/.ssh/config on your local computer:\nHost *.wynton.ucsf.edu\n  User alice\n  IdentityFile ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\n  ForwardX11 yes\n  Compression yes\nWe also add Compression yes, which corresponds to ssh -C, to improve the X11 Forwarding performance.\nIf you use ssh -Y, which you might have to do if you are on macOS, then add also ForwardX11Trusted yes:\n  ForwardX11 yes\n  ForwardX11Trusted yes\n  Compression yes\n\n\n\nThe login nodes should only be used for light-weight tasks such as submitting job scripts, checking that the status of existing jobs, and doing basic file manipulations. We should do all other type of tasks on development nodes, do avoid risk clogging up the login nodes. To avoid having to do two manual SSH steps, one to a login node followed immediately by one to the development, we can set up another SSH configuration directive that does both in one SSH call.\nFirst, make sure you have created the above Host: *.wynton.ucsf.edu directive in ~/.ssh/config on your local computer and verified that it works. Then, append another directive with:\nHost *dev?.wynton.ucsf.edu\n  ProxyJump log2.wynton.ucsf.edu\nThese two directives together will allow you to connect directly to a development host from your local machine, e.g.\n{local}$ ssh dev2.wynton.ucsf.edu\n[alice@dev2 ~]$ \nThis works, because the ProxyJump specification makes the SSH connection use log2.wynton.ucsf.edu as a “jump host” and from there automatically continue to the requested development host. The result of this latter SSH configuration directive is the same as if you would have called ssh -J log2.wynton.ucsf.edu dev2.wynton.ucsf.edu."
  },
  {
    "objectID": "hpc/howto/log-in-without-pwd.html#instruction",
    "href": "hpc/howto/log-in-without-pwd.html#instruction",
    "title": "Log in without Password",
    "section": "",
    "text": "In summary, the steps are:\n\nOn your local computer: Generate private-public key pair\nOn the cluster: Add public key\nOn your local computer: Verify password-free access\nOn your local computer: Edit ~/.ssh/config for setting default SSH options per remote host/server\n\nExpected time: &lt; 10 minutes.\n\nThese instructions are for the ssh client available on Linux, macOS, and MS Windows. If you use the PuTTY SSH client, the overall idea is similar. Please consult the PuTTY user forums for further instructions.\n\n\n\nHere, we will generate a private-public SSH key pair (stored in two files) that is unique for accessing the cluster:\n{local}$ mkdir ~/.ssh\n{local}$ chmod u=rwx,go= ~/.ssh\n{local}$ ssh-keygen -m PEM -f ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\nGenerating public/private rsa key pair.\nCreated directory '/home/alice/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/alice/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\nYour public key has been saved in /home/alice/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub.\nhe key fingerprint is:\nSHA256:2MpJL+I6rQbfhvLZAyC6fa6Y40yZhwG+FYOiHCQ94Fw alice@my_laptop\nThe key\\'s randomart image is:\n+---[RSA 2048]----+\n|o+ E             |\n|= =              |\n|o= +             |\n|O . o  o         |\n|+= .  o S        |\n|o O  o +         |\n| @ *. = .        |\n|*oB+*. .         |\n|+B*O+.           |\n+----[SHA256]-----+\n\nIf you specify a passphrase, your local operating system will ask for the passphrase the first time you try to log in to the cluster. All other login attempts will be passphrase (and password) free (until you reboot the machine). This should work out of the box on macOS and most Linux distributions - on MS Windows you need to set up your SSH agent manually (or use an empty passphrase). If you choose to use an empty passphrase, make sure that your machine is safe and uses a highly secure local login password.\n\n\n🛑 The public key you can safely share with the world, but treat your private key as a password; anyone who has access to it will have access to your account if it does not have a passphrase!"
  },
  {
    "objectID": "hpc/howto/log-in-without-pwd.html#step-2-add-the-public-ssh-key-on-cluster",
    "href": "hpc/howto/log-in-without-pwd.html#step-2-add-the-public-ssh-key-on-cluster",
    "title": "Log in without Password",
    "section": "",
    "text": "Next, we will set up the cluster to recognize your public SSH key. Assuming your cluster user name is alice in the cluster, the goal is to append the content of the public key file to ~/.ssh/authorized_keys on the cluster. There are two ways this can be done.\nAlternative 1: If you have the ssh-copy-id tool installed on your local computer, then use:\n{local}$ ssh-copy-id -i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub alice@log2.wynton.ucsf.edu\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/wynton/home/boblab/alice/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub\"\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nalice@log2.wynton.ucsf.edu:s password: \n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'alice@log2.wynton.ucsf.edu'\"\nand check to make sure that only the key(s) you wanted were added.\n\n{local}$ \nDone.\nAlternative 2: If you don’t have ssh-copy-id, you will have to copy the public key file over to the cluster, log in, append it to the target file, and validate file permissions. Assuming you already have a ~/.ssh folder on the cluster, first copy the public key file to ~/.ssh on the cluster:\n{local}$ scp ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}.pub alice@log2.wynton.ucsf.edu:.ssh/\nlaptop_to_{{ site.cluster.nickname | downcase }}.pub           100%  390     0.4KB/s   00:00\nThen, log into the cluster (still using a password) and append the public key to ~/.ssh/authorized_keys:\n{local}$ ssh -o PreferredAuthentications=keyboard-interactive,password alice@log2.wynton.ucsf.edu\nalice1@169.230.79.12\\'s password: XXXXXXXXXXXXXXXXXXX\n[alice@log2 ~]$ cd .ssh\n[alice@log2 .ssh]$ cat laptop_to_{{ site.cluster.nickname | downcase }}.pub &gt;&gt; authorized_keys\nFinally, make sure that ~/.ssh/authorized_keys on Wynton is only accessible to you (otherwise that file will be completely ignored by SSH);\n[alice@log2 .ssh]$ chmod u=rw,go= ~/.ssh/authorized_keys\n[alice@log2 .ssh]$ stat --format=%A ~/.ssh/authorized_keys\n-rw-------\nLastly, log out from the cluster:\n[alice@log2 .ssh]$ exit\n{local}$ \nDone."
  },
  {
    "objectID": "hpc/howto/log-in-without-pwd.html#step-3-test",
    "href": "hpc/howto/log-in-without-pwd.html#step-3-test",
    "title": "Log in without Password",
    "section": "",
    "text": "You should now be able to log into the cluster from your local computer without having to enter the cluster password. Try the following:\n{local}$ ssh -o PreferredAuthentications=publickey,keyboard-interactive -o IdentitiesOnly=yes -i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }} alice@log2.wynton.ucsf.edu\n[alice@log2 ~]$ \nYou will be asked to enter your passphrase, if you chose one above.\nIf you get\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic,keyboard-interactive,password).\nthen make sure you use the correct user name and that the file permissions on ~/.ssh are correct on your local machine. See Appendix Section ‘Fix non-secure file permission on ~/.ssh/’ for instructions. If it still does not work, check the ~/.ssh permissions on the cluster too.\nThe reason why we use -o PreferredAuthentications=publickey,keyboard-interactive -o IdentitiesOnly=yes in the above test, is so that we can make sure no alternative login mechanisms than our SSH keypair are in play. After having validated the above, these options can be dropped and you can now use:\n{local}$ ssh -i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }} alice@log2.wynton.ucsf.edu\n[alice@log2 ~]$"
  },
  {
    "objectID": "hpc/howto/log-in-without-pwd.html#step-4-avoid-having-to-specify-ssh-option--i-on-local-machine",
    "href": "hpc/howto/log-in-without-pwd.html#step-4-avoid-having-to-specify-ssh-option--i-on-local-machine",
    "title": "Log in without Password",
    "section": "",
    "text": "It is rather tedious having to specify what private key file to use (-i ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}) each time you use SSH. As a last step, we will set the default options for alice@log2.wynton.ucsf.edu. On your local machine, add the following directive to ~/.ssh/config (if you don’t have the file, create it):\nHost *.wynton.ucsf.edu\n  User alice\n  IdentityFile ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\nWith all of the above, you should now be able to log in to the cluster using:\n{local}$ ssh log2.wynton.ucsf.edu\n[alice@log2 ~]$ \nBecause we use globbing for Host in the above SSH config directive, it applies to SSH connections for all Wynton connections."
  },
  {
    "objectID": "hpc/howto/log-in-without-pwd.html#appendix",
    "href": "hpc/howto/log-in-without-pwd.html#appendix",
    "title": "Log in without Password",
    "section": "",
    "text": "These instructions are only for Linux and macOS.\n\nOn your local machine, open a terminal, and run\n{local}$ stat --format=%A ~/.ssh\ndrwx------\nThe stat output consists of four parts: d tells us it is a directory, rw- specifies the permission for the user (u), and the following --- and --- specifies the permissions for the group (g), and all others (o), respectively.\nIf the reported permission for group and others are anything but ---, then scp and ssh don’t trust the folder and will silently ignore your SSH key pair. To secure the folder, do:\n{local}$ chmod u=rwx,go= ~/.ssh\n{local}$ stat --format=%A ~/.ssh\ndrwx------\nExplanation: The above chmod settings specify that you as a user (u) have read (r) and write (w) permissions for this directory. In addition, you have executable (x) permission, which also means you can set it as your working directory. Continuing, the settings also specify that other users in your group (g) as well as all other (o) users on the system have no access at all (empty permission).\n\n\n\nIf you often use ssh -X when connecting to the cluster, that is, you often use X11 Forwarding for remote graphics etc., then you can make this the default by adding ForwardX11 yes to the above Host *.wynton.ucsf.edu in ~/.ssh/config on your local computer:\nHost *.wynton.ucsf.edu\n  User alice\n  IdentityFile ~/.ssh/laptop_to_{{ site.cluster.nickname | downcase }}\n  ForwardX11 yes\n  Compression yes\nWe also add Compression yes, which corresponds to ssh -C, to improve the X11 Forwarding performance.\nIf you use ssh -Y, which you might have to do if you are on macOS, then add also ForwardX11Trusted yes:\n  ForwardX11 yes\n  ForwardX11Trusted yes\n  Compression yes\n\n\n\nThe login nodes should only be used for light-weight tasks such as submitting job scripts, checking that the status of existing jobs, and doing basic file manipulations. We should do all other type of tasks on development nodes, do avoid risk clogging up the login nodes. To avoid having to do two manual SSH steps, one to a login node followed immediately by one to the development, we can set up another SSH configuration directive that does both in one SSH call.\nFirst, make sure you have created the above Host: *.wynton.ucsf.edu directive in ~/.ssh/config on your local computer and verified that it works. Then, append another directive with:\nHost *dev?.wynton.ucsf.edu\n  ProxyJump log2.wynton.ucsf.edu\nThese two directives together will allow you to connect directly to a development host from your local machine, e.g.\n{local}$ ssh dev2.wynton.ucsf.edu\n[alice@dev2 ~]$ \nThis works, because the ProxyJump specification makes the SSH connection use log2.wynton.ucsf.edu as a “jump host” and from there automatically continue to the requested development host. The result of this latter SSH configuration directive is the same as if you would have called ssh -J log2.wynton.ucsf.edu dev2.wynton.ucsf.edu."
  },
  {
    "objectID": "hpc/howto/gui-x11fwd.html",
    "href": "hpc/howto/gui-x11fwd.html",
    "title": "Graphical User Interfaces (GUI)",
    "section": "",
    "text": "The Wynton HPC environment supports running a graphical user interface (GUI) on Wynton HPC while viewing and interacting with it on your local computer. More specifically, and in more technical terms, Wynton HPC supports NX and X11 Forwarding protocols.\n\n\n\n\nDue to limitation in X2Go Client, it is not possible to connect to Wynton HPC when using SSH password(*). Instead, in order to connect to Wynton using the X2Go Client, you have to have a working SSH key pair set up and configured the X2Go Client.\nIf you are connecting to Wynton from off campus, you will also have to use Two Factor Authentication (2FA) for Wynton. This is not needed if you are connected via the UCSF VPN.\n\n\n\nFirst, you will need to install the X2Go Client on your local computer. For instructions on how to do this, see Installing X2Go Client. When you first run x2goclient:\n\nUse the menus to create a New Session …\nPick a Session name, e.g. Wynton HPC\nSet the Host to a development node: dev1, dev2, dev3, or gpudev1pdev1 or pgpudev1\nSet the Login to your Wynton HPC username, e.g. alice\nIn the Use RSA/DSA key for ssh connection, enter the path to the your private SSH Key.\nSelect Try auto login (via SSH Agent or default SSH key)\nCheck Use Proxy server for SSH connection\nThen in the Proxy server section:\n\nCheck Same login as on X2Go Server\nCheck Same password as on X2Go Server\nSet Host to a Wynton HPC login node: log1.wynton.ucsf.edu or log2.wynton.ucsf.eduplog1.wynton.ucsf.edu\nIn the RSA/DSA key: field enter the path to your private SSH Key.\nSelect SSH Agent or default SSH key\n\nIn the Session type section, choose MATE\n\n\n\n\nWith the above setup, the following instructions opens a remote desktop window on your local computer:\n\nLaunch the X2Go Client on your local machine\nDouble click the session configured (above)\nDialog Enter Passphrase to decrypt a key: Enter ssh key passphrase, if set, otherwise just ENTER\nIf connecting from an off-campus location and not already authenticated:\n\nDialog Choose DUO Authentication Method: Perform Duo Authentication as prompted\nDialog Remember connection authentication for 12 hours? [y/N]: Answer y (this is important, it does not work if you choose ‘no’)\n\nDialog MATE on dev2: Enter your Wynton HPC password (this happens for unknown reasons to some users, even when using SSH keys)\nDialog Enter passphrase to decrypt a key: Enter ssh key passphrase, if set\nDialog Enter passphrase to decrypt a key: Enter ssh key passphrase, if set (yes, twice)\nWait! It will take a long time before the MATE window to appear, looks like an empty Linux desktop. Resize the window if you wish, and start up terminals, web browsers, etc. as you would do on your regular desktop.\n\n\nIf you get a dialog saying ‘Error: Connection failed. bash: x2golistsessions: command not found’, then you have missed configuring a ‘Proxy server’ in Steps 7-8 of Section ‘Setup of the X2Go Client (once)’.\n\n\n\n\n\nYou can also use X11 forwarding over the SSH connection used to connect to Wynton HPC.\n\n\nNote that, to do this, you will need to be running an X server on your local machine. You can check this by verifying that environment variable DISPLAY is set on your local computer, e.g.\n{local}$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY=':0'\nIf DISPLAY is empty, that is, you get:\n{local}$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY=''\nthen you don’t have a local X server set up and the below will not work. If you are on macOS, we recommend installing open-source XQuartz.\n\n\n\nTo setup the X11 forwarding when connecting to the cluster, add option -X, or -Y on macOS, to your SSH call. For performance reasons, we will also add option -C to enable SSH compression. By using compression, the responsiveness and latency in GUIs will be much smaller - our benchmarks show a 5-7 times improvement when connected via the UCSF VPN (~60 Mbps download and ~5 Mbps upload). To login with X11 forwarding and compression enabled, do:\n{local}$ ssh -X -C alice@log2.wynton.ucsf.edu\nalice1@log2.wynton.ucsf.edu:s password: \n[alice@log2 ~]$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY='localhost:20.0'\n[alice@log2 ~]$ \nBy checking that DISPLAY is set, we know that X11 forwarding is in place. If DISPLAY is empty, then make sure you have specified -X (or -Y).\n\nIf you are on macOS, you need to use ssh -Y ... instead of ssh -X .... This is because macOS does not trust remote X servers by default.\n\n\n\n\nNow, since we should not run anything on the login nodes, the next step is to head over to one of the development nodes. When doing so, we have remember to keep using X11 forward, that is, we need to use -X also here;\n[alice@log2 ~]$ ssh -X dev2.wynton.ucsf.edu\nalice1@dev2:s password: \n[alice@dev2 ~]$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY='localhost:14.0'\n[alice@dev2 ~]$ \nComment: There is no need to use SSH compression in this second step. If used, it might even have a negative effect on the X11 latency.\nNow we have an X11 forward setup that runs all the way back to our local computer. This will allow us to open, for instance, an XTerm window that runs on dev2.wynton.ucsf.edu but can be interacted with on the local computer;\n[alice@dev2 ~]$ xterm\n[ ... an XTerm window is opened up ... ]\nIf you get an error here, make sure that DISPLAY is set and non-empty.\nTips: You can login into a development node in a single call by “jumping” (-J) via the login node, e.g.\n{local}$ ssh -X -C -J alice@log2.wynton.ucsf.edu alice@dev2.wynton.ucsf.edu\n[alice@dev2 ~]$"
  },
  {
    "objectID": "hpc/howto/gui-x11fwd.html#x2go-remote-desktop",
    "href": "hpc/howto/gui-x11fwd.html#x2go-remote-desktop",
    "title": "Graphical User Interfaces (GUI)",
    "section": "",
    "text": "Due to limitation in X2Go Client, it is not possible to connect to Wynton HPC when using SSH password(*). Instead, in order to connect to Wynton using the X2Go Client, you have to have a working SSH key pair set up and configured the X2Go Client.\nIf you are connecting to Wynton from off campus, you will also have to use Two Factor Authentication (2FA) for Wynton. This is not needed if you are connected via the UCSF VPN.\n\n\n\nFirst, you will need to install the X2Go Client on your local computer. For instructions on how to do this, see Installing X2Go Client. When you first run x2goclient:\n\nUse the menus to create a New Session …\nPick a Session name, e.g. Wynton HPC\nSet the Host to a development node: dev1, dev2, dev3, or gpudev1pdev1 or pgpudev1\nSet the Login to your Wynton HPC username, e.g. alice\nIn the Use RSA/DSA key for ssh connection, enter the path to the your private SSH Key.\nSelect Try auto login (via SSH Agent or default SSH key)\nCheck Use Proxy server for SSH connection\nThen in the Proxy server section:\n\nCheck Same login as on X2Go Server\nCheck Same password as on X2Go Server\nSet Host to a Wynton HPC login node: log1.wynton.ucsf.edu or log2.wynton.ucsf.eduplog1.wynton.ucsf.edu\nIn the RSA/DSA key: field enter the path to your private SSH Key.\nSelect SSH Agent or default SSH key\n\nIn the Session type section, choose MATE\n\n\n\n\nWith the above setup, the following instructions opens a remote desktop window on your local computer:\n\nLaunch the X2Go Client on your local machine\nDouble click the session configured (above)\nDialog Enter Passphrase to decrypt a key: Enter ssh key passphrase, if set, otherwise just ENTER\nIf connecting from an off-campus location and not already authenticated:\n\nDialog Choose DUO Authentication Method: Perform Duo Authentication as prompted\nDialog Remember connection authentication for 12 hours? [y/N]: Answer y (this is important, it does not work if you choose ‘no’)\n\nDialog MATE on dev2: Enter your Wynton HPC password (this happens for unknown reasons to some users, even when using SSH keys)\nDialog Enter passphrase to decrypt a key: Enter ssh key passphrase, if set\nDialog Enter passphrase to decrypt a key: Enter ssh key passphrase, if set (yes, twice)\nWait! It will take a long time before the MATE window to appear, looks like an empty Linux desktop. Resize the window if you wish, and start up terminals, web browsers, etc. as you would do on your regular desktop.\n\n\nIf you get a dialog saying ‘Error: Connection failed. bash: x2golistsessions: command not found’, then you have missed configuring a ‘Proxy server’ in Steps 7-8 of Section ‘Setup of the X2Go Client (once)’."
  },
  {
    "objectID": "hpc/howto/gui-x11fwd.html#x11-forwarding-over-ssh",
    "href": "hpc/howto/gui-x11fwd.html#x11-forwarding-over-ssh",
    "title": "Graphical User Interfaces (GUI)",
    "section": "",
    "text": "You can also use X11 forwarding over the SSH connection used to connect to Wynton HPC.\n\n\nNote that, to do this, you will need to be running an X server on your local machine. You can check this by verifying that environment variable DISPLAY is set on your local computer, e.g.\n{local}$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY=':0'\nIf DISPLAY is empty, that is, you get:\n{local}$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY=''\nthen you don’t have a local X server set up and the below will not work. If you are on macOS, we recommend installing open-source XQuartz.\n\n\n\nTo setup the X11 forwarding when connecting to the cluster, add option -X, or -Y on macOS, to your SSH call. For performance reasons, we will also add option -C to enable SSH compression. By using compression, the responsiveness and latency in GUIs will be much smaller - our benchmarks show a 5-7 times improvement when connected via the UCSF VPN (~60 Mbps download and ~5 Mbps upload). To login with X11 forwarding and compression enabled, do:\n{local}$ ssh -X -C alice@log2.wynton.ucsf.edu\nalice1@log2.wynton.ucsf.edu:s password: \n[alice@log2 ~]$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY='localhost:20.0'\n[alice@log2 ~]$ \nBy checking that DISPLAY is set, we know that X11 forwarding is in place. If DISPLAY is empty, then make sure you have specified -X (or -Y).\n\nIf you are on macOS, you need to use ssh -Y ... instead of ssh -X .... This is because macOS does not trust remote X servers by default.\n\n\n\n\nNow, since we should not run anything on the login nodes, the next step is to head over to one of the development nodes. When doing so, we have remember to keep using X11 forward, that is, we need to use -X also here;\n[alice@log2 ~]$ ssh -X dev2.wynton.ucsf.edu\nalice1@dev2:s password: \n[alice@dev2 ~]$ echo \"DISPLAY='$DISPLAY'\"\nDISPLAY='localhost:14.0'\n[alice@dev2 ~]$ \nComment: There is no need to use SSH compression in this second step. If used, it might even have a negative effect on the X11 latency.\nNow we have an X11 forward setup that runs all the way back to our local computer. This will allow us to open, for instance, an XTerm window that runs on dev2.wynton.ucsf.edu but can be interacted with on the local computer;\n[alice@dev2 ~]$ xterm\n[ ... an XTerm window is opened up ... ]\nIf you get an error here, make sure that DISPLAY is set and non-empty.\nTips: You can login into a development node in a single call by “jumping” (-J) via the login node, e.g.\n{local}$ ssh -X -C -J alice@log2.wynton.ucsf.edu alice@dev2.wynton.ucsf.edu\n[alice@dev2 ~]$"
  },
  {
    "objectID": "hpc/howto/r.html",
    "href": "hpc/howto/r.html",
    "title": "Work with R",
    "section": "",
    "text": "R is available on Wynton HPC via a contributed environment module.\n\n\nTo load the R module available in the CBI software stack, do:\n[alice@dev2 ~]$ module load CBI\n[alice@dev2 ~]$ module load r\nwhich provides access to a modern version of R:\n\n[alice@dev2 ~]$ R \n\nR version 4.5.2 (2025-10-31) -- \"[Not] Part in a Rumble\"\nCopyright (C) 2025 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; 1+2\n[1] 3\n&gt; quit()\nSave workspace image? [y/n/c]: n\n[alice@dev2 ~]$ \nTo use an older version of R, specify the version when you load R, e.g.\n[alice@dev2 ~]$ module load CBI\n[alice@dev2 ~]$ module load r/3.5.3\n\n\n\nIn order to run R in jobs, the above R environment module needs to be loaded just as when you run it interactively on a development node. For example, to run the my_script.R script, the job script should at a minimum contain:\n#! /usr/bin/env bash\n#$ -S /bin/bash\n#$ -cwd\n\nmodule load CBI\nmodule load r\nRscript my_script.R\n\n\n\n\nR 4.5.0 was released on 2025-04-11, R 4.5.1 on 2025-06-13 and R 4.5.2 on 2025-10-31. Bioconductor 3.21 was release on 2024-04-16 and Bioconductor 3.22 was release on 2024-10-30.\nWe have confirmed that more than 99% of the CRAN packages and more than 99% of the Bioconductor Software packages install out of the box when following the below instructions. The packages that failed to install do so either because they depend on a system library that is not available on the cluster, or because they have bugs preventing them from being installed out of the box. If you need to install any of those, please reach out on one of the support channels.\n\nThe majority of R packages are available from CRAN (Comprehensive R Archive Network). Another dominant repository of R packages is Bioconductor, which provides R packages with a focus on bioinformatics. Packages available from Bioconductor are not available on CRAN, and vice versa. At times, you will find online instructions for installing R packages hosted on, for instance, GitHub and GitLab. Before installing an R package from such sources, we highly recommend to install the package from CRAN or Bioconductor, if it is available there, because packages hosted on the latter are stable releases and often better tested.\nBefore continuing, it is useful to understand where R packages looks for locally installed R packages. There are three locations that R considers:\n\nYour personal R package library. This is located under ~/R/, e.g. ~/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/\n(optional) A site-wide R package library (not used on Wynton HPC)\nThe system-wide R package library part of the R installed, e.g. /wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/library\n\nFor instance, when we try to load an R package:\n&gt; library(datasets)\nR will search the above folders in order for R package ‘datasets’. When you start you fresh, the only R packages available to you are the ones installed in folder (3) - the system-wide library. The ‘datasets’ package comes with the R installation, so with a fresh setup, it will be loaded from the third location. As we will see below, when you install your own packages, they will all be installed into folder (1) - your personal library. The first time your run R, the personal library folder does not exists, so R will ask you whether or not you want to create that folder. If asked, you should always accept (answer ‘Yes’). If you had already created this folder, R will install into this folder without asking.\nFinally, R undergoes a main update once a year (around April and May). For example, R 4.5.0 was release in April 2025. The next main release will be R 4.6.0 a year later. Whenever the y component in R x.y.z version is increased, you will start out with an empty personal package folder specific for R x.y (regardless of z). This means that you will have to re-install all R packages you had installed during the year before the new main release came out. Yes, this can be tedious and can take quite some time but it will improve stability and yet allow the R developers to keep improving R. Of course, you can still keep using an older version of R and all the packages you have installed for that version - they will not be removed.\n\n\nPackages available on CRAN can be installed using the install.packages() function in R. The default behavior of R is to always ask you which one of the many CRAN mirrors you want to install from (their content is all identical). To avoid this question, tell R to always use the first one:\n&gt; chooseCRANmirror(ind = 1)\n&gt;\nNow, in order to install, for instance, the zoo package available on CRAN, call:\n&gt; install.packages(\"zoo\")\nWarning in install.packages(\"zoo\") :\n  'lib = \"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/library\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel)\nWe notice two things. First there is a warning mentioning that a “lib” folder was “not writable”. This is because your personal library folder did not yet exists and R tried to install to location (3) but failed (because you do not have write permission there). This is where R decided to ask you whether or not you want to install to a personal library. Answer ‘yes’:\nWould you like to use a personal library instead? (yes/No/cancel) yes\nWould you like to create a personal library\n'~/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\nto install packages into? (yes/No/cancel)\nR wants to make sure you are aware what is done, so it will, conservatively, also ask if you accept the default location. Answer ‘yes’ for this folder to be created. After this, the current and all future package installation in R will be installed into this folder without further questions asked. In this example, we will get:\n\nWould you like to create a personal library\n'~/R/x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\nto install packages into? (yes/No/cancel) yes\ntrying URL 'https://cloud.r-project.org/src/contrib/zoo_1.8-14.tar.gz'\nContent type 'application/x-gzip' length 778426 bytes (760 KB)\n==================================================\ndownloaded 760 KB\n\n* installing *source* package ‘zoo’ ...\n** this is package ‘zoo’ version ‘1.8-14’\n** package ‘zoo’ successfully unpacked and MD5 sums checked\n** using staged installation\n** libs\nusing C compiler: ‘gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)’\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I../inst/include  -I/usr/local/include    -fpic  -g -O2  -c coredata.c -o coredata.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I../inst/include  -I/usr/local/include    -fpic  -g -O2  -c init.c -o init.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I../inst/include  -I/usr/local/include    -fpic  -g -O2  -c lag.c -o lag.o\ngcc -std=gnu2x -shared -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -L/usr/local/lib64 -o zoo.so coredata.o init.o lag.o -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -lR\ninstalling to /wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/00LOCK-zoo/00new/zoo/libs\n** R\n** demo\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (zoo)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpVm3e6t/downloaded_packages'\n&gt;\nIf there is no mentioning of an “error” (a “warning” is ok in R but never an “error”), then the package was successfully installed. If you see * DONE (zoo) at the end, it means that the package was successfully installed. As with any other package in R, you can also verify that it is indeed installed by loading it, i.e.\n&gt; library(zoo)\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n&gt;\n\n\nIf a new version of one or more CRAN packages is released, they can be installed by calling:\n&gt; chooseCRANmirror(ind = 1)\n&gt; update.packages()\n...\n\n\n\n\nPer Bioconductor’s best practices, R packages from Bioconductor should be installed using BiocManager::install(). This is to guarantee maximum compatibility between all Bioconductor packages.\n\n\nIf you already have BiocManager installed, you can skip this section. When you start out fresh, the package BiocManager is not installed meaning that calling BiocManager::install() will fail. We need to start by installing it from CRAN (sic!);\n\ne&gt; install.packages(\"BiocManager\")\nInstalling package into '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\n(as 'lib' is unspecified)\ntrying URL 'https://cloud.r-project.org/src/contrib/BiocManager_1.30.26.tar.gz'\nContent type 'application/x-gzip' length 582690 bytes (580 KB)\n==================================================\ndownloaded 569 KB\n\n* installing *source* package ‘BiocManager’ ...\n** package ‘BiocManager’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (BiocManager)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpSRgaB4/downloaded_packages'\n&gt; \nComment: If this is the very first R package you installed, see above CRAN instructions for setting a default CRAN mirror and creating a personal library folder.\n\n\n\nWith BiocManager installed, we can now install any Bioconductor package. For instance, to install limma, and all of its dependencies, call:\n\n&gt; BiocManager::install(\"limma\")\nBioconductor version 3.21 (BiocManager 1.30.26), R 4.5.1 (2025-06-13)\nInstalling package(s) 'limma'\ntrying URL 'https://bioconductor.org/packages/3.21/bioc/src/contrib/limma_3.64.3.tar.gz'\nContent type 'application/x-gzip' length 2846680 bytes (2.7 MB)\n==================================================\ndownloaded 2.7 MB\n\n* installing *source* package ‘limma’ ...\n** this is package ‘limma’ version ‘3.64.3’\n** using staged installation\n** libs\nusing C compiler: ‘gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)’\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG   -I/usr/local/include    -fpic  -g -O2  -c init.c -o init.o                                                                                          \ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG   -I/usr/local/include    -fpic  -g -O2  -c normexp.c -o normexp.o                                                                                    \ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG   -I/usr/local/include    -fpic  -g -O2  -c weighted_lowess.c -o weighted_lowess.o                                                                    \ngcc -std=gnu2x -shared -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -L/usr/local/lib64 -o limma.so init.o normexp.o weighted_lowess.o -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -lR               \ninstalling to /wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/00LOCK-limma/00new/limma/libs\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (limma)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpIoJ3Np/downloaded_packages'\n&gt;\nThere were no “error” messages, so the installation was successful. To verify that it worked, we can load the package in R as:\n&gt; library(limma)\n&gt;\n\n\n\nTo install Bioconductor updates, call BiocManager::install() without arguments:\n&gt; BiocManager::install()\nComment: This will actually also update any CRAN packages.\n\n\n\n\n\n\n\nIf you have an R scripts, and it involves setting up a number of parallel workers in R, do not use ncores &lt;- detectCores() of the parallel package because it will result in your job hijacking all cores on the compute node regardless of how many cores the scheduler has given you. Taking up all CPU resources without permission is really bad practice and a common cause for problems. A much better solution is to use availableCores() that is available in the parallelly package, e.g. as ncores &lt;- parallelly::availableCores(). This function is backward compatible with detectCores() while respecting what the scheduler has allocated for your job.\n\n\n\nAs of 2024-04-26, the “recommended” MASS and Matrix packages require R (&gt;= 4.4.0) [2024-04-24]. If you run an older version of R, you can install older versions of them that are compatible with R (&lt; 4.4.0) using:\n&gt; install.packages(\"https://cran.r-project.org/src/contrib/Archive/MASS/MASS_7.3-60.0.1.tar.gz\", type = \"source\")\n\n&gt; install.packages(\"https://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.6-5.tar.gz\", type = \"source\")\n\n\n\nSome R packages rely on the Message Passing Interface (MPI), e.g. Rmpi, pbdMPI and bigGP. To use these, but also to install them we need to load the built-in mpi module;\n\n[alice@dev2 ~]$ module load mpi/openmpi-x86_64\n[alice@dev2 ~]$ module list\n\nCurrently Loaded Modules:\n  1) CBI   2) scl-gcc-toolset/13   3) r/4.5.2   4) mpi/openmpi-x86_64\n\n \nImportantly, make sure to specify the exact version of the mpi module as well so that your code will keep working also when a newer version becomes the new default. Note that you will have to load the same mpi module, and version(!), also whenever you run R code that requires these MPI-dependent R packages.\nIn addition to making OpenMPI available by loading the mpi module, several MPI-based R packages requires additional special care in order to install. Below sections, show how to install them.\n\n\nAfter loading the mpi module, the Rmpi package installs out-of-the-box like other R packages. After installing it, you can verify that it works by running the following example commands:\n[alice@dev2 ~]$ module load CBI r\n[alice@dev2 ~]$ module load mpi/openmpi-x86_64\n[alice@dev2 ~]$ R\n...\n&gt; library(Rmpi)\n[1684426121.677063] [c4-dev3:23125:0]            sys.c:618  UCX  ERROR shmget(size=2097152 flags=0xfb0) for mm_recv_desc failed: Operation not permitted, please check shared memory limits by 'ipcs -l'\n\n&gt; mpi.spawn.Rslaves()              ## launch one or more MPI parallel workers\n        1 slaves are spawned successfully. 0 failed.\n[1684426140.976380] [c4-dev3:23125:0]            sys.c:618  UCX  ERROR shmget(size=2097152 flags=0xb80) for ucp_am_bufs failed: Operation not permitted, please check shared memory limits by 'ipcs -l'\nmaster (rank 0, comm 1) of size 2 is running on: dev2 \nslave1 (rank 1, comm 1) of size 2 is running on: dev2\n\n&gt; mpi.remote.exec(Sys.getpid())    ## get the process ID for one of them\n     out\n1 189114\n\n\n\n\nContrary to Rmpi above, packages such pbdMPI and bigGP require more hand-holding to install. For example, after having loaded the mpi module, we can install pbdMPI in R as:\n\n&gt; install.packages(\"pbdMPI\", configure.args=\"--with-mpi-include=$MPI_INCLUDE --with-mpi-libpath=$MPI_LIB --with-mpi-type=OPENMPI\")\nInstalling package into '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\n(as 'lib' is unspecified)\ntrying URL 'https://cloud.r-project.org/src/contrib/pbdMPI_0.5-4.tar.gz'\nContent type 'application/x-gzip' length 423376 bytes (413 KB)\n==================================================\ndownloaded 413 KB\n\n* installing *source* package ‘pbdMPI’ ...\n** this is package ‘pbdMPI’ version ‘0.5-4’\n** package ‘pbdMPI’ successfully unpacked and MD5 sums checked\n** using staged installation\nchecking for sed... /usr/bin/sed\nchecking for mpicc... mpicc\nchecking for ompi_info... ompi_info\nchecking for pkg-config... /usr/bin/pkg-config\n&gt;&gt; TMP_FOUND = Nothing found from mpicc --show & sed nor pkg-config ...\nchecking for openpty in -lutil... yes\nchecking for main in -lpthread... yes\n\n******************* Results of pbdMPI package configure *****************\n\n&gt;&gt; MPIRUN = /usr/lib64/openmpi/bin/mpirun\n&gt;&gt; MPIEXEC = /usr/lib64/openmpi/bin/mpiexec\n&gt;&gt; ORTERUN = /usr/lib64/openmpi/bin/orterun\n&gt;&gt; TMP_INC =\n&gt;&gt; TMP_LIB =\n&gt;&gt; TMP_LIBNAME =\n&gt;&gt; TMP_FOUND = Nothing found from mpicc --show & sed nor pkg-config ...\n&gt;&gt; MPI_ROOT =\n&gt;&gt; MPITYPE = OPENMPI\n&gt;&gt; MPI_INCLUDE_PATH = /usr/include/openmpi-x86_64\n&gt;&gt; MPI_LIBPATH = /usr/lib64/openmpi/lib\n&gt;&gt; MPI_LIBNAME =\n&gt;&gt; MPI_LIBS =  -lutil -lpthread\n&gt;&gt; MPI_DEFS = -DMPI2\n&gt;&gt; MPI_INCL2 =\n&gt;&gt; MPI_LDFLAGS =\n&gt;&gt; PKG_CPPFLAGS = -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI\n&gt;&gt; PKG_LIBS = -L/usr/lib64/openmpi/lib -lmpi  -lutil -lpthread\n&gt;&gt; PROF_LDFLAGS =\n&gt;&gt; ENABLE_LD_LIBRARY_PATH = no\n\n*************************************************************************\n\nconfigure: creating ./config.status\nconfig.status: creating src/Makevars\nconfigure: creating ./config.status\nconfig.status: creating src/Makevars\nconfig.status: creating R/zzz.r\nconfig.status: creating R/util_execmpi.r\n** libs\nusing C compiler: ‘gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)’\necho \"MPIRUN = /usr/lib64/openmpi/bin/mpirun\" &gt; Makeconf\necho \"MPIEXEC = /usr/lib64/openmpi/bin/mpiexec\" &gt;&gt; Makeconf\necho \"ORTERUN = /usr/lib64/openmpi/bin/orterun\" &gt;&gt; Makeconf\necho \"TMP_INC = \" &gt;&gt; Makeconf\necho \"TMP_LIB = \" &gt;&gt; Makeconf\necho \"TMP_LIBNAME = \" &gt;&gt; Makeconf\necho \"TMP_FOUND = Nothing found from mpicc --show & sed nor pkg-config ...\" &gt;&gt; Makeconf\necho \"MPI_ROOT = \" &gt;&gt; Makeconf\necho \"MPITYPE = OPENMPI\" &gt;&gt; Makeconf\necho \"MPI_INCLUDE_PATH = /usr/include/openmpi-x86_64\" &gt;&gt; Makeconf\necho \"MPI_LIBPATH = /usr/lib64/openmpi/lib\" &gt;&gt; Makeconf\necho \"MPI_LIBNAME = \" &gt;&gt; Makeconf\necho \"MPI_LIBS =  -lutil -lpthread\" &gt;&gt; Makeconf\necho \"MPI_DEFS = -DMPI2\" &gt;&gt; Makeconf\necho \"MPI_INCL2 = \" &gt;&gt; Makeconf\necho \"MPI_LDFLAGS = \" &gt;&gt; Makeconf\necho \"PKG_CPPFLAGS = -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI\" &gt;&gt; Makeconf\necho \"PKG_LIBS = -L/usr/lib64/openmpi/lib -lmpi  -lutil -lpthread\" &gt;&gt; Makeconf\necho \"PROF_LDFLAGS = \" &gt;&gt; Makeconf\necho \"ENABLE_LD_LIBRARY_PATH = no\" &gt;&gt; Makeconf\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c comm_errors.c -o comm_errors.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c comm_sort_double.c -o comm_sort_double.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c comm_sort_integer.c -o comm_sort_integer.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c pkg_dl.c -o pkg_dl.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c pkg_tools.c -o pkg_tools.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd.c -o spmd.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_allgather.c -o spmd_allgather.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_allgatherv.c -o spmd_allgatherv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_allreduce.c -o spmd_allreduce.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_alltoall.c -o spmd_alltoall.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_alltoallv.c -o spmd_alltoallv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_bcast.c -o spmd_bcast.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_communicator.c -o spmd_communicator.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_communicator_spawn.c -o spmd_communicator_spawn.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_gather.c -o spmd_gather.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_gatherv.c -o spmd_gatherv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_info.c -o spmd_info.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_recv.c -o spmd_recv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_reduce.c -o spmd_reduce.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_scatter.c -o spmd_scatter.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_scatterv.c -o spmd_scatterv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_send.c -o spmd_send.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_sendrecv.c -o spmd_sendrecv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_sendrecv_replace.c -o spmd_sendrecv_replace.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_tool.c -o spmd_tool.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_utility.c -o spmd_utility.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_wait.c -o spmd_wait.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c zzz.c -o zzz.o\ngcc -std=gnu2x -shared -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -L/usr/local/lib64 -o pbdMPI.so comm_errors.o comm_sort_double.o comm_sort_integer.o pkg_dl.o pkg_tools.o spmd.o spmd_allgather.o spmd_allgatherv.o spmd_allreduce.o spmd_alltoall.o spmd_alltoallv.o spmd_bcast.o spmd_communicator.o spmd_communicator_spawn.o spmd_gather.o spmd_gatherv.o spmd_info.o spmd_recv.o spmd_reduce.o spmd_scatter.o spmd_scatterv.o spmd_send.o spmd_sendrecv.o spmd_sendrecv_replace.o spmd_tool.o spmd_utility.o spmd_wait.o zzz.o -L/usr/lib64/openmpi/lib -lmpi -lutil -lpthread -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -lR\ninstalling via 'install.libs.R' to /wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/00LOCK-pbdMPI/00new/pbdMPI\n** R\n** demo\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n--------------------------------------------------------------------------\nNo OpenFabrics connection schemes reported that they were able to be\nused on a specific port.  As such, the openib BTL (OpenFabrics\nsupport) will be disabled for this port.\n\n  Local host:           dev2\n  Local device:         mlx5_0\n  Local port:           1\n  CPCs attempted:       rdmacm, udcm\n--------------------------------------------------------------------------\n[1684168833.604597] [dev2:227206:0]       ib_iface.c:700  UCX  ERROR ibv_create_cq(cqe=4096) failed: Cannot allocate memory\n[dev2.wynton.ucsf.edu:227206] pml_ucx.c:208 Error: Failed to create UCP worker\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n--------------------------------------------------------------------------\nNo OpenFabrics connection schemes reported that they were able to be\nused on a specific port.  As such, the openib BTL (OpenFabrics\nsupport) will be disabled for this port.\n\n  Local host:           dev2\n  Local device:         mlx5_0\n  Local port:           1\n  CPCs attempted:       rdmacm, udcm\n--------------------------------------------------------------------------\n[1684168836.256287] [dev2:227248:0]       ib_iface.c:700  UCX  ERROR ibv_create_cq(cqe=4096) failed: Cannot allocate memory\n[dev2.wynton.ucsf.edu:227248] pml_ucx.c:208 Error: Failed to create UCP worker\n** testing if installed package keeps a record of temporary installation path\n* DONE (pbdMPI)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpKNz5KF/downloaded_packages'\nThe bigGP installs the same way.\n\n\n\n\n\n\n\nIf we try to install the rjags package, we’ll get the following installation error in R:\n&gt; install.packages(\"rjags\")\n...\n* installing *source* package 'rjags' ...\n** package 'rjags' successfully unpacked and MD5 sums checked\n** using staged installation\nchecking for pkg-config... /usr/bin/pkg-config\nconfigure: WARNING: pkg-config file for jags 4 unavailable\nconfigure: WARNING: Consider adding the directory containing `jags.pc`\nconfigure: WARNING: to the PKG_CONFIG_PATH environment variable\nconfigure: Attempting legacy configuration of rjags\nchecking for jags... no\nconfigure: error: \"automatic detection of JAGS failed. Please use pkg-config to locate the JAGS library. See the INSTALL file for details.\"\nERROR: configuration failed for package 'rjags'\n* removing '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/rjags'\nERROR: dependency 'rjags' is not available for package 'infercnv'\n* removing '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/infercnv'\nThe error says that the “JAGS library” is missing. It’s available via the CBI software stack. Load it before starting R:\n$ module load CBI jags\nand you’ll find that install.packages(\"rjags\") will complete successfully.\nImportantly, you need to load the jags CBI module any time you run R where the rjags R package needs to be loaded.\n\n\n\n\n\n\nIf we try to install the jqr package, it fails to compile;\n&gt; install.packages(\"jqr\")\n...\n* installing *source* package 'jqr' ...\n** package 'jqr' successfully unpacked and MD5 sums checked\n** using staged installation\nUsing PKG_CFLAGS=\nUsing PKG_LIBS=-ljq\n--------------------------- [ANTICONF] --------------------------------\nConfiguration failed because libjq was not found. Try installing:\n * deb: libjq-dev (Debian, Ubuntu).\n * rpm: jq-devel (Fedora, EPEL)\n * csw: libjq_dev (Solaris)\n * brew: jq (OSX)\nIf  is already installed set INCLUDE_DIR and LIB_DIR manually via:\nR CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'\n-------------------------- [ERROR MESSAGE] ---------------------------\n&lt;stdin&gt;:1:10: fatal error: jq.h: No such file or directory\ncompilation terminated.\n--------------------------------------------------------------------\nERROR: configuration failed for package 'jqr'\nTo fix this, load the jq module from the CBI stack before launching R, i.e.\n$ module load CBI r\n$ module load CBI jq\n$ R\nafter this, the jqr package will install out of the box.\nImportantly, you need to load the jq CBI module any time you run R where the jqr R package needs to be loaded.\n\n\n\n\n\n\n\nThe udunits2 package does not install out of the box. It seems to be due to a problem with the package itself, and the suggested instructions that the package gives on setting environment variable UDUNITS2_INCLUDE do not work. A workaround to install the package is to do:\ninstall.packages(\"udunits2\", configure.args=\"--with-udunits2-include=/usr/include/udunits2\")"
  },
  {
    "objectID": "hpc/howto/r.html#accessing-r",
    "href": "hpc/howto/r.html#accessing-r",
    "title": "Work with R",
    "section": "",
    "text": "To load the R module available in the CBI software stack, do:\n[alice@dev2 ~]$ module load CBI\n[alice@dev2 ~]$ module load r\nwhich provides access to a modern version of R:\n\n[alice@dev2 ~]$ R \n\nR version 4.5.2 (2025-10-31) -- \"[Not] Part in a Rumble\"\nCopyright (C) 2025 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; 1+2\n[1] 3\n&gt; quit()\nSave workspace image? [y/n/c]: n\n[alice@dev2 ~]$ \nTo use an older version of R, specify the version when you load R, e.g.\n[alice@dev2 ~]$ module load CBI\n[alice@dev2 ~]$ module load r/3.5.3"
  },
  {
    "objectID": "hpc/howto/r.html#using-r-in-job-scripts",
    "href": "hpc/howto/r.html#using-r-in-job-scripts",
    "title": "Work with R",
    "section": "",
    "text": "In order to run R in jobs, the above R environment module needs to be loaded just as when you run it interactively on a development node. For example, to run the my_script.R script, the job script should at a minimum contain:\n#! /usr/bin/env bash\n#$ -S /bin/bash\n#$ -cwd\n\nmodule load CBI\nmodule load r\nRscript my_script.R"
  },
  {
    "objectID": "hpc/howto/r.html#installing-r-packages",
    "href": "hpc/howto/r.html#installing-r-packages",
    "title": "Work with R",
    "section": "",
    "text": "R 4.5.0 was released on 2025-04-11, R 4.5.1 on 2025-06-13 and R 4.5.2 on 2025-10-31. Bioconductor 3.21 was release on 2024-04-16 and Bioconductor 3.22 was release on 2024-10-30.\nWe have confirmed that more than 99% of the CRAN packages and more than 99% of the Bioconductor Software packages install out of the box when following the below instructions. The packages that failed to install do so either because they depend on a system library that is not available on the cluster, or because they have bugs preventing them from being installed out of the box. If you need to install any of those, please reach out on one of the support channels.\n\nThe majority of R packages are available from CRAN (Comprehensive R Archive Network). Another dominant repository of R packages is Bioconductor, which provides R packages with a focus on bioinformatics. Packages available from Bioconductor are not available on CRAN, and vice versa. At times, you will find online instructions for installing R packages hosted on, for instance, GitHub and GitLab. Before installing an R package from such sources, we highly recommend to install the package from CRAN or Bioconductor, if it is available there, because packages hosted on the latter are stable releases and often better tested.\nBefore continuing, it is useful to understand where R packages looks for locally installed R packages. There are three locations that R considers:\n\nYour personal R package library. This is located under ~/R/, e.g. ~/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/\n(optional) A site-wide R package library (not used on Wynton HPC)\nThe system-wide R package library part of the R installed, e.g. /wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/library\n\nFor instance, when we try to load an R package:\n&gt; library(datasets)\nR will search the above folders in order for R package ‘datasets’. When you start you fresh, the only R packages available to you are the ones installed in folder (3) - the system-wide library. The ‘datasets’ package comes with the R installation, so with a fresh setup, it will be loaded from the third location. As we will see below, when you install your own packages, they will all be installed into folder (1) - your personal library. The first time your run R, the personal library folder does not exists, so R will ask you whether or not you want to create that folder. If asked, you should always accept (answer ‘Yes’). If you had already created this folder, R will install into this folder without asking.\nFinally, R undergoes a main update once a year (around April and May). For example, R 4.5.0 was release in April 2025. The next main release will be R 4.6.0 a year later. Whenever the y component in R x.y.z version is increased, you will start out with an empty personal package folder specific for R x.y (regardless of z). This means that you will have to re-install all R packages you had installed during the year before the new main release came out. Yes, this can be tedious and can take quite some time but it will improve stability and yet allow the R developers to keep improving R. Of course, you can still keep using an older version of R and all the packages you have installed for that version - they will not be removed.\n\n\nPackages available on CRAN can be installed using the install.packages() function in R. The default behavior of R is to always ask you which one of the many CRAN mirrors you want to install from (their content is all identical). To avoid this question, tell R to always use the first one:\n&gt; chooseCRANmirror(ind = 1)\n&gt;\nNow, in order to install, for instance, the zoo package available on CRAN, call:\n&gt; install.packages(\"zoo\")\nWarning in install.packages(\"zoo\") :\n  'lib = \"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/library\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel)\nWe notice two things. First there is a warning mentioning that a “lib” folder was “not writable”. This is because your personal library folder did not yet exists and R tried to install to location (3) but failed (because you do not have write permission there). This is where R decided to ask you whether or not you want to install to a personal library. Answer ‘yes’:\nWould you like to use a personal library instead? (yes/No/cancel) yes\nWould you like to create a personal library\n'~/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\nto install packages into? (yes/No/cancel)\nR wants to make sure you are aware what is done, so it will, conservatively, also ask if you accept the default location. Answer ‘yes’ for this folder to be created. After this, the current and all future package installation in R will be installed into this folder without further questions asked. In this example, we will get:\n\nWould you like to create a personal library\n'~/R/x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\nto install packages into? (yes/No/cancel) yes\ntrying URL 'https://cloud.r-project.org/src/contrib/zoo_1.8-14.tar.gz'\nContent type 'application/x-gzip' length 778426 bytes (760 KB)\n==================================================\ndownloaded 760 KB\n\n* installing *source* package ‘zoo’ ...\n** this is package ‘zoo’ version ‘1.8-14’\n** package ‘zoo’ successfully unpacked and MD5 sums checked\n** using staged installation\n** libs\nusing C compiler: ‘gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)’\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I../inst/include  -I/usr/local/include    -fpic  -g -O2  -c coredata.c -o coredata.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I../inst/include  -I/usr/local/include    -fpic  -g -O2  -c init.c -o init.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I../inst/include  -I/usr/local/include    -fpic  -g -O2  -c lag.c -o lag.o\ngcc -std=gnu2x -shared -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -L/usr/local/lib64 -o zoo.so coredata.o init.o lag.o -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -lR\ninstalling to /wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/00LOCK-zoo/00new/zoo/libs\n** R\n** demo\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (zoo)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpVm3e6t/downloaded_packages'\n&gt;\nIf there is no mentioning of an “error” (a “warning” is ok in R but never an “error”), then the package was successfully installed. If you see * DONE (zoo) at the end, it means that the package was successfully installed. As with any other package in R, you can also verify that it is indeed installed by loading it, i.e.\n&gt; library(zoo)\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n&gt;\n\n\nIf a new version of one or more CRAN packages is released, they can be installed by calling:\n&gt; chooseCRANmirror(ind = 1)\n&gt; update.packages()\n...\n\n\n\n\nPer Bioconductor’s best practices, R packages from Bioconductor should be installed using BiocManager::install(). This is to guarantee maximum compatibility between all Bioconductor packages.\n\n\nIf you already have BiocManager installed, you can skip this section. When you start out fresh, the package BiocManager is not installed meaning that calling BiocManager::install() will fail. We need to start by installing it from CRAN (sic!);\n\ne&gt; install.packages(\"BiocManager\")\nInstalling package into '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\n(as 'lib' is unspecified)\ntrying URL 'https://cloud.r-project.org/src/contrib/BiocManager_1.30.26.tar.gz'\nContent type 'application/x-gzip' length 582690 bytes (580 KB)\n==================================================\ndownloaded 569 KB\n\n* installing *source* package ‘BiocManager’ ...\n** package ‘BiocManager’ successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (BiocManager)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpSRgaB4/downloaded_packages'\n&gt; \nComment: If this is the very first R package you installed, see above CRAN instructions for setting a default CRAN mirror and creating a personal library folder.\n\n\n\nWith BiocManager installed, we can now install any Bioconductor package. For instance, to install limma, and all of its dependencies, call:\n\n&gt; BiocManager::install(\"limma\")\nBioconductor version 3.21 (BiocManager 1.30.26), R 4.5.1 (2025-06-13)\nInstalling package(s) 'limma'\ntrying URL 'https://bioconductor.org/packages/3.21/bioc/src/contrib/limma_3.64.3.tar.gz'\nContent type 'application/x-gzip' length 2846680 bytes (2.7 MB)\n==================================================\ndownloaded 2.7 MB\n\n* installing *source* package ‘limma’ ...\n** this is package ‘limma’ version ‘3.64.3’\n** using staged installation\n** libs\nusing C compiler: ‘gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)’\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG   -I/usr/local/include    -fpic  -g -O2  -c init.c -o init.o                                                                                          \ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG   -I/usr/local/include    -fpic  -g -O2  -c normexp.c -o normexp.o                                                                                    \ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG   -I/usr/local/include    -fpic  -g -O2  -c weighted_lowess.c -o weighted_lowess.o                                                                    \ngcc -std=gnu2x -shared -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -L/usr/local/lib64 -o limma.so init.o normexp.o weighted_lowess.o -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -lR               \ninstalling to /wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/00LOCK-limma/00new/limma/libs\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (limma)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpIoJ3Np/downloaded_packages'\n&gt;\nThere were no “error” messages, so the installation was successful. To verify that it worked, we can load the package in R as:\n&gt; library(limma)\n&gt;\n\n\n\nTo install Bioconductor updates, call BiocManager::install() without arguments:\n&gt; BiocManager::install()\nComment: This will actually also update any CRAN packages."
  },
  {
    "objectID": "hpc/howto/r.html#appendix",
    "href": "hpc/howto/r.html#appendix",
    "title": "Work with R",
    "section": "",
    "text": "If you have an R scripts, and it involves setting up a number of parallel workers in R, do not use ncores &lt;- detectCores() of the parallel package because it will result in your job hijacking all cores on the compute node regardless of how many cores the scheduler has given you. Taking up all CPU resources without permission is really bad practice and a common cause for problems. A much better solution is to use availableCores() that is available in the parallelly package, e.g. as ncores &lt;- parallelly::availableCores(). This function is backward compatible with detectCores() while respecting what the scheduler has allocated for your job.\n\n\n\nAs of 2024-04-26, the “recommended” MASS and Matrix packages require R (&gt;= 4.4.0) [2024-04-24]. If you run an older version of R, you can install older versions of them that are compatible with R (&lt; 4.4.0) using:\n&gt; install.packages(\"https://cran.r-project.org/src/contrib/Archive/MASS/MASS_7.3-60.0.1.tar.gz\", type = \"source\")\n\n&gt; install.packages(\"https://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.6-5.tar.gz\", type = \"source\")\n\n\n\nSome R packages rely on the Message Passing Interface (MPI), e.g. Rmpi, pbdMPI and bigGP. To use these, but also to install them we need to load the built-in mpi module;\n\n[alice@dev2 ~]$ module load mpi/openmpi-x86_64\n[alice@dev2 ~]$ module list\n\nCurrently Loaded Modules:\n  1) CBI   2) scl-gcc-toolset/13   3) r/4.5.2   4) mpi/openmpi-x86_64\n\n \nImportantly, make sure to specify the exact version of the mpi module as well so that your code will keep working also when a newer version becomes the new default. Note that you will have to load the same mpi module, and version(!), also whenever you run R code that requires these MPI-dependent R packages.\nIn addition to making OpenMPI available by loading the mpi module, several MPI-based R packages requires additional special care in order to install. Below sections, show how to install them.\n\n\nAfter loading the mpi module, the Rmpi package installs out-of-the-box like other R packages. After installing it, you can verify that it works by running the following example commands:\n[alice@dev2 ~]$ module load CBI r\n[alice@dev2 ~]$ module load mpi/openmpi-x86_64\n[alice@dev2 ~]$ R\n...\n&gt; library(Rmpi)\n[1684426121.677063] [c4-dev3:23125:0]            sys.c:618  UCX  ERROR shmget(size=2097152 flags=0xfb0) for mm_recv_desc failed: Operation not permitted, please check shared memory limits by 'ipcs -l'\n\n&gt; mpi.spawn.Rslaves()              ## launch one or more MPI parallel workers\n        1 slaves are spawned successfully. 0 failed.\n[1684426140.976380] [c4-dev3:23125:0]            sys.c:618  UCX  ERROR shmget(size=2097152 flags=0xb80) for ucp_am_bufs failed: Operation not permitted, please check shared memory limits by 'ipcs -l'\nmaster (rank 0, comm 1) of size 2 is running on: dev2 \nslave1 (rank 1, comm 1) of size 2 is running on: dev2\n\n&gt; mpi.remote.exec(Sys.getpid())    ## get the process ID for one of them\n     out\n1 189114\n\n\n\n\nContrary to Rmpi above, packages such pbdMPI and bigGP require more hand-holding to install. For example, after having loaded the mpi module, we can install pbdMPI in R as:\n\n&gt; install.packages(\"pbdMPI\", configure.args=\"--with-mpi-include=$MPI_INCLUDE --with-mpi-libpath=$MPI_LIB --with-mpi-type=OPENMPI\")\nInstalling package into '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13'\n(as 'lib' is unspecified)\ntrying URL 'https://cloud.r-project.org/src/contrib/pbdMPI_0.5-4.tar.gz'\nContent type 'application/x-gzip' length 423376 bytes (413 KB)\n==================================================\ndownloaded 413 KB\n\n* installing *source* package ‘pbdMPI’ ...\n** this is package ‘pbdMPI’ version ‘0.5-4’\n** package ‘pbdMPI’ successfully unpacked and MD5 sums checked\n** using staged installation\nchecking for sed... /usr/bin/sed\nchecking for mpicc... mpicc\nchecking for ompi_info... ompi_info\nchecking for pkg-config... /usr/bin/pkg-config\n&gt;&gt; TMP_FOUND = Nothing found from mpicc --show & sed nor pkg-config ...\nchecking for openpty in -lutil... yes\nchecking for main in -lpthread... yes\n\n******************* Results of pbdMPI package configure *****************\n\n&gt;&gt; MPIRUN = /usr/lib64/openmpi/bin/mpirun\n&gt;&gt; MPIEXEC = /usr/lib64/openmpi/bin/mpiexec\n&gt;&gt; ORTERUN = /usr/lib64/openmpi/bin/orterun\n&gt;&gt; TMP_INC =\n&gt;&gt; TMP_LIB =\n&gt;&gt; TMP_LIBNAME =\n&gt;&gt; TMP_FOUND = Nothing found from mpicc --show & sed nor pkg-config ...\n&gt;&gt; MPI_ROOT =\n&gt;&gt; MPITYPE = OPENMPI\n&gt;&gt; MPI_INCLUDE_PATH = /usr/include/openmpi-x86_64\n&gt;&gt; MPI_LIBPATH = /usr/lib64/openmpi/lib\n&gt;&gt; MPI_LIBNAME =\n&gt;&gt; MPI_LIBS =  -lutil -lpthread\n&gt;&gt; MPI_DEFS = -DMPI2\n&gt;&gt; MPI_INCL2 =\n&gt;&gt; MPI_LDFLAGS =\n&gt;&gt; PKG_CPPFLAGS = -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI\n&gt;&gt; PKG_LIBS = -L/usr/lib64/openmpi/lib -lmpi  -lutil -lpthread\n&gt;&gt; PROF_LDFLAGS =\n&gt;&gt; ENABLE_LD_LIBRARY_PATH = no\n\n*************************************************************************\n\nconfigure: creating ./config.status\nconfig.status: creating src/Makevars\nconfigure: creating ./config.status\nconfig.status: creating src/Makevars\nconfig.status: creating R/zzz.r\nconfig.status: creating R/util_execmpi.r\n** libs\nusing C compiler: ‘gcc (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)’\necho \"MPIRUN = /usr/lib64/openmpi/bin/mpirun\" &gt; Makeconf\necho \"MPIEXEC = /usr/lib64/openmpi/bin/mpiexec\" &gt;&gt; Makeconf\necho \"ORTERUN = /usr/lib64/openmpi/bin/orterun\" &gt;&gt; Makeconf\necho \"TMP_INC = \" &gt;&gt; Makeconf\necho \"TMP_LIB = \" &gt;&gt; Makeconf\necho \"TMP_LIBNAME = \" &gt;&gt; Makeconf\necho \"TMP_FOUND = Nothing found from mpicc --show & sed nor pkg-config ...\" &gt;&gt; Makeconf\necho \"MPI_ROOT = \" &gt;&gt; Makeconf\necho \"MPITYPE = OPENMPI\" &gt;&gt; Makeconf\necho \"MPI_INCLUDE_PATH = /usr/include/openmpi-x86_64\" &gt;&gt; Makeconf\necho \"MPI_LIBPATH = /usr/lib64/openmpi/lib\" &gt;&gt; Makeconf\necho \"MPI_LIBNAME = \" &gt;&gt; Makeconf\necho \"MPI_LIBS =  -lutil -lpthread\" &gt;&gt; Makeconf\necho \"MPI_DEFS = -DMPI2\" &gt;&gt; Makeconf\necho \"MPI_INCL2 = \" &gt;&gt; Makeconf\necho \"MPI_LDFLAGS = \" &gt;&gt; Makeconf\necho \"PKG_CPPFLAGS = -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI\" &gt;&gt; Makeconf\necho \"PKG_LIBS = -L/usr/lib64/openmpi/lib -lmpi  -lutil -lpthread\" &gt;&gt; Makeconf\necho \"PROF_LDFLAGS = \" &gt;&gt; Makeconf\necho \"ENABLE_LD_LIBRARY_PATH = no\" &gt;&gt; Makeconf\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c comm_errors.c -o comm_errors.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c comm_sort_double.c -o comm_sort_double.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c comm_sort_integer.c -o comm_sort_integer.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c pkg_dl.c -o pkg_dl.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c pkg_tools.c -o pkg_tools.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd.c -o spmd.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_allgather.c -o spmd_allgather.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_allgatherv.c -o spmd_allgatherv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_allreduce.c -o spmd_allreduce.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_alltoall.c -o spmd_alltoall.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_alltoallv.c -o spmd_alltoallv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_bcast.c -o spmd_bcast.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_communicator.c -o spmd_communicator.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_communicator_spawn.c -o spmd_communicator_spawn.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_gather.c -o spmd_gather.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_gatherv.c -o spmd_gatherv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_info.c -o spmd_info.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_recv.c -o spmd_recv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_reduce.c -o spmd_reduce.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_scatter.c -o spmd_scatter.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_scatterv.c -o spmd_scatterv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_send.c -o spmd_send.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_sendrecv.c -o spmd_sendrecv.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_sendrecv_replace.c -o spmd_sendrecv_replace.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_tool.c -o spmd_tool.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_utility.c -o spmd_utility.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c spmd_wait.c -o spmd_wait.o\ngcc -std=gnu2x -I\"/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/include\" -DNDEBUG -I/usr/include/openmpi-x86_64  -DMPI2 -DOPENMPI  -I/usr/local/include    -fpic  -g -O2  -c zzz.c -o zzz.o\ngcc -std=gnu2x -shared -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -L/usr/local/lib64 -o pbdMPI.so comm_errors.o comm_sort_double.o comm_sort_integer.o pkg_dl.o pkg_tools.o spmd.o spmd_allgather.o spmd_allgatherv.o spmd_allreduce.o spmd_alltoall.o spmd_alltoallv.o spmd_bcast.o spmd_communicator.o spmd_communicator_spawn.o spmd_gather.o spmd_gatherv.o spmd_info.o spmd_recv.o spmd_reduce.o spmd_scatter.o spmd_scatterv.o spmd_send.o spmd_sendrecv.o spmd_sendrecv_replace.o spmd_tool.o spmd_utility.o spmd_wait.o zzz.o -L/usr/lib64/openmpi/lib -lmpi -lutil -lpthread -L/wynton/home/cbi/shared/software/_rocky8/R-4.5.2-gcc13/lib64/R/lib -lR\ninstalling via 'install.libs.R' to /wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/00LOCK-pbdMPI/00new/pbdMPI\n** R\n** demo\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n--------------------------------------------------------------------------\nNo OpenFabrics connection schemes reported that they were able to be\nused on a specific port.  As such, the openib BTL (OpenFabrics\nsupport) will be disabled for this port.\n\n  Local host:           dev2\n  Local device:         mlx5_0\n  Local port:           1\n  CPCs attempted:       rdmacm, udcm\n--------------------------------------------------------------------------\n[1684168833.604597] [dev2:227206:0]       ib_iface.c:700  UCX  ERROR ibv_create_cq(cqe=4096) failed: Cannot allocate memory\n[dev2.wynton.ucsf.edu:227206] pml_ucx.c:208 Error: Failed to create UCP worker\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n--------------------------------------------------------------------------\nNo OpenFabrics connection schemes reported that they were able to be\nused on a specific port.  As such, the openib BTL (OpenFabrics\nsupport) will be disabled for this port.\n\n  Local host:           dev2\n  Local device:         mlx5_0\n  Local port:           1\n  CPCs attempted:       rdmacm, udcm\n--------------------------------------------------------------------------\n[1684168836.256287] [dev2:227248:0]       ib_iface.c:700  UCX  ERROR ibv_create_cq(cqe=4096) failed: Cannot allocate memory\n[dev2.wynton.ucsf.edu:227248] pml_ucx.c:208 Error: Failed to create UCP worker\n** testing if installed package keeps a record of temporary installation path\n* DONE (pbdMPI)\n\nThe downloaded source packages are in\n        '/scratch/alice/RtmpKNz5KF/downloaded_packages'\nThe bigGP installs the same way.\n\n\n\n\n\n\n\nIf we try to install the rjags package, we’ll get the following installation error in R:\n&gt; install.packages(\"rjags\")\n...\n* installing *source* package 'rjags' ...\n** package 'rjags' successfully unpacked and MD5 sums checked\n** using staged installation\nchecking for pkg-config... /usr/bin/pkg-config\nconfigure: WARNING: pkg-config file for jags 4 unavailable\nconfigure: WARNING: Consider adding the directory containing `jags.pc`\nconfigure: WARNING: to the PKG_CONFIG_PATH environment variable\nconfigure: Attempting legacy configuration of rjags\nchecking for jags... no\nconfigure: error: \"automatic detection of JAGS failed. Please use pkg-config to locate the JAGS library. See the INSTALL file for details.\"\nERROR: configuration failed for package 'rjags'\n* removing '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/rjags'\nERROR: dependency 'rjags' is not available for package 'infercnv'\n* removing '/wynton/home/boblab/alice/R/rocky8-x86_64-pc-linux-gnu-library/4.5-CBI-gcc13/infercnv'\nThe error says that the “JAGS library” is missing. It’s available via the CBI software stack. Load it before starting R:\n$ module load CBI jags\nand you’ll find that install.packages(\"rjags\") will complete successfully.\nImportantly, you need to load the jags CBI module any time you run R where the rjags R package needs to be loaded.\n\n\n\n\n\n\nIf we try to install the jqr package, it fails to compile;\n&gt; install.packages(\"jqr\")\n...\n* installing *source* package 'jqr' ...\n** package 'jqr' successfully unpacked and MD5 sums checked\n** using staged installation\nUsing PKG_CFLAGS=\nUsing PKG_LIBS=-ljq\n--------------------------- [ANTICONF] --------------------------------\nConfiguration failed because libjq was not found. Try installing:\n * deb: libjq-dev (Debian, Ubuntu).\n * rpm: jq-devel (Fedora, EPEL)\n * csw: libjq_dev (Solaris)\n * brew: jq (OSX)\nIf  is already installed set INCLUDE_DIR and LIB_DIR manually via:\nR CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'\n-------------------------- [ERROR MESSAGE] ---------------------------\n&lt;stdin&gt;:1:10: fatal error: jq.h: No such file or directory\ncompilation terminated.\n--------------------------------------------------------------------\nERROR: configuration failed for package 'jqr'\nTo fix this, load the jq module from the CBI stack before launching R, i.e.\n$ module load CBI r\n$ module load CBI jq\n$ R\nafter this, the jqr package will install out of the box.\nImportantly, you need to load the jq CBI module any time you run R where the jqr R package needs to be loaded.\n\n\n\n\n\n\n\nThe udunits2 package does not install out of the box. It seems to be due to a problem with the package itself, and the suggested instructions that the package gives on setting environment variable UDUNITS2_INCLUDE do not work. A workaround to install the package is to do:\ninstall.packages(\"udunits2\", configure.args=\"--with-udunits2-include=/usr/include/udunits2\")"
  },
  {
    "objectID": "hpc/howto/rstudio.html",
    "href": "hpc/howto/rstudio.html",
    "title": "Work with RStudio",
    "section": "",
    "text": "An RStudio Server session will time out after 120 minutes of no user input. This is to make sure there are forgotten, left-over processes running on the system. For R tasks running longer than this, the solution is to submit them via the scheduler."
  },
  {
    "objectID": "hpc/howto/rstudio.html#step-1.-launch-your-own-rstudio-server-instance",
    "href": "hpc/howto/rstudio.html#step-1.-launch-your-own-rstudio-server-instance",
    "title": "Work with RStudio",
    "section": "Step 1. Launch your own RStudio Server instance",
    "text": "Step 1. Launch your own RStudio Server instance\nAssuming you are already logged on to a development node, launch your personal RStudio Server instance as:\n[alice@dev2 ~]$ module load CBI rstudio-server-controller\n[alice@dev2 ~]$ rsc start\nalice, your personal RStudio Server 2025.05.1-513 running R 4.5.2 is available on:\n\n  &lt;http://127.0.0.1:20612&gt;\n\nImportantly, if you are running from a remote machine without direct access\nto dev2, you need to set up SSH port forwarding first, which you can do by\nrunning:\n\n  ssh -L 20612:dev2:20612 alice@log2.wynton.ucsf.edu\n\nin a second terminal from your local computer.\n\nAny R session started times out after being idle for 120 minutes.\nWARNING: You now have 10 minutes, until 2025-10-29 18:45:38-07:00, to\nconnect and log in to the RStudio Server before everything times out.\nYour one-time random password for RStudio Server is: y+IWo7rfl7Z7MRCPI3Z4\nThere are two things you should pay extra attention to here:\n\nThe one-time random password that was generated\nThe instructions how to log in to the cluster with SSH port forwarding\n\nYou will need both below."
  },
  {
    "objectID": "hpc/howto/rstudio.html#step-2.-connect-to-your-personal-rstudio-server-instance",
    "href": "hpc/howto/rstudio.html#step-2.-connect-to-your-personal-rstudio-server-instance",
    "title": "Work with RStudio",
    "section": "Step 2. Connect to your personal RStudio Server instance",
    "text": "Step 2. Connect to your personal RStudio Server instance\nOn your local computer, log into the cluster in a second terminal following the instruction above. Make sure to use your own username and make sure to use the port number (e.g. 20612) that was assigned to you.\n{local}$ ssh -L 20612:{{ site.devel.hostname}}:20612 alice@log2.wynton.ucsf.edu\nalice1@{{site.login.hostname }}:s password: XXXXXXXXXXXXXXXXXXX\n[alice@{{site.login.name }} ~]$"
  },
  {
    "objectID": "hpc/howto/rstudio.html#step-3.-open-rstudio-server-in-your-local-web-browser",
    "href": "hpc/howto/rstudio.html#step-3.-open-rstudio-server-in-your-local-web-browser",
    "title": "Work with RStudio",
    "section": "Step 3. Open RStudio Server in your local web browser",
    "text": "Step 3. Open RStudio Server in your local web browser\nIf you successfully completed the above two steps, and you made sure to use the correct port, then you should be able to open your personal RStudio Server in your local web browser by going to:\n\nhttp://127.0.0.1:20612/ (note, your port is different)\n\nYou will be presented with a ‘Sign in to RStudio’ web page where you need to enter:\n\nYour cluster username (e.g. alice)\nThe one-time random password displayed in Step 1 (e.g. y+IWo7rfl7Z7MRCPI3Z4)\n\nAfter clicking ‘Sign In’, you should be redirected to the RStudio interface.\nTo terminate the RStudio Server, start by exiting R by typing quit() at the R prompt. Then press Ctrl-C in the terminal where you called rsc start. Alternatively, run rsc stop in another terminal, e.g. the second one used in Step 2."
  },
  {
    "objectID": "hpc/howto/rstudio.html#troubleshooting",
    "href": "hpc/howto/rstudio.html#troubleshooting",
    "title": "Work with RStudio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nStuck at “R is taking longer to start than usual”?\nSome users report that they stuck when they try to log in to RStudio. After they enter their username and password, and click ‘Sign In’, they get to a page “R is taking longer to start than usual” with a spinner that never ends. The user is presented with three options ‘Reload’, ‘Safe Mode’, and ‘Terminate R’. Ideally, ‘Safe Mode’ or ‘Terminate R’ would solve the problem and let the user access the RStudio GUI. Unfortunately, for some users, none of these options help. Consecutive attempts to use rsc stop and rsc start fail for same reasons.\nAs of 2023-12-04, it is not clear why and when this happens. The one workaround we have found is to wipe the user’s RStudio sessions. For this, we recommend to use:\n$ rsc reset --which=sessions\nThis will create a local copy of your problematic RStudio setup in file rstudio-config_&lt;timestamp&gt;.tar, and then, only then, remove the actually settings. The next time you call rsc start, you should start out with a fresh RStudio setup, and the login issue should be gone.\n\n\nERROR: Failed to check process PID 12345 on dev1.wynton.ucsf.edu over SSH\nIf you get the following error when launching rsc start:\n[alice@{{ site.dev2.name }} ~]$ rsc start\nWARNING: Needs to SSH to dev1.wynton.ucsf.edu to check whether process 2132343\nis still alive [/wynton/home/boblab/alice/.config/rsc/rserver.hostname:\n21 bytes; 2024-09-28 15:56:13)]. If you don't have SSH key authentication set up,\nyou will be asked to enter your account password below. The current machine is\ndev2.wynton.ucsf.edu\nERROR: Failed to check process PID 2132343 on dev1.wynton.ucsf.edu over SSH. \nReason was: ssh: connect to host dev1.wynton.ucsf.edu port 22: No route to host\nthe reason is that rsc start tries to protect against launching more than one RStudio session at the same time on different machines. In order to confirm that you already running another RStudio session on another machine, it needs to access that machine via SSH, but if that fails you get the above error.\nTo troubleshoot this, start by making sure you can SSH to {{ site.dev1.hostname }}. (1) If you can login manually, do that and call rsc stop there. This should resolve the above problem. (2) If you cannot access the machine, it could be that you have exhausted your CPU quota on that machine and it is very slow to respond. If you suspect this is the case, see ‘Running out of memory’ below. It could also be that the machine is not working or down, which is rare, but it happens. If it is down, it’s most likely already discussed on our Slack forum - please check there to confirm it is truly down. In the rare case that the machine is really down, try to call rsc reset and the retry with rsc start. If you still get the above error, retry with rsc reset --force.\n\n\nRunning out of memory\nIf you get an ‘R Session Error’ dialog saying:\n\nThe previous R session was abnormally terminated due to an unexpected crash.\nYou may have lost workspace data as a result of this crash.\nRStudio may not have restored the previously active project as a precaution. You may switch back to it using the Projects menu.\n[OK]\n\none reason is that you ran out of memory and R was terminated by the operating system. Note that each user is limited to 96 GiB of RAM on the development node. Trying to use more than that, will cause the operating system to kill the underlying R process. When this happens, RStudio will likely keep running, but your R session was lost and reset."
  },
  {
    "objectID": "hpc/howto/install-from-source.html",
    "href": "hpc/howto/install-from-source.html",
    "title": "Install Software from Source Code",
    "section": "",
    "text": "A lot of scientific software is developed using C, C++, and Fortran. Sometimes the maintainers provide prebuilt software binaries from Linux, but more often you have to compile their software from source code. Building from source might sound daunting, especially if it’s your first time - there is new terminology to understand, new tools to use, and sometimes hard-to-understand error messages.\nThis document gives the gist of one of to the most common way to build software from source code, namely the “configure -&gt; make -&gt; make install” approach. Please be aware that not all software tools use this approach, so it’s not universal, but it is by far the most common approach.\n\n\nCompiling software from source often involves three steps:\n\nConfiguration - configure the following build step to work with your current set of tools and libraries\nBuilding - compiling the source code into binary executables\nInstallation - install the compiled binaries to its final destination\n\nLets use the samtools software as a real-world example to illustrate these steps.\n\n\nWe start by downloading the latest software “tarball” from https://github.com/samtools/samtools/releases to a temporary working location:\n[alice@dev2 ~]$ mkdir -p \"/scratch/$USER\"\n[alice@dev2 ~]$ cd \"/scratch/$USER\"\n[alice@dev2 alice]$ wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2\n[alice@dev2 alice]$ ls -l samtools-1.14.tar.bz2 \n-rw-r--r-- 1 alice boblab 7744794 Dec  7 14:41 samtools-1.14.tar.bz2\nThe next step is to extract the content of this tarball, which we can do using tar -x -f (“extract file”):\n[alice@dev2 alice]$ tar -x -f samtools-1.14.tar.bz2\n[alice@dev2 alice]$ ls -l\ntotal 7568\ndrwxr-xr-x 9 alice boblab    4096 Oct 22 04:48 samtools-1.14\n-rw-r--r-- 1 alice boblab 7744794 Dec  7 14:41 samtools-1.14.tar.bz2\nAs we see, the content of the tarball was extracted into a subfolder samtools-1.14. The tarball file is no longer needed after this stage. Let’s enter that new folder and look at its content:\n[alice@dev2 alice]$ cd samtools-1.14\n[alice@dev2 samtools-1.14]$ ls\namplicon_stats.c    bam_markdup.c       bedcov.c       Makefile\nAUTHORS             bam_mate.c          bedidx.c       misc\nbam2bcf.c           bam_md.c            bedidx.h       NEWS\nbam2bcf.h           bam_plbuf.c         ChangeLog.old  padding.c\nbam2bcf_indel.c     bam_plbuf.h         config.h.in    phase.c\nbam2depth.c         bam_plcmd.c         config.mk.in   README\nbam_addrprg.c       bam_quickcheck.c    configure      sam_opts.c\nbam_ampliconclip.c  bam_reheader.c      configure.ac   sam_opts.h\n...\nbam.h               bamtk.c             INSTALL        stats_isize.h\nbam_import.c        bam_tview.c         install-sh     test\nbam_index.c         bam_tview_curses.c  LICENSE        tmp_file.c\nbam_lpileup.c       bam_tview.h         lz4            tmp_file.h\nbam_lpileup.h       bam_tview_html.c    m4             version.sh\nWe see that there are lots of files, but a few standard files stand out: configure, INSTALL, Makefile, NEWS, and README. The files README, NEWS and INSTALL are standard file names for human-readable text files. These are often useful to read when trying to understand what the software is about and how to install it. If there is an INSTALL file, as here, it most likely contain instructions on how to install the software. In our case, INSTALL contains a section:\nBasic Installation\n==================\n\nTo build and install Samtools, 'cd' to the samtools-1.x directory containing\nthe package's source and type the following commands:\n\n    ./configure\n    make\n    make install\n...\nFamiliar, eh?\n\n\n\nDefault installation instructions, like the ones above, often assume we will install the software as an administrator to a central location available to all users on the system. That is not possible to individual users on the cluster. Instead, we need to install it to a location where we have permission to create and write files and folders.\nA common pattern is to install into a subfolder in our home folder that reflects the name of the software and its version. This way we can have multiple versions of the same software installed at the same time. Lets install samtools to the following folder:\n[alice@dev2 samtools-1.14]$ mkdir -p $HOME/software/samtools-1.14\nNow we can configure the build to install to this folder:\n[alice@dev2 samtools-1.14]$ ./configure --prefix=$HOME/software/samtools-1.14\nconfigure: running /bin/sh ./configure --disable-option-checking '--prefix=/wynton/home/boblab/alice/software/samtools-1.14'  --cache-file=/dev/null --srcdir=.\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables... \nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether gcc accepts -g... yes\nchecking for gcc option to accept ISO C89... none needed\nchecking for ranlib... ranlib\nchecking for grep that handles long lines and -e... /usr/bin/grep\nchecking for C compiler warning flags... -Wall\nchecking for pkg-config... /usr/bin/pkg-config\nchecking pkg-config is at least version 0.9.0... yes\nchecking for special C compiler options needed for large files... no\nchecking for _FILE_OFFSET_BITS value needed for large files... no\nchecking shared library type for unknown-Linux... plain .so\nchecking whether the compiler accepts -fvisibility=hidden... yes\nchecking how to run the C preprocessor... gcc -E\nchecking for egrep... /usr/bin/grep -E\nchecking for ANSI C header files... yes\nchecking for sys/types.h... yes\nchecking for sys/stat.h... yes\nchecking for stdlib.h... yes\nchecking for string.h... yes\nchecking for memory.h... yes\nchecking for strings.h... yes\nchecking for inttypes.h... yes\nchecking for stdint.h... yes\nchecking for unistd.h... yes\nchecking for stdlib.h... (cached) yes\nchecking for unistd.h... (cached) yes\nchecking for sys/param.h... yes\nchecking for getpagesize... yes\nchecking for working mmap... yes\nchecking for gmtime_r... yes\nchecking for fsync... yes\nchecking for drand48... yes\nchecking for srand48_deterministic... no\nchecking whether fdatasync is declared... yes\nchecking for fdatasync... yes\nchecking for library containing log... -lm\nchecking for zlib.h... yes\nchecking for inflate in -lz... yes\nchecking for library containing recv... none required\nchecking for bzlib.h... yes\nchecking for BZ2_bzBuffToBuffCompress in -lbz2... yes\nchecking for lzma.h... yes\nchecking for lzma_easy_buffer_encode in -llzma... yes\nchecking whether htscodecs files are present... yes\nchecking for libdeflate.h... no\nchecking for libdeflate_deflate_compress in -ldeflate... no\nchecking for curl/curl.h... yes\nchecking for curl_easy_pause in -lcurl... yes\nchecking for CCHmac... no\nchecking for library containing HMAC... -lcrypto\nchecking for library containing regcomp... none required\nchecking whether PTHREAD_MUTEX_RECURSIVE is declared... yes\nconfigure: creating ./config.status\nconfig.status: creating config.mk\nconfig.status: creating htslib.pc.tmp\nconfig.status: creating config.h\nconfig.status: linking htscodecs_bundled.mk to htscodecs.mk\n[alice@dev2 samtools-1.14]$  \nImportantly, make sure there are no errors reported. If there are, they need to be resolved before continuing. This document does not explain how to resolve configuration errors. If you cannot figure it out yourself, please reach out on one of our Support Channels.\n\n\n\nIf the configuration steps complete without errors, it is often straightforward to build (“compile”) the software my calling make. The make command will use formal build instruction in the Makefile, but we don’t have to know about those details. Just call make as in:\n[alice@dev2 samtools-1.14]$ make\n...\nconfig.mk:46: htslib-1.14/htslib_static.mk: No such file or directory\ncd htslib-1.14 && make htslib_static.mk\nmake[1]: Entering directory `/scratch/alice/samtools-1.14/htslib-1.14'\nsed -n '/^static_libs=/s/[^=]*=/HTSLIB_static_LIBS = /p;/^static_ldflags=/s/[^=]*=/HTSLIB_static_LDFLAGS = /p' htslib.pc.tmp &gt; htslib_static.mk\nmake[1]: Leaving directory `/scratch/alice/samtools-1.14/htslib-1.14'\ngcc -Wall -g -O2 -I. -Ihtslib-1.14 -I./lz4  -c -o bam.o bam.c\ngcc -Wall -g -O2 -I. -Ihtslib-1.14 -I./lz4  -c -o bam_aux.o bam_aux.c\n\n[ ... lots of output ... ]\n\ngcc -Wall -g -O2 -I. -Ihtslib-1.14 -I./lz4  -c -o test/vcf-miniview.o test/vcf-miniview.c\ngcc  -L./lz4  -o test/vcf-miniview test/vcf-miniview.o htslib-1.14/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n[alice@dev2 samtools-1.14]$ \nMake sure there are no compilation errors. If you get errors at this stage, it could be because the gcc compiler is too old. If that happens, try to use a newer compiler version following the instructions in Section ‘Too old compiler?’ below.\n\n\n\nIf we got this far, all we have to do is to install the software, we just configured and built, to its final destination, which was specified and recorded in the configure step. All we have to do now is:\n[alice@dev2 samtools-1.14]$ make install\nmkdir -p -m 755 /wynton/home/boblab/alice/software/samtools-1.14/bin /wynton/home/boblab/alice/software/samtools-1.14/bin /wynton/home/boblab/alice/software/samtools-1.14/share/man/man1\ninstall -p samtools /wynton/home/boblab/alice/software/samtools-1.14/bin\ninstall -p misc/ace2sam misc/maq2sam-long misc/maq2sam-short misc/md5fa misc/md5sum-lite misc/wgsim /wynton/home/boblab/alice/software/samtools-1.14/bin\ninstall -p misc/blast2sam.pl misc/bowtie2sam.pl misc/export2sam.pl misc/fasta-sanitize.pl misc/interpolate_sam.pl misc/novo2sam.pl misc/plot-ampliconstats misc/plot-bamstats misc/psl2sam.pl misc/sam2vcf.pl misc/samtools.pl misc/seq_cache_populate.pl misc/soap2sam.pl misc/wgsim_eval.pl misc/zoom2sam.pl /wynton/home/boblab/alice/software/samtools-1.14/bin\ninstall -p -m 644 doc/samtools*.1 misc/wgsim.1 /wynton/home/boblab/alice/software/samtools-1.14/share/man/man1\n[alice@dev2 samtools-1.14]$ \n\n\n\nIf we pay attention to the make install output, we see that the samtools executable was installed in the bin/ subfolder of our installation directory ~/software/samtools-1.14/. Since this is not yet on the search path, we have to specify the full path for our initial test:\n[alice@dev2 samtools-1.14]$ $HOME/software/samtools-1.14/bin/samtools --version | head -3\nsamtools 1.14\nUsing htslib 1.14\nCopyright (C) 2021 Genome Research Ltd.\nIn order to call this samtools executable without having to specify the full path each time, prepend its path to the PATH environment variable, e.g.\n[alice@dev2 ~]$ export PATH=~/software/samtools-1.14/bin:$PATH\n[alice@dev2 samtools-1.14]$ which samtools\n~/software/samtools-1.14/bin/samtools\n[alice@dev2 samtools-1.14]$ samtools --version | head -3\nsamtools 1.14\nUsing htslib 1.14\nCopyright (C) 2021 Genome Research Ltd.\nFor convenience, you might want to add:\nexport PATH=~/software/samtools-1.14/bin:$PATH\nto your ~/.bashrc file.\n\n\n\n\nThe GCC development tools that come built-in on our Rocky 8 system is sufficient for most needs. For example, the default gcc version is:\n\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-18)\nCopyright (C) 2018 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nThis GCC 8.5.0 compiler supports older C++ standards such as C++17, but none of the newer standards, including C++20 and C++23, cf. https://gcc.gnu.org/projects/cxx-status.html. Most software are configured to validate that we have a sufficient compiler version when built, and if a too old version is used, there is often an informative error message. Examples might be:\ng++: error: unrecognized command line option '-std=gnu++20'\nFrom this error message (-std=c++20), we can see that we need a compiler that requires C++20. From https://gcc.gnu.org/projects/cxx-status.html#cxx20, we see that we need at least GCC 8, but possibly even GCC 11 or GCC 12. Newer version of compilers are available via the ‘gcc-toolset’ Software Collections (SCLs). The easiest way to access these is via the scl-gcctoolset modules in the CBI software repository, e.g.\n\n[alice@dev2 ~]$ module load CBI scl-gcc-toolset/12\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nYou only need to load these compiler tools prior to installing the software. With very few exceptions, they are not needed for running the installed software later on."
  },
  {
    "objectID": "hpc/howto/install-from-source.html#configure---make---make-install",
    "href": "hpc/howto/install-from-source.html#configure---make---make-install",
    "title": "Install Software from Source Code",
    "section": "",
    "text": "Compiling software from source often involves three steps:\n\nConfiguration - configure the following build step to work with your current set of tools and libraries\nBuilding - compiling the source code into binary executables\nInstallation - install the compiled binaries to its final destination\n\nLets use the samtools software as a real-world example to illustrate these steps.\n\n\nWe start by downloading the latest software “tarball” from https://github.com/samtools/samtools/releases to a temporary working location:\n[alice@dev2 ~]$ mkdir -p \"/scratch/$USER\"\n[alice@dev2 ~]$ cd \"/scratch/$USER\"\n[alice@dev2 alice]$ wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2\n[alice@dev2 alice]$ ls -l samtools-1.14.tar.bz2 \n-rw-r--r-- 1 alice boblab 7744794 Dec  7 14:41 samtools-1.14.tar.bz2\nThe next step is to extract the content of this tarball, which we can do using tar -x -f (“extract file”):\n[alice@dev2 alice]$ tar -x -f samtools-1.14.tar.bz2\n[alice@dev2 alice]$ ls -l\ntotal 7568\ndrwxr-xr-x 9 alice boblab    4096 Oct 22 04:48 samtools-1.14\n-rw-r--r-- 1 alice boblab 7744794 Dec  7 14:41 samtools-1.14.tar.bz2\nAs we see, the content of the tarball was extracted into a subfolder samtools-1.14. The tarball file is no longer needed after this stage. Let’s enter that new folder and look at its content:\n[alice@dev2 alice]$ cd samtools-1.14\n[alice@dev2 samtools-1.14]$ ls\namplicon_stats.c    bam_markdup.c       bedcov.c       Makefile\nAUTHORS             bam_mate.c          bedidx.c       misc\nbam2bcf.c           bam_md.c            bedidx.h       NEWS\nbam2bcf.h           bam_plbuf.c         ChangeLog.old  padding.c\nbam2bcf_indel.c     bam_plbuf.h         config.h.in    phase.c\nbam2depth.c         bam_plcmd.c         config.mk.in   README\nbam_addrprg.c       bam_quickcheck.c    configure      sam_opts.c\nbam_ampliconclip.c  bam_reheader.c      configure.ac   sam_opts.h\n...\nbam.h               bamtk.c             INSTALL        stats_isize.h\nbam_import.c        bam_tview.c         install-sh     test\nbam_index.c         bam_tview_curses.c  LICENSE        tmp_file.c\nbam_lpileup.c       bam_tview.h         lz4            tmp_file.h\nbam_lpileup.h       bam_tview_html.c    m4             version.sh\nWe see that there are lots of files, but a few standard files stand out: configure, INSTALL, Makefile, NEWS, and README. The files README, NEWS and INSTALL are standard file names for human-readable text files. These are often useful to read when trying to understand what the software is about and how to install it. If there is an INSTALL file, as here, it most likely contain instructions on how to install the software. In our case, INSTALL contains a section:\nBasic Installation\n==================\n\nTo build and install Samtools, 'cd' to the samtools-1.x directory containing\nthe package's source and type the following commands:\n\n    ./configure\n    make\n    make install\n...\nFamiliar, eh?\n\n\n\nDefault installation instructions, like the ones above, often assume we will install the software as an administrator to a central location available to all users on the system. That is not possible to individual users on the cluster. Instead, we need to install it to a location where we have permission to create and write files and folders.\nA common pattern is to install into a subfolder in our home folder that reflects the name of the software and its version. This way we can have multiple versions of the same software installed at the same time. Lets install samtools to the following folder:\n[alice@dev2 samtools-1.14]$ mkdir -p $HOME/software/samtools-1.14\nNow we can configure the build to install to this folder:\n[alice@dev2 samtools-1.14]$ ./configure --prefix=$HOME/software/samtools-1.14\nconfigure: running /bin/sh ./configure --disable-option-checking '--prefix=/wynton/home/boblab/alice/software/samtools-1.14'  --cache-file=/dev/null --srcdir=.\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables... \nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether gcc accepts -g... yes\nchecking for gcc option to accept ISO C89... none needed\nchecking for ranlib... ranlib\nchecking for grep that handles long lines and -e... /usr/bin/grep\nchecking for C compiler warning flags... -Wall\nchecking for pkg-config... /usr/bin/pkg-config\nchecking pkg-config is at least version 0.9.0... yes\nchecking for special C compiler options needed for large files... no\nchecking for _FILE_OFFSET_BITS value needed for large files... no\nchecking shared library type for unknown-Linux... plain .so\nchecking whether the compiler accepts -fvisibility=hidden... yes\nchecking how to run the C preprocessor... gcc -E\nchecking for egrep... /usr/bin/grep -E\nchecking for ANSI C header files... yes\nchecking for sys/types.h... yes\nchecking for sys/stat.h... yes\nchecking for stdlib.h... yes\nchecking for string.h... yes\nchecking for memory.h... yes\nchecking for strings.h... yes\nchecking for inttypes.h... yes\nchecking for stdint.h... yes\nchecking for unistd.h... yes\nchecking for stdlib.h... (cached) yes\nchecking for unistd.h... (cached) yes\nchecking for sys/param.h... yes\nchecking for getpagesize... yes\nchecking for working mmap... yes\nchecking for gmtime_r... yes\nchecking for fsync... yes\nchecking for drand48... yes\nchecking for srand48_deterministic... no\nchecking whether fdatasync is declared... yes\nchecking for fdatasync... yes\nchecking for library containing log... -lm\nchecking for zlib.h... yes\nchecking for inflate in -lz... yes\nchecking for library containing recv... none required\nchecking for bzlib.h... yes\nchecking for BZ2_bzBuffToBuffCompress in -lbz2... yes\nchecking for lzma.h... yes\nchecking for lzma_easy_buffer_encode in -llzma... yes\nchecking whether htscodecs files are present... yes\nchecking for libdeflate.h... no\nchecking for libdeflate_deflate_compress in -ldeflate... no\nchecking for curl/curl.h... yes\nchecking for curl_easy_pause in -lcurl... yes\nchecking for CCHmac... no\nchecking for library containing HMAC... -lcrypto\nchecking for library containing regcomp... none required\nchecking whether PTHREAD_MUTEX_RECURSIVE is declared... yes\nconfigure: creating ./config.status\nconfig.status: creating config.mk\nconfig.status: creating htslib.pc.tmp\nconfig.status: creating config.h\nconfig.status: linking htscodecs_bundled.mk to htscodecs.mk\n[alice@dev2 samtools-1.14]$  \nImportantly, make sure there are no errors reported. If there are, they need to be resolved before continuing. This document does not explain how to resolve configuration errors. If you cannot figure it out yourself, please reach out on one of our Support Channels.\n\n\n\nIf the configuration steps complete without errors, it is often straightforward to build (“compile”) the software my calling make. The make command will use formal build instruction in the Makefile, but we don’t have to know about those details. Just call make as in:\n[alice@dev2 samtools-1.14]$ make\n...\nconfig.mk:46: htslib-1.14/htslib_static.mk: No such file or directory\ncd htslib-1.14 && make htslib_static.mk\nmake[1]: Entering directory `/scratch/alice/samtools-1.14/htslib-1.14'\nsed -n '/^static_libs=/s/[^=]*=/HTSLIB_static_LIBS = /p;/^static_ldflags=/s/[^=]*=/HTSLIB_static_LDFLAGS = /p' htslib.pc.tmp &gt; htslib_static.mk\nmake[1]: Leaving directory `/scratch/alice/samtools-1.14/htslib-1.14'\ngcc -Wall -g -O2 -I. -Ihtslib-1.14 -I./lz4  -c -o bam.o bam.c\ngcc -Wall -g -O2 -I. -Ihtslib-1.14 -I./lz4  -c -o bam_aux.o bam_aux.c\n\n[ ... lots of output ... ]\n\ngcc -Wall -g -O2 -I. -Ihtslib-1.14 -I./lz4  -c -o test/vcf-miniview.o test/vcf-miniview.c\ngcc  -L./lz4  -o test/vcf-miniview test/vcf-miniview.o htslib-1.14/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n[alice@dev2 samtools-1.14]$ \nMake sure there are no compilation errors. If you get errors at this stage, it could be because the gcc compiler is too old. If that happens, try to use a newer compiler version following the instructions in Section ‘Too old compiler?’ below.\n\n\n\nIf we got this far, all we have to do is to install the software, we just configured and built, to its final destination, which was specified and recorded in the configure step. All we have to do now is:\n[alice@dev2 samtools-1.14]$ make install\nmkdir -p -m 755 /wynton/home/boblab/alice/software/samtools-1.14/bin /wynton/home/boblab/alice/software/samtools-1.14/bin /wynton/home/boblab/alice/software/samtools-1.14/share/man/man1\ninstall -p samtools /wynton/home/boblab/alice/software/samtools-1.14/bin\ninstall -p misc/ace2sam misc/maq2sam-long misc/maq2sam-short misc/md5fa misc/md5sum-lite misc/wgsim /wynton/home/boblab/alice/software/samtools-1.14/bin\ninstall -p misc/blast2sam.pl misc/bowtie2sam.pl misc/export2sam.pl misc/fasta-sanitize.pl misc/interpolate_sam.pl misc/novo2sam.pl misc/plot-ampliconstats misc/plot-bamstats misc/psl2sam.pl misc/sam2vcf.pl misc/samtools.pl misc/seq_cache_populate.pl misc/soap2sam.pl misc/wgsim_eval.pl misc/zoom2sam.pl /wynton/home/boblab/alice/software/samtools-1.14/bin\ninstall -p -m 644 doc/samtools*.1 misc/wgsim.1 /wynton/home/boblab/alice/software/samtools-1.14/share/man/man1\n[alice@dev2 samtools-1.14]$ \n\n\n\nIf we pay attention to the make install output, we see that the samtools executable was installed in the bin/ subfolder of our installation directory ~/software/samtools-1.14/. Since this is not yet on the search path, we have to specify the full path for our initial test:\n[alice@dev2 samtools-1.14]$ $HOME/software/samtools-1.14/bin/samtools --version | head -3\nsamtools 1.14\nUsing htslib 1.14\nCopyright (C) 2021 Genome Research Ltd.\nIn order to call this samtools executable without having to specify the full path each time, prepend its path to the PATH environment variable, e.g.\n[alice@dev2 ~]$ export PATH=~/software/samtools-1.14/bin:$PATH\n[alice@dev2 samtools-1.14]$ which samtools\n~/software/samtools-1.14/bin/samtools\n[alice@dev2 samtools-1.14]$ samtools --version | head -3\nsamtools 1.14\nUsing htslib 1.14\nCopyright (C) 2021 Genome Research Ltd.\nFor convenience, you might want to add:\nexport PATH=~/software/samtools-1.14/bin:$PATH\nto your ~/.bashrc file."
  },
  {
    "objectID": "hpc/howto/install-from-source.html#too-old-compiler",
    "href": "hpc/howto/install-from-source.html#too-old-compiler",
    "title": "Install Software from Source Code",
    "section": "",
    "text": "The GCC development tools that come built-in on our Rocky 8 system is sufficient for most needs. For example, the default gcc version is:\n\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-18)\nCopyright (C) 2018 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nThis GCC 8.5.0 compiler supports older C++ standards such as C++17, but none of the newer standards, including C++20 and C++23, cf. https://gcc.gnu.org/projects/cxx-status.html. Most software are configured to validate that we have a sufficient compiler version when built, and if a too old version is used, there is often an informative error message. Examples might be:\ng++: error: unrecognized command line option '-std=gnu++20'\nFrom this error message (-std=c++20), we can see that we need a compiler that requires C++20. From https://gcc.gnu.org/projects/cxx-status.html#cxx20, we see that we need at least GCC 8, but possibly even GCC 11 or GCC 12. Newer version of compilers are available via the ‘gcc-toolset’ Software Collections (SCLs). The easiest way to access these is via the scl-gcctoolset modules in the CBI software repository, e.g.\n\n[alice@dev2 ~]$ module load CBI scl-gcc-toolset/12\n[alice@dev2 ~]$ gcc --version\ngcc (GCC) 12.2.1 20221121 (Red Hat 12.2.1-7)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nYou only need to load these compiler tools prior to installing the software. With very few exceptions, they are not needed for running the installed software later on."
  },
  {
    "objectID": "hpc/howto/python.html",
    "href": "hpc/howto/python.html",
    "title": "Work with Python",
    "section": "",
    "text": "Python 2 and Python 3 are both available via python2 and python3. There is no command python on Rocky 8; you have to specify either python2 or python3.\n\n\nThe standard way to install Python packages is by using the pip package management system. You often find installation instructions online such as:\n$ pip install pandas\nIt will not work. If you attempt to run this as-is on the cluster, you get lots of errors complaining about lack of write permissions etc., which is because it tries to install the package in the system-wide Python package folder (to which only sysadms have write permission). You might also see instructions saying you should use sudo ... - that will also not work for the same reason.\nThere are two ways for non-privileged users to install Python packages using the ‘pip’ module:\n\nInstall globally to your home directory (typically ~/.local/lib/python3.11/site-packages/) using python3 -m pip install --user ...\nInstall locally to a project-specific folder (e.g. ~/my_project/) using python3 -m pip install ... in a self-contained Python virtual environment\n\nBoth are done from the terminal. Which one you choose depends on your needs; for some projects you may want to use the virtual environment approach whereas for your everyday work you might want to work toward your global Python package stack. Installing globally is the easiest, because you don’t have to remember to activate a virtual environment and if you need the Python package in different places, you only have to install it once. However, if you are concerned about reproducibility, or being able to coming back to an old project of yours, you most likely want to use a virtual environment for that project so that its Python packages are not updated when you update or install Python packages globally. This is also true if you collaborate with others in a shared project folder.\n\n\nFirst of all, if an online installation instructions says pip install ..., replace that with python3 -m pip install .... Second, to install globally to your home directory, remember to always specify the --user option. For example,\n\n[alice@dev2 ~]$ python3 -m pip install --user pandas\nCollecting pandas\n  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 93.7 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.23.2 in /usr/lib64/python3.11/site-packages (from pandas) (1.23.5)\nCollecting python-dateutil&gt;=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 62.8 MB/s eta 0:00:00\nCollecting pytz&gt;=2020.1\n  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 508.0/508.0 kB 89.5 MB/s eta 0:00:00\nCollecting tzdata&gt;=2022.7\n  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.6/346.6 kB 77.3 MB/s eta 0:00:00\nCollecting six&gt;=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pytz, tzdata, six, python-dateutil, pandas\nSuccessfully installed pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2024.2 six-1.17.0 tzdata-2024.2\nTo see all Python packages that you have installed globally, use python3 -m pip list --user. To also see packages installed site wide on the cluster, use python3 -m pip list. Packages installed with python3 -m pip list --user are typically installed to your ~/.local/lib/python3.11/site-packages/ folder. If CLI executables are installed with one of those packages, they are often installed to ~/.local/bin/.\n\n\n\n\nVirtual environment are not used just on computer clusters - many Python users and developers choose to use virtual environment on their local computers whenever they work in Python.\n\nAn alternative to install globally to your home directory, is to install to a local folder using a, so called, Python virtual environment. A virtual environment is a self-contained folder that contains the Python executable and any Python packages you install. When you activate a virtual environment, environment variables like PATH is updated such that you will use the Python executable and the packages in the virtual environment and not the globally installed ones.\nBelow is an example on how to set up a virtual environment and install the pandas package and all of its dependencies into it.\n\n\nIn order to use virtual environments, we need the virtualenv tool. Following the above instructions, you can install it to your global stack as:\n\n[alice@dev2 ~]$ python3 -m pip install --user virtualenv\nCollecting virtualenv\n  Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 82.7 MB/s eta 0:00:00\nCollecting distlib&lt;1,&gt;=0.3.7\n  Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 85.1 MB/s eta 0:00:00\nCollecting filelock&lt;4,&gt;=3.12.2\n  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\nCollecting platformdirs&lt;5,&gt;=3.9.1\n  Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\nInstalling collected packages: distlib, platformdirs, filelock, virtualenv\nSuccessfully installed distlib-0.3.9 filelock-3.16.1 platformdirs-4.3.6 virtualenv-20.28.0\n[alice@dev2 ~]$ which virtualenv\n~/.local/bin/virtualenv\n[alice@dev2 ~]$ virtualenv --version\nvirtualenv 20.28.0 from ~/.local/lib/python3.11/site-packages/virtualenv/__init__.py\n\n\n\nStart by creating a folder specific to the project you are currently working on. Each project folder will have its own unique set of installed packages. For a project that requires Python 3, do the following (once):\n\n[alice@dev2 ~]$ virtualenv -p python3 my_project\ncreated virtual environment CPython3.11.10.final.0-64 in 7825ms\n  creator CPython3Posix(dest=~/my_project, clear=False, no_vcs_ignore=False, global=False)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=~/.local/share/virtualenv)\n    added seed packages: pip==24.3.1, python_dateutil==2.9.0.post0, pytz==2024.2, setuptools==75.6.0, six==1.17.0, tzdata==2024.2, wheel==0.45.1\n  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n\nAlways remember to specify option -p python3 or -p python2 when you call virtualenv. It makes sure that the intended version of Python is used in the virtual environment when python is called.\n\n\n\n\nNow, each time you want to work on your project, go to its folder and active the virtual environment:\n\n[alice@dev2 ~]$ cd my_project\n[alice@dev2 my_project]$ . bin/activate   ## IMPORTANT! Note period in front\n(my_project) [alice@dev2 my_project]$ \nNote how (my_project) is prepended to the shell prompt when the virtual environment my_project is activate. This tells you that you run in a customized Python environment. Specifically, python3 now points to a local, frozen version:\n\n(my_project) [alice@dev2 my_project]$ which python3\n~/my_project/bin/python3\nSimilarly, python points to:\n\n(my_project) [alice@dev2 my_project]$ which python\n~/my_project/bin/python\n(my_project) [alice@dev2 my_project]$ python --version\nPython 3.11.10\nNote how this local python command points to the local python3 command. What is interesting, and important to notice, is that if we set up a Python 3 virtual environment, then the local python command will point to the local python3 command. In other words, when we use virtual environments, the python command will be using either Python 2 or Python3 at our choice.\nTo see what Python packages are installed in the virtual environment, use:\n\n(my_project) [alice@dev2 my_project]$ python3 -m pip list\nPackage         Version\n--------------- -----------\nnumpy           2.2.0\npandas          2.2.3\npip             24.3.1\npython-dateutil 2.9.0.post0\npytz            2024.2\nsetuptools      75.6.0\nsix             1.17.0\ntzdata          2024.2\nwheel           0.45.1\n(my_project) [alice@dev2 my_project]$ \n\n\n\nWith a virtual environment enabled, you can install Python packages to the project folder using python3 -m pip install ... without specifying --user. For instance,\n\n(my_project) [alice@dev2 ~]$ python3 -m pip install pandas\nRequirement already satisfied: pandas in ./my_project/lib64/python3.11/site-packages (2.2.3)\nRequirement already satisfied: numpy&gt;=1.23.2 in ./my_project/lib64/python3.11/site-packages (from pandas) (2.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in ./my_project/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in ./my_project/lib/python3.11/site-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in ./my_project/lib/python3.11/site-packages (from pandas) (2024.2)\nRequirement already satisfied: six&gt;=1.5 in ./my_project/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nTo see which packages are now installed in the virtual environment (the “project folder”) and what their versions are, do:\n\n(my_project) [alice@dev2 my_project]$ python3 -m pip list\nPackage         Version\n--------------- -----------\nnumpy           2.2.0\npandas          2.2.3\npip             24.3.1\npython-dateutil 2.9.0.post0\npytz            2024.2\nsetuptools      75.6.0\nsix             1.17.0\ntzdata          2024.2\nwheel           0.45.1\n(my_project) [alice@dev2 my_project]$ \n\n\n\nWhenever you open a new terminal, make sure to activate the virtual environment (“project folder”), otherwise it will not find the packages you’ve installed. Pay attention to the shell prompt:\n\n[alice@dev2 ~]$ cd my_project \n[alice@dev2 my_project]$ . bin/activate   ## ACTIVATE\n(my_project) [alice@dev2 my_project]$ pip3 show pandas\nName: pandas\nVersion: 2.2.3\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https://pandas.pydata.org\nAuthor: \nAuthor-email: The Pandas Development Team &lt;pandas-dev@python.org&gt;\nLicense: BSD 3-Clause License\nLocation: ~/my_project/lib64/python3.11/site-packages\nRequires: numpy, python-dateutil, pytz, tzdata\nRequired-by: \n(my_project) [alice@dev2 my_project]$ \n\nWhen submitting a job to the scheduler, make sure the job script loads all required modules and activates the virtual environment.\n\nTo deactivate a Python virtual environment, either open a fresh terminal (e.g. log out and back in), or use:\n\n(my_project) [alice@dev2 ~]$ deactivate\n[alice@dev2 ~]$ \nNote how prefix (my_project) was dropped from the shell prompt and python3 now points to the system-wide installation;\n\n[alice@dev2 ~]$ which python3\n/usr/bin/python3\n\n\n\n\n\n\n\nYou will at times get a warning that you are running an old version of ‘pip’:\nYou are using pip version 8.1.2, however version 20.3.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nDon’t use the suggested command call in that message. Instead, use:\n\n[alice@dev2 ~]$ python3 -m pip install --user --upgrade pip\nRequirement already satisfied: pip in /usr/lib/python3.11/site-packages (22.3.1)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 31.7 MB/s eta 0:00:00\nInstalling collected packages: pip\nSuccessfully installed pip-24.3.1\nNote, if you’re using a virtual environment, drop --user, i.e.\n[alice@dev2 ~]$ python3 -m pip install --upgrade pip\nTo check the installed version of the ‘pip’ module, use:\n\n[alice@dev2 ~]$ python3 -m pip --version\npip 24.3.1 from ~/.local/lib/python3.11/site-packages/pip (python 3.11)\n\n\nPython 2 reached the end of its life on 2020-01-01 in favor of Python 3. At this point, the pip maintainers decided to stop making newer versions backward compatible with Python 2. Because of this, Python 2 only supports pip (&lt; 21). To upgrade to the latest supported pip version for Python 2, we can use:\n\n[alice@dev2 ~]$ python2 -m pip install --user --upgrade \"pip&lt;21\"\nDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\nCollecting pip&lt;21\n  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)\nInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 20.3\n    Uninstalling pip-20.3:\n      Successfully uninstalled pip-20.3\nSuccessfully installed pip-20.3.4"
  },
  {
    "objectID": "hpc/howto/python.html#installing-python-packages",
    "href": "hpc/howto/python.html#installing-python-packages",
    "title": "Work with Python",
    "section": "",
    "text": "The standard way to install Python packages is by using the pip package management system. You often find installation instructions online such as:\n$ pip install pandas\nIt will not work. If you attempt to run this as-is on the cluster, you get lots of errors complaining about lack of write permissions etc., which is because it tries to install the package in the system-wide Python package folder (to which only sysadms have write permission). You might also see instructions saying you should use sudo ... - that will also not work for the same reason.\nThere are two ways for non-privileged users to install Python packages using the ‘pip’ module:\n\nInstall globally to your home directory (typically ~/.local/lib/python3.11/site-packages/) using python3 -m pip install --user ...\nInstall locally to a project-specific folder (e.g. ~/my_project/) using python3 -m pip install ... in a self-contained Python virtual environment\n\nBoth are done from the terminal. Which one you choose depends on your needs; for some projects you may want to use the virtual environment approach whereas for your everyday work you might want to work toward your global Python package stack. Installing globally is the easiest, because you don’t have to remember to activate a virtual environment and if you need the Python package in different places, you only have to install it once. However, if you are concerned about reproducibility, or being able to coming back to an old project of yours, you most likely want to use a virtual environment for that project so that its Python packages are not updated when you update or install Python packages globally. This is also true if you collaborate with others in a shared project folder.\n\n\nFirst of all, if an online installation instructions says pip install ..., replace that with python3 -m pip install .... Second, to install globally to your home directory, remember to always specify the --user option. For example,\n\n[alice@dev2 ~]$ python3 -m pip install --user pandas\nCollecting pandas\n  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 93.7 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.23.2 in /usr/lib64/python3.11/site-packages (from pandas) (1.23.5)\nCollecting python-dateutil&gt;=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 62.8 MB/s eta 0:00:00\nCollecting pytz&gt;=2020.1\n  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 508.0/508.0 kB 89.5 MB/s eta 0:00:00\nCollecting tzdata&gt;=2022.7\n  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.6/346.6 kB 77.3 MB/s eta 0:00:00\nCollecting six&gt;=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pytz, tzdata, six, python-dateutil, pandas\nSuccessfully installed pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2024.2 six-1.17.0 tzdata-2024.2\nTo see all Python packages that you have installed globally, use python3 -m pip list --user. To also see packages installed site wide on the cluster, use python3 -m pip list. Packages installed with python3 -m pip list --user are typically installed to your ~/.local/lib/python3.11/site-packages/ folder. If CLI executables are installed with one of those packages, they are often installed to ~/.local/bin/.\n\n\n\n\nVirtual environment are not used just on computer clusters - many Python users and developers choose to use virtual environment on their local computers whenever they work in Python.\n\nAn alternative to install globally to your home directory, is to install to a local folder using a, so called, Python virtual environment. A virtual environment is a self-contained folder that contains the Python executable and any Python packages you install. When you activate a virtual environment, environment variables like PATH is updated such that you will use the Python executable and the packages in the virtual environment and not the globally installed ones.\nBelow is an example on how to set up a virtual environment and install the pandas package and all of its dependencies into it.\n\n\nIn order to use virtual environments, we need the virtualenv tool. Following the above instructions, you can install it to your global stack as:\n\n[alice@dev2 ~]$ python3 -m pip install --user virtualenv\nCollecting virtualenv\n  Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 82.7 MB/s eta 0:00:00\nCollecting distlib&lt;1,&gt;=0.3.7\n  Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 85.1 MB/s eta 0:00:00\nCollecting filelock&lt;4,&gt;=3.12.2\n  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\nCollecting platformdirs&lt;5,&gt;=3.9.1\n  Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\nInstalling collected packages: distlib, platformdirs, filelock, virtualenv\nSuccessfully installed distlib-0.3.9 filelock-3.16.1 platformdirs-4.3.6 virtualenv-20.28.0\n[alice@dev2 ~]$ which virtualenv\n~/.local/bin/virtualenv\n[alice@dev2 ~]$ virtualenv --version\nvirtualenv 20.28.0 from ~/.local/lib/python3.11/site-packages/virtualenv/__init__.py\n\n\n\nStart by creating a folder specific to the project you are currently working on. Each project folder will have its own unique set of installed packages. For a project that requires Python 3, do the following (once):\n\n[alice@dev2 ~]$ virtualenv -p python3 my_project\ncreated virtual environment CPython3.11.10.final.0-64 in 7825ms\n  creator CPython3Posix(dest=~/my_project, clear=False, no_vcs_ignore=False, global=False)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=~/.local/share/virtualenv)\n    added seed packages: pip==24.3.1, python_dateutil==2.9.0.post0, pytz==2024.2, setuptools==75.6.0, six==1.17.0, tzdata==2024.2, wheel==0.45.1\n  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n\nAlways remember to specify option -p python3 or -p python2 when you call virtualenv. It makes sure that the intended version of Python is used in the virtual environment when python is called.\n\n\n\n\nNow, each time you want to work on your project, go to its folder and active the virtual environment:\n\n[alice@dev2 ~]$ cd my_project\n[alice@dev2 my_project]$ . bin/activate   ## IMPORTANT! Note period in front\n(my_project) [alice@dev2 my_project]$ \nNote how (my_project) is prepended to the shell prompt when the virtual environment my_project is activate. This tells you that you run in a customized Python environment. Specifically, python3 now points to a local, frozen version:\n\n(my_project) [alice@dev2 my_project]$ which python3\n~/my_project/bin/python3\nSimilarly, python points to:\n\n(my_project) [alice@dev2 my_project]$ which python\n~/my_project/bin/python\n(my_project) [alice@dev2 my_project]$ python --version\nPython 3.11.10\nNote how this local python command points to the local python3 command. What is interesting, and important to notice, is that if we set up a Python 3 virtual environment, then the local python command will point to the local python3 command. In other words, when we use virtual environments, the python command will be using either Python 2 or Python3 at our choice.\nTo see what Python packages are installed in the virtual environment, use:\n\n(my_project) [alice@dev2 my_project]$ python3 -m pip list\nPackage         Version\n--------------- -----------\nnumpy           2.2.0\npandas          2.2.3\npip             24.3.1\npython-dateutil 2.9.0.post0\npytz            2024.2\nsetuptools      75.6.0\nsix             1.17.0\ntzdata          2024.2\nwheel           0.45.1\n(my_project) [alice@dev2 my_project]$ \n\n\n\nWith a virtual environment enabled, you can install Python packages to the project folder using python3 -m pip install ... without specifying --user. For instance,\n\n(my_project) [alice@dev2 ~]$ python3 -m pip install pandas\nRequirement already satisfied: pandas in ./my_project/lib64/python3.11/site-packages (2.2.3)\nRequirement already satisfied: numpy&gt;=1.23.2 in ./my_project/lib64/python3.11/site-packages (from pandas) (2.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in ./my_project/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in ./my_project/lib/python3.11/site-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in ./my_project/lib/python3.11/site-packages (from pandas) (2024.2)\nRequirement already satisfied: six&gt;=1.5 in ./my_project/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nTo see which packages are now installed in the virtual environment (the “project folder”) and what their versions are, do:\n\n(my_project) [alice@dev2 my_project]$ python3 -m pip list\nPackage         Version\n--------------- -----------\nnumpy           2.2.0\npandas          2.2.3\npip             24.3.1\npython-dateutil 2.9.0.post0\npytz            2024.2\nsetuptools      75.6.0\nsix             1.17.0\ntzdata          2024.2\nwheel           0.45.1\n(my_project) [alice@dev2 my_project]$ \n\n\n\nWhenever you open a new terminal, make sure to activate the virtual environment (“project folder”), otherwise it will not find the packages you’ve installed. Pay attention to the shell prompt:\n\n[alice@dev2 ~]$ cd my_project \n[alice@dev2 my_project]$ . bin/activate   ## ACTIVATE\n(my_project) [alice@dev2 my_project]$ pip3 show pandas\nName: pandas\nVersion: 2.2.3\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https://pandas.pydata.org\nAuthor: \nAuthor-email: The Pandas Development Team &lt;pandas-dev@python.org&gt;\nLicense: BSD 3-Clause License\nLocation: ~/my_project/lib64/python3.11/site-packages\nRequires: numpy, python-dateutil, pytz, tzdata\nRequired-by: \n(my_project) [alice@dev2 my_project]$ \n\nWhen submitting a job to the scheduler, make sure the job script loads all required modules and activates the virtual environment.\n\nTo deactivate a Python virtual environment, either open a fresh terminal (e.g. log out and back in), or use:\n\n(my_project) [alice@dev2 ~]$ deactivate\n[alice@dev2 ~]$ \nNote how prefix (my_project) was dropped from the shell prompt and python3 now points to the system-wide installation;\n\n[alice@dev2 ~]$ which python3\n/usr/bin/python3"
  },
  {
    "objectID": "hpc/howto/python.html#appendix",
    "href": "hpc/howto/python.html#appendix",
    "title": "Work with Python",
    "section": "",
    "text": "You will at times get a warning that you are running an old version of ‘pip’:\nYou are using pip version 8.1.2, however version 20.3.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nDon’t use the suggested command call in that message. Instead, use:\n\n[alice@dev2 ~]$ python3 -m pip install --user --upgrade pip\nRequirement already satisfied: pip in /usr/lib/python3.11/site-packages (22.3.1)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 31.7 MB/s eta 0:00:00\nInstalling collected packages: pip\nSuccessfully installed pip-24.3.1\nNote, if you’re using a virtual environment, drop --user, i.e.\n[alice@dev2 ~]$ python3 -m pip install --upgrade pip\nTo check the installed version of the ‘pip’ module, use:\n\n[alice@dev2 ~]$ python3 -m pip --version\npip 24.3.1 from ~/.local/lib/python3.11/site-packages/pip (python 3.11)\n\n\nPython 2 reached the end of its life on 2020-01-01 in favor of Python 3. At this point, the pip maintainers decided to stop making newer versions backward compatible with Python 2. Because of this, Python 2 only supports pip (&lt; 21). To upgrade to the latest supported pip version for Python 2, we can use:\n\n[alice@dev2 ~]$ python2 -m pip install --user --upgrade \"pip&lt;21\"\nDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\nCollecting pip&lt;21\n  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)\nInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 20.3\n    Uninstalling pip-20.3:\n      Successfully uninstalled pip-20.3\nSuccessfully installed pip-20.3.4"
  },
  {
    "objectID": "drafts/20191026-sge_shell_exit_traps.html",
    "href": "drafts/20191026-sge_shell_exit_traps.html",
    "title": "SGE and Shell EXIT Traps",
    "section": "",
    "text": "Shell traps can be used to execute a sequence of shell commands upon certain conditions or signals. For example, the ‘EXIT’ trap can be used to perform clean up tasks or similar when a shell script terminates even due to errors. See man trap for details.\nBelow is an example script trap_exit.sh that outputs “EXIT trap called” to the standard error when it terminates:\n#! /usr/bin/sh\n\ntrap '{ &gt;&2 echo \"EXIT trap called\" ; }' EXIT\n\necho \"Going to sleep ...\"\nsleep 60\n\n# Exit script\n{ &gt;&2 echo \"ERROR: boom! Exiting ...\"; exit 1; }\n\necho \"This line will never be reached\"\nIf we call this script, we’ll get after 60 seconds:\n$ ./trap_exit.sh\nGoing to sleep ...\nERROR: boom! Exiting ...\nEXIT trap called\n\n$ \n\n\nThe EXIT trap is also called when the processed is terminated by other means, e.g. using kill, e.g. kill -TERM &lt;PID&gt; or equivalently kill -15 &lt;PID&gt;. However, the EXIT trap will not be called when it is killed using SIGKILL (signal level 9) because such signals can not be intercepted.\n\n\n\nUnfortunately, SGE uses SIGKILL to terminate jobs by default, which means that EXIT traps will not be called. For example, if we submit trap_exit.sh:\n$ qsub -cwd -j yes trap_exit.sh\nYour job 8738186 (\"trap_exit.sh\") has been submitted\nwait for the job to start, and then kill it:\n$ qdel 8738186\nalice has registered the job 8738186 for deletion\nwe see that the EXIT trap was never called:\n$ cat trap_exit.sh.o8738186\nGoing to sleep ...\nFortunately, there is way to fix this. Submit the job with qsub option -notify (see man qsub). This will cause SGE to first send a SIGUSR2 signal to the job process, wait 60 seconds, before it sends the ultimate SIGKILL signal. That first SIGUSR2 signal will trigger the EXIT trap. For example, if we instead submit using:\n$ qsub -cwd -j yes -notify trap_exit.sh\nYour job 8738208 (\"trap_exit.sh\") has been submitted\nwait for the job to start, and then kill it, we see that the EXIT trap was indeed called:\n$ cat trap_exit.sh.o8738208\nGoing to sleep ...\nEXIT trap called\nThis will also work when SGE terminates a job process that consumes more resources or run longer than allowed. For example, if we submit our script with a maximum run time of 10 seconds, it will be terminated too:\n$ qsub -cwd -j yes -l h_rt=10 -notify trap_exit.sh\nYour job 8738217 (\"trap_exit.sh\") has been submitted\nFrom the output logs, we see:\n$ cat trap_exit.sh.oe8738208\nGoing to sleep ...\nEXIT trap called"
  },
  {
    "objectID": "drafts/20191026-sge_shell_exit_traps.html#known-limitations",
    "href": "drafts/20191026-sge_shell_exit_traps.html#known-limitations",
    "title": "SGE and Shell EXIT Traps",
    "section": "",
    "text": "The EXIT trap is also called when the processed is terminated by other means, e.g. using kill, e.g. kill -TERM &lt;PID&gt; or equivalently kill -15 &lt;PID&gt;. However, the EXIT trap will not be called when it is killed using SIGKILL (signal level 9) because such signals can not be intercepted."
  },
  {
    "objectID": "drafts/20191026-sge_shell_exit_traps.html#using-exit-traps-with-sge-jobs",
    "href": "drafts/20191026-sge_shell_exit_traps.html#using-exit-traps-with-sge-jobs",
    "title": "SGE and Shell EXIT Traps",
    "section": "",
    "text": "Unfortunately, SGE uses SIGKILL to terminate jobs by default, which means that EXIT traps will not be called. For example, if we submit trap_exit.sh:\n$ qsub -cwd -j yes trap_exit.sh\nYour job 8738186 (\"trap_exit.sh\") has been submitted\nwait for the job to start, and then kill it:\n$ qdel 8738186\nalice has registered the job 8738186 for deletion\nwe see that the EXIT trap was never called:\n$ cat trap_exit.sh.o8738186\nGoing to sleep ...\nFortunately, there is way to fix this. Submit the job with qsub option -notify (see man qsub). This will cause SGE to first send a SIGUSR2 signal to the job process, wait 60 seconds, before it sends the ultimate SIGKILL signal. That first SIGUSR2 signal will trigger the EXIT trap. For example, if we instead submit using:\n$ qsub -cwd -j yes -notify trap_exit.sh\nYour job 8738208 (\"trap_exit.sh\") has been submitted\nwait for the job to start, and then kill it, we see that the EXIT trap was indeed called:\n$ cat trap_exit.sh.o8738208\nGoing to sleep ...\nEXIT trap called\nThis will also work when SGE terminates a job process that consumes more resources or run longer than allowed. For example, if we submit our script with a maximum run time of 10 seconds, it will be terminated too:\n$ qsub -cwd -j yes -l h_rt=10 -notify trap_exit.sh\nYour job 8738217 (\"trap_exit.sh\") has been submitted\nFrom the output logs, we see:\n$ cat trap_exit.sh.oe8738208\nGoing to sleep ...\nEXIT trap called"
  },
  {
    "objectID": "hpc/howto/conda.html",
    "href": "hpc/howto/conda.html",
    "title": "Work with Conda",
    "section": "",
    "text": "2024-08-21: We no longer recommend using Anaconda or Miniconda that is distributed by Anaconda Inc., because of license issues. Anaconda Inc. argues that using their default package channels requires UCSF to acquire an enterprise license. If you used module load CBI miniconda3 in the past, we therefore recommend that you use module load CBI miniforge3 instead."
  },
  {
    "objectID": "hpc/howto/conda.html#loading-miniforge",
    "href": "hpc/howto/conda.html#loading-miniforge",
    "title": "Work with Conda",
    "section": "Loading Miniforge",
    "text": "Loading Miniforge\nOn Wynton HPC, up-to-date versions of the Miniforge distribution are available via the CBI software stack. There is no need for you to install this yourself. To load Miniforge, call:\n\n[alice@dev2 ~]$ module load CBI miniforge3/24.11.0-0\nThis gives access to:\n\n[alice@dev2 ~]$ conda --version\nconda 24.11.0\n[alice@dev2 ~]$ python --version\nPython 3.12.8\nTo see what software packages come with this distribution, call:\n\n[alice@dev2 ~]$ conda list\n# packages in environment at /wynton/home/boblab/shared/software/CBI/miniforge3-24.11.0-0:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\narchspec                  0.2.3              pyhd8ed1ab_0    conda-forge\nboltons                   24.0.0             pyhd8ed1ab_1    conda-forge\nbrotli-python             1.1.0           py312h2ec8cdc_2    conda-forge\nbzip2                     1.0.8                h4bc722e_7    conda-forge\nc-ares                    1.34.3               hb9d3cd8_1    conda-forge\nca-certificates           2024.8.30            hbcca054_0    conda-forge\ncertifi                   2024.8.30          pyhd8ed1ab_0    conda-forge\ncffi                      1.17.1          py312h06ac9bb_0    conda-forge\ncharset-normalizer        3.4.0              pyhd8ed1ab_1    conda-forge\ncolorama                  0.4.6              pyhd8ed1ab_1    conda-forge\nconda                     24.11.0         py312h7900ff3_0    conda-forge\nconda-libmamba-solver     24.9.0             pyhd8ed1ab_0    conda-forge\nconda-package-handling    2.4.0              pyha770c72_1    conda-forge\nconda-package-streaming   0.11.0             pyhd8ed1ab_0    conda-forge\ndistro                    1.9.0              pyhd8ed1ab_0    conda-forge\nfmt                       11.0.2               h434a139_0    conda-forge\nfrozendict                2.4.6           py312h66e93f0_0    conda-forge\nh2                        4.1.0              pyhd8ed1ab_1    conda-forge\nhpack                     4.0.0              pyhd8ed1ab_1    conda-forge\nhyperframe                6.0.1              pyhd8ed1ab_1    conda-forge\nidna                      3.10               pyhd8ed1ab_1    conda-forge\njsonpatch                 1.33               pyhd8ed1ab_1    conda-forge\njsonpointer               3.0.0           py312h7900ff3_1    conda-forge\nkeyutils                  1.6.1                h166bdaf_0    conda-forge\nkrb5                      1.21.3               h659f571_0    conda-forge\nld_impl_linux-64          2.43                 h712a8e2_2    conda-forge\nlibarchive                3.7.7                h4585015_2    conda-forge\nlibcurl                   8.10.1               hbbe4b11_0    conda-forge\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\nlibev                     4.33                 hd590300_2    conda-forge\nlibexpat                  2.6.4                h5888daf_0    conda-forge\nlibffi                    3.4.2                h7f98852_5    conda-forge\nlibgcc                    14.2.0               h77fa898_1    conda-forge\nlibgcc-ng                 14.2.0               h69a702a_1    conda-forge\nlibgomp                   14.2.0               h77fa898_1    conda-forge\nlibiconv                  1.17                 hd590300_2    conda-forge\nliblzma                   5.6.3                hb9d3cd8_1    conda-forge\nlibmamba                  1.5.11               hf72d635_0    conda-forge\nlibmambapy                1.5.11          py312hf3f0a4e_0    conda-forge\nlibnghttp2                1.64.0               h161d5f1_0    conda-forge\nlibnsl                    2.0.1                hd590300_0    conda-forge\nlibsolv                   0.7.30               h3509ff9_0    conda-forge\nlibsqlite                 3.47.2               hee588c1_0    conda-forge\nlibssh2                   1.11.1               hf672d98_0    conda-forge\nlibstdcxx                 14.2.0               hc0a3c3a_1    conda-forge\nlibstdcxx-ng              14.2.0               h4852527_1    conda-forge\nlibuuid                   2.38.1               h0b41bf4_0    conda-forge\nlibxcrypt                 4.4.36               hd590300_1    conda-forge\nlibxml2                   2.13.5               h0d44e9d_1    conda-forge\nlibzlib                   1.3.1                hb9d3cd8_2    conda-forge\nlz4-c                     1.10.0               h5888daf_1    conda-forge\nlzo                       2.10              hd590300_1001    conda-forge\nmamba                     1.5.11          py312h9460a1c_0    conda-forge\nmenuinst                  2.2.0           py312h7900ff3_0    conda-forge\nncurses                   6.5                  he02047a_1    conda-forge\nopenssl                   3.4.0                hb9d3cd8_0    conda-forge\npackaging                 24.2               pyhd8ed1ab_2    conda-forge\npip                       24.3.1             pyh8b19718_0    conda-forge\nplatformdirs              4.3.6              pyhd8ed1ab_1    conda-forge\npluggy                    1.5.0              pyhd8ed1ab_1    conda-forge\npybind11-abi              4                    hd8ed1ab_3    conda-forge\npycosat                   0.6.6           py312h66e93f0_2    conda-forge\npycparser                 2.22               pyh29332c3_1    conda-forge\npysocks                   1.7.1              pyha55dd90_7    conda-forge\npython                    3.12.8          h9e4cc4f_1_cpython    conda-forge\npython_abi                3.12                    5_cp312    conda-forge\nreadline                  8.2                  h8228510_1    conda-forge\nreproc                    14.2.5.post0         hb9d3cd8_0    conda-forge\nreproc-cpp                14.2.5.post0         h5888daf_0    conda-forge\nrequests                  2.32.3             pyhd8ed1ab_1    conda-forge\nruamel.yaml               0.18.6          py312h66e93f0_1    conda-forge\nruamel.yaml.clib          0.2.8           py312h66e93f0_1    conda-forge\nsetuptools                75.6.0             pyhff2d567_1    conda-forge\ntk                        8.6.13          noxft_h4845f30_101    conda-forge\ntqdm                      4.67.1             pyhd8ed1ab_0    conda-forge\ntruststore                0.10.0             pyhd8ed1ab_0    conda-forge\ntzdata                    2024b                hc8b5060_0    conda-forge\nurllib3                   2.2.3              pyhd8ed1ab_1    conda-forge\nwheel                     0.45.1             pyhd8ed1ab_1    conda-forge\nyaml-cpp                  0.8.0                h59595ed_0    conda-forge\nzstandard                 0.23.0          py312hef9b889_1    conda-forge\nzstd                      1.5.6                ha6fb4c9_0    conda-forge"
  },
  {
    "objectID": "hpc/howto/conda.html#creating-a-conda-environment-required",
    "href": "hpc/howto/conda.html#creating-a-conda-environment-required",
    "title": "Work with Conda",
    "section": "Creating a Conda environment (required)",
    "text": "Creating a Conda environment (required)\nA Conda environment is a mechanism for installing extra software tools and versions beyond the base Miniforge distribution in a controlled manner. When using the miniforge3 module, a Conda environment must be used to install extra software. The following command creates a new myjupyter environment:\n[alice@dev2 ~]$ conda create -n myjupyter notebook\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /wynton/home/boblab/alice/.conda/envs/myjupyter\n  \n  added / updated specs:\n    - notebook\n    \nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n  ...\n  zstandard          conda-forge/linux-64::zstandard-0.23.0-py312h3483029_0 \n  zstd               conda-forge/linux-64::zstd-1.5.6-ha6fb4c9_0 \n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\n\n...\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate myjupyter\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\n[alice@dev2 ~]$ \nBy default, the environment is created in your home directory under ~/.conda/envs/. To create the environment at a specific location, see Managing environments part of the official Conda documentation. That section covers many useful topics such as removing a Conda environment, and creating an environment with a specific Python version."
  },
  {
    "objectID": "hpc/howto/conda.html#activating-a-conda-environment-required",
    "href": "hpc/howto/conda.html#activating-a-conda-environment-required",
    "title": "Work with Conda",
    "section": "Activating a Conda environment (required)",
    "text": "Activating a Conda environment (required)\nAfter an environment is created, the next time you log in to a development node, you can set myjupyter (or any other Conda environment you’ve created) as your active environment by calling:\n[alice@dev2 ~]$ module load CBI miniforge3\n[alice@dev2 ~]$ conda activate myjupyter\n(myjupyter) [alice@dev2 ~]$ jupyter notebook --version\n7.2.1\n\n(myjupyter) [alice@dev2 ~]$ \nNote how the command-line prompt is prefixed with (myjupyter); it highlights that the Conda environment myjupyter is activated. To deactivate an environment and return to the base environment, call:\n(myjupyter) [alice@dev2 ~]$ conda deactivate\n[alice@dev2 ~]$ jupyter notebook --version\njupyter: command not found\n\n[alice@dev2 ~]$"
  },
  {
    "objectID": "hpc/howto/conda.html#speed-up-software-by-auto-staging-conda-environment-recommended",
    "href": "hpc/howto/conda.html#speed-up-software-by-auto-staging-conda-environment-recommended",
    "title": "Work with Conda",
    "section": "Speed up software by auto-staging Conda environment (recommended)",
    "text": "Speed up software by auto-staging Conda environment (recommended)\nWe highly recommend configuring Conda environment to be automatically staged only on the local disk whenever activated. This results in your software running significantly faster. Auto-staging is straightforward to configure using the conda-stage tool, e.g.\n[alice@dev2 ~]$ module load CBI miniforge3\n[alice@dev2 ~]$ module load CBI conda-stage\n[alice@dev2 ~]$ conda stage --auto-stage=enable myjupyter\nINFO: Configuring automatic staging and unstaging of original Conda environment  ...\nINFO: [ONCE] Packaging Conda environment, because it hasn't been done before ...\nCollecting packages...\nPacking environment at '/wynton/home/boblab/alice/.conda/envs/myjupyter' to\n'/wynton/home/boblab/alice/.conda/envs/.tmp.myjupyter.tar.gz'\n[########################################] | 100% Completed |  4min  5.6s\nINFO: Total 'conda-pack' time: 274 seconds\nINFO: Created conda-pack tarball: /wynton/home/boblab/alice/.conda/envs/myjupyter.tar.gz\n      (140099022 bytes; 2025-03-02 19:33:08.108806755 -0800)\nINFO: Enabled auto-staging\nINFO: Enabled auto-unstaging\n[alice@dev2 ~]$ \nThat’s it. For the complete, easy-to-follow instructions, see the conda-stage documentation."
  },
  {
    "objectID": "hpc/howto/conda.html#back-up-migrate-and-restore-conda-environments-recommended",
    "href": "hpc/howto/conda.html#back-up-migrate-and-restore-conda-environments-recommended",
    "title": "Work with Conda",
    "section": "Back up, migrate, and restore Conda environments (recommended)",
    "text": "Back up, migrate, and restore Conda environments (recommended)\nOnce you have your Conda environment built, we recommend that you back up its core configuration. The process is quick and straightforward, and fully documented in the official Conda Managing environments documentation. For example, to back up the above myjupyter environment, call:\n[alice@dev2 ~]$ conda env export --name myjupyter | grep -v \"^prefix: \" &gt; myjupyter.yml\n[alice@dev2 ~]$ ls -l myjupyter.yml\n-rw-rw-r-- 1 alice boblab 4982 Aug 21 17:56 myjupyter.yml\nThis configuration file is useful:\n\nwhen migrating the environment from Wynton to another Conda version, another computer, or another HPC environment\nfor sharing the environment with collaborators\nfor making a snapshot of the software stack used in a project\nfor disaster recovery, e.g. if you remove the Conda environment by mistake\nfor updating the dependencies in a Conda environment\n\nTo restore a backed up Conda environment from a yaml file, on the target machine:\n\ndownload the yaml file, e.g. myjupyter.yml\nmake sure there is no existing environment with the same name, i.e. check conda env list\ncreate a new Conda environment from the yaml file\n\nFor example, assume we have downloaded myjupyter.yml to our local machine. Then we start by getting the name of the backed-up Conda environment and making sure it does not already exist;\n{local}$ grep \"name:\" myjupyter.yml\nname: myjupyter\n\n{local}$ conda env list\n# conda environments:\n#\nmyjupyter /wynton/home/boblab/alice/.conda/envs/myjupyter\nbase      /path/to/miniforge3\nWhen have confirmed that there is no name clash, we can restore the backed up environment on our local machine using:\n{local}$ conda env create -f myjupyter.yml \nThis will install the exact same software versions as when we made the backup.\nWarning: This is not a fool-proof backup method, because it depends on packages to be available from the package repositories also when you try to restore the Conda environment. To lower the risk for failure, keep your environments up to date with the latest packages and test frequently that your myjupyter.yml file can be restored.\nWe can also use myjupyter.yml to update an existing environment. The gist is to edit the file to reflect what we want to be updated, and then run conda env update .... See Managing environments part of the official Conda documentation for exact instructions."
  },
  {
    "objectID": "hpc/howto/conda.html#conda-revisions",
    "href": "hpc/howto/conda.html#conda-revisions",
    "title": "Work with Conda",
    "section": "Conda revisions",
    "text": "Conda revisions\nIf you updated or installed new packages in your Conda environment and need to roll back to a previous version, it is possible to do this using Conda’s revision utility. To list available revisions in the current Conda environment, use:\n[alice@dev2 ~]$ CONDA_STAGE=false conda activate myjupyter\n(myjupyter) [alice@dev2 ~]$ conda list --revisions\n2023-06-11 03:46:03  (rev 0)\n    +_libgcc_mutex-0.1 (defaults/linux-64)\n    +_openmp_mutex-5.1 (defaults/linux-64)\n    +anyio-3.5.0 (defaults/linux-64)\n    +argon2-cffi-21.3.0 (defaults/noarch)\n    ...\n\n2023-06-11 03:52:03  (rev 1)\n    +conda-pack-0.7.0 (conda-forge/noarch)\n    +python_abi-3.11 (conda-forge/linux-64)\nTo roll back to a specific revision, say revision zero, use:\n(myjupyter) [alice@dev2 ~]$ conda install --revision 0\nWarning: This only works with packages installed using conda install. Packages installed via python3 -m pip install will not be recorded by the revision system. In such cases, we have to do manual backup snapshots (as explained above)."
  },
  {
    "objectID": "hpc/howto/conda.html#appendix",
    "href": "hpc/howto/conda.html#appendix",
    "title": "Work with Conda",
    "section": "Appendix",
    "text": "Appendix\n\nDisable automatic activation of the ‘base’ environment (highly recommended)\nIf you previously have installed Conda yourself, there is a risk that it installed itself into your ~/.bashrc file such that it automatically activates the ‘base’ environment whenever you start a new shell, e.g.\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice1@log2.wynton.ucsf.edu:s password: XXXXXXXXXXXXXXXXXXX\n(base) [alice@log2 ~]$ \nIf you see a (base) prefix in your prompt, then you have this set up and the Conda ‘base’ environment is active. You can verify this by querying conda info as:\n[alice@dev2 ~]$ conda info | grep active\nconda info | grep active\n     active environment : base\n    active env location : /wynton/home/boblab/alice/.conda\nThis auto-activation might sound convenient, but we strongly recommend against using it, because Conda software stacks have a great chance to cause conflicts (read: wreak havoc) with other software tools installed outside of Conda. For example, people that have Conda activated and then run R via module load CBI r, often report on endless, hard-to-understand problems when trying to install common R packages. Instead, we recommend to activate your Conda environments only when you need them, and leave them non-activated otherwise. This will give you a much smoother day-to-day experience. To clarify, if you never installed Conda yourself, and only used module load CBI miniforge3, then you should not have this problem.\nTo reconfigure Conda to no longer activate the ‘base’ Conda environment by default, call:\n(base) [alice@dev2 ~]$ conda config --set auto_activate_base false\n(base) [alice@dev2 ~]$ \nNext time you log in, the ‘base’ environment should no longer be activated by default.\nIf you want to completely retire you personal Conda installation, and move on to only using module load CBI miniforge3, you can uninstall the Conda setup code that were injected to your ~/.bashrc file by calling:\n[alice@dev2 ~]$ conda init --reverse\nno change     /wynton/home/boblab/alice/.conda/condabin/conda\nno change     /wynton/home/boblab/alice/.conda/bin/conda\nno change     /wynton/home/boblab/alice/.conda/bin/conda-env\nno change     /wynton/home/boblab/alice/.conda/bin/activate\nno change     /wynton/home/boblab/alice/.conda/bin/deactivate\nno change     /wynton/home/boblab/alice/.conda/etc/profile.d/conda.sh\nno change     /wynton/home/boblab/alice/.conda/etc/fish/conf.d/conda.fish\nno change     /wynton/home/boblab/alice/.conda/shell/condabin/Conda.psm1\nno change     /wynton/home/boblab/alice/.conda/shell/condabin/conda-hook.ps1\nno change     /wynton/home/boblab/alice/.conda/lib/python3.9/site-packages/xontrib/conda.xsh\nno change     /wynton/home/boblab/alice/.conda/etc/profile.d/conda.csh\nmodified      /wynton/home/boblab/alice/.bashrc\n\n==&gt; For changes to take effect, close and re-open your current shell. &lt;==\n\n[alice@dev2 ~]$"
  },
  {
    "objectID": "hpc/howto/matlab.html",
    "href": "hpc/howto/matlab.html",
    "title": "Work with MATLAB",
    "section": "",
    "text": "MATLAB is available on Wynton HPC via a built-in environment module and is supported by a UCSF-wide MATLAB license.\n\n\nTo load the MATLAB module, do:\n\n[alice@dev2 ~]$ module load matlab\n[alice@dev2 ~]$ matlab -nosplash -nodesktop\nMATLAB is selecting SOFTWARE OPENGL rendering.\n\n                     &lt; M A T L A B (R) &gt;\n           Copyright 1984-2023 The MathWorks, Inc.                                                             \n      R2023b Update 5 (23.2.0.2459199) 64-bit (glnxa64)                                                        \n                      November 28, 2023\n\n \nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n \n&gt;&gt; 1+2\n\nans =\n\n     3\n\n&gt;&gt; quit\n[alice@dev2 ~]$ \nIf you forget to load the MATLAB module, then you will get an error when attempting to start MATLAB:\n[alice@dev2 ~]$ matlab\n-bash: matlab: command not found\n\n\n\nIn order to run MATLAB in jobs, the MATLAB environment module needs to be loaded just as when you run it interactive on a development node. For example, to run the my_script.m script, the job script should at a minimum contain:\n#! /usr/bin/env bash\n#$ -S /bin/bash\n#$ -cwd          # run job in the current working directory\n\nmodule load matlab\nmatlab -singleCompThread -batch \"my_script\"\nThe -batch option tells MATLAB to call the command my_script, and since that is not a built-in command, it will look for a MATLAB script file called my_script.m, and execute that. The -singleCompThread option tells MATLAB to run in sequential mode; this prevents your job for overusing the compute nodes by mistake.\n\n\nIf your MATLAB code supports parallel processing, make sure to specify the number of CPU cores when submitting your job submit, e.g. -pe smp 4 will request four cores on one machine, which in turn will set environment variable NSLOTS to 4. To make your MATLAB script respect this, add the following at the top of your script:\n%% Make MATLAB respect the number of cores that the SGE scheduler\n%% has alloted the job.  If not specified, run with a single core,\n%% e.g. when running on a development node\nnslots = getenv('NSLOTS');              % env var is always a 'char'\nif (isempty(nslots)) nslots = '1'; end  % default value\nnslots = str2num(nslots);               % coerce to 'double'\nmaxNumCompThreads(nslots);              % number of cores MATLAB may use\nand then launch your MATLAB script without option -singleCompThread, e.g. matlab -batch \"my_script\"."
  },
  {
    "objectID": "hpc/howto/matlab.html#accessing-matlab",
    "href": "hpc/howto/matlab.html#accessing-matlab",
    "title": "Work with MATLAB",
    "section": "",
    "text": "To load the MATLAB module, do:\n\n[alice@dev2 ~]$ module load matlab\n[alice@dev2 ~]$ matlab -nosplash -nodesktop\nMATLAB is selecting SOFTWARE OPENGL rendering.\n\n                     &lt; M A T L A B (R) &gt;\n           Copyright 1984-2023 The MathWorks, Inc.                                                             \n      R2023b Update 5 (23.2.0.2459199) 64-bit (glnxa64)                                                        \n                      November 28, 2023\n\n \nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n \n&gt;&gt; 1+2\n\nans =\n\n     3\n\n&gt;&gt; quit\n[alice@dev2 ~]$ \nIf you forget to load the MATLAB module, then you will get an error when attempting to start MATLAB:\n[alice@dev2 ~]$ matlab\n-bash: matlab: command not found"
  },
  {
    "objectID": "hpc/howto/matlab.html#using-matlab-in-job-scripts",
    "href": "hpc/howto/matlab.html#using-matlab-in-job-scripts",
    "title": "Work with MATLAB",
    "section": "",
    "text": "In order to run MATLAB in jobs, the MATLAB environment module needs to be loaded just as when you run it interactive on a development node. For example, to run the my_script.m script, the job script should at a minimum contain:\n#! /usr/bin/env bash\n#$ -S /bin/bash\n#$ -cwd          # run job in the current working directory\n\nmodule load matlab\nmatlab -singleCompThread -batch \"my_script\"\nThe -batch option tells MATLAB to call the command my_script, and since that is not a built-in command, it will look for a MATLAB script file called my_script.m, and execute that. The -singleCompThread option tells MATLAB to run in sequential mode; this prevents your job for overusing the compute nodes by mistake.\n\n\nIf your MATLAB code supports parallel processing, make sure to specify the number of CPU cores when submitting your job submit, e.g. -pe smp 4 will request four cores on one machine, which in turn will set environment variable NSLOTS to 4. To make your MATLAB script respect this, add the following at the top of your script:\n%% Make MATLAB respect the number of cores that the SGE scheduler\n%% has alloted the job.  If not specified, run with a single core,\n%% e.g. when running on a development node\nnslots = getenv('NSLOTS');              % env var is always a 'char'\nif (isempty(nslots)) nslots = '1'; end  % default value\nnslots = str2num(nslots);               % coerce to 'double'\nmaxNumCompThreads(nslots);              % number of cores MATLAB may use\nand then launch your MATLAB script without option -singleCompThread, e.g. matlab -batch \"my_script\"."
  },
  {
    "objectID": "hpc/howto/bash.html",
    "href": "hpc/howto/bash.html",
    "title": "Work with Bash",
    "section": "",
    "text": "Unless you have explicitly requested to use a different shell than Bash, you will be running a Bash shell when you log onto any of Wynton HPC’s machines. You can confirm this by looking at the value of the SHELL environment variable;\n\n[alice@dev2 ~]$ echo \"$SHELL\"\n/bin/bash\nWhen a new Bash shell is launched, it will be configured according to setting in your personal ~/.bashrc startup file. On a fresh account, this file looks like:\n\n[alice@dev2 ~]$ cat /etc/skel/.bashrc\n# .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\nIn Bash, code comments start with a # symbol, which means that anything following and include this symbol that is on the same line is completely ignored. If we look at the above code, we see that almost all lines are comments. The only thing that is not a comment is the following if-then statement:\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\nThis Bash statement (i) checks if the file /etc/bashrc exists, and (ii) if it does, then that file is also “sourced” by the . /etc/bashrc line. The period in front is not a mistake; sometimes you see a period sometimes you see source, which works the same.\nIf we would look at the /etc/bashrc file, we would find a lot of things but lets not go into the details. The only thing we need to know is that . /etc/bashrc will bring in all of the essential, central configuration that the system administrators have setup for us.\n\nIt is critical that your ~/.bashrc sources the /etc/bashrc file. Do not remove the if-then statement that sources that file!"
  },
  {
    "objectID": "hpc/howto/bash.html#the-bash-startup-process",
    "href": "hpc/howto/bash.html#the-bash-startup-process",
    "title": "Work with Bash",
    "section": "",
    "text": "Unless you have explicitly requested to use a different shell than Bash, you will be running a Bash shell when you log onto any of Wynton HPC’s machines. You can confirm this by looking at the value of the SHELL environment variable;\n\n[alice@dev2 ~]$ echo \"$SHELL\"\n/bin/bash\nWhen a new Bash shell is launched, it will be configured according to setting in your personal ~/.bashrc startup file. On a fresh account, this file looks like:\n\n[alice@dev2 ~]$ cat /etc/skel/.bashrc\n# .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\nIn Bash, code comments start with a # symbol, which means that anything following and include this symbol that is on the same line is completely ignored. If we look at the above code, we see that almost all lines are comments. The only thing that is not a comment is the following if-then statement:\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\nThis Bash statement (i) checks if the file /etc/bashrc exists, and (ii) if it does, then that file is also “sourced” by the . /etc/bashrc line. The period in front is not a mistake; sometimes you see a period sometimes you see source, which works the same.\nIf we would look at the /etc/bashrc file, we would find a lot of things but lets not go into the details. The only thing we need to know is that . /etc/bashrc will bring in all of the essential, central configuration that the system administrators have setup for us.\n\nIt is critical that your ~/.bashrc sources the /etc/bashrc file. Do not remove the if-then statement that sources that file!"
  },
  {
    "objectID": "hpc/howto/jupyter.html",
    "href": "hpc/howto/jupyter.html",
    "title": "Jupyter Notebook",
    "section": "",
    "text": "Reason for not using Conda: For Conda environments installed in a home directory, both the Python interpreter and all of it’s modules reside on the shared parallel file system. The operations to look up the needed files are metadata heavy and can strain the parallel file system, resulting in a slower startup performance.\n\nAlthough you can install Jupyter via Conda, we highly recommend to install using regular Python techniques. The following is known to work:\n[alice@dev2 ~]$ python3 -m pip install --user notebook\n\n\n\nJupyter Notebook should only be run on the Wynton HPC development nodes. However, you cannot connect from outside Wynton HPC directly to a development node, but rather either need to use SSH port forwarding to establish the connection with a local web browser, else use X2Go to redirect the desktop from the development server to your local desktop and launch a remote web browser there.\nRunning Jupyter Notebook on Wynton HPC involves three steps. On an development node,\n\nfind an available TCP port\nlaunch Jupyter Notebook on select port, and\nset up a TCP port tunnel from your local machine to the {{ site.cluster.nickname }} development node where Jupyter runs\n\n\n\nJupyter Notebook makes itself available via the web browser. In order for multiple users to run Jupyter at the same time, each Jupyter instance must be served on a unique port in [1024,65535]. This port has to be free, that is, it must not already be used by any other processes on the same machine. One way to find such a port is to simply pick a random port and hope no one else is already using the port. An alternative approach, is to use the port4me tool, which will find a free port, e.g.\n[alice@dev2 ~]$ module load CBI port4me\n[alice@dev2 ~]$ port4me --tool=jupyter\n47467\nMake a note of the port number you get - you will need it next.\nComment: The port4me tool is designed to find the same free port each time you use it. It is only when that port happens to be already occupied that you will get another port, but most of the time, you will be using the same one over time.\n\n\n\nNext, we launch Jupyter Notebook on the same development node:\n[alice@dev2]$ jupyter notebook --no-browser --port 47467\n[I 2024-03-20 14:48:45.693 ServerApp] jupyter_lsp | extension was successfully linked.\n[I 2024-03-20 14:48:45.698 ServerApp] jupyter_server_terminals | extension was successfully linked.\n[I 2024-03-20 14:48:45.703 ServerApp] jupyterlab | extension was successfully linked.\n[I 2024-03-20 14:48:45.708 ServerApp] notebook | extension was successfully linked.\n[I 2024-03-20 14:48:46.577 ServerApp] notebook_shim | extension was successfully linked.\n[I 2024-03-20 14:48:46.666 ServerApp] notebook_shim | extension was successfully loaded.\n[I 2024-03-20 14:48:46.668 ServerApp] jupyter_lsp | extension was successfully loaded.\n[I 2024-03-20 14:48:46.669 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n[I 2024-03-20 14:48:46.675 LabApp] JupyterLab extension loaded from /wynton/home/boblab/alice/.local/lib/python3.11/site-packages/jupyterlab\n[I 2024-03-20 14:48:46.675 LabApp] JupyterLab application directory is /wynton/home/boblab/alice/.local/share/jupyter/lab\n[I 2024-03-20 14:48:46.677 LabApp] Extension Manager is 'pypi'.\n[I 2024-03-20 14:48:46.707 ServerApp] jupyterlab | extension was successfully loaded.\n[I 2024-03-20 14:48:46.711 ServerApp] notebook | extension was successfully loaded.\n[I 2024-03-20 14:48:46.712 ServerApp] Serving notebooks from local directory: /wynton/home/boblab/alice\n[I 2024-03-20 14:48:46.712 ServerApp] Jupyter Server 2.13.0 is running at:\n[I 2024-03-20 14:48:46.712 ServerApp] http://localhost:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\n[I 2024-03-20 14:48:46.712 ServerApp]     http://127.0.0.1:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\n[I 2024-03-20 14:48:46.712 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2024-03-20 14:48:46.725 ServerApp]\n\n    To access the server, open this file in a browser:\n        file:///wynton/home/boblab/alice/.local/share/jupyter/runtime/jpserver-2853162-open.html\n    Or copy and paste one of these URLs:\n        http://localhost:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\n        http://127.0.0.1:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\nHowever, these instructions do not work out of the box, because they are based on the assumption that you run Jupyter on your local machine. If you try to open one of these links, your browser produces a “site-not-found” error. To solve this, we need to complete the next step.\n\n\n\nAbove, Jupyter makes itself available on the local machine where it runs, which in our case is development node dev2. In order for us to access this from the web browser running on our local machine, we need to tunnel the TCP port from on your local machine to the port on the development node. This can be achieved using SSH port forward. To do this, open a terminal on your local machine, and run:\n{local}$ ssh -J alice@log2.wynton.ucsf.edu -L 47467:localhost:47467 alice@dev2\n...\n[alice@dev2 ~]$ \nImportantly, in your case, you will need to replace both instance of 47467 with the port number that you used in Step 2.\nNow your personal Jupyter Notebook instance running on dev2 is accessible directly from the web browser running on your local computer. To do this, open one of the HTTP links outputted by Jupyter in Step 2, e.g.\n\nhttp://localhost:47467/?token=57041544d4cacfdc71c2201d6bebe5b16fcec6bc8397fc98 (your port and token will be different)\n\n\n\n\n\nAn alternative method to run a Jupyter Notebook on one of the development nodes and interact with on your local computer is to use the X2Go software to connect to one of the development nodes, then launch a Jupyter Notebook and a web browser on the development node. The web browser will appear on your remote computer in the X2Go interface while the Jupyter Notebook will be running on Wynton HPC.\nSee the ‘Graphical User Interfaces (GUI)’ for how to setup and use X2Go via one of the Wynton HPC development nodes.\n\nIf you use Python via your own Conda installation, instead of the Python version provided by Wynton HPC, you can launch a Jupyter Notebook using the jupyter notebook command from the terminal connected by X2Go, which will also launch the web browser."
  },
  {
    "objectID": "hpc/howto/jupyter.html#installing-jupyter-notebook",
    "href": "hpc/howto/jupyter.html#installing-jupyter-notebook",
    "title": "Jupyter Notebook",
    "section": "",
    "text": "Reason for not using Conda: For Conda environments installed in a home directory, both the Python interpreter and all of it’s modules reside on the shared parallel file system. The operations to look up the needed files are metadata heavy and can strain the parallel file system, resulting in a slower startup performance.\n\nAlthough you can install Jupyter via Conda, we highly recommend to install using regular Python techniques. The following is known to work:\n[alice@dev2 ~]$ python3 -m pip install --user notebook"
  },
  {
    "objectID": "hpc/howto/jupyter.html#running-jupyter-notebook",
    "href": "hpc/howto/jupyter.html#running-jupyter-notebook",
    "title": "Jupyter Notebook",
    "section": "",
    "text": "Jupyter Notebook should only be run on the Wynton HPC development nodes. However, you cannot connect from outside Wynton HPC directly to a development node, but rather either need to use SSH port forwarding to establish the connection with a local web browser, else use X2Go to redirect the desktop from the development server to your local desktop and launch a remote web browser there.\nRunning Jupyter Notebook on Wynton HPC involves three steps. On an development node,\n\nfind an available TCP port\nlaunch Jupyter Notebook on select port, and\nset up a TCP port tunnel from your local machine to the {{ site.cluster.nickname }} development node where Jupyter runs\n\n\n\nJupyter Notebook makes itself available via the web browser. In order for multiple users to run Jupyter at the same time, each Jupyter instance must be served on a unique port in [1024,65535]. This port has to be free, that is, it must not already be used by any other processes on the same machine. One way to find such a port is to simply pick a random port and hope no one else is already using the port. An alternative approach, is to use the port4me tool, which will find a free port, e.g.\n[alice@dev2 ~]$ module load CBI port4me\n[alice@dev2 ~]$ port4me --tool=jupyter\n47467\nMake a note of the port number you get - you will need it next.\nComment: The port4me tool is designed to find the same free port each time you use it. It is only when that port happens to be already occupied that you will get another port, but most of the time, you will be using the same one over time.\n\n\n\nNext, we launch Jupyter Notebook on the same development node:\n[alice@dev2]$ jupyter notebook --no-browser --port 47467\n[I 2024-03-20 14:48:45.693 ServerApp] jupyter_lsp | extension was successfully linked.\n[I 2024-03-20 14:48:45.698 ServerApp] jupyter_server_terminals | extension was successfully linked.\n[I 2024-03-20 14:48:45.703 ServerApp] jupyterlab | extension was successfully linked.\n[I 2024-03-20 14:48:45.708 ServerApp] notebook | extension was successfully linked.\n[I 2024-03-20 14:48:46.577 ServerApp] notebook_shim | extension was successfully linked.\n[I 2024-03-20 14:48:46.666 ServerApp] notebook_shim | extension was successfully loaded.\n[I 2024-03-20 14:48:46.668 ServerApp] jupyter_lsp | extension was successfully loaded.\n[I 2024-03-20 14:48:46.669 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n[I 2024-03-20 14:48:46.675 LabApp] JupyterLab extension loaded from /wynton/home/boblab/alice/.local/lib/python3.11/site-packages/jupyterlab\n[I 2024-03-20 14:48:46.675 LabApp] JupyterLab application directory is /wynton/home/boblab/alice/.local/share/jupyter/lab\n[I 2024-03-20 14:48:46.677 LabApp] Extension Manager is 'pypi'.\n[I 2024-03-20 14:48:46.707 ServerApp] jupyterlab | extension was successfully loaded.\n[I 2024-03-20 14:48:46.711 ServerApp] notebook | extension was successfully loaded.\n[I 2024-03-20 14:48:46.712 ServerApp] Serving notebooks from local directory: /wynton/home/boblab/alice\n[I 2024-03-20 14:48:46.712 ServerApp] Jupyter Server 2.13.0 is running at:\n[I 2024-03-20 14:48:46.712 ServerApp] http://localhost:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\n[I 2024-03-20 14:48:46.712 ServerApp]     http://127.0.0.1:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\n[I 2024-03-20 14:48:46.712 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2024-03-20 14:48:46.725 ServerApp]\n\n    To access the server, open this file in a browser:\n        file:///wynton/home/boblab/alice/.local/share/jupyter/runtime/jpserver-2853162-open.html\n    Or copy and paste one of these URLs:\n        http://localhost:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\n        http://127.0.0.1:47467/tree?token=8e37f8d62fca6a1c9b2da429f27df5ebcec706a808c3a8f2\nHowever, these instructions do not work out of the box, because they are based on the assumption that you run Jupyter on your local machine. If you try to open one of these links, your browser produces a “site-not-found” error. To solve this, we need to complete the next step.\n\n\n\nAbove, Jupyter makes itself available on the local machine where it runs, which in our case is development node dev2. In order for us to access this from the web browser running on our local machine, we need to tunnel the TCP port from on your local machine to the port on the development node. This can be achieved using SSH port forward. To do this, open a terminal on your local machine, and run:\n{local}$ ssh -J alice@log2.wynton.ucsf.edu -L 47467:localhost:47467 alice@dev2\n...\n[alice@dev2 ~]$ \nImportantly, in your case, you will need to replace both instance of 47467 with the port number that you used in Step 2.\nNow your personal Jupyter Notebook instance running on dev2 is accessible directly from the web browser running on your local computer. To do this, open one of the HTTP links outputted by Jupyter in Step 2, e.g.\n\nhttp://localhost:47467/?token=57041544d4cacfdc71c2201d6bebe5b16fcec6bc8397fc98 (your port and token will be different)"
  },
  {
    "objectID": "hpc/howto/jupyter.html#alternative-run-jupyter-notebook-via-x2go",
    "href": "hpc/howto/jupyter.html#alternative-run-jupyter-notebook-via-x2go",
    "title": "Jupyter Notebook",
    "section": "",
    "text": "An alternative method to run a Jupyter Notebook on one of the development nodes and interact with on your local computer is to use the X2Go software to connect to one of the development nodes, then launch a Jupyter Notebook and a web browser on the development node. The web browser will appear on your remote computer in the X2Go interface while the Jupyter Notebook will be running on Wynton HPC.\nSee the ‘Graphical User Interfaces (GUI)’ for how to setup and use X2Go via one of the Wynton HPC development nodes.\n\nIf you use Python via your own Conda installation, instead of the Python version provided by Wynton HPC, you can launch a Jupyter Notebook using the jupyter notebook command from the terminal connected by X2Go, which will also launch the web browser."
  },
  {
    "objectID": "hpc/howto/conda-stage.html",
    "href": "hpc/howto/conda-stage.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Currently, the conda-stage tool has only been tested with the Bash shell, and it is unlikely it will work with other shells. Most users on Wynton use Bash, but a few have explicitly asked to use another. Type echo $SHELL if you’re not sure what shell you use."
  },
  {
    "objectID": "hpc/howto/conda-stage.html#stage-conda-environment-on-local-disk-highly-recommended",
    "href": "hpc/howto/conda-stage.html#stage-conda-environment-on-local-disk-highly-recommended",
    "title": "UCSF Wynton HPC Cluster",
    "section": "Stage Conda environment on local disk (highly recommended)",
    "text": "Stage Conda environment on local disk (highly recommended)\n\nPlease, stage your Conda environment to local disk! Your software and job scripts will run much faster, and it will significantly decrease the load on our global filesystem (BeeGFS). It is a win-win for everyone!\n\nWorking with a Conda environment that lives on local disk greatly improves the performance. This is because the local disk (/scratch) on the current machine is much faster than any network-based file system, including BeeGFS (/wynton) used on Wynton. This is particularly beneficial when running many instances of a software tool, e.g. in job scripts.\nStaging a Conda environment to local disk is straightforward using the conda-stage tool. All we have to do is configure the environment once, and from then on we can work with conda activate ... and conda deactivate as normal.\nBelow is a walk-through that illustrates the process. It assumes we have already create a Conda environment named myjupyter with some software installed.\n\nConfigure Conda environment for automatic staging (once)\nTo configure Conda environment myjupyter for automatic staging, call conda stage --auto-stage=enable myjupyter as in:\n[alice@dev2 ~]$ module load CBI miniforge3  ## or your own conda installation\n[alice@dev2 ~]$ module load CBI conda-stage\n[alice@dev2 ~]$ conda stage --auto-stage=enable myjupyter\nINFO: Configuring automatic staging and unstaging of original Conda environment  ...\nINFO: [ONCE] Packaging Conda environment, because it hasn't been done before ...\nCollecting packages...\nPacking environment at '/wynton/home/boblab/alice/.conda/envs/myjupyter' to\n'/wynton/home/boblab/alice/.conda/envs/.tmp.myjupyter.tar.gz'\n[########################################] | 100% Completed |  4min  5.6s\nINFO: Total 'conda-pack' time: 274 seconds\nINFO: Created conda-pack tarball: /wynton/home/boblab/alice/.conda/envs/myjupyter.tar.gz\n      (140099022 bytes; 2025-03-02 19:33:08.108806755 -0800)\nINFO: Enabled auto-staging\nINFO: Enabled auto-unstaging\n[alice@dev2 ~]$ \nThis configuration step is quick and needs to be done only once per environment.\nThat’s basically it! From now on, you can do what you have always done with Conda environments, as illustrated next.\n\n\nActivating and deactivating Conda environment (as usual)\nEach time you activate the environment, it is automatically staged to local disk;\n[alice@dev2 ~]$ conda activate myjupyter\nINFO: Staging current Conda environment (/wynton/home/boblab/alice/.conda/envs/myjupyter) to local disk ...\nINFO: Extracting /wynton/home/boblab/alice/.conda/envs/myjupyter.tar.gz \n      (86965746 bytes; 2022-04-15 16:53:50.000000000 -0700) to /scratch/alice/conda-stage-grWA/myjupyter\nINFO: Total extract time: 4 seconds\nINFO: Disable any /scratch/alice/conda-stage-grWA/myjupyter/etc/conda/activate.d/*.conda-stage-auto.sh scripts\nINFO: Activating staged environment\nINFO: Unpacking (relocating)\nINFO: Total 'conda-unpack' time: 0 seconds\nINFO: Making staged environment read-only (use --writable to disable)\nINFO: Activating staged Conda environment: /scratch/alice/conda-stage-grWA/myjupyter\n(/scratch/alice/conda-stage-grWA/myjupyter) [alice@dev2 ~]$ \nTo convince ourselves that, at this point, everything runs off the local disk, try this:\n(/scratch/alice/conda-stage-grWA/myjupyter) [alice@dev2 ~]$ command -v python\n/scratch/alice/conda-stage-grWA/myjupyter/bin/python\n(/scratch/alice/conda-stage-grWA/myjupyter) [alice@dev2 ~]$ command -v jupyter\n/scratch/alice/conda-stage-grWA/myjupyter/bin/jupyter\nSuccess! This means that these software tools run much faster, because they no longer rely on the much slower BeeGFS filesystem. Another advantage is that your Conda software stack adds much less load to BeeGFS, which otherwise can be quite significant when using Conda. This is a win-win for everyone. See ‘Benchmark staged Conda environment’ below for some benchmark results.\nWhen deactivated, the staged environment is automatically unstaged and all of the temporary, staged files are automatically removed. No surprises here either;\n(/scratch/alice/conda-stage-grWA/myjupyter) [alice@dev2 ~]$ conda deactivate\nINFO: Unstaging and reverting to original Conda environment  ...\nINFO: Preparing removal of staged files: /scratch/alice/conda-stage-grWA/myjupyter\nINFO: Deactivating and removing staged Conda environment: /scratch/alice/conda-stage-grWA/myjupyter\nINFO: Total unstage time: 0 seconds\n[alice@dev2 ~]$ command -v jupyter\n[alice@dev2 ~]$ command -v python\n/usr/bin/python\n\n\nUsing Conda staging in job scripts\nTo work with staged conda environments in your job scripts, make sure to first configure it to do automatic staging interactively from a development node as above. Then activate the environment as usual, e.g.\n#! /usr/bin/env bash\n#$ -S /bin/bash   # Run in bash\n#$ -cwd           # Current working directory\n#$ -j y           # Join STDERR and STDOUT\n#$ -R yes         # SGE host reservation, highly recommended\n\nmodule load CBI miniforge3\n\nconda activate myenv\ntrap 'conda deactivate' EXIT\n\n…\nIn this example, we have also added a shell “trap” that deactivates the environment when the script exits. This makes sure the staged environment is unstaged, including all of its temporary files are removed.\n\nIf you get an error on /usr/share/lmod/lmod/init/sh: line 14: 'conda-stage': not a valid identifier, make sure to declare the shell (#$ -S /bin/bash) to use in your job script.\n\n\n\nUpdate an automatically-staged Conda environment\nIf we would update or install new Conda packages to a staged environment, they will all be lost when unstaged. Because of this staged environments are by default read-only (conda-stage option --writable overrides this). Instead, for installation to be persistent, we need to install to the original Conda environment before it is staged. The easiest approach is to first disable auto-staging;\n[alice@dev2 ~]$ module load CBI conda-stage\n[alice@dev2 ~]$ conda stage --auto-stage=disable myjupyter\nINFO: Configuring automatic staging and unstaging of original Conda environment  ...\nINFO: Removed 'conda-pack' tarball /home/alice/.conda/envs/myenv.tar.gz\n      (140098670 bytes; 2025-03-02 18:09:55.975267674 -0800)\nINFO: Disabled auto-staging\nINFO: Disabled auto-unstaging\n[alice@dev2 ~]$ \nThen update it as usual as in:\n[alice@dev2 ~]$ conda enable myjupyter\n(myjupyter) [alice@dev2 ~]$ conda update --all\n…\n(myjupyter) [alice@dev2 ~]$ conda deactivate\nFinally, re-enable auto-staging as above, i.e.\n[alice@dev2 ~]$ conda stage --auto-stage=enable myjupyter\n..."
  },
  {
    "objectID": "hpc/howto/conda-stage.html#appendix",
    "href": "hpc/howto/conda-stage.html#appendix",
    "title": "UCSF Wynton HPC Cluster",
    "section": "Appendix",
    "text": "Appendix\n\nBenchmark staged Conda environment\n\nTo illustrate the benefit of staging a Conda environment to local disk, we will benchmark how long it takes for jupyter --version to complete without staging and with staging.\nWithout staging to local disk, the call takes a whopping 32 seconds to return:\n[alice@dev2 ~]$ CONDA_STAGE=false conda activate myjupyter\n(myjupyter) [alice@dev2 ~]$ command -v jupyter\n/wynton/home/boblab/alice/.conda/envs/myjupyter/bin/jupyter\n(myjupyter) [alice@dev2 ~]$ command time --portability jupyter --version &gt; /dev/null\nreal 32.06\nuser 1.42\nsys 0.76\nThis test was conducted during a time when the cluster did indeed experience heavy load on the BeeGFS file system at the time. The fact that real is much greater than user + sys suggests our process spends a lot of time just waiting. When staging to local disk, we can avoid being affected by this load. When running from the local disk, the same call takes less than a second;\n[alice@dev2 ~]$ conda activate myjupyter\n(/scratch/alice/conda-stage_wFWY/myjupyter) [alice@dev2 ~]$ command -v jupyter\n/scratch/alice/conda-stage_wFWY/myjupyter/bin/jupyter\n(/scratch/alice/conda-stage_wFWY/myjupyter) [alice@dev2 ~]$ command time --portability jupyter --version &gt; /dev/null\nreal 0.75\nuser 0.67\nsys 0.07\n\n\nProof that a staged Conda environment lives on local disk\nIf we run jupyter --version through strace to log all files accessed,\n[alice@dev2 ~]$ conda activate myjupyter\n(/scratch/alice/conda-stage_wFWY/myjupyter) [alice@dev2 ~]$ strace -e trace=stat -o jupyter.strace jupyter --version\n\nSelected Jupyter core packages...\nIPython          : 8.2.0\nipykernel        : 6.9.1\nipywidgets       : not installed\njupyter_client   : 7.1.2\njupyter_core     : 4.9.2\njupyter_server   : not installed\njupyterlab       : not installed\nnbclient         : 0.5.11\nnbconvert        : 6.4.4\nnbformat         : 5.1.3\nnotebook         : 6.4.10\nqtconsole        : not installed\ntraitlets        : 5.1.1\nand inspect the jupyter.strace log file, we find that most file-access calls go to the local disk:\n$ head -6 jupyter.strace \nstat(\"/scratch/alice/conda-stage_wFWY/myjupyter/bin/../lib/tls/x86_64\", 0x7ffc9a9ea980) = -1 ENOENT (No such file or directory)\nstat(\"/scratch/alice/conda-stage_wFWY/myjupyter/bin/../lib/tls\", 0x7ffc9a9ea980) = -1 ENOENT (No such file or directory)\nstat(\"/scratch/alice/conda-stage_wFWY/myjupyter/bin/../lib/x86_64\", 0x7ffc9a9ea980) = -1 ENOENT (No such file or directory)\nstat(\"/scratch/alice/conda-stage_wFWY/myjupyter/bin/../lib\", {st_mode=S_IFDIR|0755, st_size=8192, ...}) = 0\nstat(\"/etc/sysconfig/64bit_strstr_via_64bit_strstr_sse2_unaligned\", 0x7ffc9a9eaf10) = -1 ENOENT (No such file or directory)\nstat(\"/scratch/alice/conda-stage_wFWY/myjupyter/bin/python\", {st_mode=S_IFREG|0755, st_size=15880080, ...}) = 0\nExactly, how many of them? In this simple example where we only query the version of Jupyter Notebook and its dependencies, there are 4,027 queries to the file system;\n$ grep -c stat jupyter.strace \n4027\nOut of these, 4,021 are done toward the local disk (/scratch);\n$ grep -c 'stat(\"/scratch' jupyter.strace \n4021\nand only one toward the BeeGFS file system (/wynton):\n$ grep -v 'stat(\"/wynton' jupyter.strace \nstat(\"/wynton/home/boblab/alice/.local/lib/python3.9/site-packages\", 0x7ffc9a9ea820) = -1 ENOENT (No such file or directory)\nIn other words, by staging the Conda environment to local disk, we saved ourselves, and the system, 4,021 queries to the BeeGFS file system. And, this only for the very simple jupyter --version call."
  },
  {
    "objectID": "hpc/howto/find-files.html",
    "href": "hpc/howto/find-files.html",
    "title": "Find Files",
    "section": "",
    "text": "To find all FASTQ or SAM files under ~/data/, do\nfind `~/data/` -type f -name '*.fastq' -o -name '*.fq' -o -name '*.sam'\n\n\n\nTo find all FASTQ and SAM files larger than 50,000 KiB (~= 48.8 MiB) in your home directory (recursively), do\nfind ~ -type f -name '*.fastq' -o -name '*.fq' -o -name '*.sam' -size +50000k\nTo do the same but also list their file sizes and time stamps:\nfind ~ -type f -name '*.fastq' -o -name '*.fq' -o -name '*.sam' -size +50000k -exec ls -lh {} \\; | awk '{ print $9 \": \" $5 \" (\" $6 \" \" $7 \" \" $8 \")\" }'\n\n\n\nTo find all files under ~/transfer/ that have neither been modified nor “added” during the last 14 days, do:\nfind ~/transfer/ -type f -ctime +14\nTo remove these files interactively (rm -i), do:\nfind ~/transfer/ -type f -ctime +14 -exec rm -i {} \\;"
  },
  {
    "objectID": "hpc/howto/find-files.html#find-files-with-a-certain-filename-extension",
    "href": "hpc/howto/find-files.html#find-files-with-a-certain-filename-extension",
    "title": "Find Files",
    "section": "",
    "text": "To find all FASTQ or SAM files under ~/data/, do\nfind `~/data/` -type f -name '*.fastq' -o -name '*.fq' -o -name '*.sam'"
  },
  {
    "objectID": "hpc/howto/find-files.html#find-large-files-with-a-certain-filename-extension",
    "href": "hpc/howto/find-files.html#find-large-files-with-a-certain-filename-extension",
    "title": "Find Files",
    "section": "",
    "text": "To find all FASTQ and SAM files larger than 50,000 KiB (~= 48.8 MiB) in your home directory (recursively), do\nfind ~ -type f -name '*.fastq' -o -name '*.fq' -o -name '*.sam' -size +50000k\nTo do the same but also list their file sizes and time stamps:\nfind ~ -type f -name '*.fastq' -o -name '*.fq' -o -name '*.sam' -size +50000k -exec ls -lh {} \\; | awk '{ print $9 \": \" $5 \" (\" $6 \" \" $7 \" \" $8 \")\" }'"
  },
  {
    "objectID": "hpc/howto/find-files.html#find-files-older-than-14-days",
    "href": "hpc/howto/find-files.html#find-files-older-than-14-days",
    "title": "Find Files",
    "section": "",
    "text": "To find all files under ~/transfer/ that have neither been modified nor “added” during the last 14 days, do:\nfind ~/transfer/ -type f -ctime +14\nTo remove these files interactively (rm -i), do:\nfind ~/transfer/ -type f -ctime +14 -exec rm -i {} \\;"
  },
  {
    "objectID": "hpc/howto/change-pwd.html",
    "href": "hpc/howto/change-pwd.html",
    "title": "Wynton HPC Credentials",
    "section": "",
    "text": "Please wait 5-10 minutes before attempting to login with your new password. This is because it takes up to 10 minutes before your new password has propagated to all machines on the cluster.\n\n\nWe strongly recommend using a password vault application or website like the UCSF-provided Keeper Password Vault to generate and store your passwords.\n\n\n\nAs long as your password has not yet expired, you can change your password only via our web interface the RBVI/Wynton Password Change Website. Only passwords adhering to the Unified UCSF Enterprise Password Standard are accepted. Attempts to update to an insufficient password will produce an informative error message.\n\n\nYou can change your password before it expires using the ‘RBVI Kerberos Web Interface’ site;\n\nGo to https://www.cgl.ucsf.edu/admin/chpass.py\nTo access the page, enter your current Wynton HPC ‘Username’ (e.g. alice) and ‘Password’ (must not be expired) in the browser pop-up panel\nUpon successful login, you will reach the ‘Resource for Biocomputing, Visualization, and Informatics’ page, which allows you to set a new password\nWait 10 minutes for your new password to propagate before using it\n\n\n2023-11-13: Changing password via the command line is not supported. You must change your password using the Wynton/RBVI password change website.\n\n\n\n\n\nAfter waiting for 10 minutes for your new password to propagate, you can verify your Wynton username and password using either of the below alternatives. If neither works for you, you might have to reset your password. If so, see below for instructions.\n\n\nTo test your Wynton credentials, try to login to Wynton HPC via SSH. If you have SSH keys set up, you can force SSH to ignore those and only accept password entries by using:\n{local}$ ssh -o PreferredAuthentications=password alice@log2.wynton.ucsf.edu\nby replacing alice with your username.\n\n\n\nAlternatively, you can verify your credentials from your browser:\n\nGo to https://www.cgl.ucsf.edu/admin/kerbtest.py in your browser. A popup panel titled ‘Sign in https://www.cgl.ucsf.edu’ is opened by the browser.\nEnter your Wynton login credentials in the two fields ‘Username’ (e.g. alice) and ‘Password’ and click ‘Sign in’.\nIf you entered correct credentials, you will get to a confirmation page saying so. If you entered incorrect credentials, there will be no error message and the popup will appear again.\n\n\n\n\n\nIf your Wynton password has expired, or you forgot it, please email the dedicated password-reset team at {{ site.cluster.email_password }} to have it reset by UCSF IT support. Be aware that there are no support teams available to reset your password outside of regular business hours (Monday-Friday 08:00-17:00).\n\nAccount are personal and login credentials must not be shared with others. If detected, access to the account will be automatically disabled. It is still possible and easy for multiple users to share and collaborate on the same folders and scripts. Don’t hesitate to ask if you don’t know how to do this - we’re here to help."
  },
  {
    "objectID": "hpc/howto/change-pwd.html#change-non-expired-password",
    "href": "hpc/howto/change-pwd.html#change-non-expired-password",
    "title": "Wynton HPC Credentials",
    "section": "",
    "text": "As long as your password has not yet expired, you can change your password only via our web interface the RBVI/Wynton Password Change Website. Only passwords adhering to the Unified UCSF Enterprise Password Standard are accepted. Attempts to update to an insufficient password will produce an informative error message.\n\n\nYou can change your password before it expires using the ‘RBVI Kerberos Web Interface’ site;\n\nGo to https://www.cgl.ucsf.edu/admin/chpass.py\nTo access the page, enter your current Wynton HPC ‘Username’ (e.g. alice) and ‘Password’ (must not be expired) in the browser pop-up panel\nUpon successful login, you will reach the ‘Resource for Biocomputing, Visualization, and Informatics’ page, which allows you to set a new password\nWait 10 minutes for your new password to propagate before using it\n\n\n2023-11-13: Changing password via the command line is not supported. You must change your password using the Wynton/RBVI password change website."
  },
  {
    "objectID": "hpc/howto/change-pwd.html#verify-credentials",
    "href": "hpc/howto/change-pwd.html#verify-credentials",
    "title": "Wynton HPC Credentials",
    "section": "",
    "text": "After waiting for 10 minutes for your new password to propagate, you can verify your Wynton username and password using either of the below alternatives. If neither works for you, you might have to reset your password. If so, see below for instructions.\n\n\nTo test your Wynton credentials, try to login to Wynton HPC via SSH. If you have SSH keys set up, you can force SSH to ignore those and only accept password entries by using:\n{local}$ ssh -o PreferredAuthentications=password alice@log2.wynton.ucsf.edu\nby replacing alice with your username.\n\n\n\nAlternatively, you can verify your credentials from your browser:\n\nGo to https://www.cgl.ucsf.edu/admin/kerbtest.py in your browser. A popup panel titled ‘Sign in https://www.cgl.ucsf.edu’ is opened by the browser.\nEnter your Wynton login credentials in the two fields ‘Username’ (e.g. alice) and ‘Password’ and click ‘Sign in’.\nIf you entered correct credentials, you will get to a confirmation page saying so. If you entered incorrect credentials, there will be no error message and the popup will appear again."
  },
  {
    "objectID": "hpc/howto/change-pwd.html#reset-password",
    "href": "hpc/howto/change-pwd.html#reset-password",
    "title": "Wynton HPC Credentials",
    "section": "",
    "text": "If your Wynton password has expired, or you forgot it, please email the dedicated password-reset team at {{ site.cluster.email_password }} to have it reset by UCSF IT support. Be aware that there are no support teams available to reset your password outside of regular business hours (Monday-Friday 08:00-17:00).\n\nAccount are personal and login credentials must not be shared with others. If detected, access to the account will be automatically disabled. It is still possible and easy for multiple users to share and collaborate on the same folders and scripts. Don’t hesitate to ask if you don’t know how to do this - we’re here to help."
  },
  {
    "objectID": "hpc/howto/group-quota.html",
    "href": "hpc/howto/group-quota.html",
    "title": "Delegated Subgroup Quota Management and Delegated Group User Management",
    "section": "",
    "text": "2022-09-02: The Quota Management Tool is still being developed. If you run into any problems, please email: wynton-support@ucsf.edu.\n\nAny group with purchased storage can have a group folder in both the Wynton Regular /wynton/group/ area and the Wynton Protected /wynton/protected/group/ area, e.g. /wynton/group/boblab/ and /wynton/protected/group/boblab/. In that case, the group quota usage would include group-owned files in both areas. Of course, only members with Wynton Protected access would have access to the data under /wynton/protected/group/.\nYou can also request to create a subgroup to self-manage the quota for Protected-only data storage (e.g. boblab-phi), and it could be allocated as part of the parent quota.\nIn addition, responsible parties may request to be delegated responsibility for managing the membership of their groups.\n\n\nRequest the creation and delegation of a subgroup from wynton-support@ucsf.edu.\nOnce you are the delegated ‘owner’ of the subgroup, you will be able to allocate quota from your group’s parent quota to the subgroup:\n\nGo to CGL Unix Group Maintenance (requires login)\nGet the subgroup\nUpdate the allocatedStorage field\nSelect ‘Submit Changes’\nNotification: ‘Success! Group unix_group has been successfully updated!’\n\nThis will decrease the allocatedStorage in the parent group by the amount added to the subgroup. When the nightly script runs the quotas of both the parent group and subgroup will get updated. Every night, a script will run which reads that data and uses the allocatedStorage field to actually set the quota for the group.\n\n\n\nDelegated responsible parties are able to adjust the memberships of their delegated groups.\n\nImportant note: There are two ways a user can be a member of a Unix group. By their default Unix group and by an explicit group membership. In most cases, on Wynton, group membership is a function of a user’s default Unix group. This property can only be configured by the Wynton Sysadmins - it cannot be changed from the ‘CGL Unix Group Maintenance’ page. Usually Wynton users are only explicitly added to groups when they have secondary-group memberships in addition to their primary-group membership.\n\nRequest the delegation of group ownership from wynton-support@ucsf.edu.\nOnce you are the delegated ‘owner’ of the group, you will be able to adjust the group membership of your delegated group;\n\nGo to CGL Unix Group Maintenance (requires login)\nGet the group\nIn the ‘Members: Available’ field, select a Unix account\nSelect ‘Append’\nThe Unix Account should appear in ‘Members: Selected’\nSelect ‘Submit Changes’\nNotification: ‘Success! Group unix_group has been successfully updated!’"
  },
  {
    "objectID": "hpc/howto/group-quota.html#to-adjust-the-quota-of-a-delegated-subgroup",
    "href": "hpc/howto/group-quota.html#to-adjust-the-quota-of-a-delegated-subgroup",
    "title": "Delegated Subgroup Quota Management and Delegated Group User Management",
    "section": "",
    "text": "Request the creation and delegation of a subgroup from wynton-support@ucsf.edu.\nOnce you are the delegated ‘owner’ of the subgroup, you will be able to allocate quota from your group’s parent quota to the subgroup:\n\nGo to CGL Unix Group Maintenance (requires login)\nGet the subgroup\nUpdate the allocatedStorage field\nSelect ‘Submit Changes’\nNotification: ‘Success! Group unix_group has been successfully updated!’\n\nThis will decrease the allocatedStorage in the parent group by the amount added to the subgroup. When the nightly script runs the quotas of both the parent group and subgroup will get updated. Every night, a script will run which reads that data and uses the allocatedStorage field to actually set the quota for the group."
  },
  {
    "objectID": "hpc/howto/group-quota.html#to-adjust-the-membership-of-a-delegated-group",
    "href": "hpc/howto/group-quota.html#to-adjust-the-membership-of-a-delegated-group",
    "title": "Delegated Subgroup Quota Management and Delegated Group User Management",
    "section": "",
    "text": "Delegated responsible parties are able to adjust the memberships of their delegated groups.\n\nImportant note: There are two ways a user can be a member of a Unix group. By their default Unix group and by an explicit group membership. In most cases, on Wynton, group membership is a function of a user’s default Unix group. This property can only be configured by the Wynton Sysadmins - it cannot be changed from the ‘CGL Unix Group Maintenance’ page. Usually Wynton users are only explicitly added to groups when they have secondary-group memberships in addition to their primary-group membership.\n\nRequest the delegation of group ownership from wynton-support@ucsf.edu.\nOnce you are the delegated ‘owner’ of the group, you will be able to adjust the group membership of your delegated group;\n\nGo to CGL Unix Group Maintenance (requires login)\nGet the group\nIn the ‘Members: Available’ field, select a Unix account\nSelect ‘Append’\nThe Unix Account should appear in ‘Members: Selected’\nSelect ‘Submit Changes’\nNotification: ‘Success! Group unix_group has been successfully updated!’"
  },
  {
    "objectID": "hpc/software/apptainer.html",
    "href": "hpc/software/apptainer.html",
    "title": "Apptainer - Linux Containers",
    "section": "",
    "text": "2023-04-02: The Singularity software was migrated to Apptainer in November 2021. We will suggest using the apptainer command, but the alias singularity will continue to work for the time being."
  },
  {
    "objectID": "hpc/software/apptainer.html#instructions",
    "href": "hpc/software/apptainer.html#instructions",
    "title": "Apptainer - Linux Containers",
    "section": "Instructions",
    "text": "Instructions\nAll tasks for using Linux containers, such as downloading, building, and running containers, is done via the apptainer client and supported on Wynton HPC. The most common command calls are:\n\nUse an existing container:\n\napptainer run &lt;image&gt; - run predefined script within container\napptainer exec &lt;image&gt; - execute any command within container\napptainer shell &lt;image&gt; - run bash shell within container\n\nBuild a container:\n\napptainer build &lt;url&gt; - import an existing Apptainer or Docker container available online\napptainer build --remote &lt;image&gt; &lt;def file&gt; - build a Apptainer from local specifications in definition file\n\n\nFor full details, see apptainer --help, man apptainer, and the Apptainer website.\n\nInstall Apptainer or Docker on your own machine:\n\nCreate a Apptainer image on your own workstation and transfer the image to Wynton HPC\nCreate a Docker image on your own workstation and transfer the image to Wynton HPC"
  },
  {
    "objectID": "hpc/software/apptainer.html#example",
    "href": "hpc/software/apptainer.html#example",
    "title": "Apptainer - Linux Containers",
    "section": "Example",
    "text": "Example\n\nBuilding a Apptainer container from an existing Docker Hub image\nAs an illustration on how to use Linux containers with Apptainer, we will use the Docker container rocker/r-base available on Docker Hub. This particular container provides the latest release of the R software in an Ubuntu OS environment. Containers available from Docker Hub, Biocontainers, and elsewhere, can be downloaded and used analogously.\nTo use this rocker/r-base container, we first pull it down to a Apptainer image file ~/lxc/rocker_r-base.sif as:\n\n[alice@dev2 ~]$ mkdir lxc\nmkdir: cannot create directory ‘lxc’: File exists\n[alice@dev2 ~]$ cd lxc/\n[alice@dev2 lxc]$ apptainer build rocker_r-base.sif docker://rocker/r-base\nINFO:    Starting build...\nGetting image source signatures\nCopying blob sha256:bd556fe2886dac2a22b5e02cf43558766730e3423978522b24f2457ff2d489d0\nCopying blob sha256:4ade4cbe0c7e776cd60102751e29ec82271b08f5a086da6ccfef0574f6a040db\nCopying blob sha256:5fe7cbdf1a7c13b846f5fa6e716cf72a115b877f48d2693b9a1fbe7788d36b1d\nCopying blob sha256:ebbe46658ae1eddd748e3222cbc9dd7109f9fd7f279a4b2f9d6a32d0a58b4c16\nCopying blob sha256:4be550b6d67c5ef8b2ae801804eabe3acefc327f4abfa62a3e364f311a1e25b4\nCopying blob sha256:5790fe4db759efadbc15af34fc7b9ae6f5ebadf8ce05baec4309b73b79cec810\nCopying config sha256:6de003fbdce5b64853257f2e3d540251123c25006977d544680b6c38c21942b9\nWriting manifest to image destination\nStoring signatures\n2023/11/15 12:34:02  info unpack layer: sha256:ebbe46658ae1eddd748e3222cbc9dd7109f9fd7f279a4b2f9d6a32d0a58b4c16\n2023/11/15 12:34:04  info unpack layer: sha256:4ade4cbe0c7e776cd60102751e29ec82271b08f5a086da6ccfef0574f6a040db\n2023/11/15 12:34:04  info unpack layer: sha256:4be550b6d67c5ef8b2ae801804eabe3acefc327f4abfa62a3e364f311a1e25b4\n2023/11/15 12:34:05  info unpack layer: sha256:5fe7cbdf1a7c13b846f5fa6e716cf72a115b877f48d2693b9a1fbe7788d36b1d\n2023/11/15 12:34:05  info unpack layer: sha256:5790fe4db759efadbc15af34fc7b9ae6f5ebadf8ce05baec4309b73b79cec810\n2023/11/15 12:34:05  info unpack layer: sha256:bd556fe2886dac2a22b5e02cf43558766730e3423978522b24f2457ff2d489d0\nINFO:    Creating SIF file...\nINFO:    Build complete: rocker_r-base.sif\n[alice@dev2 lxc]$ ls -l rocker_r-base.sif\n-rwxr-xr-x. 1 alice boblab 325574656 Nov 15 12:34 rocker_r-base.sif\nThe above may take a minute or two to complete.\n\n\nRunning a container\nAfter this, we can run R within this container using:\n\n[alice@dev2 lxc]$ apptainer run rocker_r-base.sif\n\nR version 4.3.1 (2023-06-16) -- \"Beagle Scouts\"\nCopyright (C) 2023 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; sum(1:10)\n[1] 55\n&gt; q()\nSave workspace image? [y/n/c]: n\n[alice@dev2 lxc]$\nExactly what is “run” is defined by the so called %runscript of the Apptainer container, or the CMD entry if imported from a Docker container. An alternative way to launch R within this container is by explicitly executing R, e.g.\n\n[alice@dev2 lxc]$ apptainer exec rocker_r-base.sif R --quiet\n&gt; sum(1:10)\n[1] 55\n&gt; q(\"no\")\n[alice@dev2 lxc]$ \nNote that, the Apptainer image is marked as an executable, which means you can run it as any other executable, e.g.\n\n[alice@dev2 lxc]$ ./rocker_r-base.sif\n\nR version 4.3.1 (2023-06-16) -- \"Beagle Scouts\"\nCopyright (C) 2023 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; sum(1:10)\n[1] 55\n&gt; q(\"no\")\n[alice@dev2 lxc]$\nTo launch a shell within this container, and to also convince yourselves that the container runs Ubuntu (and not Rocky 8 as on the Wynton HPC host system), do:\n\n[alice@dev2 lxc]$ apptainer shell rocker_r-base.sif\nApptainer&gt; head -3 /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux bookworm/sid\"\nNAME=\"Debian GNU/Linux\"\nVERSION_CODENAME=bookworm\nApptainer&gt; Rscript --version\nRscript (R) version 4.3.1 (2023-06-16)\nApptainer&gt; exit\n\n[alice@dev2 lxc]$ head -3 /etc/os-release\nNAME=\"Rocky Linux\"\nVERSION=\"8.8 (Green Obsidian)\"\nID=\"rocky\"\n\n\nAccess other cluster folders than your home folder\nWhen running a container, only a few of the folders available “outside” are available “inside” the container. By default, you have access to the current working directory (= $PWD) and your home folder (= $HOME). In contrast, without further specifications, you will not have access to standard folders such as local /scratch and global /wynton/scratch. Similarly, lab folders such as /wynton/group/boblab are not available from inside the container.\n\n[alice@dev2 lxc]$ apptainer shell rocker_r-base.sif\nApptainer&gt; ls /scratch\nls: cannot access '/scratch': No such file or directory\nApptainer&gt; ls /wynton/scratch\nls: cannot access '/wynton/scratch': No such file or directory\nApptainer&gt; ls /wynton/group/boblab\nls: cannot access '/wynton/group/boblab': No such file or directory\nApptainer&gt; echo \"$TMPDIR\"\n/scratch/alice\nApptainer&gt; ls -dF \"$TMPDIR\"\nls: cannot access '/scratch/alice': No such file or directory\nApptainer&gt; mktemp\nmktemp: failed to create file via template ‘/scratch/alice/tmp.XXXXXXXXXX’: No such file or directory\nTo make also these folders available within the container, we can use apptainer option --bind. In its simplest form, we can just list the folders we want to make available, e.g.\n\n[alice@dev2 lxc]$ apptainer shell --bind /scratch,/wynton/scratch,/wynton/group/boblab rocker_r-base.sif\nApptainer&gt; echo \"$TMPDIR\"\n/scratch/alice\nApptainer&gt; ls -dF \"$TMPDIR\"\n/scratch/alice/\nApptainer&gt; mktemp\n/scratch/alice/tmp.UfD7e9LlxV\nApptainer&gt; ls -dF /wynton/scratch/alice\n/wynton/scratch/alice/\nApptainer&gt; ls /wynton/group/boblab\ndata1  data2\nOne can also use --bind to bind a folder inside the container to a folder at another location with a different name outside the container. For example, if a tool writes to /var/log, you can access those log files outside of the container afterward by using:\n[alice@dev2 lxc]$ mkdir -p extra/logs\n[alice@dev2 lxc]$ echo \"Hello world\" &gt; extra/logs/hello.txt\n[alice@dev2 lxc]$ apptainer shell --bind extra/logs:/var/log rocker_r-base.sif\nApptainer&gt; date &gt; /var/log/timestamp\nApptainer&gt; ls /var/log\nhello.txt  timestamp\nApptainer&gt; exit\nexit\n[alice@dev2 lxc]$ ls extra/logs\nhello.txt  timestamp\nSee apptainer help instance start for more details and other ways to mount and rename folders within the container.\n\n\nRunning a container as a job\nWhen it comes to the scheduler, there is nothing special about Apptainer per se - the Apptainer software can be used as any other software on the cluster. As a proof of concept, here is how to calculate the sum of one to ten using R within the above Linux container at the command line:\n\n[alice@dev2 lxc]$ apptainer exec \"rocker_r-base.sif\" Rscript -e \"sum(1:10)\"\n[1] 55\n[alice@dev2 lxc]$ \nTo run this as a batch job, we need to create a job script.\n[alice@dev2 lxc]$ cat demo-apptainer.sh\n#!/usr/bin/bash\n#$ -S /bin/bash\n#$ -cwd\n#$ -j y\n#$ -N demo-apptainer\n#$ -l mem_free=100M\n#$ -l h_rt=00:05:00\n\n## Remember to bind TMPDIR\napptainer exec --bind \"$TMPDIR\" rocker_r-base.sif Rscript -e \"sum(1:10)\"\nAnd now submit with qsub:\n[alice@dev2 lxc]$ qsub demo-apptainer.sh\nYour job 1657 (\"hello_world\") has been submitted\nCheck results:\n[alice@dev2 lxc]$ cat demo-apptainer.o5987\n[1] 55\n\n\nBuilding a container from scratch\nSometimes you need to build custom Linux container from a *.def definition file. In the past, when Singularity was used, this required administrative (“sudo”) privileges. However, with the introduction of Apptainer, any user can now build container images also from scratch from one of the Wynton HPC development nodes.\nFor example, consider the following isoseq3.def file, which builds upon a Docker Miniconda3 image (available at https://hub.docker.com/r/continuumio/miniconda3) and extends it by installing isoseq3 from the Bioconda channel (available at https://anaconda.org/bioconda/isoseq3):\n\nBootstrap: docker\nFrom: continuumio/miniconda3\n\n%post\n  /opt/conda/bin/conda config --set notify_outdated_conda false\n  /opt/conda/bin/conda config --add channels bioconda\n  /opt/conda/bin/conda install isoseq3\n\n%runscript\n  isoseq3 \"$@\"\nTo build a container image from this definition file, use apptainer build as in:\n\n[alice@dev2 ~]$ mkdir lxc\nmkdir: cannot create directory ‘lxc’: File exists\n[alice@dev2 ~]$ cd lxc/\n[alice@dev2 lxc]$ apptainer build isoseq3.sif isoseq3.def\nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    The %post section will be run under fakeroot\nINFO:    Starting build...\nGetting image source signatures\nCopying blob sha256:129bc9a4304fe3a6ef0435e6698ab6bc2728b6f92078718fb28cb4b54ac59e96\nCopying blob sha256:e67fdae3559346105027c63e7fb032bba57e62b1fe9f2da23e6fdfb56384e00b\nCopying blob sha256:62aa66a9c405da603a06d242539b8f0dd178ae4179bf52584bbcce7a0471795f\nCopying config sha256:6fbaadd54391b461351b02c0ddaf2bf284a2dcc9817f5685e07b2602e30f2b5c\nWriting manifest to image destination\nStoring signatures\n2023/11/15 13:18:59  info unpack layer: sha256:e67fdae3559346105027c63e7fb032bba57e62b1fe9f2da23e6fdfb56384e00b\n2023/11/15 13:19:01  info unpack layer: sha256:62aa66a9c405da603a06d242539b8f0dd178ae4179bf52584bbcce7a0471795f\n2023/11/15 13:19:03  info unpack layer: sha256:129bc9a4304fe3a6ef0435e6698ab6bc2728b6f92078718fb28cb4b54ac59e96\nINFO:    Running post scriptlet\n+ /opt/conda/bin/conda config --set notify_outdated_conda false\n+ /opt/conda/bin/conda config --add channels bioconda\n+ /opt/conda/bin/conda install isoseq3\nCollecting package metadata (current_repodata.json): ...working... done\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - isoseq3\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    isoseq-4.0.0               |       h9ee0642_0         2.9 MB  bioconda\n    isoseq3-4.0.0              |       h9ee0642_0           7 KB  bioconda\n    openssl-3.0.12             |       h7f8727e_0         5.2 MB\n    ------------------------------------------------------------\n                                           Total:         8.1 MB\n\nThe following NEW packages will be INSTALLED:\n\n  isoseq             bioconda/linux-64::isoseq-4.0.0-h9ee0642_0 \n  isoseq3            bioconda/linux-64::isoseq3-4.0.0-h9ee0642_0 \n\nThe following packages will be UPDATED:\n\n  openssl                                 3.0.11-h7f8727e_2 --&gt; 3.0.12-h7f8727e_0 \n\n\nProceed ([y]/n)? \n\nDownloading and Extracting Packages: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: isoseq3.sif\nThe results is a container image file named isoseq3.sif:\n\n[alice@dev2 lxc]$ ls -l isoseq3.sif\n-rwxr-xr-x. 1 alice boblab 214814720 Nov 15  2023 isoseq3.sif\n[alice@dev2 lxc]$ \nBecause the definition file has a %runscript entry, we can call this image directly as-is, e.g.\n\n[alice@dev2 lxc]$ ./isoseq3.sif --version\nisoseq 4.0.0 (commit v4.0.0)\n\nUsing:\n  pbbam     : 2.4.99 (commit v2.4.0-16-g5cc6e4b)\n  pbcopper  : 2.3.99 (commit v2.3.0-14-g5ac5693)\n  pbmm2     : 1.11.99 (commit v1.11.0-1-g1b5a417)\n  minimap2  : 2.15\n  parasail  : 2.1.3\n  boost     : 1.77\n  htslib    : 1.17\n  zlib      : 1.2.13\n[alice@dev2 lxc]$"
  },
  {
    "objectID": "hpc/software/apptainer.html#frequently-asked-questions-faq",
    "href": "hpc/software/apptainer.html#frequently-asked-questions-faq",
    "title": "Apptainer - Linux Containers",
    "section": "Frequently asked questions (FAQ)",
    "text": "Frequently asked questions (FAQ)\nQ. Why not Docker?\nA. Docker is one of the most popular and well-known software solutions for using Linux Containers. However, contrary to Apptainer, it turns out that it is hard to get Docker to play well with multi-tenant HPC environments.\nQ. What happened to Singularity?\nA. The Apptainer software is a fork of the Singularity software from 2021. For backward compatibility, the singularity command is an alias to the apptainer command. We suggest that you update your script to use apptainer.\nQ. What’s the filename extension *.sif?\nA. First of all, the filename extension is optional, and some prefer to drop them, e.g. rocker_r-base instead of rocker_r-base.sif. SIF, which is short for the Singularity Container Image Format, is a file format that can hold a Linux container environments in a single file. You might also run into old Singularity images named *.img and *.simg, which are legacy file formats that Singularity used in the past, where *.img indicates a writable (ext3) images whereas *.simg indicates a read-only (squashfs) image."
  },
  {
    "objectID": "hpc/software/software-modules.html",
    "href": "hpc/software/software-modules.html",
    "title": "Software Modules",
    "section": "",
    "text": "In addition to the core software tools that are available by default, additional software is available via different Software Repositories. Note that some of these software tools are installed and maintained by other users or research groups of the cluster who have kindly agreed on sharing their efforts with other cluster users. Currently known and publicly shared repositories are:\n\n\n\nRepository\n\n\nDescription\n\n\n\n\nCBI\n\n\nThe Computational Biology and Informatics (CBI) Software Repository Repository of software shared by the Computational Biology and Informatics (https://cbi.ucsf.edu) at the UCSF Helen Diller Family Comprehensive Cancer Center. Usage: module load CBI\n\n\n\n\nSali\n\n\nSali Lab Software Repository Repository of software shared by the UCSF Sali Lab (https://salilab.org/). Usage: module load Sali\n\n\n\n\nTo get access to the software available in one or more of these repositories, load the repository using module load &lt;repos&gt; (on command line and in script). After loading a software repository, all of its software tools are available as environment modules, that is, they can in turn be loaded using module load &lt;software&gt;.\n\nThe names of software repositories are always capitalized (e.g. CBI and Sali) whereas the names of the software themselves are typically in all lower case (e.g. r and bwa). This makes it easier to distinguish between repositories and software.\n\n\n\nThe R software is available in software repository CBI. To use that R installation, first make sure to enable (“load”) the repository and then the software as in:\nmodule load CBI   ## Enables the software repository\nmodule load r     ## Enables R\nor, shorter (order is important):\nmodule load CBI r\nAfter this, the R and the Rscript commands are available on the search path (PATH), e.g.\n$ Rscript --version\nR scripting front-end version 4.0.3 (2020-10-10)\n\nFor jobs, software modules need to be loaded in the submitted job script - it is not enough to load them in the terminal before submitting the job to the scheduler.\n\nTo disable (“unload”) R, that is, remove it from the search path, do:\nmodule unload r\n\n\nIf you unload a repository, any of its software modules that are loaded will be deactivated - they remain listed but act as they never have been loaded. If you then reload the repository, such software modules will be activated again.\n\nTo see what software modules you have currently loaded, use:\nmodule list\nTo disable all loaded software modules and repositories, use:\nmodule purge\nTo see what software modules are currently available (in the software repositories you have loaded), use:\nmodule avail\nor alternative,\nmodule spider\nIf the software repository provides more than one version of each software, specific versions can be loaded using the module load &lt;software&gt;/&lt;version&gt; format. For instance, the CBI repository provides a large number of historical R versions. To load R 3.2.0, use:\nmodule load r/3.2.0\nComment: If another version of R is already loaded, that will automatically be unloaded before loading the new version.\n\n\n\nSince module is only available on the development and compute nodes, its use in a login script (.profile, .bash_profile, .bashrc), we can only call it where its supported. It is supported when environment variable MODULEPATH is set. The below ~/.bashrc code snippet shows how to check for this. Moreover, many of the modules provide software tools that are only useful in interactive mode on a development node. The below example shows how to load modules only when running in interactive mode.\n## Are environment modules supported?\nif [[ -n \"$MODULEPATH\" ]]; then\n    ## Load modules for interactive use\n    ## (these will not be loaded when running jobs)\n    if [[ $- == *i* ]]; then\n        module load CBI htop mc \n    fi\nfi\n\n\n\nFor more information on how to use modules and the module command, see module --help, man module, and the official Lmod documentation.\n\n\n\nInstead of the classical Tcl-based environment module system commonly available on Linux, the cluster uses a Lua-based environment module system called Lmod. Lmod has a several advantages over the Tcl-based module system while being backward compatible, i.e. users of Tcl modules can still use them with Lmod. There are a few rare corner cases where a Tcl module might fail and the module has to be rewritten as a Lua-based module.\nWhen loading a software repository (module load &lt;repos&gt;), it will append its module folder to the $MODULEPATH. Unloading it (module unload &lt;repos&gt;) will undo any changes. For instance, module load &lt;repos&gt; appends $MODULEPATH_ROOT/&lt;repos&gt; to your $MODULEPATH. Multiple software repositories can be loaded in one call, e.g. module load &lt;repos1&gt; &lt;repos2&gt;. It is also possible to load a software repository and some of its software tools in one call, e.g. module load &lt;repos&gt; &lt;software1&gt; &lt;software2&gt;."
  },
  {
    "objectID": "hpc/software/software-modules.html#example",
    "href": "hpc/software/software-modules.html#example",
    "title": "Software Modules",
    "section": "",
    "text": "The R software is available in software repository CBI. To use that R installation, first make sure to enable (“load”) the repository and then the software as in:\nmodule load CBI   ## Enables the software repository\nmodule load r     ## Enables R\nor, shorter (order is important):\nmodule load CBI r\nAfter this, the R and the Rscript commands are available on the search path (PATH), e.g.\n$ Rscript --version\nR scripting front-end version 4.0.3 (2020-10-10)\n\nFor jobs, software modules need to be loaded in the submitted job script - it is not enough to load them in the terminal before submitting the job to the scheduler.\n\nTo disable (“unload”) R, that is, remove it from the search path, do:\nmodule unload r\n\n\nIf you unload a repository, any of its software modules that are loaded will be deactivated - they remain listed but act as they never have been loaded. If you then reload the repository, such software modules will be activated again.\n\nTo see what software modules you have currently loaded, use:\nmodule list\nTo disable all loaded software modules and repositories, use:\nmodule purge\nTo see what software modules are currently available (in the software repositories you have loaded), use:\nmodule avail\nor alternative,\nmodule spider\nIf the software repository provides more than one version of each software, specific versions can be loaded using the module load &lt;software&gt;/&lt;version&gt; format. For instance, the CBI repository provides a large number of historical R versions. To load R 3.2.0, use:\nmodule load r/3.2.0\nComment: If another version of R is already loaded, that will automatically be unloaded before loading the new version."
  },
  {
    "objectID": "hpc/software/software-modules.html#using-within-a-login-shell",
    "href": "hpc/software/software-modules.html#using-within-a-login-shell",
    "title": "Software Modules",
    "section": "",
    "text": "Since module is only available on the development and compute nodes, its use in a login script (.profile, .bash_profile, .bashrc), we can only call it where its supported. It is supported when environment variable MODULEPATH is set. The below ~/.bashrc code snippet shows how to check for this. Moreover, many of the modules provide software tools that are only useful in interactive mode on a development node. The below example shows how to load modules only when running in interactive mode.\n## Are environment modules supported?\nif [[ -n \"$MODULEPATH\" ]]; then\n    ## Load modules for interactive use\n    ## (these will not be loaded when running jobs)\n    if [[ $- == *i* ]]; then\n        module load CBI htop mc \n    fi\nfi"
  },
  {
    "objectID": "hpc/software/software-modules.html#see-also",
    "href": "hpc/software/software-modules.html#see-also",
    "title": "Software Modules",
    "section": "",
    "text": "For more information on how to use modules and the module command, see module --help, man module, and the official Lmod documentation."
  },
  {
    "objectID": "hpc/software/software-modules.html#technical-details",
    "href": "hpc/software/software-modules.html#technical-details",
    "title": "Software Modules",
    "section": "",
    "text": "Instead of the classical Tcl-based environment module system commonly available on Linux, the cluster uses a Lua-based environment module system called Lmod. Lmod has a several advantages over the Tcl-based module system while being backward compatible, i.e. users of Tcl modules can still use them with Lmod. There are a few rare corner cases where a Tcl module might fail and the module has to be rewritten as a Lua-based module.\nWhen loading a software repository (module load &lt;repos&gt;), it will append its module folder to the $MODULEPATH. Unloading it (module unload &lt;repos&gt;) will undo any changes. For instance, module load &lt;repos&gt; appends $MODULEPATH_ROOT/&lt;repos&gt; to your $MODULEPATH. Multiple software repositories can be loaded in one call, e.g. module load &lt;repos1&gt; &lt;repos2&gt;. It is also possible to load a software repository and some of its software tools in one call, e.g. module load &lt;repos&gt; &lt;software1&gt; &lt;software2&gt;."
  },
  {
    "objectID": "hpc/software/rocky-8-linux.html",
    "href": "hpc/software/rocky-8-linux.html",
    "title": "Wynton runs Rocky 8 Linux as of November 2023",
    "section": "",
    "text": "Last updated: 2023-12-22\nWynton now runs Rocky 8 Linux. Previously, we were running CentOS 7 Linux, which will reach the end of life (EOL) on June 30, 2024.\n\n\nRocky 8 is a modern Linux distribution that is similar and supersedes CentOS 7. It comes with newer versions of software tools and system libraries, which means that more software will install out of the box without having to go an extra mile to get them installed. We expect a lot of things to keep working as before, but some software tools may require you to tweak your scripts and to re-install for instance R and Python packages.\nHere are some of the hurdles you may run into:\n\nSoftware Repositories : Not all, but the majority of the existing built-in, CBI, and Sali environment modules work also on Rocky 8. A small number of modules specific to CentOS 7 have been removed from Rocky 8. Many of the modules are no longer necessary to use on Rocky 8, because they provided software and libraries that were otherwise outdated on CentOS 7. In contrast, Rocky 8 comes with newer versions making the need for loading newer versions via modules less necessary - when in doubt, try first without loading the module.\nSoftware Collections (SCL) : Compared to CentOS 7, Rocky 8 provides only SCLs for using modern GCC development tools, e.g. C and C++ compilers. These SCLs, called gcc-toolset, corresponds somewhat to the devtoolset SCLs that are available on CentOS 7. The built-in GCC version is now 8.5.0 (2021-05-14), whereas on CentOS 7 it was 4.8.5 (2015-06-23). Python and Ruby SCLs are no longer available.\nPython : There is no longer a python command; Python 3 has to be called as python3 and legacy Python 2 is available as python2. On CentOS 7, python corresponded to python2. Python 3.6 is available via python3.6, Python 3.8 via python3.8, and Python 3.11 via python3.11.\n\nIf you get errors like /opt/rh/rh-python38/root/usr/bin/python3:   bad interpreter: No such file or directory when you run a Python based software, then you need to reinstall that software tool.\n\nR : R via the CBI software stack works as previously. All versions have been been re-installed for Rocky 8. Previously installed R packages are no longer available, and have to re-installed.\nMPI : OpenMPI is available via the built-in module mpi. On Rocky 8, the default, and only available version is OpenMPI 4.1, which you get via module mpi/openmpi-x86_64 (sic!). Note that, on CentOS 7, mpi/openmpi-x86_64 provided legacy OpenMPI 1.10. On CentOS 7, there was also mpi/openmpi3-x86_64 for OpenMPI 3.1. Because Rocky 8 and CentOS 7 have no versions of OpenMPI in common, it is likely that software tools that were compiled with OpenMPI as a dependency have to be rebuilt for Rocky 8.\nSelf-compiled software: If you have compiled software to run from your home directory, depending on how the libraries are linked, it may need to be recompiled to function in Rocky 8."
  },
  {
    "objectID": "hpc/software/rocky-8-linux.html#what-to-expect",
    "href": "hpc/software/rocky-8-linux.html#what-to-expect",
    "title": "Wynton runs Rocky 8 Linux as of November 2023",
    "section": "",
    "text": "Rocky 8 is a modern Linux distribution that is similar and supersedes CentOS 7. It comes with newer versions of software tools and system libraries, which means that more software will install out of the box without having to go an extra mile to get them installed. We expect a lot of things to keep working as before, but some software tools may require you to tweak your scripts and to re-install for instance R and Python packages.\nHere are some of the hurdles you may run into:\n\nSoftware Repositories : Not all, but the majority of the existing built-in, CBI, and Sali environment modules work also on Rocky 8. A small number of modules specific to CentOS 7 have been removed from Rocky 8. Many of the modules are no longer necessary to use on Rocky 8, because they provided software and libraries that were otherwise outdated on CentOS 7. In contrast, Rocky 8 comes with newer versions making the need for loading newer versions via modules less necessary - when in doubt, try first without loading the module.\nSoftware Collections (SCL) : Compared to CentOS 7, Rocky 8 provides only SCLs for using modern GCC development tools, e.g. C and C++ compilers. These SCLs, called gcc-toolset, corresponds somewhat to the devtoolset SCLs that are available on CentOS 7. The built-in GCC version is now 8.5.0 (2021-05-14), whereas on CentOS 7 it was 4.8.5 (2015-06-23). Python and Ruby SCLs are no longer available.\nPython : There is no longer a python command; Python 3 has to be called as python3 and legacy Python 2 is available as python2. On CentOS 7, python corresponded to python2. Python 3.6 is available via python3.6, Python 3.8 via python3.8, and Python 3.11 via python3.11.\n\nIf you get errors like /opt/rh/rh-python38/root/usr/bin/python3:   bad interpreter: No such file or directory when you run a Python based software, then you need to reinstall that software tool.\n\nR : R via the CBI software stack works as previously. All versions have been been re-installed for Rocky 8. Previously installed R packages are no longer available, and have to re-installed.\nMPI : OpenMPI is available via the built-in module mpi. On Rocky 8, the default, and only available version is OpenMPI 4.1, which you get via module mpi/openmpi-x86_64 (sic!). Note that, on CentOS 7, mpi/openmpi-x86_64 provided legacy OpenMPI 1.10. On CentOS 7, there was also mpi/openmpi3-x86_64 for OpenMPI 3.1. Because Rocky 8 and CentOS 7 have no versions of OpenMPI in common, it is likely that software tools that were compiled with OpenMPI as a dependency have to be rebuilt for Rocky 8.\nSelf-compiled software: If you have compiled software to run from your home directory, depending on how the libraries are linked, it may need to be recompiled to function in Rocky 8."
  },
  {
    "objectID": "hpc/software/missing-software.html",
    "href": "hpc/software/missing-software.html",
    "title": "Missing Software?",
    "section": "",
    "text": "Missing Software?\nDon’t find the software you need for your analysis?  If so, you have a few options:\n\nInstall it yourself to your own account\n\nDownload a pre-built binary\nBuild software from source (traditional, e.g. ./configure --prefix ~/software, make, and make install)  \n\nUse Apptainer to run it via a Linux container\n\nPull down an existing image from public repositories such as Docker Hub and Biocontainers (e.g. apptainer build blast.sif docker://biocontainers/blast:2.2.31 and apptainer exec blast.sif blastp -help)\nCreate your own Linux containers \n\nReach out to the Wynton HPC user community\n\nIt could be that someone else has already installed it, is interested in also installing it, or is willing to help \n\nMissing core software or missing SCL?\n\nIf a core software or a Software Collection (SCL) is missing, it is likely that the the system administrators are willing to add it - please let us know"
  },
  {
    "objectID": "hpc/about/user-agreement.html",
    "href": "hpc/about/user-agreement.html",
    "title": "User Agreement and Disclaimer",
    "section": "",
    "text": "2025-07-08: Termination of Wynton Support for P4/PHI data\n\nStarting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton.\n\n\n\n\nUser Agreement and Disclaimer\nLast Updated October 30, 2025\nWynton user responsibilities and requirements in support of this Agreement include:\n\nBy using the Wynton HPC environment, you agree to abide by the statement of its Purpose, Principles and Governance as well as the UCSF Minimum Security Standards.\nP4 data, which includes PHI, is prohibited.\nP3 data may only be used in “Wynton Protected” via a “protected” account. Refer to UCSF Policy 650-16 Addendum F, UCSF Data Classification Standard. \nUsers who wish to access P3 protected data are required to log into node(s) that implement enhanced security, such as additional access restrictions.\nThe PI is responsible for ensuring that users are listed on the Institutional Review Board (IRB) approval letter before they approve those users’ Wynton access to data that require IRB approval. The PI is also responsible for ensuring they are in compliance with their grant requirements (e.g. platform security, data retention policies).\nIt is the responsibility of the PIs to ensure the integrity of their data. The Wynton filesystem is not the primary source of data (not a system of record). Once the PI has write access to the data, they are entitled to manipulate the data as they see fit.\nAll Wynton users are responsible for ensuring they have their own back-ups so they are able to revert to an earlier version if a mistake is made or they want access to their data during an outage. Wynton does not offer a back-up service, other than disaster recovery back-ups of Home directories that could be restored only in the event of a major disaster, such as data center loss or ransomware. Very large Home directories do not have disaster recovery back-ups.\nUpon separation from UCSF, a Wynton user’s home directory and its contents may be transferred to the user’s PI in accordance with the Authorized and Acceptable Use of Electronic Information Resources policy (650-18) “i. An account that is not deleted upon loss of affiliation shall be transferred to another UCSF faculty or staff person designated as being responsible for the account.” Accounts and data that are not claimed after 90 days may be permanently deleted.\nBecause IT security requirements, operating systems, and hardware are continually evolving, difficult situations may arise in the future. For example, it is possible that future requirements imposed by the University of California Office of the President (UCOP) or UCSF concerning protected data would in turn negatively impact the ability of the cluster to serve its founding purpose, as defined in the Governance document. In such a case, the Steering Committee may roll back configurational changes or otherwise re-configure the cluster, so that the founding needs are still satisfied, even if that occurs at the expense of some applications. Therefore, while every effort will continue to be made to maximize the number of applications, it is impossible to guarantee that once supported applications will always remain supported. If a rollback is required the Wynton OPS team and Shared Compute governance will make it a priority to stand up a separate cluster supporting the required functionality.\nUse of the Wynton HPC cluster is subject to UCSF’s policy on Authorized and Acceptable Use of Electronic Information Resources.\nThe Wynton Operations team is tasked with implementing the policies established by the Faculty Steering Committee and with ensuring that the cluster runs effectively and efficiently. Users agree not to attempt to circumvent policies nor avoid restrictions imposed by the Wynton Operations team.\nAccounts are personal, and login credentials must not be shared.\nUsers are prohibited from changing system-wide configurations on Wynton nodes.\nWynton operations installs and maintains packages provided by the OS vendor and select system-wide application software.\nSoftware not managed by Wynton admins is the user’s responsibility. This includes but is not limited to: software installed in Wynton user and group directories, software in containers, and software installed using the Conda package manager. Additionally, Lmod modules that are not maintained by the Wynton admins themselves are the responsibility of the individual labs and users that maintain the module repositories (e.g. the CBI and Sali repositories)."
  },
  {
    "objectID": "hpc/about/code-of-conduct.html",
    "href": "hpc/about/code-of-conduct.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "This is a mockup page!"
  },
  {
    "objectID": "hpc/about/code-of-conduct.html#our-pledge",
    "href": "hpc/about/code-of-conduct.html#our-pledge",
    "title": "Code of Conduct",
    "section": "Our Pledge",
    "text": "Our Pledge\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation."
  },
  {
    "objectID": "hpc/about/code-of-conduct.html#our-standards",
    "href": "hpc/about/code-of-conduct.html#our-standards",
    "title": "Code of Conduct",
    "section": "Our Standards",
    "text": "Our Standards\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "hpc/about/code-of-conduct.html#our-responsibilities",
    "href": "hpc/about/code-of-conduct.html#our-responsibilities",
    "title": "Code of Conduct",
    "section": "Our Responsibilities",
    "text": "Our Responsibilities\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "hpc/about/code-of-conduct.html#scope",
    "href": "hpc/about/code-of-conduct.html#scope",
    "title": "Code of Conduct",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "hpc/about/code-of-conduct.html#enforcement",
    "href": "hpc/about/code-of-conduct.html#enforcement",
    "title": "Code of Conduct",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "hpc/about/code-of-conduct.html#attribution",
    "href": "hpc/about/code-of-conduct.html#attribution",
    "title": "Code of Conduct",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4"
  },
  {
    "objectID": "hpc/about/shares.html",
    "href": "hpc/about/shares.html",
    "title": "Contributing Member Shares",
    "section": "",
    "text": "2025-01-15: Temporary Hold on Storage and Compute Requests\nAs Wynton continues to expand, we are approaching the limits of space, cooling, and power capacity in the Byers Hall server room. In addition, as we work to move the administration of the cluster to the Academic Research Systems (ARS) Team, we are currently reprioritizing our workload with the support of Wynton faculty leadership to ensure that we continue to meet our most critical objectives. We understand that this may cause some delays and appreciate your patience and understanding during this period.\nSee also the Wynton-announcement email titled ‘Important Updates on Wynton Storage and Compute Requests’ sent to all users on 2024-11-25."
  },
  {
    "objectID": "hpc/about/shares.html#compute-shares",
    "href": "hpc/about/shares.html#compute-shares",
    "title": "Contributing Member Shares",
    "section": "Compute Shares",
    "text": "Compute Shares\nCurrently, the Wynton HPC cluster has in total member.qtotal = 7023 slots available on the member.q queue. Jobs on the member.q queue will launch and finish sooner than jobs on the communal, lower-priority long.q queue. A member.q job will have higher-priority on the CPU than a long.q job in case they run on the same compute node. It is only contributing members who have access to the member.q queue - non-contributing members will only have access to queues such as the long.q queue. Contributors get non-expiring, lifetime access to a number of these member.q slots in proportion to their hardware contribution to the cluster. The number of member.q slots a particular hardware contribution, which can be monetary(*) or physical(*), adds, is based on how much compute power the contribution adds to the cluster. The amount of compute power that contributed hardware adds is based on benchmarking(*), which result in a processing-unit score (PU) for the contribution. Currently, there are in total PUtotal = 20186 contributed processing units on Wynton HPC.\n\nA lab’s contributed processing units (PUlab) will never expire - it will remain the same until the lab makes additional contributions to the cluster.\n\nAs other labs contribute to the cluster, the total computer power (PUtotal) and the total number of member.q slots (member.qtotal) will increase over time. This will result in the lab’s relative compute share (PUlab / PUtotal) to decrease over time while their number of member.q slots (member.qlab) will stay approximately(**) the same.\n\nExample: Additional contribution from the Charlie Lab\nAssume that the last addition was from the Charlie Lab contributing 4 compute nodes. Each of these machines has a 12-core 2.2 GHz Opteron 6174 CPU and clocks in at 1.6 PUs based on the benchmarking, resulting in the processing power added for this lab, but also to the cluster as a whole, to be 4 * 1.6 PUs = +6.4 PUs. In addition to increasing the total amount of contributed PUs, the lab’s contribution also increased the total number of member.q slots on the cluster by 4 * 12 = +48 slots.\nIf this was Charlie Lab’s first contribution to Wynton HPC, their share on the member.q queue will be PUlab / PUtotal = 6.4 / 20186 = 0.032%. This PU share translates to member.qlab = (PUlab / PUtotal) *member.qtotal = 2 member.q slots (2.21 rounded off to the closest integer). Instead, if they already had contributed, say, in total 16.3 PUs in the past, their computational share would had become PUlab = (16.3 + 6.4) / 20186 = 0.113%, which, would corresponds to 8 member.q slots (7.85 rounded off).\n\n\n\nCurrent Compute Shares\nBelow table shows the current amount of contributions in terms of Processing Units (PU) and the corresponding number of member.q slots per contributing lab.\n\n\n\n\n\n\n\nSource: compute_shares.tsv produced on . These data were compiled from the current SGE configuration (qconf -srqs member_queue_limits and qconf -sprj &lt;project&gt;). In SGE terms, a processing unit (PU) corresponds to a functional share (“fshare”).\n (*) To be documented. (**) The reason for member.qlab not remaining exactly the same when PUlab does not change, is that the compute power per core is greater for newer hardware compared with older hardware. Because of this, a lab’s number of member.q slots is likely to, ever so slightly, decrease in the long run as the cluster keeps growing. But don’t worry, as the average compute power per member.q slot increases over time, your lab’s total compute power on the member.q queue remains constant per definition (unless your lab adds further contributions)."
  },
  {
    "objectID": "hpc/about/pricing-compute.html",
    "href": "hpc/about/pricing-compute.html",
    "title": "Pricing for Extra Compute",
    "section": "",
    "text": "2025-01-15: Temporary Hold on Storage and Compute Requests\nAs Wynton continues to expand, we are approaching the limits of space, cooling, and power capacity in the Byers Hall server room. In addition, as we work to move the administration of the cluster to the Academic Research Systems (ARS) Team, we are currently reprioritizing our workload with the support of Wynton faculty leadership to ensure that we continue to meet our most critical objectives. We understand that this may cause some delays and appreciate your patience and understanding during this period.\nSee also the Wynton-announcement email titled ‘Important Updates on Wynton Storage and Compute Requests’ sent to all users on 2024-11-25."
  },
  {
    "objectID": "hpc/about/pricing-compute.html#pricing-for-prioritized-cpu-compute",
    "href": "hpc/about/pricing-compute.html#pricing-for-prioritized-cpu-compute",
    "title": "Pricing for Extra Compute",
    "section": "Pricing for prioritized “CPU” compute",
    "text": "Pricing for prioritized “CPU” compute\nCluster compute nodes are purchased by Wynton in modules that have 4 “2U” compute nodes in a single chassis. If partial nodes are requested, contributions will be pooled with other requests towards a full system. Another option is to pay 180 USD/member.q slot.\n\nConfiguration: SMC RM224Q 2U Quad-Node Server\nThis configuration comes with four compute modules, where each node includes:\n\nCPU/node: 2 × Intel Xeon Gold 5520+ 2.2GHz 28-core processor\nRAM/node: 256GiB DDR5-4800 ECC (16 × 16 GiB)\nStorage/node: 1 × 1.92TB U.2 NVMe SSD, 1DWPD\n\nTotal price for a module with four (4) node: ~37,000 USD excluding taxes.\nAs of August 2024, lead time for compute nodes is approximate three months from order to delivery. Once the hardware is received, we will schedule time to configure hardware as time allows given priority projects. Unless you are purchasing a four-compute-node module, we will need to wait until we have enough requests to place an order for a whole unit. If we have capacity, we may be able to provision some shares in advance of the billing."
  },
  {
    "objectID": "hpc/about/pricing-compute.html#pricing-for-prioritized-graphics-processing-unit-gpu-processing",
    "href": "hpc/about/pricing-compute.html#pricing-for-prioritized-graphics-processing-unit-gpu-processing",
    "title": "Pricing for Extra Compute",
    "section": "Pricing for prioritized Graphics Processing Unit (GPU) processing",
    "text": "Pricing for prioritized Graphics Processing Unit (GPU) processing\n\nWynton HPC has 61 GPU nodes with a total of 235 GPUs available to all users. Among these, 39 GPU nodes, with a total of 147 GPUs, were contributed by different research groups. GPU jobs are limited to 2 hours in length when run on GPUs not contributed by the running user’s lab. In contrast, contributors are not limited to 2-hour GPU jobs on nodes they contributed. There is also one GPU development node that is available to all users.\n\nSingle GPU cards cannot be purchased - only a full GPU node. However, just like partial compute nodes can be purchased, GPU purchase requests can also be pooled towards a full system.\nThere are three common configurations of GPU compute nodes available, which all share the same base setup, and differs only by type of GPU.\n\nConfiguration: Mercury GPU208 2U Server\n\nCPU: 1 × AMD EPYC 7543P 2.8GHz 32-core processor\nRAM: 512GiB DDR4-3200 (8 × 64GiB)\nOperating-system storage: 1 × 512GB SATA SSD\nStorage: 1 × 960GB U.2 NVMe SSD\n\nThen choose one of:\n\nGPU: 4 × Nvidia A40 with 48GB GPU memory (~25,000 USD)\nGPU: 4 × Nvidia L40 with 48GB GPU memory (~33,000 USD)\n\nNote: A100s are no longer available\nThe prices listed are total prices in USD excluding taxes. Each price includes the base setup and the corresponding GPU configuration. Warranty is included. Taxes are calculated and added once hardware is received.\nAs of October 2023, lead time for GPUs is approximately 3 months from order to delivery. Once the hardware is received, we will schedule time to configure hardware as time allows given priority projects."
  },
  {
    "objectID": "hpc/about/pricing-compute.html#how-to-purchase",
    "href": "hpc/about/pricing-compute.html#how-to-purchase",
    "title": "Pricing for Extra Compute",
    "section": "How to purchase",
    "text": "How to purchase\nTo purchase compute nodes or GPU nodes, please contact wynton-compute@ucsf.edu. Please include what you are considering contributing and your UCSF COA SpeedType or chartstring for billing, along with any questions.\nIf you are not ready to purchase but have technical questions while you explore options, please contact wynton-support@ucsf.edu."
  },
  {
    "objectID": "hpc/about/pricing-storage.html",
    "href": "hpc/about/pricing-storage.html",
    "title": "Pricing for Extra Storage",
    "section": "",
    "text": "2025-01-15: Temporary Hold on Storage and Compute Requests\nAs Wynton continues to expand, we are approaching the limits of space, cooling, and power capacity in the Byers Hall server room. In addition, as we work to move the administration of the cluster to the Academic Research Systems (ARS) Team, we are currently reprioritizing our workload with the support of Wynton faculty leadership to ensure that we continue to meet our most critical objectives. We understand that this may cause some delays and appreciate your patience and understanding during this period.\n\nWaitlist for New Storage Brick: If you are already on the waitlist for the new storage brick, please be assured that you will be notified when this service is brought online.\nPending Requests: If you have recently submitted a request and have not yet received a response, we will evaluate whether we can accommodate some or all of your requests at this time. The Wynton Project Manager will reach out to you with more information.\nUrgent Needs: If you have an urgent need, please contact {{ site.cluster.email_support }} with the details of your request and its urgency. We will do our best to accommodate your needs.\n\nSee also the Wynton-announcement email titled ‘Important Updates on Wynton Storage and Compute Requests’ sent to all users on 2024-11-25."
  },
  {
    "objectID": "hpc/about/pricing-storage.html#summary",
    "href": "hpc/about/pricing-storage.html#summary",
    "title": "Pricing for Extra Storage",
    "section": "Summary",
    "text": "Summary\n\nAll user accounts come with a quota of non-expandable 500 GiB of storage in /wynton/home or /wynton/protected/home, which is free of charge\nGlobal /wynton/scratch and /wynton/protected/scratch (for Wynton Protected users) may be used for smaller, short-term project needs of a few TBs with the caveat that files older than two weeks are deleted automatically\nAdditional, long-term storage can be purchased in /wynton/group, /wynton/protected/group (for Wynton Protected users), or in /wynton/protected/project (for specific Wynton Protected projects) at $160/TiB (one-time fee). Both will count towards a lab’s storage quota total.\nAfter 5 years from purchase, when the warranty runs out, a small “maintenance fee” might be introduced\nTo purchase more storage, please contact wynton-storage@ucsf.edu"
  },
  {
    "objectID": "hpc/about/pricing-storage.html#lab-specific-hpc-storage-pricing-model",
    "href": "hpc/about/pricing-storage.html#lab-specific-hpc-storage-pricing-model",
    "title": "Pricing for Extra Storage",
    "section": "Lab-Specific HPC Storage Pricing Model",
    "text": "Lab-Specific HPC Storage Pricing Model\nWe are currently offering the ability for research labs to purchase additional storage at a one-time charge of $160/TiB for RAID 6 equivalent storage (or $320/TiB for mirrored RAID 6). In contrast to /wynton/home, purchased storage will be mounted on /wynton/group, /wynton/protected/group, or /wynton/protected/project and dedicated to the lab that bought it – no oversubscription will occur. The purchased storage will have similar performance to the rest of the BeeGFS infrastructure (e.g. /wynton/scratch, /wynton/protected/scratch, /wynton/home, and /wynton/protected/home).\nPlease note, storage in /wynton/group/, /wynton/protected/group, and /wynton/protected/project count towards a lab’s storage quota total.\nGiven prices of hard drives, the stated rate might seem high, but there are three mitigating factors. First, we have enabled ZFS compression, so the actual available space might be significantly more. Second, the price includes the cost of the networking, metadata servers, storage server, maintenance, and administration. Third, we have proven that the performance of our BeeGFS infrastructure is much higher than the typical NFS server (in some respects, the performance is more than an order of magnitude faster). In the future, if absolutely necessary, we may also charge a “maintenance fee” for storage after the initial 5-year hardware warranty expires, but nothing has been decided as of yet. Similarly, any future storage purchases may be priced differently than that described here, to reflect the situation present at that time.\nThere are some additional parameters:\n\nUnlike nodes, the storage cannot be extracted from the system once it’s been brought online. Once a lab or a group has “bought in”, they will not be able to retrieve their portion of storage hardware if they choose to leave Wynton (unlike compute nodes/shares).\nThe storage is not available for mounting outside of the cluster.\nThe storage is not backed up."
  },
  {
    "objectID": "hpc/about/pricing-storage.html#frequently-asked-questions",
    "href": "hpc/about/pricing-storage.html#frequently-asked-questions",
    "title": "Pricing for Extra Storage",
    "section": "Frequently Asked Questions",
    "text": "Frequently Asked Questions\n\nQ: If I already have purchased group storage, can I also use that storage quota for Wynton Protected data in /wynton/protected/group?\nA: Yes, but Wynton Protected data must not be stored in /wynton/group. We can create group folders for you under /wynton/protected for Wynton Protected data storage and collaboration. If you choose to have your current storage allocation updated to include Wynton Protected storage, you have three options:\n\nWe can create a group folder in both /wynton/group and /wynton/protected/group. Both folders will share the same quota. In this case, the same quota will be applied to two folders, so it can be tricky to track where the group’s quota usage is, Wynton Protected or Wynton Regular.\nWe can create a group folder in /wynton/group and a folder with a separate sub-quota subtracted from the group quota in /wynton/protected/group, for example, /wynton/protected/group/group-phi. This makes it easier to track separate Wynton Protected and Wynton Regular group quota usage.\nIf the group using the Wynton Protected data does not include the whole lab group membership, or includes members from outside of the group, then a folder in /wynton/protected/project can be created and the child quota subtracted from the parent group.\n\nOnly members of the lab group with Wynton Protected access will be able to access files stored under /wynton/protected.\n\n\nQ: Does purchasing additional group storage increase the quotas available to lab users in /wynton/home or /wynton/protected/home?\nA: No, the home directory quotas are managed separately from group quotas and additional purchased group space cannot be applied to home directories."
  },
  {
    "objectID": "hpc/about/pricing-storage.html#background",
    "href": "hpc/about/pricing-storage.html#background",
    "title": "Pricing for Extra Storage",
    "section": "Background",
    "text": "Background\nWe have an immediate need to provide for Wynton HPC storage expansion to meet the demands of various research groups on campus. Given the difficulty of predicting longer-term costs and issues, the current pricing is considered short-term and may change as we understand the evolving needs and operational realities of Wynton HPC. This model is based on some assumptions that are important to understand:\n\nVarious components of the shared storage environment are considered “infrastructure” and are currently funded from the ongoing support provided by the campus. These components include the networking infrastructure, and management and metadata servers that are part of the overall storage infrastructure. We don’t know with certainty that these components will continue to be funded by the campus and this introduces additional uncertainty as to the future pricing for storage, beyond the current offering described here.\nThe $160/TiB price is for non-mirrored, potentially non-geographically redundant storage. While we hope to always purchase storage servers in pairs, providing failover between servers, there is no guarantee that we will always be able to do that. If you wish to protect your data beyond the level of RAID 6 (allows for two failed disks), we suggest you consider purchasing mirrored storage which completely duplicates data using a separate set of (RAID 6) disk drives.\nAny new storage will be added to our existing BeeGFS installation and will not use separate instances of BeeGFS (which would significantly increase the potential costs, if not in hardware, certainly in terms of personnel effort)."
  },
  {
    "objectID": "hpc/about/pricing-storage.html#price-calculations",
    "href": "hpc/about/pricing-storage.html#price-calculations",
    "title": "Pricing for Extra Storage",
    "section": "Price calculations",
    "text": "Price calculations\n\nWe assume that purchases during 2019 represented a reasonable scalable unit of storage. This purchase provided 1,200 TB (raw) storage and two storage servers. At ~$112,500, this results in a cost of $93/TB (raw) or $136/TB after accounting for RAID-Z2 and BeeGFS filesystem costs. We also need to add two additional metadata servers at a cost of $20,000. Taken together, this results in the $160/TB price.\nThe current storage hardware increment is ~$160,000, which may result in some delay between the first contributions and an actual purchase, although there is already pent-up demand and hence we are trying to proceed with the purchase as quickly as possible.\nUpdate 2024-07-29: We have sold storage in units of TB = 1000^4 bytes, but allocated in units of TiB = 1024^4 bytes, which is a 10% over-allocation. That said, given that prices has been going down, we have made the decision to change the charge to $160/TiB. There is also current work in revisit the storage price, so the current price might be adjusted again.\n\nLast updated: July 29, 2024"
  },
  {
    "objectID": "hpc/about/governance.html",
    "href": "hpc/about/governance.html",
    "title": "Shared Cluster Computing at UCSF",
    "section": "",
    "text": "The Wynton high performance computer cluster at UCSF (defined here as the computer cluster named Wynton HPC as of June 2024 and the one primarily located in Byers Hall room 101, Mission Bay, UCSF) was founded by basic scientists to support their research programs. In the process, researchers benefited and continue to benefit from maximized economies of scale in building, administering, and using the cluster. The cluster provides computing capacity, network connectivity internally at UCSF and externally to research networks such as the Pacific Research Platform, data storage, job queueing with accounting, and systems administration. As a result, the cluster contributes directly to the mission of UCSF, including research, education, and outreach.\nThe cluster is built on a co-op model rather than a fee-for-service model, in contrast to, for example, most technology cores at UCSF and high-performance computing provided by Amazon Web Services. In the co-op model, every contributing research group has immediate access to the cluster computing power and storage capacity proportional to their contribution; contributions are preferably made in cash, although suitable existing hardware may also be integrated. This funding model suits best the often unpredictable timing and amount of research funds available to individual projects. In addition, our goal is to maximize the number of UCSF users of the shared cluster. Therefore, every member of the UCSF community has access to a fraction of the cluster computing power and storage capacity regardless of their contribution, reflecting the contribution by UCSF and shared instrumentation grants from NIH and NSF. Finally, any unused compute power is available to any user on a first-come, first-served basis. Every effort is made to accommodate and expand the set of applications that can be run efficiently on the cluster, as long as they don’t negatively impact our founding goals. If a contributing member leaves the consortium, they are entitled to take their contribution with them (except for communal storage).\n\n\n\nThe governance pertaining only to the Wynton cluster as defined above is described here; Other high performance computing initiatives at UCSF are outside the scope of this document. The Wynton cluster is governed by the Wynton Faculty Steering Committee (WFSC), guided by the cluster’s purpose and organizational principles outlined above. The Committee consists of major contributing basic scientists, a representative of guest users, and a campus Information Technology representative, all of whom have some expertise in cluster computing. The Committee meets as required to review overall utilization of the system, plan future expansions, discuss funding opportunities, etc. The Committee also resolves any disputes concerning use and governance of the cluster. Committee matters are addressed at meetings and by email as needed. To ensure continuity of governance, the Committee appoints new members by a majority vote. All decision rights for the Wynton Cluster rest with the Wynton Faculty Steering Committee decided by a majority vote, with the Chair breaking any ties. Decision rights include but are not limited to queuing rules, expansion of the environment, proposals for new uses of the environment, administrative aspects of the environment such as proposals for recharges, collaborations with other Faculty Subcommittees on investment requests/proposals, best practices, innovations, submissions of funding requests to outside agencies, and/or campus entities. The computer security aspects of Wynton are specified by UCSF IT. The Committee currently consists of Kliment Verba (Co-Chair), Sergio Baranzini, Tanja Kortemme, Scott Pegg, Brian Shoichet, Tom Ferrin (Co-Chair), and Mandy Terrill (Campus IT) (as of August 2024).\n\n\n\n\nThe budget request and approval process for campus funds and resources will flow through the Committee for Research Technology (CRT), with subsequent oversight by the CRT and then final approval by the Executive Vice Chancellor and Provost (EVCP). This ensures alignment with institutional strategies and financial policies while respecting the voices of all stakeholders in the prioritization and operations of the compute cluster.\nThe Wynton Faculty Steering Committee (WFSC) is tasked with formulating recommendations and articulating the needs and concerns regarding the budget for the Wynton computational environment. These recommendations are a critical component of the process as we seek approval from the Executive Vice Chancellor and Provost (EVCP), ensuring that the budgetary needs align with the operational and strategic imperatives of UCSF’s research infrastructure."
  },
  {
    "objectID": "hpc/about/governance.html#context",
    "href": "hpc/about/governance.html#context",
    "title": "Shared Cluster Computing at UCSF",
    "section": "",
    "text": "The Wynton high performance computer cluster at UCSF (defined here as the computer cluster named Wynton HPC as of June 2024 and the one primarily located in Byers Hall room 101, Mission Bay, UCSF) was founded by basic scientists to support their research programs. In the process, researchers benefited and continue to benefit from maximized economies of scale in building, administering, and using the cluster. The cluster provides computing capacity, network connectivity internally at UCSF and externally to research networks such as the Pacific Research Platform, data storage, job queueing with accounting, and systems administration. As a result, the cluster contributes directly to the mission of UCSF, including research, education, and outreach.\nThe cluster is built on a co-op model rather than a fee-for-service model, in contrast to, for example, most technology cores at UCSF and high-performance computing provided by Amazon Web Services. In the co-op model, every contributing research group has immediate access to the cluster computing power and storage capacity proportional to their contribution; contributions are preferably made in cash, although suitable existing hardware may also be integrated. This funding model suits best the often unpredictable timing and amount of research funds available to individual projects. In addition, our goal is to maximize the number of UCSF users of the shared cluster. Therefore, every member of the UCSF community has access to a fraction of the cluster computing power and storage capacity regardless of their contribution, reflecting the contribution by UCSF and shared instrumentation grants from NIH and NSF. Finally, any unused compute power is available to any user on a first-come, first-served basis. Every effort is made to accommodate and expand the set of applications that can be run efficiently on the cluster, as long as they don’t negatively impact our founding goals. If a contributing member leaves the consortium, they are entitled to take their contribution with them (except for communal storage)."
  },
  {
    "objectID": "hpc/about/governance.html#governance-of-the-wynton-cluster",
    "href": "hpc/about/governance.html#governance-of-the-wynton-cluster",
    "title": "Shared Cluster Computing at UCSF",
    "section": "",
    "text": "The governance pertaining only to the Wynton cluster as defined above is described here; Other high performance computing initiatives at UCSF are outside the scope of this document. The Wynton cluster is governed by the Wynton Faculty Steering Committee (WFSC), guided by the cluster’s purpose and organizational principles outlined above. The Committee consists of major contributing basic scientists, a representative of guest users, and a campus Information Technology representative, all of whom have some expertise in cluster computing. The Committee meets as required to review overall utilization of the system, plan future expansions, discuss funding opportunities, etc. The Committee also resolves any disputes concerning use and governance of the cluster. Committee matters are addressed at meetings and by email as needed. To ensure continuity of governance, the Committee appoints new members by a majority vote. All decision rights for the Wynton Cluster rest with the Wynton Faculty Steering Committee decided by a majority vote, with the Chair breaking any ties. Decision rights include but are not limited to queuing rules, expansion of the environment, proposals for new uses of the environment, administrative aspects of the environment such as proposals for recharges, collaborations with other Faculty Subcommittees on investment requests/proposals, best practices, innovations, submissions of funding requests to outside agencies, and/or campus entities. The computer security aspects of Wynton are specified by UCSF IT. The Committee currently consists of Kliment Verba (Co-Chair), Sergio Baranzini, Tanja Kortemme, Scott Pegg, Brian Shoichet, Tom Ferrin (Co-Chair), and Mandy Terrill (Campus IT) (as of August 2024)."
  },
  {
    "objectID": "hpc/about/governance.html#budget-request-and-approval-process",
    "href": "hpc/about/governance.html#budget-request-and-approval-process",
    "title": "Shared Cluster Computing at UCSF",
    "section": "",
    "text": "The budget request and approval process for campus funds and resources will flow through the Committee for Research Technology (CRT), with subsequent oversight by the CRT and then final approval by the Executive Vice Chancellor and Provost (EVCP). This ensures alignment with institutional strategies and financial policies while respecting the voices of all stakeholders in the prioritization and operations of the compute cluster.\nThe Wynton Faculty Steering Committee (WFSC) is tasked with formulating recommendations and articulating the needs and concerns regarding the budget for the Wynton computational environment. These recommendations are a critical component of the process as we seek approval from the Executive Vice Chancellor and Provost (EVCP), ensuring that the budgetary needs align with the operational and strategic imperatives of UCSF’s research infrastructure."
  },
  {
    "objectID": "hpc/about/gpus.html",
    "href": "hpc/about/gpus.html",
    "title": "GPU Compute Nodes",
    "section": "",
    "text": "GPU Compute Nodes\nSeveral of {{ site.cluster.name}} compute nodes have Graphics Processing Units (GPUs):\n\n\nGPU nodes\n\n\n61 GPU compute nodes (22 communal and 39 contributed nodes)\n\n\nGPUs\n\n\n235 GPUs (88 communal and 147 contributed GPUs)\n\n\nThe tables below contain the list of the {{ site.cluster.nickname}} GPU compute nodes and whether the node is contributed by a lab or a communal node contributed by the institution. Members of groups with contributed GPUs have extra privileges on the job scheduler for jobs running on their GPU nodes. If you are a lab interested in contributing a GPU node, please see the Pricing for Extra Compute page.\n\n\n\n\n\n\n\nSource: gpu_nodes.tsv produced on . These data are manually updated."
  },
  {
    "objectID": "hpc/about/join.html",
    "href": "hpc/about/join.html",
    "title": "Join the Cluster",
    "section": "",
    "text": "Wynton HPC is a large, shared high-performance compute (HPC) cluster underlying UCSF’s Research Computing Capability. Funded and administered cooperatively by UCSF campus IT and key research groups, it is available to all UCSF researchers, and consists of different profiles suited to various biomedical and health science computing needs. Researchers can participate using the “co-op” model of resource contribution and sharing.\n\n\nThe end goal is that all UCSF researchers should have access to the cluster without charge. Free accounts will be limited by the number of concurrent cores and will have lower priority on the job queue. Participating co-op members that contribute to the cluster will get priority on the job queue and will be able to utilize a large number of concurrent cores (proportionate to the contribution).\n\n\n\nThe Wynton HPC environment is available for free to all UCSF researchers, which includes faculty and principal investigators (PIs) as well as research staff, students, and postdocs affiliated with a faculty or a PI. If you don’t fit into one of these categories, please don’t hesitate to contact us.\nIf you are a non-UCSF affiliate, you will need to have the UCSF staff, student, postdoc, or PI you are associated with request your account for you.\n\n\n\nThe Wynton HPC staff will make every effort to process your account request and create the account within five business days of confirmation of your affiliation and contact information.\n\n\n\nTo apply for an account on the Wynton HPC cluster, please follow the link below to request a Wynton Account:\n\nGladstone affiliates: Request a Wynton HPC Account via the Gladstone Service Desk. This will ensure that we get the proper information for your Wynton Account before it is created. Warning: Gladstone-affiliated Wynton HPC account requests submitted independently of the Gladstone Service Desk will be canceled.\nEverybody else: Request a Wynton Account / Modify an Existing Account (requires UCSF MyAccess login)\n\nNote: The form linked above is only available to UCSF staff, students, postdocs, and faculty. If you are a non-UCSF affiliate who does not have a UCSF Guest Account, (Request a UCSF Guest Account), you will need to have the UCSF staff, student, postdoc, or PI you are associated with request the account for you.\n\nAccounts are personal and login credentials must not be shared with others. However, multiple users can easily share and collaborate on the same folders and scripts. Don’t hesitate to ask if you don’t know how to do this - we’re here to help. Review /hpc/support/ for ways to get support.\n\n\n\n\nIf you have an existing Wynton HPC account and would like to modify it, e.g. change it to a Wynton Protected account, please use the following ServiceNow form:\n\nEveryone: Request a Wynton Account / Modify an Existing Account (requires UCSF MyAccess login)\n\n\n\n\n\n\nIf you have successfully logged into MyAccess, the “Requested By Name” and “Requested by Email” should be autopopulated with your information from the UCSF Directory.\nIn the “UCSF Principle Investigator (PI) Approver Name”, enter the name of your faculty member or supervisor you are working with and select the matching name from the pull down. This should populate the “UCSF PI Email” and “UCSF PI Department or Program” fields.\n\n\n\n\n\nSelect “Yes” or “No” from the “Does this user already have an existing account on Wynton?” pull down.\nSelect “Yes” or “No” from the “Do you Require Access to Wynton Protected” pull down. More information here regarding Wynton Protected: Wynton Protected Quickstart. Note: P4/PHI data is prohibited on Wynton.\nIf you are filling the form out for yourself, when you click the box to indicate that, the “Requested For” field will be autopopulated with your name. If you are requesting an account for another user, enter their name and select it from the pull down. If they are a non-UCSF affiliate, select “Check this box if you cannot find their User Name in the database” and enter their First Name, Last Name, and contact email address.\nSelect a “UCSF Title” from the pull down menu (if there isn’t an exact match, select something that approximates your title/role).\nSelect the “Country” and “State” from which you work. If you are associated with an institution other than UCSF, enter it in the “Institution” field.\nIf you have had accounts with any of the affiliated computing environments, select them.\nEnter your desired Wynton Username.\nEnter a desired alternative Wynton Username (in case your first choice is already in use).\n\n\n\n\n\nProvide any additional comments in the text field (e.g.lab changes, PI changes, responding to an audit message).\nCheck the box consenting to, “If the individual(s) for whom I am requesting access leaves their appointment with UCSF or transitions to a new role that does not necessitate this access, then I will inform the Wynton Team immediately that this access should be revoked for this individual.”\nClick the “Submit” button on the right.\n\n\n\n\n\nIf you are the “Requestor” of the account or “Requested For”, you will shortly receive an email confirming the creation of the ticket with a subject similar to, “Request Item RITM0305233 – opened on your behalf”.\nIf you are the person for whom the account has been requested, you will also receive an email with the subject similar to, “Email Validation required for RITM0305233 Wynton HPC Account Request”. Please reply to the “Email Validation required…” email, taking care not to alter the subject line. Also, please include the body of the email in your reply. NOTE: This message is a different email than the email acknowledging the request.\nOnce your email contact address has been validated, and any necessary approvals, training or attestations have been completed, Wynton Staff will make every effort to process your new account request within five business days.\n\nIf you have any problems with the form or questions about filling it out, please don’t hesitate to contact us."
  },
  {
    "objectID": "hpc/about/join.html#co-op-model",
    "href": "hpc/about/join.html#co-op-model",
    "title": "Join the Cluster",
    "section": "",
    "text": "The end goal is that all UCSF researchers should have access to the cluster without charge. Free accounts will be limited by the number of concurrent cores and will have lower priority on the job queue. Participating co-op members that contribute to the cluster will get priority on the job queue and will be able to utilize a large number of concurrent cores (proportionate to the contribution)."
  },
  {
    "objectID": "hpc/about/join.html#who-can-join",
    "href": "hpc/about/join.html#who-can-join",
    "title": "Join the Cluster",
    "section": "",
    "text": "The Wynton HPC environment is available for free to all UCSF researchers, which includes faculty and principal investigators (PIs) as well as research staff, students, and postdocs affiliated with a faculty or a PI. If you don’t fit into one of these categories, please don’t hesitate to contact us.\nIf you are a non-UCSF affiliate, you will need to have the UCSF staff, student, postdoc, or PI you are associated with request your account for you."
  },
  {
    "objectID": "hpc/about/join.html#how-long-will-it-take-for-my-wynton-account-request-to-be-processed",
    "href": "hpc/about/join.html#how-long-will-it-take-for-my-wynton-account-request-to-be-processed",
    "title": "Join the Cluster",
    "section": "",
    "text": "The Wynton HPC staff will make every effort to process your account request and create the account within five business days of confirmation of your affiliation and contact information."
  },
  {
    "objectID": "hpc/about/join.html#request-a-new-account",
    "href": "hpc/about/join.html#request-a-new-account",
    "title": "Join the Cluster",
    "section": "",
    "text": "To apply for an account on the Wynton HPC cluster, please follow the link below to request a Wynton Account:\n\nGladstone affiliates: Request a Wynton HPC Account via the Gladstone Service Desk. This will ensure that we get the proper information for your Wynton Account before it is created. Warning: Gladstone-affiliated Wynton HPC account requests submitted independently of the Gladstone Service Desk will be canceled.\nEverybody else: Request a Wynton Account / Modify an Existing Account (requires UCSF MyAccess login)\n\nNote: The form linked above is only available to UCSF staff, students, postdocs, and faculty. If you are a non-UCSF affiliate who does not have a UCSF Guest Account, (Request a UCSF Guest Account), you will need to have the UCSF staff, student, postdoc, or PI you are associated with request the account for you.\n\nAccounts are personal and login credentials must not be shared with others. However, multiple users can easily share and collaborate on the same folders and scripts. Don’t hesitate to ask if you don’t know how to do this - we’re here to help. Review /hpc/support/ for ways to get support."
  },
  {
    "objectID": "hpc/about/join.html#update-an-existing-account",
    "href": "hpc/about/join.html#update-an-existing-account",
    "title": "Join the Cluster",
    "section": "",
    "text": "If you have an existing Wynton HPC account and would like to modify it, e.g. change it to a Wynton Protected account, please use the following ServiceNow form:\n\nEveryone: Request a Wynton Account / Modify an Existing Account (requires UCSF MyAccess login)\n\n\n\n\n\n\nIf you have successfully logged into MyAccess, the “Requested By Name” and “Requested by Email” should be autopopulated with your information from the UCSF Directory.\nIn the “UCSF Principle Investigator (PI) Approver Name”, enter the name of your faculty member or supervisor you are working with and select the matching name from the pull down. This should populate the “UCSF PI Email” and “UCSF PI Department or Program” fields.\n\n\n\n\n\nSelect “Yes” or “No” from the “Does this user already have an existing account on Wynton?” pull down.\nSelect “Yes” or “No” from the “Do you Require Access to Wynton Protected” pull down. More information here regarding Wynton Protected: Wynton Protected Quickstart. Note: P4/PHI data is prohibited on Wynton.\nIf you are filling the form out for yourself, when you click the box to indicate that, the “Requested For” field will be autopopulated with your name. If you are requesting an account for another user, enter their name and select it from the pull down. If they are a non-UCSF affiliate, select “Check this box if you cannot find their User Name in the database” and enter their First Name, Last Name, and contact email address.\nSelect a “UCSF Title” from the pull down menu (if there isn’t an exact match, select something that approximates your title/role).\nSelect the “Country” and “State” from which you work. If you are associated with an institution other than UCSF, enter it in the “Institution” field.\nIf you have had accounts with any of the affiliated computing environments, select them.\nEnter your desired Wynton Username.\nEnter a desired alternative Wynton Username (in case your first choice is already in use).\n\n\n\n\n\nProvide any additional comments in the text field (e.g.lab changes, PI changes, responding to an audit message).\nCheck the box consenting to, “If the individual(s) for whom I am requesting access leaves their appointment with UCSF or transitions to a new role that does not necessitate this access, then I will inform the Wynton Team immediately that this access should be revoked for this individual.”\nClick the “Submit” button on the right.\n\n\n\n\n\nIf you are the “Requestor” of the account or “Requested For”, you will shortly receive an email confirming the creation of the ticket with a subject similar to, “Request Item RITM0305233 – opened on your behalf”.\nIf you are the person for whom the account has been requested, you will also receive an email with the subject similar to, “Email Validation required for RITM0305233 Wynton HPC Account Request”. Please reply to the “Email Validation required…” email, taking care not to alter the subject line. Also, please include the body of the email in your reply. NOTE: This message is a different email than the email acknowledging the request.\nOnce your email contact address has been validated, and any necessary approvals, training or attestations have been completed, Wynton Staff will make every effort to process your new account request within five business days.\n\nIf you have any problems with the form or questions about filling it out, please don’t hesitate to contact us."
  },
  {
    "objectID": "hpc/about/warning-hold-on-compute-requests.html",
    "href": "hpc/about/warning-hold-on-compute-requests.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "2025-01-15: Temporary Hold on Storage and Compute Requests\nAs Wynton continues to expand, we are approaching the limits of space, cooling, and power capacity in the Byers Hall server room. In addition, as we work to move the administration of the cluster to the Academic Research Systems (ARS) Team, we are currently reprioritizing our workload with the support of Wynton faculty leadership to ensure that we continue to meet our most critical objectives. We understand that this may cause some delays and appreciate your patience and understanding during this period.\nSee also the Wynton-announcement email titled ‘Important Updates on Wynton Storage and Compute Requests’ sent to all users on 2024-11-25."
  },
  {
    "objectID": "hpc/get-started/development-prototyping.html",
    "href": "hpc/get-started/development-prototyping.html",
    "title": "Development / Prototyping",
    "section": "",
    "text": "Although you should always run analyses via the job scheduler, there are times when you may need to develop parts of it interactively at the command-line prompt. For instance, you may need to install some software, a few R packages, or run some quick tests on your new pipeline. Wynton HPC provides development nodes dedicated for such short-term usages and that are configured similarly to the compute nodes.\n\nPlease do not run your real-world analysis on the development nodes. Instead, run it as a job via the scheduler - this will assert that your analysis runs as fast as possible and with all the resources it needs. For further instruction, see the getting-started page Submit Job to Cluster.\n\n\nAny shell session that has been idle for more than eight hours will timeout and exit automatically.\n\n\nCPU use is throttled to 4 CPU cores/user, and memory use is limited to 96 GiB/user. If a process overuses the memory, it will be killed by the operating system.\n\n\n\n\n\ndev1.wynton.ucsf.edu\ndev2.wynton.ucsf.edu\ndev3.wynton.ucsf.edu\ngpudev1.wynton.ucsf.edu\n\n\n\nThe following development nodes are available to Wynton Protected users:\n\npdev1.wynton.ucsf.edu\npgpudev1.wynton.ucsf.edu\n\n\n\n\n\nTo access one of the development node, say, dev2.wynton.ucsf.edu,\n\nmake sure you are logged in to the cluster, and\ntype ssh dev2.wynton.ucsf.edu and press ENTER.\n\n\n\nFrom one of the login nodes, do:\n[alice@log2 ~]$ ssh dev2.wynton.ucsf.edu\nalice1@dev2:s password: XXXXXXXXXXXXXXXXXXX\n[alice@dev2 ~]$ \n\nAs from the login nodes, it is possible to submit jobs also from the development nodes."
  },
  {
    "objectID": "hpc/get-started/development-prototyping.html#list-of-development-nodes",
    "href": "hpc/get-started/development-prototyping.html#list-of-development-nodes",
    "title": "Development / Prototyping",
    "section": "",
    "text": "dev1.wynton.ucsf.edu\ndev2.wynton.ucsf.edu\ndev3.wynton.ucsf.edu\ngpudev1.wynton.ucsf.edu\n\n\n\nThe following development nodes are available to Wynton Protected users:\n\npdev1.wynton.ucsf.edu\npgpudev1.wynton.ucsf.edu"
  },
  {
    "objectID": "hpc/get-started/development-prototyping.html#instructions",
    "href": "hpc/get-started/development-prototyping.html#instructions",
    "title": "Development / Prototyping",
    "section": "",
    "text": "To access one of the development node, say, dev2.wynton.ucsf.edu,\n\nmake sure you are logged in to the cluster, and\ntype ssh dev2.wynton.ucsf.edu and press ENTER.\n\n\n\nFrom one of the login nodes, do:\n[alice@log2 ~]$ ssh dev2.wynton.ucsf.edu\nalice1@dev2:s password: XXXXXXXXXXXXXXXXXXX\n[alice@dev2 ~]$ \n\nAs from the login nodes, it is possible to submit jobs also from the development nodes."
  },
  {
    "objectID": "hpc/get-started/hello-world-job.html",
    "href": "hpc/get-started/hello-world-job.html",
    "title": "‘Hello World’ Job",
    "section": "",
    "text": "The Wynton HPC cluster farm consists of a large number of compute nodes that are ready to serve users’ compute tasks (aka jobs). Since all compute nodes are configured the same way, for instance, they have the exact same set software installed, it does not matter on which compute node your analysis runs.\nAt any time, there will be many users using the cluster where some users run a single analysis whereas other run many multi-day jobs in parallel. In order for users not to step on each others toes and for users to get a fair share of the compute resources, the cluster uses a so called job scheduler to orchestrate the compute requests. This works by users submitting their compute jobs to the job queue. Then the scheduler will locate a compute node with enough free resources to process the submitted job and launch the job on that compute node. The scheduler is configured to distribute the compute load across all compute nodes for overall maximum performance and fare share among the users.\n\n\nThe most common way of running compute tasks on the Wynton HPC cluster, consists of:\n\ncreating a script,\nsubmitting the script to the queue,\nwaiting for the script to start and finish, and\nlooking at the results, e.g. output data files and text logs.\n\nThe Wynton HPC cluster uses Son of Grid Engine (SGE) as its scheduler. SGE provides command qsub to submit a script (“job”) and command qstat to check the status of a job.\n\nFurther information with detailed examples on job submissions can be found on separate pages under the ‘Scheduler’ menu.\n\n\n\nIn this example we will run compute jobs that outputs the name of the compute node that runs the job, waits ten seconds to emulate some processing, and the time it runs. The name of the current machine is available in environment variable HOSTNAME (standard in Unix) and the current time is outputted when calling the command date. To do this as a compute job, create a script ~/tests/hello_world containing:\n#! /usr/bin/env bash\n#$ -S /bin/bash     # run job as a Bash shell [IMPORTANT]\n#$ -cwd             # run job in the current working directory\n#$ -l h_rt=00:01:00 # request a one-minute run time\n\necho \"Hello world, I am running on node $HOSTNAME\"\nsleep 10\ndate\nHint: To create this file, make sure that the folder exists first. If doesn’t, call mkdir ~/tests.\nAlthough not critical for the job scheduler, it is always convenient to set the file permission on this script file to be executable, e.g.\n[alice@dev2 ~]$ cd tests/\n[alice@dev2 tests]$ chmod ugo+x hello_world\nThis, in combination with the so called “she-bang” (#! ...) on the first line, allows you call the script just any other software, e.g.\n[alice@dev2 tests]$ ./hello_world\nHello world, I am running on node dev2.\nMon Aug 28 16:31:29 PDT 2017\nNote how it takes ten seconds between the Hello world message and the time stamp. We have now confirmed that the shell script does what we expect it to do, and we are ready to submit it to the job queue of the scheduler. To do this, do:\n[alice@dev2 tests]$ qsub -cwd -j yes hello_world\nYour job 201 (\"hello_world\") has been submitted\nExplanation of command-line options: The -cwd option tells the scheduler to launch the hello_world script and output the job log files to the current working directory (here ~/tests/). The -j yes option specifies that error message should be merged with regular output (instead of outputting to separate log files).\nWhen submitting a job, the scheduler assigned the job an identifier (“job id”). In the above example, the job id is ‘201’. Immediately after the job has been submitted, we can see that it queued but not launched;\n[alice@dev2 tests]$ qstat\njob-ID prior   name       user   state submit/start at     queue              slots ja-task-ID\n----------------------------------------------------------------------------------------------\n   201 0.00000 hello_worl alice  qw    08/01/2017 03:34:19                        1        \nLater, when the job has been launched on one of the compute nodes, and we will something like:\n[alice@dev2 tests]$ qstat\njob-ID prior   name       user   state submit/start at     queue              slots ja-task-ID\n----------------------------------------------------------------------------------------------\n   201 0.95000 hello_worl alice  r     08/01/2017 03:34:19 member.q@cin-hmid1     1\nEventually, when the job script finished, qstat will no longer list it (if you have no other jobs on the queue, qstat will not output anything).\nSo where is the output of the job? Since we used -j yes and -cwd we will find a job output file in the current directory named hello_world.o201 that contains:\n[alice@dev2 tests]$ cat hello_world.o201\nHello world, I am running on node cin-hmid1\nMon Aug 28 16:32:12 PDT 2017\n[alice@dev2 tests]$ \nThere is of course nothing preventing us from submitting the same script multiple times. If done, each submission will result in the script be launched on a compute node and a unique log file hello_world.o&lt;job_id&gt; will be outputted. Please try that and see what qstat outputs. Now, you may want to pass different arguments to your script each time, e.g. each job should process a different input data file. For information on how to do this, see the Submit Jobs page."
  },
  {
    "objectID": "hpc/get-started/hello-world-job.html#instructions",
    "href": "hpc/get-started/hello-world-job.html#instructions",
    "title": "‘Hello World’ Job",
    "section": "",
    "text": "The most common way of running compute tasks on the Wynton HPC cluster, consists of:\n\ncreating a script,\nsubmitting the script to the queue,\nwaiting for the script to start and finish, and\nlooking at the results, e.g. output data files and text logs.\n\nThe Wynton HPC cluster uses Son of Grid Engine (SGE) as its scheduler. SGE provides command qsub to submit a script (“job”) and command qstat to check the status of a job.\n\nFurther information with detailed examples on job submissions can be found on separate pages under the ‘Scheduler’ menu.\n\n\n\nIn this example we will run compute jobs that outputs the name of the compute node that runs the job, waits ten seconds to emulate some processing, and the time it runs. The name of the current machine is available in environment variable HOSTNAME (standard in Unix) and the current time is outputted when calling the command date. To do this as a compute job, create a script ~/tests/hello_world containing:\n#! /usr/bin/env bash\n#$ -S /bin/bash     # run job as a Bash shell [IMPORTANT]\n#$ -cwd             # run job in the current working directory\n#$ -l h_rt=00:01:00 # request a one-minute run time\n\necho \"Hello world, I am running on node $HOSTNAME\"\nsleep 10\ndate\nHint: To create this file, make sure that the folder exists first. If doesn’t, call mkdir ~/tests.\nAlthough not critical for the job scheduler, it is always convenient to set the file permission on this script file to be executable, e.g.\n[alice@dev2 ~]$ cd tests/\n[alice@dev2 tests]$ chmod ugo+x hello_world\nThis, in combination with the so called “she-bang” (#! ...) on the first line, allows you call the script just any other software, e.g.\n[alice@dev2 tests]$ ./hello_world\nHello world, I am running on node dev2.\nMon Aug 28 16:31:29 PDT 2017\nNote how it takes ten seconds between the Hello world message and the time stamp. We have now confirmed that the shell script does what we expect it to do, and we are ready to submit it to the job queue of the scheduler. To do this, do:\n[alice@dev2 tests]$ qsub -cwd -j yes hello_world\nYour job 201 (\"hello_world\") has been submitted\nExplanation of command-line options: The -cwd option tells the scheduler to launch the hello_world script and output the job log files to the current working directory (here ~/tests/). The -j yes option specifies that error message should be merged with regular output (instead of outputting to separate log files).\nWhen submitting a job, the scheduler assigned the job an identifier (“job id”). In the above example, the job id is ‘201’. Immediately after the job has been submitted, we can see that it queued but not launched;\n[alice@dev2 tests]$ qstat\njob-ID prior   name       user   state submit/start at     queue              slots ja-task-ID\n----------------------------------------------------------------------------------------------\n   201 0.00000 hello_worl alice  qw    08/01/2017 03:34:19                        1        \nLater, when the job has been launched on one of the compute nodes, and we will something like:\n[alice@dev2 tests]$ qstat\njob-ID prior   name       user   state submit/start at     queue              slots ja-task-ID\n----------------------------------------------------------------------------------------------\n   201 0.95000 hello_worl alice  r     08/01/2017 03:34:19 member.q@cin-hmid1     1\nEventually, when the job script finished, qstat will no longer list it (if you have no other jobs on the queue, qstat will not output anything).\nSo where is the output of the job? Since we used -j yes and -cwd we will find a job output file in the current directory named hello_world.o201 that contains:\n[alice@dev2 tests]$ cat hello_world.o201\nHello world, I am running on node cin-hmid1\nMon Aug 28 16:32:12 PDT 2017\n[alice@dev2 tests]$ \nThere is of course nothing preventing us from submitting the same script multiple times. If done, each submission will result in the script be launched on a compute node and a unique log file hello_world.o&lt;job_id&gt; will be outputted. Please try that and see what qstat outputs. Now, you may want to pass different arguments to your script each time, e.g. each job should process a different input data file. For information on how to do this, see the Submit Jobs page."
  },
  {
    "objectID": "hpc/get-started/duo-signup.html",
    "href": "hpc/get-started/duo-signup.html",
    "title": "Two-Factor Authentication for SSH",
    "section": "",
    "text": "Access to Wynton HPC from outside of the UCSF network requires two-factor authentication (2FA). If you connecting via the UCSF campus network, 2FA is not required. Likewise, if you are on the UCSF VPN, you are already fully authenticated on the campus network and no further 2FA is needed to access Wynton HPC. In all other cases, you will be prompted to authenticate through a Wynton-specific 2FA method when SSH:ing directly to the cluster.\n\n\nSimilarly to the UCSF VPN, Wynton HPC requires two-factor authentication via the Duo 2FA system. Duo supports authentication via:\n\npush confirmation in the Duo Mobile App,\na phone call, or\nSMS passcode.\n\nKnown issues:\n\nIt is not possible to register multiple authentication methods, e.g. multiple devices and phone numbers.\nIf you wish to change your 2FA method, phone number, or SMS number, you will need to contact the Wynton HPC support staff to reset your existing 2FA registration or resend the registration link.\nIf you receive a message from Duo that you, “have been locked out due to excessive authentication failures,” you will need to contact the Wynton HPC support staff to have your Duo account re-enabled.\nSupport for hardware 2FA keys (e.g. YubiKey, Feitian, etc.) is limited and might not even work. For example, when registering a hardware key, that will be your only option. Also, if you have already registered your hardware 2FA key with UCSF (e.g. UCSF VPN), then that physical key can not be used with the Wynton 2FA system. This is also true if the hardware key supports two or more security keys (e.g. short press and long press on a YubiKey).\n\nGot a new smart phone? After installing the Duo Mobile App on the new device, you can transfer the Wynton 2FA credentials from your old phone to your new phone via ‘Connect a new phone’ in the Duo Mobile App’s ‘Settings’ on both devices.\n\n\n\nAfter having completed the 2FA registration (see below) for your account, you can access Wynton HPC via SSH from outside the UCSF network. A typical SSH login will then start by you authenticating yourself via 2FA as illustrated by:\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 1\nSuccess. Logging you in...\nRemember connection authentication from 24.5.83.75 for 12 hours? [y/N] n\n\nLast login: Tue Oct 13 11:56:19 2020 from 24.5.83.75\nWelcome to the Wynton login nodes. [...]\n\n[alice@log2]$ \nFor examples on what it looks like when you authenticate via other options, see the examples below.\n\n\n\nIn order to authenticate via 2FA, you will first have to register your Duo 2FA setup with Wynton HPC. Since they are different 2FA systems, you have to complete this registration regardless whether or not you have already registered Duo 2FA for the UCSF VPN. Below are detailed instructions on how to register 2FA for Wynton HPC.\nComment: If you are asked to ‘Please contact your help or support desk’ during the Duo 2FA registration, please contact the Wynton HPC support staff (do not contact the UCSF IT Service Desk).\n\nWe recommend that you read through the below instructions before starting the registration. You can only register once and you cannot reconfigure your 2FA setup afterwards. The only way to update the settings is to contact the Wynton HPC support staff to reset your 2FA setup.\n\n\n\nIn order to register with Duo 2FA, you need to obtain a registration link (URL) to your personal registration page. This is done by attempting to log into Wynton via SSH.\nIf this is the first time you access Wynton HPC via SSH, then you will have to do two SSH logins - the first login is just a “trigger” and the second login one will display the registration URL. If you have priorly logged into to Wynton HPC, then you can skip to the second SSH-login instructions below.\n\nSSH to log2.wynton.ucsf.edu using your Wynton username\n\nIf you are asked the question Are you sure you want to continue connecting (yes/no/[fingerprint])?, then answer yes\nEnter your Wynton password\nThe connection will be closed automatically. This is expected\n\nAgain, SSH to log2.wynton.ucsf.edu using your Wynton username\n\nEnter your Wynton password\nAfter entering you password, you should then see a message saying to enroll at a particular Duo URL\nThis will followed by a ‘Permission denied’. The connection will close. This is expected\n\nGo to the enrollment page by copying the URL into your web browser\n\nHere is what the above steps will look like:\n{local}$ ssh alice@log2.wynton.ucsf.edu\nThe authenticity of host log2.wynton.ucsf.edu (169.230.79.12) can not be established.\nECDSA key fingerprint is SHA256:DrCbFJouT3pRHoPO6rzGNJxX4OOIBuLy/ZdxjIQrx3M.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added log2.wynton.ucsf.edu,169.230.79.12 (ECDSA) to the list of known hosts.\nConnection closed by 169.230.79.12 port 22\n\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nPlease enroll at https://api-6747fbb1.duosecurity.com/portal?code=61c954f6d6124546&akey=DBPXF7JZIKINNMVHIHZK\n\nPlease enroll at https://api-6747fbb1.duosecurity.com/portal?code=61c954f6d6124546&akey=DBPXF7JZIKINNMVHIHZK\n\nPlease enroll at https://api-6747fbb1.duosecurity.com/portal?code=61c954f6d6124546&akey=DBPXF7JZIKINNMVHIHZK\n\nalice@log2.wynton.ucsf.edu: Permission denied (publickey,gssapi-with-mic,keyboard-interactive).\n\n\n\nAfter copying & pasting the registration link into your web browser, should end up on a page titled ‘Protect Your UCSF Account’ on ‘duosecurity.com’:\n\nFor security, verify that you are on the ‘duosecurity.com’ webpage\nClick the ‘Start setup’ button to begin the Duo 2FA registration\n\n\n\nThe first registration page will ask ‘What type of device are you adding?’:\n\n\nMobile phone [recommended]\nTablet (iPad, Nexus 7, etc.)\nLandline\nSecurity Key (YubiKey, Feitian, etc.)\nTouch ID\n\n\n\nSelect the type of device you are using;\n\n‘Mobile phone’ is recommended. It allows you do authenticate with the Duo Mobile App, SMS, and phone calls\nWe do not recommend using ‘Security Key’ because then you will not be able to use the ‘Mobile phone’ options. Also, if you have already registered your ‘Security Key’ for the UCSF VPN, it will not work with the Wynton HPC 2FA.\n\nClick the ‘Continue’ button\n\n\n\n\nIf you selected ‘Mobile phone’ for your device type, the next page will ask you to ‘Enter your phone number’:\n\nEnter your phone number in the box\nA checkbox below the phone number will ask you to verify your number is correct\nMake sure the number is correct and check the box\nClick the ‘Continue’ button\n\n\n\n\nThe next page ‘What type of phone is this number?’ will ask you to select the type of phone:\n\n\niPhone\nAndroid\nWindows Phone\nOther (and cell phones)\n\n\n\nSelect the type of phone you have\nClick the ‘Continue’ button\n\n\n\n\nThe next page ‘Install Duo Mobile’ will instruct you to install the Duo Mobile app on your device:\n\nIf you don’t already have the Duo Mobile App installed on your smart phone, please install it\nVerify that you can open the Duo Mobile App\nClick the ‘I have Duo Mobile installed’ button\n\n\n\n\nFollow the instructions on the ‘Activate Duo Mobile’ page:\n\nOpen the Duo Mobile App on your smart phone\nTap the ‘+’ button\nHold your phone’s camera up to scan the barcode presented in your computer’s web browser\nAfter scanning the barcode with the app on your phone, an entry titled ‘UCSF PharmChem’ should appear in the app\nClick the ‘Continue’ button\n\nComment: If you have problems scanning the barcode, there is also an option to receive a on-time activation link via email.\n\n\n\n\nVerify that the ‘My settings & Devices’ lists your phone number\nUse the default ‘When I log in’ method\nClick the ‘Finish Enrollment’ button\n\nNote: It does not matter which default ‘When I log in’ method you choose, you will still be prompted which one to use via the SSH login as with the default ‘Ask me to choose an authentication method’ option.\nThis concludes the Duo 2FA registration. You can now close the web browser.\n\n\n\n\nAfter having completed the above 2FA registration, you should be able to access Wynton HPC via SSH. As illustrated below, verify that this works by SSH:ing to a Wynton HPC login or data-transfer node using your Wynton username and:\n\nEnter your Wynton password\nAt the prompt, select a 2FA method, e.g. ‘Duo Push’\nOpen the Duo Mobile App and click the ‘Accept’ button\nThe SSH session\n\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 1\nSuccess. Logging you in...\nRemember connection authentication from 24.5.83.75 for 12 hours? [y/N] n\nLast failed login: Wed Oct 14 13:34:11 PDT 2020 from 73.70.236.131 on ssh:notty\nThere was 2 failed login attempts since the last successful login.\n\nWelcome to the Wynton login nodes. [...]\n\n[alice@log2]$ \n\n\n\n\n\n\nIf you choose to authenticate via a phone call, below is what you will see. The phone call will be made momentarily to your registered phone number. Please listen to the prompt and follow the instructions. If you did not request a Duo 2FA phone call, hang up.\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 2\nSuccess. Logging you in...\n\n\n\nIf you choose to authenticate via an SMS passcode, below is what you will see. The passcode to be entered is sent momentarily to your registered mobile number as an SMS.\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 3\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999 (next code starts with: 1)\n\nPasscode or option (1-3): 1443743\nSuccess. Logging you in..."
  },
  {
    "objectID": "hpc/get-started/duo-signup.html#requirements",
    "href": "hpc/get-started/duo-signup.html#requirements",
    "title": "Two-Factor Authentication for SSH",
    "section": "",
    "text": "Similarly to the UCSF VPN, Wynton HPC requires two-factor authentication via the Duo 2FA system. Duo supports authentication via:\n\npush confirmation in the Duo Mobile App,\na phone call, or\nSMS passcode.\n\nKnown issues:\n\nIt is not possible to register multiple authentication methods, e.g. multiple devices and phone numbers.\nIf you wish to change your 2FA method, phone number, or SMS number, you will need to contact the Wynton HPC support staff to reset your existing 2FA registration or resend the registration link.\nIf you receive a message from Duo that you, “have been locked out due to excessive authentication failures,” you will need to contact the Wynton HPC support staff to have your Duo account re-enabled.\nSupport for hardware 2FA keys (e.g. YubiKey, Feitian, etc.) is limited and might not even work. For example, when registering a hardware key, that will be your only option. Also, if you have already registered your hardware 2FA key with UCSF (e.g. UCSF VPN), then that physical key can not be used with the Wynton 2FA system. This is also true if the hardware key supports two or more security keys (e.g. short press and long press on a YubiKey).\n\nGot a new smart phone? After installing the Duo Mobile App on the new device, you can transfer the Wynton 2FA credentials from your old phone to your new phone via ‘Connect a new phone’ in the Duo Mobile App’s ‘Settings’ on both devices."
  },
  {
    "objectID": "hpc/get-started/duo-signup.html#example-authentication-with-a-duo-push",
    "href": "hpc/get-started/duo-signup.html#example-authentication-with-a-duo-push",
    "title": "Two-Factor Authentication for SSH",
    "section": "",
    "text": "After having completed the 2FA registration (see below) for your account, you can access Wynton HPC via SSH from outside the UCSF network. A typical SSH login will then start by you authenticating yourself via 2FA as illustrated by:\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 1\nSuccess. Logging you in...\nRemember connection authentication from 24.5.83.75 for 12 hours? [y/N] n\n\nLast login: Tue Oct 13 11:56:19 2020 from 24.5.83.75\nWelcome to the Wynton login nodes. [...]\n\n[alice@log2]$ \nFor examples on what it looks like when you authenticate via other options, see the examples below."
  },
  {
    "objectID": "hpc/get-started/duo-signup.html#registration-for-2fa-on",
    "href": "hpc/get-started/duo-signup.html#registration-for-2fa-on",
    "title": "Two-Factor Authentication for SSH",
    "section": "",
    "text": "In order to authenticate via 2FA, you will first have to register your Duo 2FA setup with Wynton HPC. Since they are different 2FA systems, you have to complete this registration regardless whether or not you have already registered Duo 2FA for the UCSF VPN. Below are detailed instructions on how to register 2FA for Wynton HPC.\nComment: If you are asked to ‘Please contact your help or support desk’ during the Duo 2FA registration, please contact the Wynton HPC support staff (do not contact the UCSF IT Service Desk).\n\nWe recommend that you read through the below instructions before starting the registration. You can only register once and you cannot reconfigure your 2FA setup afterwards. The only way to update the settings is to contact the Wynton HPC support staff to reset your 2FA setup.\n\n\n\nIn order to register with Duo 2FA, you need to obtain a registration link (URL) to your personal registration page. This is done by attempting to log into Wynton via SSH.\nIf this is the first time you access Wynton HPC via SSH, then you will have to do two SSH logins - the first login is just a “trigger” and the second login one will display the registration URL. If you have priorly logged into to Wynton HPC, then you can skip to the second SSH-login instructions below.\n\nSSH to log2.wynton.ucsf.edu using your Wynton username\n\nIf you are asked the question Are you sure you want to continue connecting (yes/no/[fingerprint])?, then answer yes\nEnter your Wynton password\nThe connection will be closed automatically. This is expected\n\nAgain, SSH to log2.wynton.ucsf.edu using your Wynton username\n\nEnter your Wynton password\nAfter entering you password, you should then see a message saying to enroll at a particular Duo URL\nThis will followed by a ‘Permission denied’. The connection will close. This is expected\n\nGo to the enrollment page by copying the URL into your web browser\n\nHere is what the above steps will look like:\n{local}$ ssh alice@log2.wynton.ucsf.edu\nThe authenticity of host log2.wynton.ucsf.edu (169.230.79.12) can not be established.\nECDSA key fingerprint is SHA256:DrCbFJouT3pRHoPO6rzGNJxX4OOIBuLy/ZdxjIQrx3M.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added log2.wynton.ucsf.edu,169.230.79.12 (ECDSA) to the list of known hosts.\nConnection closed by 169.230.79.12 port 22\n\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nPlease enroll at https://api-6747fbb1.duosecurity.com/portal?code=61c954f6d6124546&akey=DBPXF7JZIKINNMVHIHZK\n\nPlease enroll at https://api-6747fbb1.duosecurity.com/portal?code=61c954f6d6124546&akey=DBPXF7JZIKINNMVHIHZK\n\nPlease enroll at https://api-6747fbb1.duosecurity.com/portal?code=61c954f6d6124546&akey=DBPXF7JZIKINNMVHIHZK\n\nalice@log2.wynton.ucsf.edu: Permission denied (publickey,gssapi-with-mic,keyboard-interactive).\n\n\n\nAfter copying & pasting the registration link into your web browser, should end up on a page titled ‘Protect Your UCSF Account’ on ‘duosecurity.com’:\n\nFor security, verify that you are on the ‘duosecurity.com’ webpage\nClick the ‘Start setup’ button to begin the Duo 2FA registration\n\n\n\nThe first registration page will ask ‘What type of device are you adding?’:\n\n\nMobile phone [recommended]\nTablet (iPad, Nexus 7, etc.)\nLandline\nSecurity Key (YubiKey, Feitian, etc.)\nTouch ID\n\n\n\nSelect the type of device you are using;\n\n‘Mobile phone’ is recommended. It allows you do authenticate with the Duo Mobile App, SMS, and phone calls\nWe do not recommend using ‘Security Key’ because then you will not be able to use the ‘Mobile phone’ options. Also, if you have already registered your ‘Security Key’ for the UCSF VPN, it will not work with the Wynton HPC 2FA.\n\nClick the ‘Continue’ button\n\n\n\n\nIf you selected ‘Mobile phone’ for your device type, the next page will ask you to ‘Enter your phone number’:\n\nEnter your phone number in the box\nA checkbox below the phone number will ask you to verify your number is correct\nMake sure the number is correct and check the box\nClick the ‘Continue’ button\n\n\n\n\nThe next page ‘What type of phone is this number?’ will ask you to select the type of phone:\n\n\niPhone\nAndroid\nWindows Phone\nOther (and cell phones)\n\n\n\nSelect the type of phone you have\nClick the ‘Continue’ button\n\n\n\n\nThe next page ‘Install Duo Mobile’ will instruct you to install the Duo Mobile app on your device:\n\nIf you don’t already have the Duo Mobile App installed on your smart phone, please install it\nVerify that you can open the Duo Mobile App\nClick the ‘I have Duo Mobile installed’ button\n\n\n\n\nFollow the instructions on the ‘Activate Duo Mobile’ page:\n\nOpen the Duo Mobile App on your smart phone\nTap the ‘+’ button\nHold your phone’s camera up to scan the barcode presented in your computer’s web browser\nAfter scanning the barcode with the app on your phone, an entry titled ‘UCSF PharmChem’ should appear in the app\nClick the ‘Continue’ button\n\nComment: If you have problems scanning the barcode, there is also an option to receive a on-time activation link via email.\n\n\n\n\nVerify that the ‘My settings & Devices’ lists your phone number\nUse the default ‘When I log in’ method\nClick the ‘Finish Enrollment’ button\n\nNote: It does not matter which default ‘When I log in’ method you choose, you will still be prompted which one to use via the SSH login as with the default ‘Ask me to choose an authentication method’ option.\nThis concludes the Duo 2FA registration. You can now close the web browser.\n\n\n\n\nAfter having completed the above 2FA registration, you should be able to access Wynton HPC via SSH. As illustrated below, verify that this works by SSH:ing to a Wynton HPC login or data-transfer node using your Wynton username and:\n\nEnter your Wynton password\nAt the prompt, select a 2FA method, e.g. ‘Duo Push’\nOpen the Duo Mobile App and click the ‘Accept’ button\nThe SSH session\n\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 1\nSuccess. Logging you in...\nRemember connection authentication from 24.5.83.75 for 12 hours? [y/N] n\nLast failed login: Wed Oct 14 13:34:11 PDT 2020 from 73.70.236.131 on ssh:notty\nThere was 2 failed login attempts since the last successful login.\n\nWelcome to the Wynton login nodes. [...]\n\n[alice@log2]$"
  },
  {
    "objectID": "hpc/get-started/duo-signup.html#examples-of-other-authentication-methods",
    "href": "hpc/get-started/duo-signup.html#examples-of-other-authentication-methods",
    "title": "Two-Factor Authentication for SSH",
    "section": "",
    "text": "If you choose to authenticate via a phone call, below is what you will see. The phone call will be made momentarily to your registered phone number. Please listen to the prompt and follow the instructions. If you did not request a Duo 2FA phone call, hang up.\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 2\nSuccess. Logging you in...\n\n\n\nIf you choose to authenticate via an SMS passcode, below is what you will see. The passcode to be entered is sent momentarily to your registered mobile number as an SMS.\n{local}$ ssh alice@log2.wynton.ucsf.edu\nalice@log2.wynton.ucsf.edu password:\nDuo two-factor login for alice\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999\n\nPasscode or option (1-3): 3\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999 (next code starts with: 1)\n\nPasscode or option (1-3): 1443743\nSuccess. Logging you in..."
  },
  {
    "objectID": "hpc/get-started/good-practices.html",
    "href": "hpc/get-started/good-practices.html",
    "title": "Good Practices",
    "section": "",
    "text": "Good Practices\nThe Wynton HPC cluster is high-performance compute resource used by a variety of UCSF researchers. It is designed to handle a large number of analyses run in parallel by a large number of groups and users. Despite this, there will times when the demand is higher than the supply, resulting in longer queueing times and possibly also slower processing times when the jobs are started.\nIn order to maximize the efficiency of the cluster and decrease your (and others) queuing and processing times, please try make yourself familiar with the following guidelines:\n\nIdentify and specify the amount of resources your jobs need (⇒ better utilization and load balancing)\nMake your script agile to SGE resources allocated to your job (⇒ scalable and lower risk of using resources not requested)\nWork toward local scratch on the compute nodes (⇒ faster processing and less load on shared disk I/O)\n\nSome additional guidelines for optimum use of the BeeGFS file system on Wynton:\n\nPrefer fewer, large files over many small ones\nIf writing many files, spread them out over a number of directories including SGE output and error files\nUsers are strongly encouraged to keep the number of reads and writes to a single directory to a reasonable number\nDon’t include anything in /wynton in your default LD_LIBRARY_PATH"
  },
  {
    "objectID": "hpc/transfers/files-and-directories.html",
    "href": "hpc/transfers/files-and-directories.html",
    "title": "Transfer Files and Directories",
    "section": "",
    "text": "To set up password-free file transfers, see Log in without Password. Then there is also no need to specify your cluster username."
  },
  {
    "objectID": "hpc/transfers/files-and-directories.html#from-your-local-machine-to-the-file-system",
    "href": "hpc/transfers/files-and-directories.html#from-your-local-machine-to-the-file-system",
    "title": "Transfer Files and Directories",
    "section": " From your local machine to the Wynton HPC file system",
    "text": "From your local machine to the Wynton HPC file system\n\nCopy a single file on your local file system to your Wynton HPC home directory\nTo copy a single file to your home directory (~/) on the cluster, use\n{local}$ scp one_file.tsv alice@dt2.wynton.ucsf.edu:~/\n\n\nCopy one or more files to a folder on the cluster\nTo copy multiple files to Wynton HPC so they appear directly under ~/study/files/, use\n{local}$ scp *.txt *.R ../some/path/another_file.tsv alice@dt2.wynton.ucsf.edu:study/files/\n\n\nRecursively copy a directory to a folder on the cluster\nTo copy all content of directory dataset/ to Wynton HPC so that it appears as ~/study/dataset/, use\n{local}$ scp -r dataset/ alice@dt2.wynton.ucsf.edu:study/"
  },
  {
    "objectID": "hpc/transfers/files-and-directories.html#from-the-file-system-to-your-local-machine",
    "href": "hpc/transfers/files-and-directories.html#from-the-file-system-to-your-local-machine",
    "title": "Transfer Files and Directories",
    "section": " From the Wynton HPC file system to your local machine",
    "text": "From the Wynton HPC file system to your local machine\n\nCopy a single file from your Wynton HPC home directory to your local machine\nTo copy a single file in your Wynton HPC home directory to the working directory of your local machine, use\n{local}$ cd /path/project\n{local}$ scp alice@dt2.wynton.ucsf.edu:one_file.tsv .\nNote: Don’t forget that period (.) at the end - it indicates copy [the file] “to the current directory”.\n\n\nCopy one or more files from the cluster\nTo copy multiple files from ~/study/files/ on the cluster to ~/study/ on your local machine, do:\n{local}$ scp alice@dt2.wynton.ucsf.edu:study/files/*.txt alice@dt2.wynton.ucsf.edu:study/files/*.R ~/study/\n\n\nRecursively copy a folder from the cluster\nTo copy all content of directory dataset/ on the cluster so that it appears as dataset/ in your local working directory, use\n{local}$ cd /path/project\n{local}$ scp -r alice@dt2.wynton.ucsf.edu:dataset/ .\nNote: Don’t forget that period (.) at the end - it indicates copy [the folder] “to the current directory”."
  },
  {
    "objectID": "hpc/transfers/files-and-directories.html#gui-file-transfer-clients",
    "href": "hpc/transfers/files-and-directories.html#gui-file-transfer-clients",
    "title": "Transfer Files and Directories",
    "section": "GUI file-transfer clients",
    "text": "GUI file-transfer clients\nIn addition to using command-line file transfer clients, some users might use graphical desktop clients to perform file transfers.\n\nWynton requires multifactor authentication so there are a couple additional configuration steps that might be necessary.\n\n\nCyberduck\nWhen using Cyberduck, from the menu:\n\nnavigate to Preferences -&gt; Transfers -&gt; General\nchange the Transfer Files setting ‘Use browser connection’ instead of ‘Open Multiple connections’\n\n\n\nFileZilla\nWhen using FileZilla, do:\n\nin the General tab, select ‘SFTP’ as the Protocol instead of ‘FTP’\nfor Logon Type, select ‘Interactive’ instead of ‘Ask for Password’\nunder the Transfer Settings tab, you might need to click the ‘Limit number of simultaneous connections’ and make sure the ‘Maximum number of connections’ is set to 1"
  },
  {
    "objectID": "hpc/transfers/globus.html",
    "href": "hpc/transfers/globus.html",
    "title": "Globus",
    "section": "",
    "text": "Globus is a non-profit service for moving, syncing, and sharing large amounts of data asynchronously in the background. Transfers are done from and to, so called, Collections. In order to perform a file transfer from one location to another using the Globus service, both ends must have a Collection. UCSF has a site license for Globus, and several UCSF departments and services, including the Wynton HPC environment, provide Globus Collection. This will allow you to transfer and share data efficiently with any other Globus user in the world.\n\n\n\n\nIf you only need to transfer data to or from remote Globus collections to or from your local machine or departmental share, you do NOT need a Wynton HPC Account.\nInstead, follow the instructions below to install Globus Connect Personal on your computer.\n\n\n\nIf you want to transfer files from or to your local machine, you need to set up a personal Collection on that machine. Below is an outline on how to do this. For full details, see the Globus Docs How To.\n\n[local] Make sure Globus Connect Personal is installed on your local machine (available for macOS, Linux, and MS Windows)\n[local] (optional) The default is that Globus will have access to all of the content under your home directory, e.g. when connected to Globus you will be able to browse it from the Global website online. To limit this, create a folder to be used solely for Globus transfers, e.g. ~/globus/. Launch the ‘Globus Connect Personal’ software, go to ‘Preferences’ and change the ‘Access Path Configuration’ to ~/globus. Then, click ‘Save’.\n[online] Setup a Globus Connect Personal (GCP) Collection for your local machine. Use one GCP collection per machine. This step will produce a GCP Security Key for your local machine. Make sure to write it down in a safe place. If you lose it, you will have to create a new GCP collection.\n[local] Launch the ‘Globus Connect Personal’ software, and enter your GCP Security Key code to connect.\n[online] Go to Collection -&gt; ‘Administered by You’, go to on your GCP Collection, and click on ‘Open in File Manager’. This will display the files and folders on your local computer. If you restricted access to ~/globus (Step 2), then it is only that folder that is accessible via Globus.\n[local] In the Globus Connect Personal software, make sure to disconnect when no longer needed.\n[online] (Optional) If you require to transfer data to or from Globus High Assurance Collections, your account must be associated with the “University of California San Francisco High Assurance Globus Plus” Group. To join the group, login to globus.org with your UCSF MyAccess credentials, select the groups side tab, deselect “My Groups”, and search for “University of California San Francisco” - locate the “University of California San Francisco High Assurance Globus Plus” group and hit the join. The person who manages the UCSF Globus subscription will approve any account associated with a UCSF Email Address.\n\n\n\n\nIf you want to transfer files from or to your Wynton account, you need to set up the ‘UCSF Wynton HPC’ Collection. Below is an outline on how to do this.\n\n[online] Go to Globus.org and log in with your UCSF MyAccess credentials.\nThen find the Collections menu on the left and click it.\nThen locate the [‘UCSF Wynton HPC’ collection].\nThis will bring up the “Overview” of the [‘UCSF Wynton HPC’ collection].\nClick “Open in File Manager”. There should be a new pane which says, “Authentication Required”.\nClick “Continue”.\nSelect your Globus Wynton Identity.\nEnter your Wynton credential, i.e. your Wynton username and password.\nYour Wynton Home Directory should display in File Manager.\n\nNOTE: You can only use Wynton Globus Collections to share data from Wynton Servers and the Wynton File systems. You cannot use the Wynton Globus Collections to share data mounted on Wynton servers via NFS mounts of remote servers.\n\n\n\nIf you receive a message similar to Missing Identity Information. Unable to complete the authentication process. Your identity Provider did not release the attributes(s): {{email}} please follow these steps, “This error is the result of email address privacy settings in the directory. Please ask them to go to UCSF Directory and click the “Edit My Record” button at the top right of the page. They’ll be asked to login via MyAccess (if they haven’t already done so). To the right of the “Email” field on the Edit Your UCSF Directory Entry page that appears, click the pop-up menu (which probably shows “Private” currently) and select either “UCSF Only” or “Public” then click the “Save Changes” button at the bottom of the page. This will tell the SSO system that it’s okay to release the email address to CILogon and other InCommon Federation registered applications.”\n\n\n\n\n\nTo use the Wynton Protected Compatible Globus [‘UCSF Wynton PDT’ Collection], you must be signed up for Wynton Two-Factor Authentication.\nThe regular ‘UCSF Wynton HPC’ Collection is not compliant with Wynton Protected use. Instead, all Wynton Protected data must use the Wynton-Protected-approved Globus Collection ‘UCSF Wynton PDT’, which is associated with the pdt1 and pdt2 data-transfer nodes.\n\nLog in to Globus with your UCSF Campus ID.\nOn the next screen, you will be notified ‘Authentication Required’.\nSelect your identity.\nThis will take you to a screen where you are prompted to enter your Wynton Username and Password. Enter these.\nYour default Two-Factor Authentication will be notified.\n\nApprove, and Globus will continue to a screen representing the Wynton file system.\n\n\n\n\n\n\n\nYou must have:\n\na UCSF Associated Globus Account\na UCSF Account that includes access to UCSF Box\n\n\n\n\n\nGo to Globus.org and log in with your UCSF MyAccess credentials.\nIn the “Collections” search field enter “UCSF Wynton HPC Box Connector”.\nSelect the “UCSF Wynton HPC Box Connector” collection.\nYou will see a dialog “Authentication Required”, click “continue”.\nNext you will see a dialog “Identity Required”. Select your UCSF associated identity, (it will be something like “123456@ucsf.edu”).\nAuthenticate with MyAccess.\nA representation of your UCSF Box files will appear in the Globus panel.\nLog in to another Globus Collection to transfer files to or from your UCSF Box file repository to another collection.\n\nTroubleshooting:\n\nIf step 4/5 above fails and you receive a message similar to “Identity set contains identity from an allowed domain but it does not map to a valid username for this connector.” contact wynton-support@ucsf.edu.\nIf you receive a message similar to Missing Identity Information. Unable to complete the authentication process. Your identity Provider did not release the attributes(s): {{email}} or “Identity set contains identity from an allowed domain but it does not map to a valid username for this connector.” please follow these steps, “This error is the result of email address privacy settings in the directory. Please ask them to go to UCSF Directory and click the “Edit My Record” button at the top right of the page. They’ll be asked to login via MyAccess (if they haven’t already done so). To the right of the “Email” field on the Edit Your UCSF Directory Entry page that appears, click the pop-up menu (which probably shows “Private” currently) and select either “UCSF Only” or “Public” then click the “Save Changes” button at the bottom of the page. This will tell the SSO system that it’s okay to release the email address to CILogon and other InCommon Federation registered applications.” If you choose to keep your email address “private”, you will be unable to use the Globus Box Plugin.\n\n\n\n\n\nTo transfer files via the Globus network:\n\nGo to the Globus Online File Manager\nSpecify the two Collections you wish to transfer files from and to.\nSelect the files and folders to be transferred and click ‘Start’.\n\n\n\n\n\nYour personal GCP Collection\n[‘UCSF Wynton HPC’ Collection] (UUID 68f06816-cd00-4bf3-ae1f-0597a07ed118)\n[‘UCSF Wynton PDT’ Collection] (UUID ef126934-6f8a-4af8-81cd-d5b0b7afe51f)\n[‘{{ site.globus-box.name }}’ Collection] (UUID { site.globus-box.uuid })\n\n[‘UCSF Wynton HPC’ Collection]: https://app.globus.org/file-manager/collections/68f06816-cd00-4bf3-ae1f-0597a07ed118/overview [‘UCSF Wynton PDT’ Collection]: https://app.globus.org/file-manager/collections/ef126934-6f8a-4af8-81cd-d5b0b7afe51f/overview [‘{{ site.globus-box.name }}’ Collection]: https://app.globus.org/file-manager/collections/{{ site.globus-box.uuid }}/overview"
  },
  {
    "objectID": "hpc/transfers/globus.html#setup-of-globus-collections",
    "href": "hpc/transfers/globus.html#setup-of-globus-collections",
    "title": "Globus",
    "section": "",
    "text": "If you only need to transfer data to or from remote Globus collections to or from your local machine or departmental share, you do NOT need a Wynton HPC Account.\nInstead, follow the instructions below to install Globus Connect Personal on your computer.\n\n\n\nIf you want to transfer files from or to your local machine, you need to set up a personal Collection on that machine. Below is an outline on how to do this. For full details, see the Globus Docs How To.\n\n[local] Make sure Globus Connect Personal is installed on your local machine (available for macOS, Linux, and MS Windows)\n[local] (optional) The default is that Globus will have access to all of the content under your home directory, e.g. when connected to Globus you will be able to browse it from the Global website online. To limit this, create a folder to be used solely for Globus transfers, e.g. ~/globus/. Launch the ‘Globus Connect Personal’ software, go to ‘Preferences’ and change the ‘Access Path Configuration’ to ~/globus. Then, click ‘Save’.\n[online] Setup a Globus Connect Personal (GCP) Collection for your local machine. Use one GCP collection per machine. This step will produce a GCP Security Key for your local machine. Make sure to write it down in a safe place. If you lose it, you will have to create a new GCP collection.\n[local] Launch the ‘Globus Connect Personal’ software, and enter your GCP Security Key code to connect.\n[online] Go to Collection -&gt; ‘Administered by You’, go to on your GCP Collection, and click on ‘Open in File Manager’. This will display the files and folders on your local computer. If you restricted access to ~/globus (Step 2), then it is only that folder that is accessible via Globus.\n[local] In the Globus Connect Personal software, make sure to disconnect when no longer needed.\n[online] (Optional) If you require to transfer data to or from Globus High Assurance Collections, your account must be associated with the “University of California San Francisco High Assurance Globus Plus” Group. To join the group, login to globus.org with your UCSF MyAccess credentials, select the groups side tab, deselect “My Groups”, and search for “University of California San Francisco” - locate the “University of California San Francisco High Assurance Globus Plus” group and hit the join. The person who manages the UCSF Globus subscription will approve any account associated with a UCSF Email Address.\n\n\n\n\nIf you want to transfer files from or to your Wynton account, you need to set up the ‘UCSF Wynton HPC’ Collection. Below is an outline on how to do this.\n\n[online] Go to Globus.org and log in with your UCSF MyAccess credentials.\nThen find the Collections menu on the left and click it.\nThen locate the [‘UCSF Wynton HPC’ collection].\nThis will bring up the “Overview” of the [‘UCSF Wynton HPC’ collection].\nClick “Open in File Manager”. There should be a new pane which says, “Authentication Required”.\nClick “Continue”.\nSelect your Globus Wynton Identity.\nEnter your Wynton credential, i.e. your Wynton username and password.\nYour Wynton Home Directory should display in File Manager.\n\nNOTE: You can only use Wynton Globus Collections to share data from Wynton Servers and the Wynton File systems. You cannot use the Wynton Globus Collections to share data mounted on Wynton servers via NFS mounts of remote servers.\n\n\n\nIf you receive a message similar to Missing Identity Information. Unable to complete the authentication process. Your identity Provider did not release the attributes(s): {{email}} please follow these steps, “This error is the result of email address privacy settings in the directory. Please ask them to go to UCSF Directory and click the “Edit My Record” button at the top right of the page. They’ll be asked to login via MyAccess (if they haven’t already done so). To the right of the “Email” field on the Edit Your UCSF Directory Entry page that appears, click the pop-up menu (which probably shows “Private” currently) and select either “UCSF Only” or “Public” then click the “Save Changes” button at the bottom of the page. This will tell the SSO system that it’s okay to release the email address to CILogon and other InCommon Federation registered applications.”\n\n\n\n\n\nTo use the Wynton Protected Compatible Globus [‘UCSF Wynton PDT’ Collection], you must be signed up for Wynton Two-Factor Authentication.\nThe regular ‘UCSF Wynton HPC’ Collection is not compliant with Wynton Protected use. Instead, all Wynton Protected data must use the Wynton-Protected-approved Globus Collection ‘UCSF Wynton PDT’, which is associated with the pdt1 and pdt2 data-transfer nodes.\n\nLog in to Globus with your UCSF Campus ID.\nOn the next screen, you will be notified ‘Authentication Required’.\nSelect your identity.\nThis will take you to a screen where you are prompted to enter your Wynton Username and Password. Enter these.\nYour default Two-Factor Authentication will be notified.\n\nApprove, and Globus will continue to a screen representing the Wynton file system."
  },
  {
    "objectID": "hpc/transfers/globus.html#transfer-between-wynton-and-ucsf-box-using-ucsf-wynton-hpc-box-globus-connector",
    "href": "hpc/transfers/globus.html#transfer-between-wynton-and-ucsf-box-using-ucsf-wynton-hpc-box-globus-connector",
    "title": "Globus",
    "section": "",
    "text": "You must have:\n\na UCSF Associated Globus Account\na UCSF Account that includes access to UCSF Box\n\n\n\n\n\nGo to Globus.org and log in with your UCSF MyAccess credentials.\nIn the “Collections” search field enter “UCSF Wynton HPC Box Connector”.\nSelect the “UCSF Wynton HPC Box Connector” collection.\nYou will see a dialog “Authentication Required”, click “continue”.\nNext you will see a dialog “Identity Required”. Select your UCSF associated identity, (it will be something like “123456@ucsf.edu”).\nAuthenticate with MyAccess.\nA representation of your UCSF Box files will appear in the Globus panel.\nLog in to another Globus Collection to transfer files to or from your UCSF Box file repository to another collection.\n\nTroubleshooting:\n\nIf step 4/5 above fails and you receive a message similar to “Identity set contains identity from an allowed domain but it does not map to a valid username for this connector.” contact wynton-support@ucsf.edu.\nIf you receive a message similar to Missing Identity Information. Unable to complete the authentication process. Your identity Provider did not release the attributes(s): {{email}} or “Identity set contains identity from an allowed domain but it does not map to a valid username for this connector.” please follow these steps, “This error is the result of email address privacy settings in the directory. Please ask them to go to UCSF Directory and click the “Edit My Record” button at the top right of the page. They’ll be asked to login via MyAccess (if they haven’t already done so). To the right of the “Email” field on the Edit Your UCSF Directory Entry page that appears, click the pop-up menu (which probably shows “Private” currently) and select either “UCSF Only” or “Public” then click the “Save Changes” button at the bottom of the page. This will tell the SSO system that it’s okay to release the email address to CILogon and other InCommon Federation registered applications.” If you choose to keep your email address “private”, you will be unable to use the Globus Box Plugin."
  },
  {
    "objectID": "hpc/transfers/globus.html#transferring-files",
    "href": "hpc/transfers/globus.html#transferring-files",
    "title": "Globus",
    "section": "",
    "text": "To transfer files via the Globus network:\n\nGo to the Globus Online File Manager\nSpecify the two Collections you wish to transfer files from and to.\nSelect the files and folders to be transferred and click ‘Start’."
  },
  {
    "objectID": "hpc/transfers/globus.html#collections-of-interest",
    "href": "hpc/transfers/globus.html#collections-of-interest",
    "title": "Globus",
    "section": "",
    "text": "Your personal GCP Collection\n[‘UCSF Wynton HPC’ Collection] (UUID 68f06816-cd00-4bf3-ae1f-0597a07ed118)\n[‘UCSF Wynton PDT’ Collection] (UUID ef126934-6f8a-4af8-81cd-d5b0b7afe51f)\n[‘{{ site.globus-box.name }}’ Collection] (UUID { site.globus-box.uuid })\n\n[‘UCSF Wynton HPC’ Collection]: https://app.globus.org/file-manager/collections/68f06816-cd00-4bf3-ae1f-0597a07ed118/overview [‘UCSF Wynton PDT’ Collection]: https://app.globus.org/file-manager/collections/ef126934-6f8a-4af8-81cd-d5b0b7afe51f/overview [‘{{ site.globus-box.name }}’ Collection]: https://app.globus.org/file-manager/collections/{{ site.globus-box.uuid }}/overview"
  },
  {
    "objectID": "hpc/transfers/globus-sharing.html",
    "href": "hpc/transfers/globus-sharing.html",
    "title": "Globus File Sharing",
    "section": "",
    "text": "Below are instructions on how to share a specific folder on Wynton with specific Globus users. These users do neither have to have a Wynton account or have to be UCSF employees - you can share with anyone who has a Globus account. When sharing, you can specify whether the person you share with should have read-only access or write access. Typically, if you want to make large data files available for download, you will share with read-only permission. If you want to receive data from a collaborator, you can give them write permission.\n\n\n\nNote, Guest Collections/Shared Globus Collections are not allowed for Wynton Protected users.\n\n\nMake sure you have a working setup of Globus for Wynton\nEmail support@wynton.ucsf.edu with the directory you want to share. The admins will send back the path the Globus share will appear as, e.g. /wynton/globus/share/alice\nOn the ‘UCSF Wynton’ Collection page, click the ‘Collections’ tab to get to ‘Guest Collections’:\n\nClick the ‘Add a Guest Collection’ button\n\nFill out the ‘Create New Guest Collection’ form:\n\nDirectory: Enter the path you received from the admins, e.g. /wynton/globus/share/alice\nDisplay Name: Enter a descriptive name, e.g. ‘Alice Wynton Share with Bob’\nOptionally add a “Description” or “Keywords”.\nClick ‘Create Collection’\n\nOn the ‘Permission’ tab, click ‘Add Permissions - Share With’ and Enter:\n\nDirectory: /\nShare with: ‘user’ (the default)\nUsername of Email: the Globus ID email address of the user you’ll be sharing with. If they don’t have a Globus ID email address, use their regular email – they’ll be prompted to create a Globus ID.\nPermissions: ‘read’ (user can download) or ‘write’ (user can also upload)\nClick ‘Add Permission’\n\n\nAt that point you should be all set. Globus will send an email to the person you are sharing the folder with, and you can let them know the share is active.\n\nIt is only possible to have one Globus Guest Collection per Wynton user. However, you can add multiple permissions to your Guest Collection, giving different people access to different subdirectories. If you have questions, send mail to support@wynton.ucsf.edu.\n\n\n\n\nWhen you’re done, log into Globus, go to ‘Collections’ and then ‘Administered by You’. Click on the shared Collection and then ‘Delete Collection’. Email support@wynton.ucsf.edu to let us know you’ve deleted the Collection."
  },
  {
    "objectID": "hpc/transfers/globus-sharing.html#creating-a-shared-globus-collection",
    "href": "hpc/transfers/globus-sharing.html#creating-a-shared-globus-collection",
    "title": "Globus File Sharing",
    "section": "",
    "text": "Note, Guest Collections/Shared Globus Collections are not allowed for Wynton Protected users.\n\n\nMake sure you have a working setup of Globus for Wynton\nEmail support@wynton.ucsf.edu with the directory you want to share. The admins will send back the path the Globus share will appear as, e.g. /wynton/globus/share/alice\nOn the ‘UCSF Wynton’ Collection page, click the ‘Collections’ tab to get to ‘Guest Collections’:\n\nClick the ‘Add a Guest Collection’ button\n\nFill out the ‘Create New Guest Collection’ form:\n\nDirectory: Enter the path you received from the admins, e.g. /wynton/globus/share/alice\nDisplay Name: Enter a descriptive name, e.g. ‘Alice Wynton Share with Bob’\nOptionally add a “Description” or “Keywords”.\nClick ‘Create Collection’\n\nOn the ‘Permission’ tab, click ‘Add Permissions - Share With’ and Enter:\n\nDirectory: /\nShare with: ‘user’ (the default)\nUsername of Email: the Globus ID email address of the user you’ll be sharing with. If they don’t have a Globus ID email address, use their regular email – they’ll be prompted to create a Globus ID.\nPermissions: ‘read’ (user can download) or ‘write’ (user can also upload)\nClick ‘Add Permission’\n\n\nAt that point you should be all set. Globus will send an email to the person you are sharing the folder with, and you can let them know the share is active.\n\nIt is only possible to have one Globus Guest Collection per Wynton user. However, you can add multiple permissions to your Guest Collection, giving different people access to different subdirectories. If you have questions, send mail to support@wynton.ucsf.edu."
  },
  {
    "objectID": "hpc/transfers/globus-sharing.html#removing-a-shared-globus-collection",
    "href": "hpc/transfers/globus-sharing.html#removing-a-shared-globus-collection",
    "title": "Globus File Sharing",
    "section": "",
    "text": "When you’re done, log into Globus, go to ‘Collections’ and then ‘Administered by You’. Click on the shared Collection and then ‘Delete Collection’. Email support@wynton.ucsf.edu to let us know you’ve deleted the Collection."
  },
  {
    "objectID": "hpc/scheduler/envvars.html",
    "href": "hpc/scheduler/envvars.html",
    "title": "Useful Job Environment Variables",
    "section": "",
    "text": "Some of the environment variables set by the scheduler and available to a job at runtime:\n\nHOSTNAME (string) - the name of the machine where the job is launched\nJOB_ID (integer) - a unique job identifier\nJOB_NAME (string) - the name of the job as it appears on the queue\nNHOSTS (integer) - the number of hosts for a parallel job (default: 1)\nNSLOTS (integer) - the number of slots allocated for a parallel job (default: 1)\nPE_HOSTFILE (string) - the absolute path of a file with rows of machines allocated to a parallel job\nSGE_GPU (comma-separated integers or undefined) - set of GPU core indices allocated to a GPU job (default: undefined)\nSGE_TASK_ID (integer or undefined) - subtask identifier for array jobs (default: undefined)\nTMPDIR (string) - the absolute path to a job-specific temporary directory (local on the compute node and owned by $USER) that is automatically removed when the job finishes\n\n\n\nTo see all environment variable available to a job, we can submit a quick job that outputs all environment variables and their values sorted by name;\n$ cd ~/test/\n$ echo \"env | sort\" | qsub -cwd -j yes -pe smp 4 -l h_rt=00:01:00 -N envvar\nYour job 453717 (\"envvar\") has been submitted\nWhen finished, check the content of the job output file:\n$ grep -E \"(HOSTNAME|PWD|TMPDIR|NSLOTS|NHOSTS|SGE_|JOB_|PE_)\" envvar.o453717\nHOSTNAME=qb3-id14\nJOB_ID=453717\nJOB_NAME=envvar\nJOB_SCRIPT=/var/spool/sge/wynton/qb3-id14/job_scripts/453717\nNHOSTS=1\nNSLOTS=4\nPE_HOSTFILE=/var/spool/sge/wynton/qb3-id14/active_jobs/453717.1/pe_hostfile\nPWD=/wynton/home/boblab/alice/test\nSGE_ACCOUNT=sge\nSGE_ARCH=lx-amd64\nSGE_BINARY_PATH=/opt/sge/bin/lx-amd64\nSGE_CELL=wynton\nSGE_CLUSTER_NAME=wynton_cluster\nSGE_CWD_PATH=/wynton/home/boblab/alice/test\nSGE_JOB_SPOOL_DIR=/var/spool/sge/wynton/qb3-id14/active_jobs/453717.1\nSGE_O_HOME=/wynton/home/boblab/alice\nSGE_O_HOST=dev3\nSGE_O_LOGNAME=alice\nSGE_O_MAIL=/var/spool/mail/alice\nSGE_O_PATH=/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/wynton/home/boblab/alice/.local/bin:/wynton/home/boblab/alice/bin\nSGE_O_SHELL=/bin/bash\nSGE_O_WORKDIR=/wynton/home/boblab/alice/test\nSGE_ROOT=/opt/sge\nSGE_RSH_COMMAND=builtin\nSGE_STDERR_PATH=/wynton/home/boblab/alice/test/envvar.o453717\nSGE_STDIN_PATH=/dev/null\nSGE_STDOUT_PATH=/wynton/home/boblab/alice/test/envvar.o453717\nSGE_TASK_FIRST=undefined\nSGE_TASK_ID=undefined\nSGE_TASK_LAST=undefined\nSGE_TASK_STEPSIZE=undefined\nTMPDIR=/scratch/453717.1.long.q\n\n\n\nHere are some examples how to get the value of environment variable NSLOTS in some of the most popular programming languages. The value is assigned to a local variable nslots, and if not set, 1 is used as the default value. All examples coerce the value to a numeric value and then outputs a message with the value.\n\n\nnslots=${NSLOTS:-1}\necho \"Number of slots available: ${nslots}\"\n\n\n\nnslots = getenv('NSLOTS');              % env var is always a 'char'\nif (isempty(nslots)) nslots = '1'; end  % default value\nnslots = str2num(nslots);               % coerce to 'double'\nfprintf('Number of slots available: %d\\n', nslots);\nSee also the how-to page on ‘Work with MATLAB’.\n\n\n\nimport os\nnslots = os.getenv('NSLOTS', '1')  # env var is always a 'str'\nnslots = int(nslots)               # coerce to an 'int'\nprint('Number of slots available: ' + nslots)\n\n\n\nnslots &lt;- Sys.getenv(\"NSLOTS\", \"1\")  # env var is always a 'character'\nnslots &lt;- as.integer(nslots)         # coerce to an 'integer'\nmessage(\"Number of slots available: \", nslots)\nSee also the how-to page on ‘Work with R’.\n\n\n\nnslots = ENV[\"NSLOTS\"] || \"1\"        # env var is always a 'String'\nnslots = nslots.to_i                 # coerce to 'Integer'\nputs \"Number of slots available: #{nslots}\""
  },
  {
    "objectID": "hpc/scheduler/envvars.html#example",
    "href": "hpc/scheduler/envvars.html#example",
    "title": "Useful Job Environment Variables",
    "section": "",
    "text": "To see all environment variable available to a job, we can submit a quick job that outputs all environment variables and their values sorted by name;\n$ cd ~/test/\n$ echo \"env | sort\" | qsub -cwd -j yes -pe smp 4 -l h_rt=00:01:00 -N envvar\nYour job 453717 (\"envvar\") has been submitted\nWhen finished, check the content of the job output file:\n$ grep -E \"(HOSTNAME|PWD|TMPDIR|NSLOTS|NHOSTS|SGE_|JOB_|PE_)\" envvar.o453717\nHOSTNAME=qb3-id14\nJOB_ID=453717\nJOB_NAME=envvar\nJOB_SCRIPT=/var/spool/sge/wynton/qb3-id14/job_scripts/453717\nNHOSTS=1\nNSLOTS=4\nPE_HOSTFILE=/var/spool/sge/wynton/qb3-id14/active_jobs/453717.1/pe_hostfile\nPWD=/wynton/home/boblab/alice/test\nSGE_ACCOUNT=sge\nSGE_ARCH=lx-amd64\nSGE_BINARY_PATH=/opt/sge/bin/lx-amd64\nSGE_CELL=wynton\nSGE_CLUSTER_NAME=wynton_cluster\nSGE_CWD_PATH=/wynton/home/boblab/alice/test\nSGE_JOB_SPOOL_DIR=/var/spool/sge/wynton/qb3-id14/active_jobs/453717.1\nSGE_O_HOME=/wynton/home/boblab/alice\nSGE_O_HOST=dev3\nSGE_O_LOGNAME=alice\nSGE_O_MAIL=/var/spool/mail/alice\nSGE_O_PATH=/opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/wynton/home/boblab/alice/.local/bin:/wynton/home/boblab/alice/bin\nSGE_O_SHELL=/bin/bash\nSGE_O_WORKDIR=/wynton/home/boblab/alice/test\nSGE_ROOT=/opt/sge\nSGE_RSH_COMMAND=builtin\nSGE_STDERR_PATH=/wynton/home/boblab/alice/test/envvar.o453717\nSGE_STDIN_PATH=/dev/null\nSGE_STDOUT_PATH=/wynton/home/boblab/alice/test/envvar.o453717\nSGE_TASK_FIRST=undefined\nSGE_TASK_ID=undefined\nSGE_TASK_LAST=undefined\nSGE_TASK_STEPSIZE=undefined\nTMPDIR=/scratch/453717.1.long.q"
  },
  {
    "objectID": "hpc/scheduler/envvars.html#environment-variables-in-different-languages",
    "href": "hpc/scheduler/envvars.html#environment-variables-in-different-languages",
    "title": "Useful Job Environment Variables",
    "section": "",
    "text": "Here are some examples how to get the value of environment variable NSLOTS in some of the most popular programming languages. The value is assigned to a local variable nslots, and if not set, 1 is used as the default value. All examples coerce the value to a numeric value and then outputs a message with the value.\n\n\nnslots=${NSLOTS:-1}\necho \"Number of slots available: ${nslots}\"\n\n\n\nnslots = getenv('NSLOTS');              % env var is always a 'char'\nif (isempty(nslots)) nslots = '1'; end  % default value\nnslots = str2num(nslots);               % coerce to 'double'\nfprintf('Number of slots available: %d\\n', nslots);\nSee also the how-to page on ‘Work with MATLAB’.\n\n\n\nimport os\nnslots = os.getenv('NSLOTS', '1')  # env var is always a 'str'\nnslots = int(nslots)               # coerce to an 'int'\nprint('Number of slots available: ' + nslots)\n\n\n\nnslots &lt;- Sys.getenv(\"NSLOTS\", \"1\")  # env var is always a 'character'\nnslots &lt;- as.integer(nslots)         # coerce to an 'integer'\nmessage(\"Number of slots available: \", nslots)\nSee also the how-to page on ‘Work with R’.\n\n\n\nnslots = ENV[\"NSLOTS\"] || \"1\"        # env var is always a 'String'\nnslots = nslots.to_i                 # coerce to 'Integer'\nputs \"Number of slots available: #{nslots}\""
  },
  {
    "objectID": "hpc/scheduler/list-jobs.html",
    "href": "hpc/scheduler/list-jobs.html",
    "title": "List Jobs",
    "section": "",
    "text": "You can list all your submitted jobs, queued and running, using:\nqstat\nTo get detailed information on a specific job, your or others, use:\nqstat -j &lt;job_id&gt;\n\n\nTo see jobs of another user, use:\nqstat -u &lt;user&gt;\nTo see jobs of all users, use:\nqstat -u '*'\nFor example,\n$ qstat -u '*'\njob-ID  prior   name       user state submit/start at     queue           slots ja-task-ID\n------------------------------------------------------------------------------------------\n380725 0.05508 job_qb3.sh alice     r 04/20/2018 15:24:13 long.q@mac-inst-hmid1      24\n380768 0.16819 proc0009.s carol     r 04/20/2018 15:00:28 long.q@msg-hmio3            4\n380815 0.05508 job_qb3.sh alice     r 04/20/2018 15:59:58 long.q@mac-inst-id1        24\n382375 0.05508 job_qb3.sh alice     r 04/21/2018 13:37:13 ondemand.q@mac-inst-id3    24\n386295 0.06649 se_psev_sc bob       r 04/25/2018 08:42:39 long.q@msg-id19             6\n386435 0.16819 proc0010.s carol     r 04/25/2018 10:54:24 member.q@cdhi-idgpu1        4\n386532 0.05055 GATK_SUB_G charlie   r 04/25/2018 12:46:54 ondemand.q@mac-inst-id1    24\n386533 0.05055 GATK_SUB_G charlie   r 04/25/2018 12:46:54 ondemand.q@mac-inst-hmid1  24\n386435 0.16819 proc0010.s carol     r 04/25/2018 10:54:24 member.q@cdhi-idgpu1        4\n386594 0.05055 GATK_SUB_G charlie   r 04/26/2018 00:22:30 long.q@cdhi-idgpu1         24\n387048 0.17577 C3Db2_REST bob       r 04/25/2018 23:02:30 gpu.q@msg-iogpu11           4\n387524 0.16566 class3d    alice     r 04/26/2018 10:34:00 gpu.q@msg-iogpu9            2\n387674 0.16566 class3d    alice     r 04/26/2018 13:40:30 gpu.q@msg-ihgpu3            2\n388578 0.05004 pedigree1  bob       r 04/27/2018 10:57:45 long.q@msg-ihgpu2           1 2\n388578 0.05004 pedigree2  bob       r 04/27/2018 10:57:45 long.q@msg-iogpu4           1 3\n388806 0.05004 run_cluste bob       r 04/27/2018 15:06:33 long.q@qb3-id3              1 20\n388693 0.50000 run_0423.s carol    qw 04/27/2018 12:53:02                             2\n388745 0.41908 test       carol   Eqw 04/27/2018 14:00:42                             3\n372081 0.16240 DIAGS      alice    qw 04/12/2018 10:44:20                             1\n\n\n\nIn the output of qstat, there is a state column, which shows the current state of each job. Commonly seen states are:\n\nq - the job is queued\nw - the job is waiting to be launched\nh - the job is held on the queue, e.g. waiting for another job dependency to finish\nr - the job is currently running on a compute node\nE - there was an error launching the job. See qstat -j &lt;job_id&gt; for the reason why the job failed\nd - the job is being deleted (from calling qdel)\n\nFor more details, see man qstat.\n\n\n\nWhen your jobs will be launched depends on your jobs’ current priority on the queue. If one of your jobs is on the top of the priority queue and the resources (CPU, memory, …) you have requested are available, then that job will be launched next. If sufficient resources are not available, then a lower-priority jobs with lower resource may be launched in the meantime.\nThe priority scores of all jobs can be seen in column prior in the qstat -u '*' output (*). The “priority scores” are constantly recalculated as a function of all users’ jobs currently queued and running on the cluster. They are a function of:\n\nyour group’s current priority\nyour personal current priority relative to other users in your group\n\nThere is no memory, that is, what you, your group, or others have run in the past does not matter.\n(*) The priorities of already running jobs (those with an r in column state) are irrelevant."
  },
  {
    "objectID": "hpc/scheduler/list-jobs.html#list-jobs-of-other-users",
    "href": "hpc/scheduler/list-jobs.html#list-jobs-of-other-users",
    "title": "List Jobs",
    "section": "",
    "text": "To see jobs of another user, use:\nqstat -u &lt;user&gt;\nTo see jobs of all users, use:\nqstat -u '*'\nFor example,\n$ qstat -u '*'\njob-ID  prior   name       user state submit/start at     queue           slots ja-task-ID\n------------------------------------------------------------------------------------------\n380725 0.05508 job_qb3.sh alice     r 04/20/2018 15:24:13 long.q@mac-inst-hmid1      24\n380768 0.16819 proc0009.s carol     r 04/20/2018 15:00:28 long.q@msg-hmio3            4\n380815 0.05508 job_qb3.sh alice     r 04/20/2018 15:59:58 long.q@mac-inst-id1        24\n382375 0.05508 job_qb3.sh alice     r 04/21/2018 13:37:13 ondemand.q@mac-inst-id3    24\n386295 0.06649 se_psev_sc bob       r 04/25/2018 08:42:39 long.q@msg-id19             6\n386435 0.16819 proc0010.s carol     r 04/25/2018 10:54:24 member.q@cdhi-idgpu1        4\n386532 0.05055 GATK_SUB_G charlie   r 04/25/2018 12:46:54 ondemand.q@mac-inst-id1    24\n386533 0.05055 GATK_SUB_G charlie   r 04/25/2018 12:46:54 ondemand.q@mac-inst-hmid1  24\n386435 0.16819 proc0010.s carol     r 04/25/2018 10:54:24 member.q@cdhi-idgpu1        4\n386594 0.05055 GATK_SUB_G charlie   r 04/26/2018 00:22:30 long.q@cdhi-idgpu1         24\n387048 0.17577 C3Db2_REST bob       r 04/25/2018 23:02:30 gpu.q@msg-iogpu11           4\n387524 0.16566 class3d    alice     r 04/26/2018 10:34:00 gpu.q@msg-iogpu9            2\n387674 0.16566 class3d    alice     r 04/26/2018 13:40:30 gpu.q@msg-ihgpu3            2\n388578 0.05004 pedigree1  bob       r 04/27/2018 10:57:45 long.q@msg-ihgpu2           1 2\n388578 0.05004 pedigree2  bob       r 04/27/2018 10:57:45 long.q@msg-iogpu4           1 3\n388806 0.05004 run_cluste bob       r 04/27/2018 15:06:33 long.q@qb3-id3              1 20\n388693 0.50000 run_0423.s carol    qw 04/27/2018 12:53:02                             2\n388745 0.41908 test       carol   Eqw 04/27/2018 14:00:42                             3\n372081 0.16240 DIAGS      alice    qw 04/12/2018 10:44:20                             1"
  },
  {
    "objectID": "hpc/scheduler/list-jobs.html#explanation-of-the-job-state",
    "href": "hpc/scheduler/list-jobs.html#explanation-of-the-job-state",
    "title": "List Jobs",
    "section": "",
    "text": "In the output of qstat, there is a state column, which shows the current state of each job. Commonly seen states are:\n\nq - the job is queued\nw - the job is waiting to be launched\nh - the job is held on the queue, e.g. waiting for another job dependency to finish\nr - the job is currently running on a compute node\nE - there was an error launching the job. See qstat -j &lt;job_id&gt; for the reason why the job failed\nd - the job is being deleted (from calling qdel)\n\nFor more details, see man qstat."
  },
  {
    "objectID": "hpc/scheduler/list-jobs.html#when-will-queued-jobs-start",
    "href": "hpc/scheduler/list-jobs.html#when-will-queued-jobs-start",
    "title": "List Jobs",
    "section": "",
    "text": "When your jobs will be launched depends on your jobs’ current priority on the queue. If one of your jobs is on the top of the priority queue and the resources (CPU, memory, …) you have requested are available, then that job will be launched next. If sufficient resources are not available, then a lower-priority jobs with lower resource may be launched in the meantime.\nThe priority scores of all jobs can be seen in column prior in the qstat -u '*' output (*). The “priority scores” are constantly recalculated as a function of all users’ jobs currently queued and running on the cluster. They are a function of:\n\nyour group’s current priority\nyour personal current priority relative to other users in your group\n\nThere is no memory, that is, what you, your group, or others have run in the past does not matter.\n(*) The priorities of already running jobs (those with an r in column state) are irrelevant."
  },
  {
    "objectID": "hpc/scheduler/job-summary.html",
    "href": "hpc/scheduler/job-summary.html",
    "title": "Job Summary",
    "section": "",
    "text": "The more accurately you can specify the required resources (memory, running time, local scratch needs, …) of your jobs, the better the job scheduler can serve your needs and often your jobs will be processed sooner. For instance, if you have a good sense of the amount of memory and run time your job needs, then you can specify these via SGE resource options mem_free and h_rt. If you don’t specify them, your job will use the default settings.\nIf you don’t know how much resources your job consumes, you can add qstat -j $JOB_ID to the end of your job script. This will output a summary of your job to the job output log. Here is an example of a job that runs R, draws 100 million random numbers and calculates their summary statistics. We also call the garbage collector 1,000 times to emulate some CPU processing to give the scheduler enough time to snapshot the job. At the end, we output the job summary.\n#!/bin/env bash\n#$ -S /bin/bash  # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd          # use current working directory\n#$ -j yes        # merge stdout and stderr\n\n## Summarize 100 million random numbers in R, which occupies\n## 100e6 * 8 bytes = 0.80 GB of RAM.\n## We also run the garbage collector 1,000 times to\n## emulate some CPU processing time\nRscript -e \"x &lt;- rnorm(100e6); for (i in 1:1e3) gc(); summary(x)\"\n\n## End-of-job summary, if running as a job\n[[ -n \"$JOB_ID\" ]] && qstat -j \"$JOB_ID\"\nAs a first guess, we can assume that this script takes at most 5 minutes to run, but let’s assume we don’t have a good sense on how much memory it will consume, so we submit it as:\n$ qsub -l h_rt=00:05:00 job_summary.sge\nYour job 2854740 (\"job_summary.sge\") has been submitted\nWhen the job completes, we can find the resources as part of the output file:\n$ grep \"usage\" job_summary.sge.o2854740\nusage         1:            cpu=00:00:14, mem=6.82412 GB s, io=0.00903 GB, vmem=810.203M, maxvmem=810.203M\nThe full details are available at the end.\nWith this information, we can narrow down that the total processing time was 14 seconds (cpu=00:00:14) and that the maximum amount of virtual memory used was ~810 MB (maxvmem=810.203M). With the help of cpu and maxvmem from previous runs, we can re-submit this job script with more relevant resource specifications;\n$ qsub -l h_rt=00:01:00 -l mem_free=1G job_summary.sge\nComment: Note that the mem_free value has unit GB s (GB * seconds), which is because it is the “accumulated memory usage of the job in Gbytes seconds”.\nHere is an example of the job summary outputted from the above job script:\n$ cat job_summary.sge.o2854740\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n-5.379004 -0.674312  0.000100  0.000069  0.674541  6.314641\n==============================================================\njob_number:                 2854740\nexec_file:                  job_scripts/2854740\nsubmission_time:            Fri Nov  2 22:32:57 2018\nowner:                      alice\nuid:                        59999\ngroup:                      boblab\ngid:                        34001\nsge_o_home:                 /wynton/home/alice\nsge_o_log_name:             alice\nsge_o_path:                 /opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/wynton/home/alice/.local/bin:/wynton/home/alice/bin\nsge_o_shell:                /bin/bash\nsge_o_workdir:              /wynton/home/alice/ex\nsge_o_host:                 dev1\naccount:                    sge\ncwd:                        /wynton/home/alice/ex\nmerge:                      y\nhard resource_list:         h_rt=300\nmail_list:                  alice@dev1\nnotify:                     FALSE\njob_name:                   job_summary.sge\njobshare:                   0\nhard_queue_list:            !gpu.q\nenv_list:                   TERM=NONE\nscript_file:                job_summary.sge\nproject:                    boblab\nbinding:                    NONE\njob_type:                   NONE\nusage         1:            cpu=00:00:14, mem=6.82412 GB s, io=0.00903 GB, vmem=810.203M, maxvmem=810.203M\nbinding       1:            NONE\nscheduling info:            queue instance \"long.q@msg-test3\" dropped because it is temporarily not available\n                            queue instance \"long.q@msg-iogpu14\" dropped because it is temporarily not available\n                            ...\n                            queue instance \"long.q@qb3-test3\" dropped because it is disabled\n                            queue instance \"long.q@qb3-test7\" dropped because it is disabled\n                            ...\n                            queue instance \"gpu.q@msg-iogpu12\" dropped because it is full\n                            queue instance \"gpu.q@msg-ihgpu2\" dropped because it is full\n                            queue instance \"gpu.q@msg-iogpu13\" dropped because it is full\n\n\n\nSometimes your job “just dies”. There is often a simply explanation to this but finding out why can be complicated at first, especially if there are no clues in the job log files.\nOne common reason for jobs terminating early is that it ran out of the requested runtime (-l h_rt=&lt;runtime&gt;). When that happens, the log files appear to have ended abruptly (which is not too far from the truth). It is tempting to try us qstat -j &lt;jobid&gt; to find out more;\n$ qstat -j 191442\nFollowing jobs do not exist: \n191442\nUnfortunately, that does not work because the job no longer exist. Instead, we have to go look into the SGE logs. More specifically, we can scan the SGE “accounting” file, which records summaries of all jobs, for information on our terminated job. The SGE accounting file is huge so we don’t want to scan all of it. Instead, we search only the end of it but making a best guess of what “end of it” means. Below, we start at the 100,000 last rows and scan for our job. If no output is produced, try to increase the number of lines scanned.\n$ tail -100000 /opt/sge/wynton/common/accounting | qacct -f - -j 191442\n==============================================================\nqname        long.q\nhostname     qb3-id95\ngroup        boblab\nowner        alice\nproject      boblab\ndepartment   defaultdepartment\njobname      run.sge\njobnumber    191442\ntaskid       undefined\naccount      sge\npriority     19\nqsub_time    Wed Feb 12 20:59:18 2020\nstart_time   Wed Feb 12 21:01:50 2020\nend_time     Wed Feb 12 22:01:50 2020\ngranted_pe   smp\nslots        10\nfailed       37  : qmaster enforced h_rt, h_cpu, or h_vmem limit\nexit_status  137                  (Killed)\nru_wallclock 3600s\nru_utime     1.373s\nru_stime     0.883s\nru_maxrss    58.508KB\nru_ixrss     0.000B\nru_ismrss    0.000B\nru_idrss     0.000B\nru_isrss     0.000B\nru_minflt    194382\nru_majflt    44\nru_nswap     0\nru_inblock   26278\nru_oublock   563\nru_msgsnd    0\nru_msgrcv    0\nru_nsignals  0\nru_nvcsw     10787\nru_nivcsw    316\ncpu          12195.120s\nmem          4.030TBs\nio           379.956GB\niow          0.000s\nmaxvmem      20.610GB\narid         undefined\nar_sub_time  undefined\ncategory     -u alice -q !gpu.q -l h_rt=3600,mem_free=25G -pe smp 10\nWhen looking at this output, we first focus on the lines:\nfailed       37  : qmaster enforced h_rt, h_cpu, or h_vmem limit\nexit_status  137                  (Killed)\nFirst of all, the exit_status line is not zero (0); any software with an exit code other than zero indicates that something went wrong. It could be due to an error (typically exit_status = 1), or as here 137 with suggests that the job was “killed”. If we look at failed, we see that some rules were enforced, which in our case suggests that the rule for resource h_rt was enforced.\nNext, if we look at:\nru_wallclock 3600s\nru_utime     1.373s\nru_stime     0.883s\nWe see that the job maxed out at a ru_wallclock runtime at 3600 seconds, i.e. 1 hour. This is indeed the maximum runtime requested, which we can infer from:\ncategory     -u alice -q !gpu.q -l h_rt=3600,mem_free=25G -pe smp 10\nConclusion: Job 191442 was killed by the job scheduler because it ran out of its requested runtime.\nComment: For jobs that finished on or prior to 2022-03-02, use the SGE accounting file /opt/sge/wynton/common/accounting-20220302.\n\n\nYou can read about all qacct output fields and what they mean in man sge_status and man accounting."
  },
  {
    "objectID": "hpc/scheduler/job-summary.html#output-job-details-at-end-of-job",
    "href": "hpc/scheduler/job-summary.html#output-job-details-at-end-of-job",
    "title": "Job Summary",
    "section": "",
    "text": "The more accurately you can specify the required resources (memory, running time, local scratch needs, …) of your jobs, the better the job scheduler can serve your needs and often your jobs will be processed sooner. For instance, if you have a good sense of the amount of memory and run time your job needs, then you can specify these via SGE resource options mem_free and h_rt. If you don’t specify them, your job will use the default settings.\nIf you don’t know how much resources your job consumes, you can add qstat -j $JOB_ID to the end of your job script. This will output a summary of your job to the job output log. Here is an example of a job that runs R, draws 100 million random numbers and calculates their summary statistics. We also call the garbage collector 1,000 times to emulate some CPU processing to give the scheduler enough time to snapshot the job. At the end, we output the job summary.\n#!/bin/env bash\n#$ -S /bin/bash  # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd          # use current working directory\n#$ -j yes        # merge stdout and stderr\n\n## Summarize 100 million random numbers in R, which occupies\n## 100e6 * 8 bytes = 0.80 GB of RAM.\n## We also run the garbage collector 1,000 times to\n## emulate some CPU processing time\nRscript -e \"x &lt;- rnorm(100e6); for (i in 1:1e3) gc(); summary(x)\"\n\n## End-of-job summary, if running as a job\n[[ -n \"$JOB_ID\" ]] && qstat -j \"$JOB_ID\"\nAs a first guess, we can assume that this script takes at most 5 minutes to run, but let’s assume we don’t have a good sense on how much memory it will consume, so we submit it as:\n$ qsub -l h_rt=00:05:00 job_summary.sge\nYour job 2854740 (\"job_summary.sge\") has been submitted\nWhen the job completes, we can find the resources as part of the output file:\n$ grep \"usage\" job_summary.sge.o2854740\nusage         1:            cpu=00:00:14, mem=6.82412 GB s, io=0.00903 GB, vmem=810.203M, maxvmem=810.203M\nThe full details are available at the end.\nWith this information, we can narrow down that the total processing time was 14 seconds (cpu=00:00:14) and that the maximum amount of virtual memory used was ~810 MB (maxvmem=810.203M). With the help of cpu and maxvmem from previous runs, we can re-submit this job script with more relevant resource specifications;\n$ qsub -l h_rt=00:01:00 -l mem_free=1G job_summary.sge\nComment: Note that the mem_free value has unit GB s (GB * seconds), which is because it is the “accumulated memory usage of the job in Gbytes seconds”.\nHere is an example of the job summary outputted from the above job script:\n$ cat job_summary.sge.o2854740\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n-5.379004 -0.674312  0.000100  0.000069  0.674541  6.314641\n==============================================================\njob_number:                 2854740\nexec_file:                  job_scripts/2854740\nsubmission_time:            Fri Nov  2 22:32:57 2018\nowner:                      alice\nuid:                        59999\ngroup:                      boblab\ngid:                        34001\nsge_o_home:                 /wynton/home/alice\nsge_o_log_name:             alice\nsge_o_path:                 /opt/sge/bin:/opt/sge/bin/lx-amd64:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/wynton/home/alice/.local/bin:/wynton/home/alice/bin\nsge_o_shell:                /bin/bash\nsge_o_workdir:              /wynton/home/alice/ex\nsge_o_host:                 dev1\naccount:                    sge\ncwd:                        /wynton/home/alice/ex\nmerge:                      y\nhard resource_list:         h_rt=300\nmail_list:                  alice@dev1\nnotify:                     FALSE\njob_name:                   job_summary.sge\njobshare:                   0\nhard_queue_list:            !gpu.q\nenv_list:                   TERM=NONE\nscript_file:                job_summary.sge\nproject:                    boblab\nbinding:                    NONE\njob_type:                   NONE\nusage         1:            cpu=00:00:14, mem=6.82412 GB s, io=0.00903 GB, vmem=810.203M, maxvmem=810.203M\nbinding       1:            NONE\nscheduling info:            queue instance \"long.q@msg-test3\" dropped because it is temporarily not available\n                            queue instance \"long.q@msg-iogpu14\" dropped because it is temporarily not available\n                            ...\n                            queue instance \"long.q@qb3-test3\" dropped because it is disabled\n                            queue instance \"long.q@qb3-test7\" dropped because it is disabled\n                            ...\n                            queue instance \"gpu.q@msg-iogpu12\" dropped because it is full\n                            queue instance \"gpu.q@msg-ihgpu2\" dropped because it is full\n                            queue instance \"gpu.q@msg-iogpu13\" dropped because it is full"
  },
  {
    "objectID": "hpc/scheduler/job-summary.html#post-mortem-job-details",
    "href": "hpc/scheduler/job-summary.html#post-mortem-job-details",
    "title": "Job Summary",
    "section": "",
    "text": "Sometimes your job “just dies”. There is often a simply explanation to this but finding out why can be complicated at first, especially if there are no clues in the job log files.\nOne common reason for jobs terminating early is that it ran out of the requested runtime (-l h_rt=&lt;runtime&gt;). When that happens, the log files appear to have ended abruptly (which is not too far from the truth). It is tempting to try us qstat -j &lt;jobid&gt; to find out more;\n$ qstat -j 191442\nFollowing jobs do not exist: \n191442\nUnfortunately, that does not work because the job no longer exist. Instead, we have to go look into the SGE logs. More specifically, we can scan the SGE “accounting” file, which records summaries of all jobs, for information on our terminated job. The SGE accounting file is huge so we don’t want to scan all of it. Instead, we search only the end of it but making a best guess of what “end of it” means. Below, we start at the 100,000 last rows and scan for our job. If no output is produced, try to increase the number of lines scanned.\n$ tail -100000 /opt/sge/wynton/common/accounting | qacct -f - -j 191442\n==============================================================\nqname        long.q\nhostname     qb3-id95\ngroup        boblab\nowner        alice\nproject      boblab\ndepartment   defaultdepartment\njobname      run.sge\njobnumber    191442\ntaskid       undefined\naccount      sge\npriority     19\nqsub_time    Wed Feb 12 20:59:18 2020\nstart_time   Wed Feb 12 21:01:50 2020\nend_time     Wed Feb 12 22:01:50 2020\ngranted_pe   smp\nslots        10\nfailed       37  : qmaster enforced h_rt, h_cpu, or h_vmem limit\nexit_status  137                  (Killed)\nru_wallclock 3600s\nru_utime     1.373s\nru_stime     0.883s\nru_maxrss    58.508KB\nru_ixrss     0.000B\nru_ismrss    0.000B\nru_idrss     0.000B\nru_isrss     0.000B\nru_minflt    194382\nru_majflt    44\nru_nswap     0\nru_inblock   26278\nru_oublock   563\nru_msgsnd    0\nru_msgrcv    0\nru_nsignals  0\nru_nvcsw     10787\nru_nivcsw    316\ncpu          12195.120s\nmem          4.030TBs\nio           379.956GB\niow          0.000s\nmaxvmem      20.610GB\narid         undefined\nar_sub_time  undefined\ncategory     -u alice -q !gpu.q -l h_rt=3600,mem_free=25G -pe smp 10\nWhen looking at this output, we first focus on the lines:\nfailed       37  : qmaster enforced h_rt, h_cpu, or h_vmem limit\nexit_status  137                  (Killed)\nFirst of all, the exit_status line is not zero (0); any software with an exit code other than zero indicates that something went wrong. It could be due to an error (typically exit_status = 1), or as here 137 with suggests that the job was “killed”. If we look at failed, we see that some rules were enforced, which in our case suggests that the rule for resource h_rt was enforced.\nNext, if we look at:\nru_wallclock 3600s\nru_utime     1.373s\nru_stime     0.883s\nWe see that the job maxed out at a ru_wallclock runtime at 3600 seconds, i.e. 1 hour. This is indeed the maximum runtime requested, which we can infer from:\ncategory     -u alice -q !gpu.q -l h_rt=3600,mem_free=25G -pe smp 10\nConclusion: Job 191442 was killed by the job scheduler because it ran out of its requested runtime.\nComment: For jobs that finished on or prior to 2022-03-02, use the SGE accounting file /opt/sge/wynton/common/accounting-20220302.\n\n\nYou can read about all qacct output fields and what they mean in man sge_status and man accounting."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html",
    "href": "hpc/scheduler/submit-jobs.html",
    "title": "Submit Jobs",
    "section": "",
    "text": "Here is what a typical job submission of shell script script.sh would look like from the command line:\nqsub -cwd -pe smp 4 -l mem_free=2G -l scratch=50G -l h_rt=00:20:00 script.sh\nThis job submission will submit script.sh to the job scheduler which will eventually launch the job on one the compute nodes that can meet the resource needs of the job. Exactly, what these options are is explained below sections, but in summary, the above will result in:\n\n-S /bin/bash: the job scheduler will run the script via Bash (important not to forget)\n-cwd: the working directory will be set to the same directory as from where the submission was done\n-pe smp 4: the job will be allotted four slots (“cores”) on a single machine\n-l mem_free=2G: the job will be allotted 2 GiB of RAM per slot, i.e. 8 GiB in total\n-l scratch=50G: the job will be launched on a compute node with at least 50 GiB of local /scratch available\n-l h_rt=00:20:00: the scheduler knows that the job to run no longer than 20 minutes allowing it to be scheduled much sooner than if no run-time was specified\nscript.sh: the shell script to be run\n\n\n\nBefore you can submit jobs to the compute nodes, you should prepare a script like the one below. Split your jobs into smaller tasks varying only in input parameters. You can then submit the jobs from a login node or a dev node.\n#!/bin/bash           # the shell language when run outside of the job scheduler\n#                     # lines starting with #$ is an instruction to the job scheduler\n#$ -S /bin/bash       # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd               # job should run in the current working directory\n#$ -j y               # STDERR and STDOUT should be joined\n#$ -l mem_free=1G     # job requires up to 1 GiB of RAM per slot\n#$ -l scratch=2G      # job requires up to 2 GiB of local /scratch space\n#$ -l h_rt=24:00:00   # job requires up to 24 hours of runtime\n##$ -t 1-10           # array job with 10 tasks (remove first '#' to enable)\n#$ -r y               # if job crashes, it should be restarted\n\n## If you array jobs (option -t), this script will run T times, once per task.\n## For each run, $SGE_TASK_ID is set to the corresponding task index (here 1-10).\n## To configure different parameters for each task index, one can use a Bash \n## array to map from the task index to a parameter string.\n\n## All possible parameters\n# params=(1bac 2xyz 3ijk 4abc 5def 6ghi 7jkl 8mno 9pqr 10stu)\n\n## Select the parameter for the current task index\n## Arrays are indexed from 0, so we subtract one from the task index\n# param=\"${params[$((SGE_TASK_ID - 1))]}\"\n\ndate\nhostname\n\n## End-of-job summary, if running as a job\n[[ -n \"$JOB_ID\" ]] && qstat -j \"$JOB_ID\"  # This is useful for debugging and usage purposes,\n                                          # e.g. \"did my job exceed its memory request?\"\n\n\n\nTo submit a shell script to the scheduler such that it will run in the current working directory (-cwd), use:\nqsub -cwd script.sh\nThe scheduler will assign your job a unique (numeric) job ID.\n\n\n\nUnless specified, the maximum amount of memory used at any time is 1 GiB per slot (-l mem_free=1G). A job that need to use more memory, need to request that when submitted. For example, a job that needs (at most) 10 GiB of memory should be submitted as:\nqsub -cwd -l mem_free=10G script.sh\nThe scheduler will launch this jobs on the first available compute node with that amount of memory available.\nTIPS: Add qstat -j $JOB_ID to the end of your script to find out how much memory and CPU time your job needed. See Job Summary page for more details.\n\nA job that consumes more memory than requested may be terminated by the administrators and in the future possibly automatically by the scheduler. Because of this, you may request a bit more memory in order to give your job some leeway.\n\n\nNote that -l mem_free=size specifies memory per slot, not per job.\n\n\n\n\n\nSpecifying the run time will shorten the queuing time - significantly so for short running jobs.\n\nBy specifying the how long each job will take, the better the scheduler can manage resources and allocate jobs to different nodes. This will also decrease the average waiting time the job will sit in the queue before being launched on a compute node. You can specify the maximum run time (= wall time, not CPU time) for a job using option -l h_rt=HH:MM:SS where HH:MM:SS specifies the number of hours (HH), the number of minutes (MM), and the number of seconds (SS) - all parts must be specified. For instance, the following job is expected to run for at most 3 minutes (180 seconds):\nqsub -cwd -l mem_free=2G -l h_rt=00:03:00 script.sh\n\nIf not specified, the default run time is 14 days, i.e. the maximum allowed. If your job will run more quickly than that, a more accurate estimate will help the scheduler plan better. Note, though, that jobs are killed if they hit the run time request, so be sure to leave some leeway in your request.\n\n\n\n\nEach compute node has 0.1-1.8 TiB of local scratch storage which is fast and ideal for temporary, intermediate data files that are only needed for the length of a job. This scratch storage is unique to each machine and shared among all users and jobs running on the same machine. To minimize the risk of launching a job on a node that have little scratch space left, specify the -l scratch=size resource. For instance, if your job requires 200 GiB of local /scratch space, submit the job using:\nqsub -cwd -l scratch=200G script.sh\nYour job is only guaranteed the amount of available scratch space that you request when it is launched. For more information and best practices, see Using Local /scratch on Compute Nodes.\n\nPlease specify -l scratch=size when using local /scratch and please clean up afterward. This maximizes the chance for compute nodes having enough available space, reduces the queuing times, and minimizes the risk for running out of local scratch.\n\n\nNote that -l scratch=size specifies space per job, not per slot.\n\nIf your job would benefit from extra-fast local scratch storage, then you can request a node with either a SSD or NVMe scratch drive via the following flag:\nqsub -l ssd_scratch=1\n\n\n\nBy default, the scheduler will allocate a single core for your job. To allow the job to use multiple CPU cores, you must run your job in a SGE parallel environment (PE) and tell SGE how many cores the job will use. Please note that jobs using multiple cores running outside of a parallel environment are subject to termination without warning by the Wynton admins. There are four parallel environments on Wynton:\n\nsmp: for single-host parallel jobs using ‘Symmetric multiprocessing (SMP)’\nmpi: for multiple-host parallel jobs based on MPI parallelization\nmpi_onehost: for single-host parallel jobs based on MPI parallelization\nmpi-8: for multi-threaded multi-host jobs based on MPI parallelization\n\nFor any of the above environments, you must request the number of slots needed when you submit the job, e.g. -pe smp 4, -pe mpi_onehost 14, -pe mpi 38, and -pe mpi-8 40.\n\n\nTo request four slots (NSLOTS=4) on a single host, where each slot gets 2 GiB of RAM, for a total of 8 GiB RAM, use:\nqsub -pe smp 4 -l mem_free=2G script.sh\nThe scheduler will make sure your job is launched on a node with at least four slots available.\nNote, when writing your script, use SGE environment variable NSLOTS, which is set to the number of cores that your job was allocated. This way you don’t have to update your script if you request a different number of cores. For instance, if your script runs the BWA alignment, have it specify the number of parallel threads as:\nbwa aln -t \"${NSLOTS:-1}\" ...\nBy using ${NSLOTS:-1}, instead of just ${NSLOTS}, this script will fall back to use a single thread if NSLOTS is not set, e.g. when running the script on your local computer.\n\nDo not use more cores than requested! - a common reason for compute nodes being clogged up and jobs running slowly. A typically mistake is to hard-code the number of cores in the script and then request a different number when submitting the job - using NSLOTS avoids this problem. Another problem is software that by default use all of the machine’s cores - make sure to control for this, e.g. use dedicated command-line option or environment variable for that software. One such environment variable is OMP_NUM_THREADS. For bash scripts, use export OMP_NUM_THREADS=${NSLOTS:-1}.\n\n\n\n\nSimilarly to the SMP parallel environment, we can request slots on a single host for the main purpose of using MPI to orchestrate the parallel processing. For example, to request ten such slots, use:\nqsub -pe mpi_onehost 10 -l mem_free=2G script.sh\nwhere the job script looks like something like:\n#! /usr/bin/env bash\n#$ -S /bin/bash       # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd               # job should run in the current working directory\n\nmodule load mpi/openmpi-x86_64\n\n## Important: Make sure to $NHOSTS here\nmpirun -np $NSLOTS /path/to/the_app\n\n\n\nMPI supports parallelization across multiple hosts where the different parallel processes communicating over the network. To request a, possible, multi-host job, use -pi mpi &lt;slots&gt;. For example,\nqsub -pe mpi 24 -l mem_free=2G script.sh\nrequests 24 slots, each allotted 2 GiB RAM, with a total of 48 GiB RAM.\n\n\n\nIn addition to -pe mpi_onehost &lt;nslots&gt; and -pe mpi &lt;nslots&gt;, Wynton HPC provides a special MPI parallel environment (PE) called mpi-8 that allocates exactly eight (8) slots per node across one or more compute nodes. For instance, to request a Hybrid MPI job with in total forty slots (40), submit it as:\nqsub -pe mpi-8 40 hybrid_mpi.sh\nand make sure that the script (here hybrid_mpi.sh) exports OMP_NUM_THREADS=8 (the eight slots per node) and then launches the MPI application using mpirun -np $(wc -l &lt; \"$PE_HOSTFILE\") /path/to/the_app:\n#! /usr/bin/env bash\n#$ -S /bin/bash       # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd               # job should run in the current working directory\n\nmodule load mpi/openmpi-x86_64\nexport OMP_NUM_THREADS=8\n\n## Important: Don't use -np $NHOSTS here\nmpirun -np $(wc -l &lt; \"$PE_HOSTFILE\") /path/to/the_app\n\nNote: When working with MPI, it is important to use the exact same version as was used to built the software using MPI. Because of this, we always specify the full mpi/&lt;version&gt; path.\n\nNote that mpi-8 jobs must request a multiple of exactly eight (8) slots. If NSLOTS is not a multiple of eight, then the job will be stuck in the queue forever and never run.\n\n\n2023-01-23: Due to an SGE issue, you must not use NHOSTS as in mpirun -np $NHOSTS .... If done, you might overuse one node and underuse another, when requesting 16 or more slots. Until resolved, please use the mpirun -np $(wc -l &lt; \"$PE_HOSTFILE\") ... solution instead.\n\n\n\n\n\nAll Wynton HPC compute nodes have x86-64-compliant CPUs, but they differ in generation. There are four major generations of CPUs - x86-64-v1, x86-64-v2, x86-64-v3, and x86-64-v4 - formally referred to as CPU microarchitecture levels. Most software tools are compiled such they can run on any of these generations, that is, they only require a CPU supporting x86-64-v1. However, some are compiled to take advantage of more modern CPU instructions, and thereby become more performant. Such software tools cannot run on an older generation of CPUs. For example, if a tool uses x86-64-v4 CPU instructions, but is launched on a x86-64-v3 machine, it will terminate, often with an obscure error, e.g.\n *** caught illegal operation ***\naddress 0x2b3a8b234ccd, cause 'illegal operand'\nor\nIllegal instruction (core dumped)\nIf you have a software tool that requires a specific x86-64 level, specify it via the x86-64-v=&lt;level&gt; resource. For example,\nqsub -cwd -l x86-64-v=3 script.sh\nwill launch the job on a compute node with x86-64-v3 CPUs or newer.\n\n\n\nThe majority of the compute nodes have 1 Gbps and 10 Gbps network cards while a few got 40 Gbps cards. A job that requires 10-40 Gbps network speed can request this by specifying the eth_speed=10 (sic!) resource, e.g.\nqsub -cwd -l eth_speed=10 script.sh\nA job requesting eth_speed=40 will end up on a 40 Gbps node, and a job requesting eth_speed=1 (default) will end up on any node.\n\n\n\nYou can pass arguments to a job script similarly to how one passes argument to a script executed on the command line, e.g.\nqsub -cwd -l mem_free=1G script.sh --first=2 --second=true --third='\"some value\"' --debug\nArguments are then passed as if you called the script as script.sh --first=2 --second=true --third=\"some value\" --debug. Note how you have to have an extra layer of single quotes around \"some value\", otherwise script.sh will see --third=some value as two independent arguments (--third=some and value).\n\n\n\nThe qsub command typically takes a job-script file as input, but, alternatively, it can take a sequence of commands on the standard input (stdin) as input. For example, consider you want to run two commands via the job scheduler;\nhostname\ndate --iso-8601=seconds\nInstead of creating a script file with those two commands, you can pass them to sub using echo:\n$ echo \"hostname; date --iso-8601=seconds\" | qsub -S /bin/bash -cwd -l mem_free=1G\nYour job 2225213 (\"STDIN\") has been submitted\n\n\n\nIt is currently not possible to request interactive jobs (aka qlogin). Instead, there are dedicated development nodes that can be used for short-term interactive development needs such building software and prototyping scripts before submitting them to the scheduler.\nComment: MPI stands for ‘Message Passing Interface’.\n\n\n\n\nAn array job is a collection of similar serial jobs which can be submitted and controlled together. For example, -t 1-10 runs 10 tasks in an array. Each task runs the same script, but gets a different value for the $SGE_TASK_ID environment variable (from ‘1’ to ‘10’ in this example). You can use this to choose different inputs or other parameters for each task.\nThe SGE manual pages are installed on the login and dev nodes. For more info on any SGE command, just type the name of command (e.g., man qsub).\n\n\n\n\n\nFor further options and advanced usage, see Advanced Usage of the scheduler."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#sample-submit-script",
    "href": "hpc/scheduler/submit-jobs.html#sample-submit-script",
    "title": "Submit Jobs",
    "section": "",
    "text": "Before you can submit jobs to the compute nodes, you should prepare a script like the one below. Split your jobs into smaller tasks varying only in input parameters. You can then submit the jobs from a login node or a dev node.\n#!/bin/bash           # the shell language when run outside of the job scheduler\n#                     # lines starting with #$ is an instruction to the job scheduler\n#$ -S /bin/bash       # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd               # job should run in the current working directory\n#$ -j y               # STDERR and STDOUT should be joined\n#$ -l mem_free=1G     # job requires up to 1 GiB of RAM per slot\n#$ -l scratch=2G      # job requires up to 2 GiB of local /scratch space\n#$ -l h_rt=24:00:00   # job requires up to 24 hours of runtime\n##$ -t 1-10           # array job with 10 tasks (remove first '#' to enable)\n#$ -r y               # if job crashes, it should be restarted\n\n## If you array jobs (option -t), this script will run T times, once per task.\n## For each run, $SGE_TASK_ID is set to the corresponding task index (here 1-10).\n## To configure different parameters for each task index, one can use a Bash \n## array to map from the task index to a parameter string.\n\n## All possible parameters\n# params=(1bac 2xyz 3ijk 4abc 5def 6ghi 7jkl 8mno 9pqr 10stu)\n\n## Select the parameter for the current task index\n## Arrays are indexed from 0, so we subtract one from the task index\n# param=\"${params[$((SGE_TASK_ID - 1))]}\"\n\ndate\nhostname\n\n## End-of-job summary, if running as a job\n[[ -n \"$JOB_ID\" ]] && qstat -j \"$JOB_ID\"  # This is useful for debugging and usage purposes,\n                                          # e.g. \"did my job exceed its memory request?\""
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#submit-a-script-to-run-in-the-current-working-directory",
    "href": "hpc/scheduler/submit-jobs.html#submit-a-script-to-run-in-the-current-working-directory",
    "title": "Submit Jobs",
    "section": "",
    "text": "To submit a shell script to the scheduler such that it will run in the current working directory (-cwd), use:\nqsub -cwd script.sh\nThe scheduler will assign your job a unique (numeric) job ID."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#specifying-maximum-memory-usage--l-mem_freeamount",
    "href": "hpc/scheduler/submit-jobs.html#specifying-maximum-memory-usage--l-mem_freeamount",
    "title": "Submit Jobs",
    "section": "",
    "text": "Unless specified, the maximum amount of memory used at any time is 1 GiB per slot (-l mem_free=1G). A job that need to use more memory, need to request that when submitted. For example, a job that needs (at most) 10 GiB of memory should be submitted as:\nqsub -cwd -l mem_free=10G script.sh\nThe scheduler will launch this jobs on the first available compute node with that amount of memory available.\nTIPS: Add qstat -j $JOB_ID to the end of your script to find out how much memory and CPU time your job needed. See Job Summary page for more details.\n\nA job that consumes more memory than requested may be terminated by the administrators and in the future possibly automatically by the scheduler. Because of this, you may request a bit more memory in order to give your job some leeway.\n\n\nNote that -l mem_free=size specifies memory per slot, not per job."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#specifying-maximum-run-time--l-h_rttime",
    "href": "hpc/scheduler/submit-jobs.html#specifying-maximum-run-time--l-h_rttime",
    "title": "Submit Jobs",
    "section": "",
    "text": "Specifying the run time will shorten the queuing time - significantly so for short running jobs.\n\nBy specifying the how long each job will take, the better the scheduler can manage resources and allocate jobs to different nodes. This will also decrease the average waiting time the job will sit in the queue before being launched on a compute node. You can specify the maximum run time (= wall time, not CPU time) for a job using option -l h_rt=HH:MM:SS where HH:MM:SS specifies the number of hours (HH), the number of minutes (MM), and the number of seconds (SS) - all parts must be specified. For instance, the following job is expected to run for at most 3 minutes (180 seconds):\nqsub -cwd -l mem_free=2G -l h_rt=00:03:00 script.sh\n\nIf not specified, the default run time is 14 days, i.e. the maximum allowed. If your job will run more quickly than that, a more accurate estimate will help the scheduler plan better. Note, though, that jobs are killed if they hit the run time request, so be sure to leave some leeway in your request."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#using-local-scratch-storage--l-scratchamount",
    "href": "hpc/scheduler/submit-jobs.html#using-local-scratch-storage--l-scratchamount",
    "title": "Submit Jobs",
    "section": "",
    "text": "Each compute node has 0.1-1.8 TiB of local scratch storage which is fast and ideal for temporary, intermediate data files that are only needed for the length of a job. This scratch storage is unique to each machine and shared among all users and jobs running on the same machine. To minimize the risk of launching a job on a node that have little scratch space left, specify the -l scratch=size resource. For instance, if your job requires 200 GiB of local /scratch space, submit the job using:\nqsub -cwd -l scratch=200G script.sh\nYour job is only guaranteed the amount of available scratch space that you request when it is launched. For more information and best practices, see Using Local /scratch on Compute Nodes.\n\nPlease specify -l scratch=size when using local /scratch and please clean up afterward. This maximizes the chance for compute nodes having enough available space, reduces the queuing times, and minimizes the risk for running out of local scratch.\n\n\nNote that -l scratch=size specifies space per job, not per slot.\n\nIf your job would benefit from extra-fast local scratch storage, then you can request a node with either a SSD or NVMe scratch drive via the following flag:\nqsub -l ssd_scratch=1"
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#parallel-processing--pe-type-slots",
    "href": "hpc/scheduler/submit-jobs.html#parallel-processing--pe-type-slots",
    "title": "Submit Jobs",
    "section": "",
    "text": "By default, the scheduler will allocate a single core for your job. To allow the job to use multiple CPU cores, you must run your job in a SGE parallel environment (PE) and tell SGE how many cores the job will use. Please note that jobs using multiple cores running outside of a parallel environment are subject to termination without warning by the Wynton admins. There are four parallel environments on Wynton:\n\nsmp: for single-host parallel jobs using ‘Symmetric multiprocessing (SMP)’\nmpi: for multiple-host parallel jobs based on MPI parallelization\nmpi_onehost: for single-host parallel jobs based on MPI parallelization\nmpi-8: for multi-threaded multi-host jobs based on MPI parallelization\n\nFor any of the above environments, you must request the number of slots needed when you submit the job, e.g. -pe smp 4, -pe mpi_onehost 14, -pe mpi 38, and -pe mpi-8 40.\n\n\nTo request four slots (NSLOTS=4) on a single host, where each slot gets 2 GiB of RAM, for a total of 8 GiB RAM, use:\nqsub -pe smp 4 -l mem_free=2G script.sh\nThe scheduler will make sure your job is launched on a node with at least four slots available.\nNote, when writing your script, use SGE environment variable NSLOTS, which is set to the number of cores that your job was allocated. This way you don’t have to update your script if you request a different number of cores. For instance, if your script runs the BWA alignment, have it specify the number of parallel threads as:\nbwa aln -t \"${NSLOTS:-1}\" ...\nBy using ${NSLOTS:-1}, instead of just ${NSLOTS}, this script will fall back to use a single thread if NSLOTS is not set, e.g. when running the script on your local computer.\n\nDo not use more cores than requested! - a common reason for compute nodes being clogged up and jobs running slowly. A typically mistake is to hard-code the number of cores in the script and then request a different number when submitting the job - using NSLOTS avoids this problem. Another problem is software that by default use all of the machine’s cores - make sure to control for this, e.g. use dedicated command-line option or environment variable for that software. One such environment variable is OMP_NUM_THREADS. For bash scripts, use export OMP_NUM_THREADS=${NSLOTS:-1}.\n\n\n\n\nSimilarly to the SMP parallel environment, we can request slots on a single host for the main purpose of using MPI to orchestrate the parallel processing. For example, to request ten such slots, use:\nqsub -pe mpi_onehost 10 -l mem_free=2G script.sh\nwhere the job script looks like something like:\n#! /usr/bin/env bash\n#$ -S /bin/bash       # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd               # job should run in the current working directory\n\nmodule load mpi/openmpi-x86_64\n\n## Important: Make sure to $NHOSTS here\nmpirun -np $NSLOTS /path/to/the_app\n\n\n\nMPI supports parallelization across multiple hosts where the different parallel processes communicating over the network. To request a, possible, multi-host job, use -pi mpi &lt;slots&gt;. For example,\nqsub -pe mpi 24 -l mem_free=2G script.sh\nrequests 24 slots, each allotted 2 GiB RAM, with a total of 48 GiB RAM.\n\n\n\nIn addition to -pe mpi_onehost &lt;nslots&gt; and -pe mpi &lt;nslots&gt;, Wynton HPC provides a special MPI parallel environment (PE) called mpi-8 that allocates exactly eight (8) slots per node across one or more compute nodes. For instance, to request a Hybrid MPI job with in total forty slots (40), submit it as:\nqsub -pe mpi-8 40 hybrid_mpi.sh\nand make sure that the script (here hybrid_mpi.sh) exports OMP_NUM_THREADS=8 (the eight slots per node) and then launches the MPI application using mpirun -np $(wc -l &lt; \"$PE_HOSTFILE\") /path/to/the_app:\n#! /usr/bin/env bash\n#$ -S /bin/bash       # the shell language when run via the job scheduler [IMPORTANT]\n#$ -cwd               # job should run in the current working directory\n\nmodule load mpi/openmpi-x86_64\nexport OMP_NUM_THREADS=8\n\n## Important: Don't use -np $NHOSTS here\nmpirun -np $(wc -l &lt; \"$PE_HOSTFILE\") /path/to/the_app\n\nNote: When working with MPI, it is important to use the exact same version as was used to built the software using MPI. Because of this, we always specify the full mpi/&lt;version&gt; path.\n\nNote that mpi-8 jobs must request a multiple of exactly eight (8) slots. If NSLOTS is not a multiple of eight, then the job will be stuck in the queue forever and never run.\n\n\n2023-01-23: Due to an SGE issue, you must not use NHOSTS as in mpirun -np $NHOSTS .... If done, you might overuse one node and underuse another, when requesting 16 or more slots. Until resolved, please use the mpirun -np $(wc -l &lt; \"$PE_HOSTFILE\") ... solution instead."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#cpu-architecture-generation--l-x86-64-vlevel",
    "href": "hpc/scheduler/submit-jobs.html#cpu-architecture-generation--l-x86-64-vlevel",
    "title": "Submit Jobs",
    "section": "",
    "text": "All Wynton HPC compute nodes have x86-64-compliant CPUs, but they differ in generation. There are four major generations of CPUs - x86-64-v1, x86-64-v2, x86-64-v3, and x86-64-v4 - formally referred to as CPU microarchitecture levels. Most software tools are compiled such they can run on any of these generations, that is, they only require a CPU supporting x86-64-v1. However, some are compiled to take advantage of more modern CPU instructions, and thereby become more performant. Such software tools cannot run on an older generation of CPUs. For example, if a tool uses x86-64-v4 CPU instructions, but is launched on a x86-64-v3 machine, it will terminate, often with an obscure error, e.g.\n *** caught illegal operation ***\naddress 0x2b3a8b234ccd, cause 'illegal operand'\nor\nIllegal instruction (core dumped)\nIf you have a software tool that requires a specific x86-64 level, specify it via the x86-64-v=&lt;level&gt; resource. For example,\nqsub -cwd -l x86-64-v=3 script.sh\nwill launch the job on a compute node with x86-64-v3 CPUs or newer."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#minimum-network-speed-1-gbps-10-gbps-40-gbps--l-eth_speed-bandwidth",
    "href": "hpc/scheduler/submit-jobs.html#minimum-network-speed-1-gbps-10-gbps-40-gbps--l-eth_speed-bandwidth",
    "title": "Submit Jobs",
    "section": "",
    "text": "The majority of the compute nodes have 1 Gbps and 10 Gbps network cards while a few got 40 Gbps cards. A job that requires 10-40 Gbps network speed can request this by specifying the eth_speed=10 (sic!) resource, e.g.\nqsub -cwd -l eth_speed=10 script.sh\nA job requesting eth_speed=40 will end up on a 40 Gbps node, and a job requesting eth_speed=1 (default) will end up on any node."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#passing-arguments-to-script",
    "href": "hpc/scheduler/submit-jobs.html#passing-arguments-to-script",
    "title": "Submit Jobs",
    "section": "",
    "text": "You can pass arguments to a job script similarly to how one passes argument to a script executed on the command line, e.g.\nqsub -cwd -l mem_free=1G script.sh --first=2 --second=true --third='\"some value\"' --debug\nArguments are then passed as if you called the script as script.sh --first=2 --second=true --third=\"some value\" --debug. Note how you have to have an extra layer of single quotes around \"some value\", otherwise script.sh will see --third=some value as two independent arguments (--third=some and value)."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#submitting-a-string-of-commands-as-job",
    "href": "hpc/scheduler/submit-jobs.html#submitting-a-string-of-commands-as-job",
    "title": "Submit Jobs",
    "section": "",
    "text": "The qsub command typically takes a job-script file as input, but, alternatively, it can take a sequence of commands on the standard input (stdin) as input. For example, consider you want to run two commands via the job scheduler;\nhostname\ndate --iso-8601=seconds\nInstead of creating a script file with those two commands, you can pass them to sub using echo:\n$ echo \"hostname; date --iso-8601=seconds\" | qsub -S /bin/bash -cwd -l mem_free=1G\nYour job 2225213 (\"STDIN\") has been submitted"
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#interactive-jobs",
    "href": "hpc/scheduler/submit-jobs.html#interactive-jobs",
    "title": "Submit Jobs",
    "section": "",
    "text": "It is currently not possible to request interactive jobs (aka qlogin). Instead, there are dedicated development nodes that can be used for short-term interactive development needs such building software and prototyping scripts before submitting them to the scheduler.\nComment: MPI stands for ‘Message Passing Interface’."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#tips",
    "href": "hpc/scheduler/submit-jobs.html#tips",
    "title": "Submit Jobs",
    "section": "",
    "text": "An array job is a collection of similar serial jobs which can be submitted and controlled together. For example, -t 1-10 runs 10 tasks in an array. Each task runs the same script, but gets a different value for the $SGE_TASK_ID environment variable (from ‘1’ to ‘10’ in this example). You can use this to choose different inputs or other parameters for each task.\nThe SGE manual pages are installed on the login and dev nodes. For more info on any SGE command, just type the name of command (e.g., man qsub)."
  },
  {
    "objectID": "hpc/scheduler/submit-jobs.html#see-also",
    "href": "hpc/scheduler/submit-jobs.html#see-also",
    "title": "Submit Jobs",
    "section": "",
    "text": "For further options and advanced usage, see Advanced Usage of the scheduler."
  },
  {
    "objectID": "hpc/scheduler/advanced-usage.html",
    "href": "hpc/scheduler/advanced-usage.html",
    "title": "Advanced Scheduler Usage",
    "section": "",
    "text": "When submitting a job, the job id is outputted to standard output (stdout) as part of a long message, e.g.\n$ qsub -cwd hello_world\nYour job 151711 (\"hello_world\") has been submitted\nAlthough it possible to parse this output string to infer the job id, by adding option -terse only the job id itself is outputted removing any needs for parsing, e.g.\n$ qsub -terse -cwd hello_world\n151712\nUsing Bash syntax, you can capture the job id when submitting the job as:\n$ job_id=$(qsub -terse -cwd hello_world)\n$ echo $job_id\n151720\nThis allows you to pass it in downstream calls, e.g. qstat -j $job_id and qdel $job_id.\n\n\n\nSGE resource hostname can be used to specify which compute nodes to include and exclude when submitting a job. At times, some of the compute nodes have issues, resulting in any job ending up on such a node to fail. Until the admins have disabled the problematic compute node, you can manually avoid it via the hostname resource specification. For example, to avoid compute node qb3-idgpu11, submit the job as:\n$ qsub -l hostname='!qb3-idgpu11' ...\nThe ! symbol means “not”. Note that we must put the hostname specification withing single quotation marks. To avoid more than one problematic compute node, use:\n$ qsub -l hostname='!(qb3-idgpu11|qb3-idgpu13|qb3-idgpu18)' ...\nThe | symbol means “or” and the ! applies to everything within parenthesis. By De Morgan’s Laws, the latter is equivalent to:\n$ qsub -l hostname='!qb3-idgpu11&!qb3-idgpu13&!qb3-idgpu18' ...\nwhere the & symbols means “and”.\nIt is also possible to exclude a set of compute nodes via basic globbing, e.g.\n$ qsub -l hostname='!qb3-idgpu*' ...\n\n\n\nFor more help on the SGE scheduler, please see the Grid Engine HOWTOs page."
  },
  {
    "objectID": "hpc/scheduler/advanced-usage.html#programmatically-get-job-id",
    "href": "hpc/scheduler/advanced-usage.html#programmatically-get-job-id",
    "title": "Advanced Scheduler Usage",
    "section": "",
    "text": "When submitting a job, the job id is outputted to standard output (stdout) as part of a long message, e.g.\n$ qsub -cwd hello_world\nYour job 151711 (\"hello_world\") has been submitted\nAlthough it possible to parse this output string to infer the job id, by adding option -terse only the job id itself is outputted removing any needs for parsing, e.g.\n$ qsub -terse -cwd hello_world\n151712\nUsing Bash syntax, you can capture the job id when submitting the job as:\n$ job_id=$(qsub -terse -cwd hello_world)\n$ echo $job_id\n151720\nThis allows you to pass it in downstream calls, e.g. qstat -j $job_id and qdel $job_id."
  },
  {
    "objectID": "hpc/scheduler/advanced-usage.html#exclude-one-or-more-compute-nodes",
    "href": "hpc/scheduler/advanced-usage.html#exclude-one-or-more-compute-nodes",
    "title": "Advanced Scheduler Usage",
    "section": "",
    "text": "SGE resource hostname can be used to specify which compute nodes to include and exclude when submitting a job. At times, some of the compute nodes have issues, resulting in any job ending up on such a node to fail. Until the admins have disabled the problematic compute node, you can manually avoid it via the hostname resource specification. For example, to avoid compute node qb3-idgpu11, submit the job as:\n$ qsub -l hostname='!qb3-idgpu11' ...\nThe ! symbol means “not”. Note that we must put the hostname specification withing single quotation marks. To avoid more than one problematic compute node, use:\n$ qsub -l hostname='!(qb3-idgpu11|qb3-idgpu13|qb3-idgpu18)' ...\nThe | symbol means “or” and the ! applies to everything within parenthesis. By De Morgan’s Laws, the latter is equivalent to:\n$ qsub -l hostname='!qb3-idgpu11&!qb3-idgpu13&!qb3-idgpu18' ...\nwhere the & symbols means “and”.\nIt is also possible to exclude a set of compute nodes via basic globbing, e.g.\n$ qsub -l hostname='!qb3-idgpu*' ..."
  },
  {
    "objectID": "hpc/scheduler/advanced-usage.html#additional-resources",
    "href": "hpc/scheduler/advanced-usage.html#additional-resources",
    "title": "Advanced Scheduler Usage",
    "section": "",
    "text": "For more help on the SGE scheduler, please see the Grid Engine HOWTOs page."
  },
  {
    "objectID": "hpc/index.html",
    "href": "hpc/index.html",
    "title": "Wynton HPC - UCSF Research Computing at Scale",
    "section": "",
    "text": "2025-07-08: Termination of Wynton Support for P4/PHI data\n\nStarting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton.\n\n\n\nNEWS: (For upcoming and current incidents, see the Status page)\n2025-07-08: Termination of Wynton Support for P4/PHI data. Starting today, all work on P4/PHI level data must be ceased and all P4/PHI data removed from Wynton.\n2025-04-22: PHI Freeze: Effective immediately, we are pausing requests for new projects that process P4 data, specifically Protected Health Information (PHI).\n2025-01-16: The memory limit on development nodes was decreased from 68 GiB to 48 GiB, in order to further lower the risk for these machines to run low on memory resulting in non-responsiveness.\nMore …\n\n\nWynton HPC - UCSF Research Computing at Scale\nWynton HPC is a large, shared high-performance compute (HPC) cluster underlying UCSF’s Research Computing Capability. Funded and administered cooperatively by UCSF campus IT and key research groups, it is available to all UCSF researchers, and consists of different profiles suited to various biomedical and health science computing needs. Researchers can participate using the “co-op” model of resource contribution and sharing.\nThe Wynton HPC environment is available for free to all UCSF researchers. To join, please follow the instructions for requesting an account.\nThe Wynton HPC environment grew as more users discovered it and more groups bought into the co-op model. Each contributing member brings more resources and compute power for everyone based on a fair-share model where contributors get higher access priority than non-contributing members. In addition, several UCSF centers have joined by contributing a large amount of their compute hardware to the cluster, e.g. Memory and Aging Center and QB3.\nPlease note: P4 data, which includes PHI, is prohibited. P3 data may only be used in “Wynton Protected” via a “protected” account. Refer to UCSF Policy 650-16 Addendum F, UCSF Data Classification Standard. For compliant alternatives for working with P4 data, please visit our UCSF Data Storage for Research Page found here: https://secureresearch.ucsf.edu/ucsf-data-storage-research, or contact our cloud services team for a consultation: https://it.ucsf.edu/service/secured-enterprise-cloud-aws.\n\n\nWynton’s Future\nWynton HPC has been a vital resource for UCSF’s computational research, providing shared HPC capabilities to support diverse scientific projects. However, Wynton HPC is being gradually phased out to align with UCSF’s evolving research infrastructure strategy.\nWhile Wynton will remain operational for the near term, researchers are encouraged to plan for its eventual absorption into the FAC and CoreHPC. UCSF is committed to supporting researchers during this transition and ensuring continued access to robust computational resources.\nFor more details on the upcoming changes, visit the Wynton HPC Transition Announcement.\nLearn about the Facility for Advanced Computing (FAC), including pricing options for FAC Capacity Storage."
  },
  {
    "objectID": "hpc/status/incidents-2019.html",
    "href": "hpc/status/incidents-2019.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2019\n\nFull downtime:\n\nScheduled: 96 hours = 4.0 days = 1.1%\nUnscheduled: 83.5 hours = 3.5 days = 1.0%\nTotal: 179.5 hours = 7.5 days = 2.0%\nExternal factors: 15% of the above downtime, corresponding to 26 hours (=1.1 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2021-01-09 (1.0 hours) - job scheduler updates\n2021-07-08 (95 hours)\n\nTotal downtime: 96.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2019-01-22 (up to 14 days)\n2019-03-21 (up to 14 days)\n2019-10-29 (up to 14 days)\n2019-12-22 (up to 14 days)\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2019-07-30 (6.5 hour) - Byers Hall power outage\n2019-08-15 (5.5 hour) - Diller power outage\n2019-10-25 (1.0 hour) - Byers Hall power outage\n2019-10-22 (13.0 hour) - Diller power backup failed during power maintenance\n\nTotal downtime: 26.0 hours of which 26.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2019-01-08 (2.0 hours) - BeeGFS server non-responsive\n2019-01-14 (1.5 hours) - BeeGFS non-responsive\n2019-05-15 (24.5 hours) - BeeGFS non-responsive\n2019-05-17 (5.0 hours) - BeeGFS slowdown\n2019-06-17 (10.5 hours) - BeeGFS non-responsive\n2019-08-23 (4.0 hours) - BeeGFS server non-responsive\n2019-09-24 (3.0 hours) - BeeGFS server non-responsive\n2019-12-18 (3.5 hours) - Network switch upgrade\n2019-12-22 (5.5 hours) - BeeGFS server non-responsive\n\nTotal downtime: 58.5 hours of which 0 hours were due to external factors\n\n\n\nAccounts\n\nNumber of user account: 478 (change: +280 during the year)\n\n\n\n\nDecember 20, 2019 - January 4, 2020\n\nKernel maintenance\nResolved: All compute nodes have been updated and rebooted. Jan 4, 11:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. December 20, 10:20 PT\n\n\n\nDecember 22, 2019\n\nBeeGFS failure\nResolved: No further hiccups were needed during the BeeGFS resynchronization. Everything is working as expected. December 23, 10:00 PT\nUpdate: The issues with login was because the responsiveness of one of the BeeGFS file servers became unreliable around 04:20. Rebooting that server resolved the problem. The cluster is fully functional again although slower than usual until the file system have been resynced. After this, there might be a need for one more, brief, reboot. December 22, 14:40 PT\nNotice: It is not possible to log in to the Wynton HPC environment. The reason is currently not known. December 22, 09:15 PT\n\n\n\n\nDecember 18, 2019\n\nNetwork/login issues\nResolved: The Wynton HPC environment is fully functional again. The BeeGFS filesystem was not working properly during 18:30-22:10 on December 18 resulting in no login access to the cluster and job file I/O being backed up. December 19, 08:50 PT\nUpdate: The BeeGFS filesystem is non-responsive, which we believe is due to the network switch upgrade. December 18, 21:00 PT\nNotice: One of two network switches will be upgraded on Wednesday December 18 starting at 18:00 and lasting a few hours. We do not expect this to impact the Wynton HPC environment other than slowing down the network performance to 50%. December 17, 10:00 PT\n\n\n\n\nOctober 29-November 11, 2019\n\nKernel maintenance\nResolved: All compute nodes have been updated and rebooted. Nov 11, 01:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). GPU nodes will be rebooted as soon as all GPU jobs complete. During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,000 cores) in the graph above. Oct 29, 16:30 PT\n\n\n\nOctober 25, 2019\n\nByers Hall power outage glitch\nResolved: Development node qb3-dev2 was rebooted. Data-transfer node dt1.wynton.ucsf.edu is kept offline because it is scheduled to be upgraded next week. October 28, 15:00 PT\nUpdate: Most compute nodes that went down due to the power glitch has been rebooted. Data-transfer node dt1.wynton.ucsf.edu and development node qb3-dev2 are still down - they will be brought back online on Monday October 28. October 25, 14:00 PT\nNotice: A very brief power outage in the Byers Hall building caused several compute nodes in its Data Center to go down. Jobs that were running on those compute nodes at the time of the power failure did unfortunately fail. Log-in, data-transfer, and development nodes were also affected. All these hosts are currently being rebooted. October 25, 13:00 PT\n\n\n\n\nOctober 24, 2019\n\nLogin non-functional\nResolved: Log in works again. October 24, 09:45 PT\nNotice: It is not possible to log in to the Wynton HPC environment. This is due to a recent misconfiguration of the LDAP server. October 24, 09:30 PT\n\n\n\nOctober 22-23, 2019\n\nBeeGFS failure\nResolved: The Wynton HPC BeeGFS file system is fully functional again. During the outage, /wynton/group and /wynton/scratch was not working properly, whereas /wynton/home was unaffected. October 23, 10:35 PT\nNotice: The Wynton HPC BeeGFS file system is non-functional. It is expected to be resolved by noon on October 23. The underlying problem is that the power backup at the Diller data center did not work as expected during a planned power maintenance. October 22, 21:45 PT\n\n\n\n\nSeptember 24, 2019\n\nBeeGFS failure\nResolved: The Wynton HPC environment is up and running again. September 24, 20:25 PT\nNotice: The Wynton HPC environment is unresponsive. Problem is being investigated. September 24, 17:30 PT\n\n\n\n\nAugust 23, 2019\n\nBeeGFS failure\nResolved: The Wynton HPC environment is up and running again. The reason for this downtime was the BeeGFS file server became unresponsive. August 23, 20:45 PT\nNotice: The Wynton HPC environment is unresponsive. August 23, 16:45 PT\n\n\n\n\nAugust 15, 2019\n\nPower outage\nResolved: The Wynton HPC environment is up and running again. August 15, 21:00 PT\nNotice: The Wynton HPC environment is down due to a non-planned power outage at the Diller data center. Jobs running on compute nodes located in that data center, were terminated. Jobs running elsewhere may also have been affected because /wynton/home went down as well (despite it being mirrored). August 15, 15:45 PT\n\n\n\n\nJuly 30, 2019\n\nPower outage\nResolved: The Wynton HPC environment is up and running again. July 30, 14:40 PT\nNotice: The Wynton HPC environment is down due to a non-planned power outage at the main data center. July 30, 08:20 PT\n\n\n\n\nJuly 8-12, 2019\n\nFull system downtime\nResolved: The Wynton HPC environment and the BeeGFS file system are fully functional after updates and upgrades. July 12, 11:15 PT\nNotice: The Wynton HPC environment is down for maintenance. July 8, 12:00 PT\nNotice: Updates to the BeeGFS file system and the operating system that require to bring down all of Wynton HPC will start on the morning of Monday July 8. Please make sure to log out before then. The downtime might last the full week. July 1, 14:15 PT\n\n\n\n\nJune 17-18, 2019\n\nSignificant file-system outage\nResolved: The BeeGFS file system is fully functional again. June 18, 01:30 PT\nInvestigating: Parts of /wynton/scratch and /wynton/group are currently unavailable. The /wynton/home space should be unaffected. June 17, 15:05 PT\n\n\n\n\nMay 17, 2019\n\nMajor outage due to file-system issues\nResolved: The BeeGFS file system and the cluster is functional again. May 17, 16:00 PT\nInvestigating: There is a major slowdown of the BeeGFS file system (/wynton), which in turn causes significant problems throughout the Wynton HPC environment. May 17, 10:45 PT\n\n\n\n\nMay 15-16, 2019\n\nMajor outage due to file-system issues\nResolved: The BeeGFS file system, and thereby also the cluster itself, is functional again. May 16, 10:30 PT\nInvestigating: The BeeGFS file system (/wynton) is experiencing major issues. This caused all on Wynton HPC to become non-functional. May 15, 10:00 PT\n\n\n\n\nMay 15, 2019\n\nNetwork/login issues\nResolved: The UCSF-wide network issue that affected access to Wynton HPC has been resolved. May 15, 15:30 PT\nUpdate: The login issue is related to UCSF-wide network issues. May 15, 13:30 PT\nInvestigating: There are issues logging in to Wynton HPC. May 15, 10:15 PT\n\n\n\nMarch 21-April 5, 2019\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. April 5, 12:00 PT\nUpdate: Nearly all compute nodes have been rebooted (~5,200 cores are now available). Mar 29, 12:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 5,424 cores) in the graph above. Mar 21, 15:30 PT\n\n\n\nMarch 22, 2019\n\nKernel maintenance\nResolved: The login, development and transfer hosts have been rebooted. March 22, 10:35 PT\nNotice: On Friday March 22 at 10:30am, all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. These hosts should be offline for less than 5 minutes. Mar 21, 15:30 PT\n\n\n\nJanuary 22-February 5, 2019\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. Feb 5, 11:30 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target 1,944 cores) in the graph above. Jan 22, 16:45 PT\n\n\n\nJanuary 23, 2019\n\nKernel maintenance\nResolved: The login, development and transfer hosts have been rebooted. Jan 23, 13:00 PT\nNotice: On Wednesday January 23 at 12:00 (noon), all of the login, development, and data transfer hosts will be rebooted. Please be logged out before then. The hosts should be offline for less than 5 minutes. Jan 22, 16:45 PT\n\n\n\nJanuary 14, 2019\n\nBlocking file-system issues\nResolved: The file system under /wynton/ is back up again. We are looking into the cause and taking steps to prevent this from happening again. Jan 9, 12:45 PT\nInvestigating: The file system under /wynton/ went down around 11:30 resulting is several critical failures including the scheduler failing. Jan 14, 11:55 PT\n\n\n\n\nJanuary 9, 2019\n\nJob scheduler maintenance downtime\nResolved: The SGE job scheduler is now back online and accepts new job submission again. Jan 9, 12:45 PT\nUpdate: The downtime of the job scheduler will begin on Wednesday January 9 @ noon and is expected to be completed by 1:00pm. Jan 8, 16:00 PT\nNotice: There will be a short job-scheduler downtime on Wednesday January 9 due to SGE maintenance. During this downtime, already running jobs will keep running and queued jobs will remain in the queue, but no new jobs can be submitted. Dec 20, 12:00 PT\n\n\n\n\nJanuary 8, 2019\n\nFile-system server crash\nInvestigating: One of the parallel file-system servers (BeeGFS) appears to have crashed on Monday January 7 at 07:30 and was recovered on 9:20pm. Right now we are monitoring its stability, and investigating the cause and what impact it might have had. Currently, we believe users might have experienced I/O errors on /wynton/scratch/ whereas /wynton/home/ was not affected. Jan 8, 10:15 PT"
  },
  {
    "objectID": "hpc/status/incidents-2022.html",
    "href": "hpc/status/incidents-2022.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2022\n\nFull downtime:\n\nScheduled: 94.0 hours = 3.9 days = 1.1%\nUnscheduled: 220.0 hours = 9.2 days = 2.5%\nTotal: 314.0 hours = 13.1 days = 3.6%\nExternal factors: 36% of the above downtime, corresponding to 114 hours (= 4.8 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2022-02-08 (53.5 hours)\n2022-09-27 (40.5 hours)\n\nTotal downtime: 94.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2022-08-05 (up to 14 days)\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\n2022-09-06 (66 hours)\n\nTotal downtime: 66 hours of which 66 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2022-03-28 (1 hours): Major BeeGFS issues\n2022-03-26 (5 hours): Major BeeGFS issues\n2022-03-18 (100 hours): Major BeeGFS issues\n\nTotal downtime: 106.0 hours of which 0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2022-03-26 (48 hours): Data-center cooling issues\n\nTotal downtime: 48 hours of which 48 hours were due to external factors\n\n\n\nAccounts\n\nNumber of user account: 1,643 (change: +369 during the year)\n\n\n\n\nNovember 2, 2022\n\nMajor BeeGFS issues\nResolved: The BeeGFS issues have been resolved. At 05:29 this morning, a local file system hosting one of our 12 BeeGFS meta daemons crashed. Normally, BeeGFS detects this and redirects processing to a secondary, backup daemon. In this incident, this failback did not get activated and a manual intervention was needed. November 2, 09:30 PT\nNotice: The BeeGFS file system started to experience issues early morning on Tuesday 2022-11-02. The symptoms are missing files and folders. November 2, 08:15 PT\n\n\n\n\nNovember 1, 2022\n\nScheduler not available\nResolved: The job scheduler is responsive again, but we are not certain what caused the problem. We will keep monitoring the issue. November 1, 16:30 PT\nNotice: The job scheduler, SGE, does not respond to user requests, e.g. qstat and qsub. No new jobs can be submitted at this time. The first reports on problems came in around 09:00 this morning. We are troubleshooting the problem. November 1, 10:25 PT\n\n\n\n\nSeptember 27-29, 2022\n\nFull downtime\nResolved: The cluster maintenance has completed and the cluster is now fully operational again. September 29, 13:30 PT\nUpdate: The cluster has been shut down for maintenance. September 27, 21:00 PT\nNotice: Wynton will be shut down on Tuesday September 27, 2022 at 21:00. We expect the cluster to be back up by the end of the workday on Thursday September 29. This is done to avoid file-system and hardware failures that otherwise may occur when the UCSF Facilities performs maintenance to the power system in Byers Hall. We will take the opportunity to perform cluster maintenance after the completion of the power-system maintenance. September 14, 17:00 PT\n\n\n\n\nSeptember 6-9, 2022\n\nOutage following campus power glitch\nResolved: As of 09:20 on 2022-09-09, the cluster is back in full operation. The queues are enabled, jobs are running, and the development nodes are accepting logins. September 9, 09:35 PT\nUpdate: Login and data-transfer nodes are disabled to minimize the risk for file corruption. September 7, 12:45 PT\nNotice: The Wynton system experiencing system-wide issues, including the file system, due to a campus power glitch. To minimize the risk of corrupting the file system, it was decided to shut down the job scheduler and terminate all running jobs. The power outage at Mission Bay campus happened at 15:13. Despite diesel-generated backup power started up momentarily, it was enough to affect some of our servers. The job scheduler will be offline until the impact on Wynton is fully investigated. September 6, 16:20 PT\n\n\n\n\nAugust 5-9, 2022\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. Aug 9, 12:00 PT\nNotice: New operating-system kernels are deployed. Login, data-transfer, and development nodes will be rebooted on Monday August 8 at 14:00. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~14,500 cores) in the graph above. Aug 5, 10:30 PT\n\n\n\nAugust 4, 2022\n\nSoftware repository maintenance\nResolved: The Sali lab software module repository is back. Aug 4, 12:00 PT\nNotice: The Sali lab software module repository is back will be unavailable from around 10:30-11:30 today August 4 for maintenance. Aug 4, 03:30 PT\n\n\n\nMarch 28-April 6, 2022\n\nMajor BeeGFS issues\nResolved: The patch of the BeeGFS servers were successfully deployed by 14:30 and went without disruptions. As a side effect, rudimentary benchmarking shows that this patch also improves the overall performance. Since the troubleshooting, bug fixing, and testing started on 2022-03-28, we managed to keep the impact of the bugs to a minimum resulting in only one hour of BeeGFS stall. April 6, 17:00 PT\nUpdate: The BeeGFS servers will be updated tomorrow April 6 at 14:00. The cluster should work as usual during the update. April 5, 17:00 PT\nUpdate: Our load tests over the weekend went well. Next, we will do discrepancy validation tests between our current version and the patch versions. When those pass, we will do a final confirmation with the BeeGFS vendor. We hope to deploy the patch to Wynton in a few days. April 4, 10:30 PT\nUpdate: After a few rounds, we now have a patch that we have confirmed work on our test BeeGFS system. The plan is to do additional high-load testing today and over the weekend. April 1, 10:30 PT\nUpdate: The BeeGFS vendors will send us a patch by tomorrow Tuesday, which we will test on our separate BeeGFS test system. After being validated there, will will deploy it to the main system. We hope to have a patch deploy by the end of the week. March 28, 11:30 PT\nUpdate: We have re-enabled the job scheduler after manually having resolved the BeeGFS meta server issues. We will keep monitoring the problem and send more debug data to the BeeGFS vendors. March 28, 11:00 PT\nNotice: On Monday 2022-03-28 morning at 10:30 the BeeGFS hung again. We put a hold on the job scheduler for now. March 28, 10:30 PT\n\n\n\n\nMarch 26, 2022\n\nJob scheduler is disabled due to cooling issues\nResolved: The compute nodes and the job scheduler are up and running again. March 26, 11:00 PT\nNotice: The job scheduler as disabled and running jobs where terminated on Saturday 2022-03-26 around 09:00. This was done due to an emergency shutdown because the ambient temperature in the data center started to rise around 08:00 and at 09:00 it hit the critical level, where our monitoring system automatically shuts down compute nodes to prevent further damage. This resulted in the room temperature coming down to normal levels again. We are waiting on UCSF Facilities to restore cooling in the data center. March 26, 10:30 PT\n\n\n\n\nMarch 26, 2022\n\nMajor BeeGFS issues\nResolved: Just after 03:00 on Saturday 2022-03-26 morning BeeGFS hung. Recover actions were taken at 07:30 and the problem was resolved before 08:00. We have tracked down the problem occur when a user runs more than one rm -r /wynton/path/to/folder concurrently on the same folder. This is a bug in BeeGFS that vendors is aware of. March 26, 10:30 PT\n\n\n\n\nMarch 18-22, 2022\n\nJob scheduler is disabled because of BeeGFS issues\nResolved: We have re-enabled the job scheduler, which now processes all queued jobs. We will keep working with the BeeGFS vendor to find a solution to avoid this issue from happening again. March 22, 16:30 PT\nUpdate: The BeeGFS issue has been identified. We identified a job that appears to trigger a bug in BeeGFS, which we can reproduce. The BeeGFS vendor will work on a bug fix. The good news is that the job script that triggers the problem can be tweaked to avoid hitting the bug. This means we can enable the job scheduler as soon as all BeeGFS metadata servers have synchronized, which we expect to take a few hours. March 22, 12:00 PT\nUpdate: The BeeGFS file system troubleshooting continues. The job queue is still disabled. You might experience login and non-responsive prompt issues while we troubleshoot this. We have met with the BeeGFS vendors this morning and we are collecting debug information to allow them to troubleshoot the problem on their end. At the same time, we hope to narrow in on the problem further on our end by trying to identify whether there is a particular job or software running on the queue that might cause this. Currently, we have no estimate when this problem will be fixed. We have another call scheduled with the vendor tomorrow morning. March 21, 11:45 PT\nUpdate: The BeeGFS file system is back online and the cluster can be accessed again. However, we had to put SGE in maintenance mode, which means no jobs will be started until the underlying problem, which is still unknown, has been identified and resolved. The plan is to talk to the BeeGFS vendor as soon as possible after the weekend. Unfortunately, in order to stabilize BeeGFS, we had to kill, at 16:30 today, all running jobs and requeue them on the SGE job scheduler. They are now listed as status ‘Rq’. For troubleshooting purposes, please do not delete any of your ‘Rq’ jobs. March 18, 17:05 PT\nNotification: The Wynton environment cannot be accessed at the moment. This is because the global file system, BeeGFS, is experiencing issues. The problem, which started around 11:45 today, is being investigated. March 18, 11:55 PT\n\n\n\n\nMarch 14-15, 2022\n\nBrief network outage\nNoticed: UCSF Network IT will be performing maintenance on several network switches in the evening and overnight on Monday March 14. This will not affect jobs running on the cluster. One of the switches is the one which provides Wynton with external network access. When that switch is rebooted, Wynton will be inaccessible for about 15 minutes. This is likely to happen somewhere between 22:00 and 23:00 that evening, but the outage window extends from 21:00 to 05:00 the following morning, so it could take place anywhere in that window. March 11, 10:15 PT\n\n\n\nFebruary 28-March 2, 2022\n\nFull downtime\nResolved: Wynton is available again. March 2, 15:30 PT\nUpdate: The Wynton environment is now offline for maintenance work. February 28, 10:00 PT\nClarification: The shutdown will take place early Monday morning February 28, 2022. Also, this is on a Monday and not on a Tuesday (as previously written below). February 22, 11:45 PT\nUpdate: We confirm that this downtime will take place as scheduled. February 14, 15:45 PT\nNotice: We are planning a full file-system maintenance starting on Tuesday Monday February 28, 2022. As this requires a full shutdown of the cluster environment, we will start decreasing the job queue, on February 14, two weeks prior to the shutdown. On February 14, jobs that requires 14 days or less to run will be launched. On February 15, only jobs that requires 13 days or less will be launched, and so on until the day of the downtime. Submitted jobs that would go into the downtime window if launched, will only be launched after the downtime window. November 22, 11:45 PT"
  },
  {
    "objectID": "hpc/status/beegfs.html",
    "href": "hpc/status/beegfs.html",
    "title": "Wynton HPC Status Dashboard",
    "section": "",
    "text": "The BeeGFS Metrics graphs are now on the status page."
  },
  {
    "objectID": "hpc/status/incidents-2020.html",
    "href": "hpc/status/incidents-2020.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2020\n\nFull downtime:\n\nScheduled: 123 hours = 5.1 days = 1.4%\nUnscheduled: 91.5 hours = 3.8 days = 1.0%\nTotal: 214.5 hours = 8.9 days = 2.4%\nExternal factors: 12% of the above downtime, corresponding to 26.5 hours (=1.1 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2020-08-10 (93 hours)\n2020-12-07 (30 hours)\n\nTotal downtime: 123 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\n2020-06-11 (up to 14 days)\n2020-12-11 (up to 14 days)\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nNone\n\nTotal downtime: 0 hours\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2020-01-22 (2.5 hours) - BeeGFS failure to failed upgrade\n2020-01-29 (1.0 hours) - BeeGFS non-responsive\n2020-02-05 (51.5 hours) - Legacy NetApp file system failed\n2020-05-22 (0.5 hours) - BeeGFS non-responsive to failed upgrade\n2020-08-19 (1.5 hours) - BeeGFS non-responsive\n2020-10-21 (3 hours) - BeeGFS non-responsive\n2020-11-05 (3 hours) - BeeGFS non-responsive\n\nTotal downtime: 63.0 hours\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2020-05-28 (26.5 hours) - MSG Data Center outage affecting many GPU compute nodes\n2020-07-04 (2 hours) - SGE scheduler failed\n2020-11-04 (288 hours) - ~80 compute nodes lost due to network switch failure\n\nTotal downtime: 28.5 hours of which 26.5 hours were due to external factors\n\n\n\nAccounts\n\nNumber of user account: 864 (change: +386 during the year)\n\n\n\n\nDecember 8-17, 2020\n\nLimited accessibility of Login node log1\nResolved: Login node ‘log1.wynton.ucsf.edu’ can again be accessed from outside of the UCSF network. December 17, 14:20 PT\nNotice: Login node ‘log1.wynton.ucsf.edu’ is only accessible from within UCSF network. This is a side effect of the recent network upgrades. We are waiting for The UCSF IT Network to resolve this for us. Until resolved, please use the alternative ‘log2.wynton.ucsf.edu’ login node when connecting from outside of the UCSF network. December 8, 23:00 PT\n\n\n\nDecember 11-16, 2020\n\nRebooting compute nodes\nResolved: All compute nodes have been rebooted. December 16, 05:00 PT\nNotice: The new BeeGFS setting introduced during the upgrades earlier this week caused problems throughout the system and we need to roll them back. The compute nodes will no longer take on new jobs until they have been rebooted. A compute node will be automatically rebooted as soon as all of its running jobs have completed. Unfortunately, we have to kill jobs that run on compute nodes that are stalled and suffer from the BeeGFS issues. December 11, 13:50 PT\n\n\n\nDecember 11, 2020\n\nRebooting login and development nodes\nResolved: All login and development nodes have been rebooted. December 12, 17:00 PT\nNotice: Login node ‘log1.wynton.ucsf.edu’ and all the development nodes will be rebooted at 4:30 PM today Friday. This is needed in order to roll back the new BeeGFS setting introduced during the upgrades earlier this week. December 11, 13:50 PT\n\n\n\nDecember 7-8, 2020\n\nMajor upgrades (full downtime)\nResolved: The upgrade has been completed. The cluster back online, including all of the login, data-transfer, and development nodes, as well as the majority of the compute nodes. The scheduler is processing jobs again. All hosts now run CentOS 7.9. December 8, 16:30 PT\nUpdate: The upgrade is paused and will resume tomorrow. We hope to be bring all of the cluster back online by the end of tomorrow. For now, login node ‘log2’ (but not ‘log1’), and data-transfer nodes ‘dt1’, and ‘dt2’ are back online and can be used for accessing files. Development nodes ‘dev1’ and ‘dev3’ are also available (please make sure to leave room for others). The scheduler remains down, i.e. it is is not be possible to submit jobs. December 7, 17:00 PT\nUpdate: The upgrades have started. Access to Wynton HPC has been disable as of 10:30 this morning. The schedulers stopped launching queued jobs as of 23:30 last night. December 7, 10:30 PT\nRevised notice: We have decided to hold back on upgrading BeeGFS during the downtime and only focus on the remain parts including operating system and network upgrades. The scope of the work is still non-trivial. There is a risk that the downtime will extend into Thursday December 10. However, if everything go smoothly, we hope that Wynton HPC will be back up by the end of Monday or during the Tuesday. There will only be one continuous downtime, that is, when the cluster comes back up, it will stay up. December 3, 09:00 PT\nNotice: Starting early Monday December 7, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the operating system, the network, and the BeeGFS file system. We anticipate that the cluster will be available again by the end of Tuesday December 8, when load testing of the upgraded BeeGFS file system will start. If these tests fail, we will have to unroll the BeeGFS upgrade, which in case we anticipate that the cluster is back online by the end of Wednesday December 9. November 23, 16:50 PT\n\n\n\n\nNovember 4-16, 2020\n\nCompute nodes not serving jobs (due to network switch failure)\nResolved: All 74 compute nodes that were taken off the job scheduler on 2020-11-04 are back up and running November 16, 12:00 PT\nNotice: 74 compute nodes, including several GPU nodes, were taken off the job scheduler around 14:00 on 2020-11-04 due to a faulty network switch. The network switch needs to be replaced in order to resolve this. November 4, 16:10 PT\n\n\n\n\nNovember 5, 2020\n\nCluster inaccessible (due to BeeGFS issues)\nResolved: Our BeeGFS file system was non-responsive during 01:20-04:00 on 2020-11-05 because one of the meta servers hung. November 5, 08:55 PT\n\n\n\n\nOctober 21, 2020\n\nCluster inaccessible (due to BeeGFS issues)\nResolved: Our BeeGFS file system was non-responsive because one of its meta servers hung, which now has been restarted. October 21, 11:15 PT\nNotice: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 09:30 today. October 21, 10:45 PT\n\n\n\n\nAugust 19, 2020\n\nCluster inaccessible (due to BeeGFS issues)\nResolved: Our BeeGFS file system was non-responsive between 17:22 and 18:52 today because one of its meta servers hung while the other attempted to synchronize to it. August 19, 19:00 PT\nNotice: The cluster is currently inaccessible for unknown reasons. The problem was first reported around 17:30 today. August 19, 18:15 PT\n\n\n\n\nAugust 10-13, 2020\n\nNetwork and hardware upgrades (full downtime)\nResolved: The cluster is fully back up and running. Several compute nodes still need to be rebooted but we consider this upgrade cycle completed. The network upgrade took longer than expected, which delayed the processes. We hope to bring the new lab storage online during the next week. August 13, 21:00 PT\nUpdate: All login, data-transfer, and development nodes are online. Additional compute nodes are being upgraded and are soon part of the pool serving jobs. August 13, 14:50 PT\nUpdate: Login node log1, data-transfer node dt2, and the development nodes are available again. Compute nodes are going through an upgrade cycle and will soon start serving jobs again. The upgrade work is taking longer than expected and will continue tomorrow Thursday August 13. August 12, 16:10 PT\nNotice: All of the Wynton HPC environment is down for maintenance and upgrades. August 10, 00:00 PT\nNotice: Starting early Monday August 10, the cluster will be powered down entirely for maintenance and upgrades, which includes upgrading the network and adding lab storage purchased by several groups. We anticipate that the cluster will be available again by the end of Wednesday August 12. July 24, 15:45 PT\n\n\n\n\nJuly 6, 2020\n\nDevelopment node failures\nResolved: All three development nodes have been rebooted. July 6, 15:20 PT\nNotice: The three regular development nodes have all gotten themselves hung up on one particular process. This affects basic system operations and preventing such basic commands as ps and w. To clear this state, we’ll be doing an emergency reboot of the dev nodes at about 15:15. July 6, 15:05 PT\n\n\n\nJuly 5, 2020\n\nJob scheduler non-working\nResolved: The SGE scheduler produced errors when queried or when jobs were submitted or launched. The problem started 00:30 and lasted until 02:45 early Sunday 2020-07-05. July 6, 22:00 PT\n\n\n\n\nJune 11-26, 2020\n\nKernel maintenance\nResolved: All compute nodes have been rebooted. June 26, 10:45 PT\nUpdate: Development node dev3 is back online. June 15, 11:15 PT\nUpdate: Development node dev3 is not available. It failed to reboot and requires on-site attention, which might not be possible for several days. All other log-in, data-transfer, and development nodes were rebooted successfully. June 11, 15:45 PT\nNotice: New operating-system kernels are deployed. Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer than usual slots available on the queues. To follow the progress, see the green ‘Available CPU cores’ curve (target ~10,400 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Thursday June 11. June 11, 10:45 PT\n\n\n\nJune 5-9, 2020\n\nNo internet access on development nodes\nResolved: Internet access from the development nodes is available again. A new web-proxy server had to be built and deploy. June 9, 09:15 PT\nNotice: Internet access from the development nodes is not available. This is because the proxy server providing them with internet access had a critical hardware failure around 08-09 this morning. At the most, we cannot provide an estimate when we get to restore this server. June 5, 16:45 PT\n\n\n\nMay 18-22, 2020\n\nFile-system maintenance\nUpdate: The upgrade of the BeeGFS filesystem introduced new issues. We decided to rollback the upgrade and we are working with the vendor. There is no upgrade planned for the near term. June 8, 09:00 PT\nUpdate: The BeeGFS filesystem has been upgraded using a patch from the vendor. The patch was designed to lower the amount of resynchronization needed between the two metadata servers. Unfortunately, after the upgrade we observe an increase of resynchronization. We will keep monitoring the status. If the problem remains, we will consider a rollback to the BeeGFS version used prior to May 18. May 22, 01:25 PT\nUpdate: For a short moment around 01:00 early Friday, both of our BeeGFS metadata servers were down. This may have lead to some applications experiencing I/O errors around this time. May 22, 01:25 PT\nNotice: Work to improve the stability of the BeeGFS filesystem (/wynton) will be conducted during the week of May 18-22. This involves restarting the eight pairs of metadata server processes, which may result in several brief stalls of the file system. Each should last less than 5 minutes and operations will continue normally after each one. May 6, 15:10 PT\n\n\n\n\nMay 28-29, 2020\n\nGPU compute nodes outage\nResolved: The GPU compute nodes are now fully available to serve jobs. May 29, 12:00 PT\nUpdate: The GPU compute nodes that went down yesterday have been rebooted. May 29, 11:10 PT\nInvestigating: A large number of GPU compute nodes in the MSG data center are currently down for unknown reasons. We are investigating the cause. May 28, 09:35 PT\n\n\n\n\nFebruary 5-7, 2020\n\nMajor outage due to NetApp file-system failure\nResolved: The Wynton HPC system is considered fully functional again. The legacy, deprecated NetApp storage was lost. February 10, 10:55 PT\nUpdate: The majority of the compute nodes have been rebooted and are now online and running jobs. We will actively monitor the system and assess the how everything works before we considered this incident resolved. February 7, 13:40 PT\nUpdate: The login, development and data transfer nodes will be rebooted at 01:00 today Friday February 7. February 7, 12:00 PT\nUpdate: The failed legacy NetApp server is the cause to the problems, e.g. compute nodes not being responsive causing problems for SGE etc. Because of this, all of the cluster - login, development, transfer, and computes nodes - will be rebooted tomorrow Friday 2020-02-07. February 6, 10:00 PT\nNotice: Wynton HPC is experience major issues due to NetApp file-system failure, despite this is being deprecated and not used much these days. The first user report on this came in around 09:00 and the job-queue logs suggests the problem began around 02:00. It will take a while for everything to come back up and there will be brief BeeGFS outage while we reboot the BeeGFS management node. February 5, 10:15 PT\n\n\n\n\nJanuary 29, 2020\n\nBeeGFS failure\nResolved: The BeeGFS file-system issue has been resolved by rebooting two meta servers. January 29, 17:00 PT\nNotice: There’s currently an issue with the BeeGFS file system. Users reporting that they cannot log in. January 29, 16:00 PT\n\n\n\n\nJanuary 22, 2020\n\nFile-system maintenance\nResolved: The BeeGFS upgrade issue has been resolved. Jan 22, 14:30 PT\nUpdate: The planned upgrade caused unexpected problems to the BeeGFS file system resulting in /wynton/group becoming unstable. Jan 22, 13:35 PT\nNotice: One of the BeeGFS servers, which serve our cluster-wide file system, will be swapped out starting at noon (11:59am) on Wednesday January 22, 2020 and the work is expected to last one hour. We don’t anticipate any downtime because the BeeGFS servers are mirrored for availability. Jan 16, 14:40 PT\n\n\n\n\nDecember 20, 2019 - January 4, 2020\n\nKernel maintenance\nResolved: All compute nodes have been updated and rebooted. Jan 4, 11:00 PT\nNotice: Compute nodes will no longer accept new jobs until they have been rebooted. A node will be rebooted as soon as any existing jobs have completed, which may take up to two weeks (maximum runtime). During this update period, there will be fewer available slots on the queues than usual. To follow the progress, see the green ‘Available CPU cores’ curve (target ~7,500 cores) in the graph above. Log-in, data-transfer, and development nodes will be rebooted at 15:30 on Friday December 20. GPU nodes already run the new kernel and are not affected. December 20, 10:20 PT"
  },
  {
    "objectID": "hpc/status/incidents-20xx.html",
    "href": "hpc/status/incidents-20xx.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 20?? (this far)\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 0.0 hours (= 0.0 days)\nTotal: 0.0 hours (= 0.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\nNo incidents thus far :)"
  },
  {
    "objectID": "hpc/status/beta/index.html",
    "href": "hpc/status/beta/index.html",
    "title": "Wynton HPC Dashboard",
    "section": "",
    "text": "Wynton HPC Dashboard\n\n\nSystem Metrics Job Metrics\n\n\n\nSystem Metrics\n\n\n\nAvailable Nodes85 nodes (98.8%)\n\n\n\n\n\n\n\nCore Usage1385 cores (76.2%)\n\n\n\n\n\n\n\nMemory Usage8.6 TiB (40.0%)\n\n\n\n\n\n\n\n\nJob Metrics\n\n\n\nActive Jobs356 jobs\n\n\n\n\n\n\n\nBacklog45 jobs"
  },
  {
    "objectID": "hpc/status/incidents-2026.html",
    "href": "hpc/status/incidents-2026.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2026 (this far)\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 0.0 hours (= 0.0 days)\nTotal: 0.0 hours (= 0.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\nNo incidents thus far :)"
  },
  {
    "objectID": "hpc/status/incidents-2029.html",
    "href": "hpc/status/incidents-2029.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2029 (this far)\n\nFull downtime:\n\nScheduled: 0.0 hours (= 0.0 days)\nUnscheduled: 0.0 hours (= 0.0 days)\nTotal: 0.0 hours (= 0.0 days)\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\nNo incidents thus far :)"
  },
  {
    "objectID": "hpc/status/incidents-2023.html",
    "href": "hpc/status/incidents-2023.html",
    "title": "UCSF Wynton HPC Cluster",
    "section": "",
    "text": "Operational Summary for 2023\n\nFull downtime:\n\nScheduled: 141.0 hours = 5.9 days = 1.6%\nUnscheduled: 742.25 hours = 30.9 days = 8.5%\nTotal: 883.25 hours = 35.3 days = 10.1%\nExternal factors: 0% of the above downtime, corresponding to 0.0 hours (= 0.0 days), were due to external factors\n\n\n\nScheduled maintenance downtimes\n\nImpact: No file access, no compute resources available\nDamage: None\nOccurrences:\n\n2023-02-22 (17.0 hours)\n2023-05-17 (20.0 hours)\n2023-10-30 – 2023-11-03 (104.0 hours)\n\nTotal downtime: 141.0 hours\n\n\n\nScheduled kernel maintenance\n\nImpact: Fewer compute nodes than usual until rebooted\nDamage: None\nOccurrences:\n\nN/A\n\n\n\n\nUnscheduled downtimes due to power outage\n\nImpact: No file access, no compute resources available\nDamage: Running jobs (&lt;= 14 days) failed, file-transfers failed, possible file corruptions\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to file-system failures\n\nImpact: No file access\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\n2023-05-17 – 2023-06-01 (359.0 hours)\n2023-10-27 – 2023-11-15 (347.25 hours, excluding the scheduled 5-day downtime)\n\nTotal downtime: 742.25 hours of which 0.0 hours were due to external factors\n\n\n\nUnscheduled downtimes due to other reasons\n\nImpact: Less compute resources\nDamage: Running jobs (&lt;= 14 days) may have failed, file-transfers may have failed, cluster not accessible\nOccurrences:\n\nN/A\n\nTotal downtime: 0.0 hours of which 0.0 hours were due to external factors\n\n\n\n\nNovember 15-December 15, 2023\n\nUpgrading compute nodes\nResolved: All compute nodes are up and running. December 15, 09:00 PT\nUpdate: A total of ~15,000 CPU cores are now up and running. November 27, 15:00 PT\nUpdate: A total of ~14,000 CPU cores are now up and running. November 26, 02:00 PT\nUpdate: A total of ~13,000 CPU cores are now up and running. November 22, 15:30 PT\nUpdate: A total of ~12,000 CPU cores are now up and running. November 22, 01:00 PT\nUpdate: A total of ~10,000 CPU cores are now up and running. November 21, 01:00 PT\nUpdate: 98 compute nodes with a total of 2,780 CPU cores are now up and running. November 16, 15:00 PT\nNotice: As we come back from the downtime, we start out with 36 out of 490 compute nodes available to process jobs. Work continues to migrating the remaining nodes to Rocky 8. November 15, 14:15 PT\n\n\n\n\nOctober 30-November 15, 2023\n\nFull downtime\nUpdate: The job scheduler is available and jobs are running. The data-transfer nodes are available. At this time, 36 out of 490 compute nodes have been re-enabled. Work has begun booting up the remaining ones. The first jobs were processed around 09:00 this morning. November 15, 14:15 PT\nUpdate: We plan to re-enable the job scheduler and start processing jobs by the end of today. It is possible to submit jobs already now, but they will remain queued until we re-enable the scheduler. November 15, 10:30 PT\nUpdate: The BeeGFS issue has been resolved, which allows us to move forward on the remaining Rocky-8 updates. We hope to start bringing compute nodes online as soon as tomorrow (2023-11-15). November 14, 13:15 PT\nUpdate: Still status quo; the BeeGFS issue holds us back from bringing the scheduler back up. We’re rather certain that we will not be able to resolve it today or tomorrow. November 13, 13:45 PT\nUpdate: Login and development nodes are available. Write access to the BeeGFS file system has been re-enabled. Due to continued issues in getting BeeGFS back in stable state, we are still not ready for opening up the scheduler and compute nodes. November 11, 00:30 PT\nUpdate: Unfortunately, we will not bring up Wynton to run jobs today. We are evaluating what, if anything, may be possible to bring up before the long weekend. The reason being that the required metadata resynchronization failed late yesterday. The vendor has provided us with a script to fix the failure. That script is running, and once it’s done, we’ll reattempt to resynchronize. November 9, 10:30 PT\nUpdate: We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. At that time, we expect all login, all data-transfer, and most development nodes will be available. A large number of the compute nodes will also be available via the scheduler. November 8, 10:30 PT\nUpdate: The team makes progress on the scheduled downtime activities, which was delayed due to the BeeGFS incident. We estimate to bring Wynton back up by the end of day Thursday November 9, 2023. November 7, 11:20 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Monday October 30 through end of business on Friday November 3, 2023 (2023W44). The operating system will be upgraded system wide (all machines) from CentOS 7.9 to Rocky 8 Linux, the BeeGFS will be upgrade, and old hardware will be replaced. UCSF Facilities will perform scheduled work. After the downtime, there will no longer be any machine running CentOS 7.9. All machines will have their local disks (including /scratch and /tmp) wiped. Anything under /wynton (including /wynton/scratch, /wynton/home, …) should be unaffected, but please note that Wynton does not back anything up, so we recommend you to back up critical data. For more information about the Rocky 8 Linux migration project and how you can prepare for it is available at on the Migration to Rocky 8 Linux from CentOS 7 page. October 13, 11:15 PT\n\n\n\n\nOctober 27-November 14, 2023\n\nFile-system failures\nResolved: The BeeGFS metadata resynchronization is complete around 02:30 this morning. November 14, 13:15 PT\nUpdate: The BeeGFS metadata resynchronization is still unresolved. We are looking into other strategies, which we are currently testing. If those tests are successful, we will attempt to deploy the fix in the production. November 13, 13:45 PT\nUpdate: After resynchronization of the BeeGFS metadata kept failing, we identified a possible culprit. We suspect BeeGFS cannot handle the folders with many millions of files, causing the resynchronization to fail. We keep working on stabilizing BeeGFS. November 11, 00:45 PT\nUpdate: The BeeGFS metadata resynchronization that had been running for several hours, failed late yesterday. The vendor has provided us with a script tailored to fix the issue we ran into. That script is running, and once it’s done, we’ll start the resynchronization again. November 9, 10:30 PT\nUpdate: The recovery from the BeeGFS incident goes as planned. We estimate to have resolved this issue by the end of November 9, 2023, when full read-write access to /wynton will be available again. November 8, 10:30 PT\nUpdate: The Wynton team works on fixing and stabilizing the BeeGFS incident. We estimate to have resolved this issue by the end of November 9, 2023. November 7, 11:20 PT\nUpdate: Read-only access to Wynton has been enabled for users to retrieve their files. Login nodes log1 and plog1 are available for this. If going through the Wynton 2FA, make sure to answer “no” (default) when prompted for “Remember connection authentication from 98.153.103.186 for 12 hours? [y/N]”; answering “yes” causes the SSH connection to fail. November 5, 00:30 PT\nUpdate: Wynton admins can retrieve user files under /wynton/ upon requests until 18:00 today, when the UCSF network will go down. We are not able to share the PHI data under /wynton/protected/. Please contact support with all details including full path of the data to be retrieved. October 30, 15:30 PT\nUpdate: The BeeGFS issue is related to a CentOS 7-kernel bug in one of our BeeGFS metadata servers. To minimize the risk of data loss on the /wynton file system, we took the decision to shut down Wynton immediately. At the moment, we do not have an estimate on how long it will take to resolve this problem. It has to be resolved before we can begin the major upgrade scheduled for 2023W44. October 27, 16:30 PT\nNotice: The BeeGFS file system, which hosts /wynton, is experiencing unexpected, major issues. Some or all files on /wynton cannot be accessed, and when attempted, an Communication error on send error is seen. The problem started around 13:45 on Friday 2023-10-27. October 27, 15:10 PT\n\n\n\n\nOctober 23-October 26, 2023\nResolve: Login node log2 and data-transfer node dt1 are available again. October 26, 12:15 PT\nUpdate: Development node dev2 is available again. October 24, 12:45 PT\nNotice: Access to login node log2, data-transfer nodes dt1, and development node dev2 will be disabled from Monday-Friday October 23-27, 2023 (2023W43) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node log1, data-transfer nodes dt2, and development nodes dev1 and dev3 are unaffected, so are the Wynton HPC Globus endpoints. October 23, 11:10 PT\n\n\nOctober 16-October 20, 2023\nResolved: Login node log1, data-transfer nodes dt2 and pdt2 are available again and are now running Rocky 8. October 20, 17:00 PT\nNotice: Data-transfer nodes dt2 will be disabled this week instead of dt1 as previously announced. October 16, 14:30 PT\nNotice: Access to login node log1, data-transfer nodes dt1, and pdt2 will be disabled from Monday-Friday October 16-20, 2023 (2023W42) to upgrade the operating system to Rocky 8 Linux. They might return sooner. The alternative login node log2, data-transfer nodes dt2, and pdt1 are unaffected, so are the Wynton HPC Globus endpoints. October 13, 11:20 PT\n\n\nJune 1, 2023 - April 3, 2024\n\nPost file-system failure incidents\nResolved: All corrupted and orphaned files have now been deleted. There might be orphaned directories remaining, which we leave to each user to remove, if they exist. April 3, 2024, 11:15 PT\nUpdate: Reading files whose data was lost on the unrecovered storage targets back in May no longer results in an error message. Instead, the portion of the file that was lost will be replaced by null bytes. Obviously, this results in a file with corrupt content. The admins will be going through and deleting all the corrupted files as soon as possible. It’s a big task and will take some time. July 13, 14:15 PT\nUpdate: The remaining two ZFS storage targets (22004 and 22006) are back online again. July 11, 10:30 PT\nUpdate: Four out of the six ZFS storage targets have been brought back online. Two targets (22004 and 22006) remain offline. If you encounter a “Communication error on send” error, please do not delete or move the affected file. July 6, 17:00 PT\nUpdate: Six ZFS storage targets (22001-22006) are down, because one of the recovered storage targets encountered latent damage that had gone undetected since the recovery in May. This locked up the server and thus all six targets on that server. July 6, 08:30 PT\nUpdate: The final two ZFS storage targets are now serving the BeeGFS file system (/wynton) again. June 30, 11:00 PT\nUpdate: We will be reintroducing the final two ZFS storage targets back into the BeeGFS file system (/wynton) on Friday June 30. The work will start at 10 am and should take an hour or so. During that time, there will be a couple of brief “blips” as we reconfigure the storage. June 29, 23:55 PT\nUpdate: Organizing the data recovered from ZFS storage target 22004 into a form suitable for BeeGFS is taking long than expected. Thus far, we’ve properly replaced 10,354,873 of the 11,351,926 recovered files. Approximately one million files remain. We now hope to complete the work this week. The automatic clean up of old files on /wynton/scratch and /wynton/protected/scratch have been resumed. June 27, 17:00 PT\nUpdate: There are two broken ZFS storage targets (22004 and 21002). We expect to recover most files on target 22004 (approximately 14 TB). The reason it takes this long to recover that storage target is that the file chunks are there, but we have to puzzle them together to reconstruct the original files, which is a slow process. We estimate this process to complete by the end of the week. The files on the other target, target 21002, are unfortunately not recoverable. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 21, 23:30 PT\nNotice: There are two ZFS storage targets that are still failing and offline. We have hopes to be able to recover files from one of them. As of June 9, about 12 TB of low-level, raw file data (out of ~15 TB) was recovered. When that is completed, we will start the tedious work on reconstructing the actual files lost. The consultants are less optimistic about recovering data from second storage target, because it was much more damaged. They will give us the final verdict by the end of the week. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 12, 16:00 PT\n\n\n\nMay 16-June 1, 2023\n\nFull downtime followed by network and file-system recovery\nResolved: The job scheduler is now available. Access to /wynton/group, /wynton/protected/group, and /wynton/protected/project has been restored. If you encounter a “Communication error on send” error, please do not delete or move the affected file. June 1, 16:00 PT\nUpdate: Wynton will be fully available later today, meaning the job scheduler and access to /wynton/group, /wynton/protected/group, and /wynton/protected/project will be re-enabled. Note, two ZFS storage targets are still faulty and offline, but the work of trying to recover them will continue while we go live. This means that any files on the above re-opened /wynton subfolders that are stored, in part or in full, on those two offline storage targets will be inaccessible. Any attempt to read such files will result in a “Communication error on send” error and stall. To exit, press Ctrl-C. Importantly, do not attempt to remove, move, or update such files! That will make it impossible to recover them! June 1, 12:15 PT\nUpdate: In total 22 (92%) out of 24 failed storage targets has been recovered. The consultant hopes to recover the bulk of the data from one of the two remaining damaged targets. The final damage target is heavily damaged, work on it will continue a few more days, but it is likely it cannot be recovered. The plan is to open up /wynton/group tomorrow Thursday with instructions what to expect for files on the damaged targets. The compute nodes and the job scheduler will also be enabled during the day tomorrow. May 31, 22:45 PT\nUpdate: In total 22 (92%) out of 24 failed storage targets has been recovered. The remaining two targets are unlikely to be fully recovered. We’re hoping to restore the bulk of the files from them, but there is a risk that we will get none back. Then plan is to bring back /wynton/group, /wynton/protected/group, and /wynton/protected/project, and re-enable the job queue, on Thursday. May 31, 01:00 PT\nUpdate: The login, data-transfer, and development nodes (except gpudev1) are now online an available for use. The job scheduler and compute nodes are kept offline, to allow for continued recovery of the failed ZFS storage pools. For the same reason, folders under /wynton/group, /wynton/protected/group, and /wynton/protected/project are locked down, except for groups who have mirrored storage. /wynton/home and /wynton/scratch are fully available. We have suspended the automatic cleanup of old files under /wynton/scratch and /wynton/protected/scratch. The ZFS consultant recovered 3 of the 6 remaining storage targets. We have now recovered in total 21 (88%) out of 24 failed targets. The recovery work will continue on Monday (sic!). May 26, 17:00 PT\nUpdate: All 12 ZFS storage targets on one server pair have been recovered and are undergoing final verification, after which that server pair is back in production. On the remaining server pair with also 12 failed ZFS storage targets, 4 targets have been recovered, 4 possibly have been, and 4 are holding out. We’re continuing our work with the consultant on those targets. These storage servers were installed on 2023-03-28, so it is only files written after that date that may be affected. We are tentatively planning on bringing up the login, data transfer and development nodes tomorrow Friday, prior to the long weekend, but access to directories in /wynton/group, /wynton/protected/group, or /wynton/protected/project will be blocked with the exception for a few groups with mirrored storage. /wynton/home and /wynton/scratch would be fully accessible. May 25, 17:00 PT\nUpdate: 8 more ZFS storage targets were recovered today. We have now recovered in total 17 (71%) out of 24 failed targets. The content of the recovered targets is now being verified. We will continue working with the consultant tomorrow on the remaining 7 storage targets. May 24, 17:00 PT\nUpdate: The maintenance and upgrade of the Wynton network switch was successful and is now completed. We also made progress of recovering the failed ZFS storage targets - 9 (38%) out of 24 failed targets have been recovered. To maximize our chances at a full recovery, Wynton will be kept down until the consultant completes their initial assessment. Details: The contracted ZFS consultant started to work on recovering the failed ZFS storage targets that we have on four servers. During the two hours of work, they quickly recovered another three targets on on the first server, leaving us with only one failed target on that server. Attempts of the same recovery method on the second and third servers were not successful. There was no time today to work on the fourth server. The work to recover the remaining targets will resume tomorrow. After the initial recovery attempt has been attempted on all targets, the consultant, who is one of the lead ZFS developers, plans to load a development version of ZFS on the servers in order to perform more thorough and deep-reaching recovery attempts. May 23, 17:00 PT\nUpdate: Wynton will be kept down until the ZFS-recovery consultant has completed their initial assessment. If they get everything back quickly, Wynton will come back up swiftly. If recovery takes longer, or is less certain, we will look at coming back up without the problematic storage targets. As the purchase is being finalized, we hope that the consultant can start their work either on Tuesday or Wednesday. The UCSF Networking Team is performing more maintenance on the switch tonight. May 22, 23:30 PT\nUpdate: The cluster will be kept offline until at least Tuesday May 23. The BeeGFS file-system failure is because 24 out of 144 ZFS storage targets got corrupted. These 24 storage targets served our “group” storage, which means only files written to /wynton/group, /wynton/protected/group, and /wynton/protected/project within the past couple of months are affected. Files under /wynton/home and /wynton/scratch are not affected. We are scanning the BeeGFS file system to identify exactly which files are affected. Thus far, we have managed to recover 6 (25%) out of the 24 failed targets. The remaining 18 targets are more complicated and we are working with a vendor to start helping us recover them next week. May 19, 10:15 PT\nUpdate: Automatic cleanup of /wynton/scratch has been disabled. May 18, 23:00 PT\nUpdate: Several ZFS storage targets that are used by BeeGFS experienced failures during the scheduled maintenance window. There is a very high risk of partial data loss, but we will do everything possible to minimize the loss. In addition, the Wynton core network switch failed and needs to be replaced. The UCSF IT Infrastructure Network Services Team works with the vendor to get a rapid replacement. May 17, 16:30 PT\nUpdate: The cluster is down and unavailable because of maintenance. May 16, 21:00 PT\nUpdate: There will be a one-day downtime starting at 21:00 on Tuesday May 16 and ending at 17:00 on Wednesday May 17. This is aligned with a planned PG&E power-outage maintenance on May 17. Starting May 2, the maximum job runtime will be decreased on a daily basis from the maximum 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will only be started after the downtime. The default run time is 14 days, so make sure to specify qsub -l h_rt=&lt;run-time&gt; ... if you want something shorter. May 3, 10:00 PT\nUpdate: The updated plan is to only have a 24-hour downtime starting the evening of Tuesday May 16 and end by the end of Wednesday May 17. This is aligned with a planned PG&E power-outage maintenance on May 17. April 24, 11:00 PT\nUpdate: The updated plan is to have the downtime during the week of May 15, 2023 (2023W20). This is aligned with a planned PG&E power-outage maintenance during the same week. March 27, 11:00 PT\nNotice: We will performing a full-week major update to the cluster during late Spring 2023. Current plan is to do this during either the week of May 8, 2023 (2023W19) or the week of May 15, 2023 (2023W20). February 27, 11:00 PT\n\n\n\n\n\nFebruary 22-23, 2023\n\nFull downtime\nResolved: The cluster maintenance has completed and the cluster is now fully operational again. February 23, 14:00 PT\nUpdate: The cluster has been shut down for maintenance. February 22, 21:00 PT\nNotice: The cluster will be shut down for maintenance from 9 pm on Wednesday February 22 until 5:00 pm on Thursday February 23, 2023. This is done to avoid possible file-system and hardware failures when the UCSF Facilities performs power-system maintenance. During this downtime, we will perform cluster maintenance. Starting February 8, the maximum job runtime will be decreased on a daily basis from the current 14 days so that jobs finish in time. Jobs with runtimes going into the maintenance window, will be started after the downtime. February 9, 09:00 PT\n\n\n\n\nJanuary 24, 2023\n\nNo access to login and data-transfer hosts\nResolve: Network issues has been resolved and access to all login and data-transfer has been re-established. The problem was physical (a cable was disconnected). January 24, 16:00 PT\nNotice: There is no access to non-PHI login and data-transfer hosts (log[1-2], dt[1-2]). We suspect a physical issue (e.g. somebody kicked a cable), which means we need to send someone onsite to fix the problem. January 24, 14:45 PT\n\n\n\nJanuary 11, 2023\n\nNo internet access on development nodes\nResolved: The network issue for the proxy servers has been fixed. All development nodes now have working internet access. January 11, 16:00 PT\nWorkarounds: Until this issue has been resolved, and depending on needs, you might try to use a data-transfer node.Some of the software tools on the development nodes are also available on the data-transfer nodes, e.g. curl, wget, and git. January 11, 09:50 PT\nNotice: The development nodes have no internet access, because the network used by out proxy servers is down for unknown reasons. The problem most likely started on January 10 around 15:45. January 11, 09:00 PT"
  },
  {
    "objectID": "hpc/myaccount/unlock-account.html",
    "href": "hpc/myaccount/unlock-account.html",
    "title": "Your Account Has Been Locked",
    "section": "",
    "text": "If you are reading this, your Wynton account has been locked. This can happen for several reasons:\n\nYour password has expired\n\nPasswords must be updated at least every 12 months per University of California requirements. Wynton sends out automatic email notifications reminding any user that their password is about to expire. These email messages are sent each Sunday starting two weeks before a password expires[^1]. To avoid this from happening again, please make sure to update your password at least every 12 months. If you cannot remember when you last changed it, you can always change your password at any time.\n\nYour UCSF affiliation has expired\n\nWhen leaving the UCSF, your UCSF affiliation expires and your account is locked. At times, this can happen also when you transition from one UCSF position to another.\n\nYour Wynton guest account has expired\n\nIf you are not a UCSF affiliate, but have a Wynton account that was sponsored by a UCSF collaborator, then your UCSF sponsor needs to renew this every 12 months.\n\n\n\n\n\nIf the only reason your account was locked was because of a recently expired password (less than 30 days ago), you can reset it by sending email to wynton-password@ucsf.edu.\nOtherwise, you’ll need to unlock your account via ServiceNow. Do not send an email to Wynton support - they can only unlock your account if you have created a ServiceNow ticket.\nInstructions:\n\nGo to the Wynton Account Request/Wynton Account Modification ServiceNow form\n(Login to ServiceNow requires UCSF MyAccess credentials. If you do not have access, your faculty sponsor, or another approved representative, will need to fill out this form for you)\nUnder ‘Account Details’, choose ‘Yes’ for “Does this user already have an existing account on Wynton?”\nFill out the remaining required fields\nSubmit the form\n\nThe Wynton Support Team will get notified and take action on it on a daily basis.\nFootnotes:\n[^1] If you did not receive such a message, please let support know, so that we can investigate. The messages are titled ‘[Wynton] Your Wynton password is expiring’ and are sent from do-not-reply@wynton.ucsf.edu to the email address that is associated with your Wynton account."
  },
  {
    "objectID": "hpc/myaccount/unlock-account.html#what-happened-and-why",
    "href": "hpc/myaccount/unlock-account.html#what-happened-and-why",
    "title": "Your Account Has Been Locked",
    "section": "",
    "text": "If you are reading this, your Wynton account has been locked. This can happen for several reasons:\n\nYour password has expired\n\nPasswords must be updated at least every 12 months per University of California requirements. Wynton sends out automatic email notifications reminding any user that their password is about to expire. These email messages are sent each Sunday starting two weeks before a password expires[^1]. To avoid this from happening again, please make sure to update your password at least every 12 months. If you cannot remember when you last changed it, you can always change your password at any time.\n\nYour UCSF affiliation has expired\n\nWhen leaving the UCSF, your UCSF affiliation expires and your account is locked. At times, this can happen also when you transition from one UCSF position to another.\n\nYour Wynton guest account has expired\n\nIf you are not a UCSF affiliate, but have a Wynton account that was sponsored by a UCSF collaborator, then your UCSF sponsor needs to renew this every 12 months."
  },
  {
    "objectID": "hpc/myaccount/unlock-account.html#instructions-for-unlocking-your-account",
    "href": "hpc/myaccount/unlock-account.html#instructions-for-unlocking-your-account",
    "title": "Your Account Has Been Locked",
    "section": "",
    "text": "If the only reason your account was locked was because of a recently expired password (less than 30 days ago), you can reset it by sending email to wynton-password@ucsf.edu.\nOtherwise, you’ll need to unlock your account via ServiceNow. Do not send an email to Wynton support - they can only unlock your account if you have created a ServiceNow ticket.\nInstructions:\n\nGo to the Wynton Account Request/Wynton Account Modification ServiceNow form\n(Login to ServiceNow requires UCSF MyAccess credentials. If you do not have access, your faculty sponsor, or another approved representative, will need to fill out this form for you)\nUnder ‘Account Details’, choose ‘Yes’ for “Does this user already have an existing account on Wynton?”\nFill out the remaining required fields\nSubmit the form\n\nThe Wynton Support Team will get notified and take action on it on a daily basis.\nFootnotes:\n[^1] If you did not receive such a message, please let support know, so that we can investigate. The messages are titled ‘[Wynton] Your Wynton password is expiring’ and are sent from do-not-reply@wynton.ucsf.edu to the email address that is associated with your Wynton account."
  },
  {
    "objectID": "hpc/support/index.html",
    "href": "hpc/support/index.html",
    "title": "Support Channels",
    "section": "",
    "text": "Whenever asking for help, here or elsewhere, stop, take a big breathe, and then ask yourself:\n👉 - “How can I help the helper help me?” 👈\nBelow is a checklist that helps us better assist you with your issue and get to a resolution sooner. Please make sure to:\n\n  include your username on the cluster (e.g. “My user name is alice.”).\n  specify what machine the problem occurs on (e.g. “I tried this on dev1 and dev3.”).\n  if you get an error, cut-and-paste the full error message (e.g. R: command not found).\n  for failing jobs, specify the job ID of the job that has issues (e.g. “Jobs that failed are 9702394 and 9709933, but it worked for 9704439.”).\n  for failing jobs, specify how you submit the job and what your script is (e.g. “I submit using qsub -cwd -j y -l         mem_free=500M my_script.sh. My script is /wynton/home/boblab/alice/my_script.sh.”). If the script is not too long, consider pasting it in your message.\n  cut-and-paste text from the console. By including also the prompt (e.g. [alice@dev2 ~]$ somecmd         input.txt), you convey useful information, such as your username, current machine, and current working directory, and possibly more.\n  if using Slack, please format code and output as a “Code Block” (there’s a button for it too); it makes it much easier to read. Avoid screenshot images.\n  use ‘Reply All’ in all your email correspondence. That way others in the team can pitch with additional knowledge and suggestions.\n  never include protected health information (PHI), or other sensitive, data in unencrypted emails or Slack channels. If you need to include sensitive data, use the UCSF encrypted email system (invoke by including “Secure:” at the start of the email subject line.) Do not include sensitive data in your Slack posts.\n\n\n\n\nIf you have questions which require Wynton staff assistance, please don’t hesitate to drop us a {{ site.cluster.name }} Support email at:\n\nwynton-support@ucsf.edu - reaches the {{ site.cluster.nickname }} support staff (Monday-Friday 08:00-17:00; Wynton support)\nwynton-password@ucsf.edu - if you forgot your {{ site.cluster.name }} password (Monday-Friday 08:00-17:00; UCSF IT support)\n\nFor questions, discussion, and feature requests, please use our Slack workspace:\n\nucsf-wynton - a communal forum for Wynton HPC users to interact in real time. We encourage everyone to engage, e.g. ask questions, answer questions, share tips and tricks, and participate in discussions. To join: (i) go to the signup page, and (ii) enter your UCSF email address. (*) If you don’t have one of the recognized email domains, or it for other reasons does not work, reach out to us on the above email address and we will send you an invite to your preferred email address.\n\nIf you prefer in-person support, please visit our Office Hours:\n\nWe hold online Office Hours every Tuesday at 11-noon (1 hour)\n\nyou can find the Zoom URL in (i) the message-of-the-day (MOTD) that you get when you log into Wynton, and (ii) on Slack."
  },
  {
    "objectID": "hpc/support/index.html#checklist-when-asking-for-help",
    "href": "hpc/support/index.html#checklist-when-asking-for-help",
    "title": "Support Channels",
    "section": "",
    "text": "Whenever asking for help, here or elsewhere, stop, take a big breathe, and then ask yourself:\n👉 - “How can I help the helper help me?” 👈\nBelow is a checklist that helps us better assist you with your issue and get to a resolution sooner. Please make sure to:\n\n  include your username on the cluster (e.g. “My user name is alice.”).\n  specify what machine the problem occurs on (e.g. “I tried this on dev1 and dev3.”).\n  if you get an error, cut-and-paste the full error message (e.g. R: command not found).\n  for failing jobs, specify the job ID of the job that has issues (e.g. “Jobs that failed are 9702394 and 9709933, but it worked for 9704439.”).\n  for failing jobs, specify how you submit the job and what your script is (e.g. “I submit using qsub -cwd -j y -l         mem_free=500M my_script.sh. My script is /wynton/home/boblab/alice/my_script.sh.”). If the script is not too long, consider pasting it in your message.\n  cut-and-paste text from the console. By including also the prompt (e.g. [alice@dev2 ~]$ somecmd         input.txt), you convey useful information, such as your username, current machine, and current working directory, and possibly more.\n  if using Slack, please format code and output as a “Code Block” (there’s a button for it too); it makes it much easier to read. Avoid screenshot images.\n  use ‘Reply All’ in all your email correspondence. That way others in the team can pitch with additional knowledge and suggestions.\n  never include protected health information (PHI), or other sensitive, data in unencrypted emails or Slack channels. If you need to include sensitive data, use the UCSF encrypted email system (invoke by including “Secure:” at the start of the email subject line.) Do not include sensitive data in your Slack posts."
  },
  {
    "objectID": "hpc/support/index.html#how-to-ask-for-help",
    "href": "hpc/support/index.html#how-to-ask-for-help",
    "title": "Support Channels",
    "section": "",
    "text": "If you have questions which require Wynton staff assistance, please don’t hesitate to drop us a {{ site.cluster.name }} Support email at:\n\nwynton-support@ucsf.edu - reaches the {{ site.cluster.nickname }} support staff (Monday-Friday 08:00-17:00; Wynton support)\nwynton-password@ucsf.edu - if you forgot your {{ site.cluster.name }} password (Monday-Friday 08:00-17:00; UCSF IT support)\n\nFor questions, discussion, and feature requests, please use our Slack workspace:\n\nucsf-wynton - a communal forum for Wynton HPC users to interact in real time. We encourage everyone to engage, e.g. ask questions, answer questions, share tips and tricks, and participate in discussions. To join: (i) go to the signup page, and (ii) enter your UCSF email address. (*) If you don’t have one of the recognized email domains, or it for other reasons does not work, reach out to us on the above email address and we will send you an invite to your preferred email address.\n\nIf you prefer in-person support, please visit our Office Hours:\n\nWe hold online Office Hours every Tuesday at 11-noon (1 hour)\n\nyou can find the Zoom URL in (i) the message-of-the-day (MOTD) that you get when you log into Wynton, and (ii) on Slack."
  }
]